_unit_id,_created_at,_golden,_id,_missed,_started_at,_tainted,_channel,_trust,_worker_id,_country,_region,_city,_ip,check_the_box_if_the_original_question_is_similar_to_any_of_the_questions_below,check_the_box_if_the_original_question_is_similar_to_any_of_the_questions_below_gold,question_1_body,question_1_title,question_2_body,question_2_title,question_3_body,question_3_title,question_4_body,question_4_title,question_5_body,question_5_title,user_question_body,user_question_title,user_selected_num
940847290,4/26/2016 17:08:54,false,1969446913,,4/26/2016 17:07:44,false,neodev,1.0,36167043,GBR,G6,Hull,77.86.101.69,0,,"<p>Hi everyone,åÊ</p>
<p></p>
<p>We&#39;ve had a chance to look through submissions for HW3 and while it&#39;ll be a few days before grades are ready to be sent out, we wanted to offer some feedback and clarification. Most of you had really high accuracies and many of you noted that accuracy was &#34;higher&#34; when you eliminated cross-validation because of overfitting (in the same way scores are higher when the midterm matches the practice test very closely).åÊ</p>
<p></p>
<p>X represented the full set of articles. y represented the labels. There were hundreds of thousands of unigram features, because features corresponded to each word that appeared (repeats did not contribute to # of features -- see &#39;that guy shot that other guy&#39; for an example).åÊ</p>
<p></p>
<p>This week, you are re-training your classifier on the full set of articles that we gave you last week (&#39;articles&#39; is the name of the text file) and then output predictions on the new URLs you collected.åÊYou&#39;ll want to modify the code for your statistical classifier to generate the labels for crowdworkers to check - the file on the assignment page is a great start towards that.åÊ</p>
<p></p>
<p>If you have any questions, please come to office hours sooner rather than later and post to Piazza if your question hasn&#39;t already been asked. Some parts of this assignment just take time to finish -- crawling, waiting for crowd worker judgements, the questionnaire -- and it&#39;s better to leave yourself time to debug.åÊ</p>",Using your classifier in HW4: Becoming a Requester,"<p>Hi all!</p>
<p></p>
<p>We have finished grading your classifier assignments, and you should get your grades shortly if you have not already (look for an email from Kate!). You guys did very well overall. It was not an easy assignment and there wereåÊa lot of new concepts that you had to take in (and new packages you had to install) all at once, so very well done.åÊ</p>
<p></p>
<p>There were twoåÊcommon points of confusion. Please feel free to ask questions here or in OHs if you are having trouble with these concepts, since they are important if you intend to do more ML in the future.</p>
<p></p>
<p><strong>What exactly areåÊX and y? What are their dimensions?åÊ</strong></p>
<p>In short, y is a vector of labels (one label per article). X is a matrix of features (one row per article, one column per feature). (You mightåÊpreferåÊto think of X as aåÊlist of vectors of features).åÊA key point is thatåÊ<span style=""text-decoration:underline"">every article has a value for every feature</span>-- so every article (row) has the same number of features (columns) associated with it. What changes areåÊthe values of those features (e.g. 0 or 1 in our assignment). Since the &#34;features&#34; in our caseåÊwere words, <span style=""text-decoration:underline"">the number of features (number of columns of X) is equal to the number of words in our vocabulary</span> (this was probably ~300,000-500,000 in our dataset, depending on your preprocessing). If theåÊword appeared in the article, theåÊvalue for theåÊword is 1 for that article, and if it did not appear, theåÊvalue is 0.åÊ</p>
<p></p>
<p>You are not alone if you find this concept a bit abstract. The biggest hurdle to overcome when learning about ML is getting comfortable thinking about your data as a feature matrix. Once you have internalized this notion, the rest of ML is reasonably straightforward. Please, ask us questions!</p>
<p></p>
<p><strong>False positives and false negativesåÊ</strong></p>
<p>Many of you were confused by the concept of false positives and false negatives. <span style=""text-decoration:underline"">A *false positive* in our case is an article that should have been labeled &#34;not gun related&#34; (0) but was falsely labeled &#34;gun related&#34; (1).</span> These were likely articles with words like &#34;shoot&#34; but which referred to sports or movies instead of violent crimes. A *false negative* isåÊan article that should have been labeled &#34;gun related&#34; (1) but was falsely labeled &#34;not gun related&#34; (0). Its good to think about these different types of errors when you are working on prediction tasks-- often different types of errors have different costs, or one type of error is worse than another.åÊåÊ(E.g. in medicine, tests that produce many false negativesåÊare usually considered worse that ones that produce false positives. I&#39;m sure you can think of some reasons why?)</p>
<p>åÊ</p>
<p><strong>But what now? My life has been so empty since turning in that classifier assignment...åÊ</strong></p>
<p>Many of you have expressed interest in learning more about machine learning, and you should absolutely consider enrolling in <a href=""https://alliance.seas.upenn.edu/~cis520/wiki/"" target=""_blank"">penn&#39;s ML course</a>åÊor <a href=""http://www.seas.upenn.edu/~cis519/fall2015/"" target=""_blank"">penn&#39;s intro ML course</a>. Or check out the <a href=""https://www.coursera.org/learn/machine-learning"" target=""_blank"">Coursera course</a>åÊ(crowdsourcing woot woot!). If you can&#39;t wait that long,åÊyou have all the freedom in the world to build an ML component into your final project, and we are more than happy to help you do so!</p>
<p></p>
<p>Hope everyone hasåÊa relaxing, happy weekend!åÊ</p>
<p></p>
<p>#pin</p>",Take-aways from you classifier assignments,"<p>When I run the classifier, I get the error:åÊ</p>
<p>Reading raw data</p>
<p>Loading training data</p>
<p>Traceback (most recent call last):</p>
<p>åÊ File &#34;predict_unlabelled.py&#34;, line 65, in &lt;module&gt;</p>
<p>åÊ åÊ y, X, texts, dv, le = get_matricies(training_data)</p>
<p>åÊ File &#34;predict_unlabelled.py&#34;, line 31, in get_matricies</p>
<p>åÊ åÊ texts = [d[1] for d in data]</p>
<p>IndexError: list index out of range</p>
<p></p>
<p>Which is an error indicated in part of the code we were given. Does anyone know why this is?åÊ</p>",Error when running classifier,<p>ShouldåÊwe have a single if-and-or statement with all of the keywords we use? Or should we have a series of if statements when checking keywords?åÊ</p>,rule-based classifier,"<p>Can we physically draw a picture of the tree resulting from the classifier? Ours seems like it may be pretty extensive, so it would take quite a while to draw it using draw.io/other online tools...</p>",Large classifier tree...,<p>Resolved!</p>,Error when running classifier template,1
940847290,4/26/2016 17:16:52,false,1969451842,,4/26/2016 17:08:45,false,neodev,1.0,33973110,VEN,23,Maracaibo,186.94.238.104,0,,"<p>Hi everyone,åÊ</p>
<p></p>
<p>We&#39;ve had a chance to look through submissions for HW3 and while it&#39;ll be a few days before grades are ready to be sent out, we wanted to offer some feedback and clarification. Most of you had really high accuracies and many of you noted that accuracy was &#34;higher&#34; when you eliminated cross-validation because of overfitting (in the same way scores are higher when the midterm matches the practice test very closely).åÊ</p>
<p></p>
<p>X represented the full set of articles. y represented the labels. There were hundreds of thousands of unigram features, because features corresponded to each word that appeared (repeats did not contribute to # of features -- see &#39;that guy shot that other guy&#39; for an example).åÊ</p>
<p></p>
<p>This week, you are re-training your classifier on the full set of articles that we gave you last week (&#39;articles&#39; is the name of the text file) and then output predictions on the new URLs you collected.åÊYou&#39;ll want to modify the code for your statistical classifier to generate the labels for crowdworkers to check - the file on the assignment page is a great start towards that.åÊ</p>
<p></p>
<p>If you have any questions, please come to office hours sooner rather than later and post to Piazza if your question hasn&#39;t already been asked. Some parts of this assignment just take time to finish -- crawling, waiting for crowd worker judgements, the questionnaire -- and it&#39;s better to leave yourself time to debug.åÊ</p>",Using your classifier in HW4: Becoming a Requester,"<p>Hi all!</p>
<p></p>
<p>We have finished grading your classifier assignments, and you should get your grades shortly if you have not already (look for an email from Kate!). You guys did very well overall. It was not an easy assignment and there wereåÊa lot of new concepts that you had to take in (and new packages you had to install) all at once, so very well done.åÊ</p>
<p></p>
<p>There were twoåÊcommon points of confusion. Please feel free to ask questions here or in OHs if you are having trouble with these concepts, since they are important if you intend to do more ML in the future.</p>
<p></p>
<p><strong>What exactly areåÊX and y? What are their dimensions?åÊ</strong></p>
<p>In short, y is a vector of labels (one label per article). X is a matrix of features (one row per article, one column per feature). (You mightåÊpreferåÊto think of X as aåÊlist of vectors of features).åÊA key point is thatåÊ<span style=""text-decoration:underline"">every article has a value for every feature</span>-- so every article (row) has the same number of features (columns) associated with it. What changes areåÊthe values of those features (e.g. 0 or 1 in our assignment). Since the &#34;features&#34; in our caseåÊwere words, <span style=""text-decoration:underline"">the number of features (number of columns of X) is equal to the number of words in our vocabulary</span> (this was probably ~300,000-500,000 in our dataset, depending on your preprocessing). If theåÊword appeared in the article, theåÊvalue for theåÊword is 1 for that article, and if it did not appear, theåÊvalue is 0.åÊ</p>
<p></p>
<p>You are not alone if you find this concept a bit abstract. The biggest hurdle to overcome when learning about ML is getting comfortable thinking about your data as a feature matrix. Once you have internalized this notion, the rest of ML is reasonably straightforward. Please, ask us questions!</p>
<p></p>
<p><strong>False positives and false negativesåÊ</strong></p>
<p>Many of you were confused by the concept of false positives and false negatives. <span style=""text-decoration:underline"">A *false positive* in our case is an article that should have been labeled &#34;not gun related&#34; (0) but was falsely labeled &#34;gun related&#34; (1).</span> These were likely articles with words like &#34;shoot&#34; but which referred to sports or movies instead of violent crimes. A *false negative* isåÊan article that should have been labeled &#34;gun related&#34; (1) but was falsely labeled &#34;not gun related&#34; (0). Its good to think about these different types of errors when you are working on prediction tasks-- often different types of errors have different costs, or one type of error is worse than another.åÊåÊ(E.g. in medicine, tests that produce many false negativesåÊare usually considered worse that ones that produce false positives. I&#39;m sure you can think of some reasons why?)</p>
<p>åÊ</p>
<p><strong>But what now? My life has been so empty since turning in that classifier assignment...åÊ</strong></p>
<p>Many of you have expressed interest in learning more about machine learning, and you should absolutely consider enrolling in <a href=""https://alliance.seas.upenn.edu/~cis520/wiki/"" target=""_blank"">penn&#39;s ML course</a>åÊor <a href=""http://www.seas.upenn.edu/~cis519/fall2015/"" target=""_blank"">penn&#39;s intro ML course</a>. Or check out the <a href=""https://www.coursera.org/learn/machine-learning"" target=""_blank"">Coursera course</a>åÊ(crowdsourcing woot woot!). If you can&#39;t wait that long,åÊyou have all the freedom in the world to build an ML component into your final project, and we are more than happy to help you do so!</p>
<p></p>
<p>Hope everyone hasåÊa relaxing, happy weekend!åÊ</p>
<p></p>
<p>#pin</p>",Take-aways from you classifier assignments,"<p>When I run the classifier, I get the error:åÊ</p>
<p>Reading raw data</p>
<p>Loading training data</p>
<p>Traceback (most recent call last):</p>
<p>åÊ File &#34;predict_unlabelled.py&#34;, line 65, in &lt;module&gt;</p>
<p>åÊ åÊ y, X, texts, dv, le = get_matricies(training_data)</p>
<p>åÊ File &#34;predict_unlabelled.py&#34;, line 31, in get_matricies</p>
<p>åÊ åÊ texts = [d[1] for d in data]</p>
<p>IndexError: list index out of range</p>
<p></p>
<p>Which is an error indicated in part of the code we were given. Does anyone know why this is?åÊ</p>",Error when running classifier,<p>ShouldåÊwe have a single if-and-or statement with all of the keywords we use? Or should we have a series of if statements when checking keywords?åÊ</p>,rule-based classifier,"<p>Can we physically draw a picture of the tree resulting from the classifier? Ours seems like it may be pretty extensive, so it would take quite a while to draw it using draw.io/other online tools...</p>",Large classifier tree...,<p>Resolved!</p>,Error when running classifier template,1
940847290,4/26/2016 17:19:05,false,1969453026,,4/26/2016 17:16:43,false,elite,1.0,25411289,HRV,"","",31.147.119.175,0,,"<p>Hi everyone,åÊ</p>
<p></p>
<p>We&#39;ve had a chance to look through submissions for HW3 and while it&#39;ll be a few days before grades are ready to be sent out, we wanted to offer some feedback and clarification. Most of you had really high accuracies and many of you noted that accuracy was &#34;higher&#34; when you eliminated cross-validation because of overfitting (in the same way scores are higher when the midterm matches the practice test very closely).åÊ</p>
<p></p>
<p>X represented the full set of articles. y represented the labels. There were hundreds of thousands of unigram features, because features corresponded to each word that appeared (repeats did not contribute to # of features -- see &#39;that guy shot that other guy&#39; for an example).åÊ</p>
<p></p>
<p>This week, you are re-training your classifier on the full set of articles that we gave you last week (&#39;articles&#39; is the name of the text file) and then output predictions on the new URLs you collected.åÊYou&#39;ll want to modify the code for your statistical classifier to generate the labels for crowdworkers to check - the file on the assignment page is a great start towards that.åÊ</p>
<p></p>
<p>If you have any questions, please come to office hours sooner rather than later and post to Piazza if your question hasn&#39;t already been asked. Some parts of this assignment just take time to finish -- crawling, waiting for crowd worker judgements, the questionnaire -- and it&#39;s better to leave yourself time to debug.åÊ</p>",Using your classifier in HW4: Becoming a Requester,"<p>Hi all!</p>
<p></p>
<p>We have finished grading your classifier assignments, and you should get your grades shortly if you have not already (look for an email from Kate!). You guys did very well overall. It was not an easy assignment and there wereåÊa lot of new concepts that you had to take in (and new packages you had to install) all at once, so very well done.åÊ</p>
<p></p>
<p>There were twoåÊcommon points of confusion. Please feel free to ask questions here or in OHs if you are having trouble with these concepts, since they are important if you intend to do more ML in the future.</p>
<p></p>
<p><strong>What exactly areåÊX and y? What are their dimensions?åÊ</strong></p>
<p>In short, y is a vector of labels (one label per article). X is a matrix of features (one row per article, one column per feature). (You mightåÊpreferåÊto think of X as aåÊlist of vectors of features).åÊA key point is thatåÊ<span style=""text-decoration:underline"">every article has a value for every feature</span>-- so every article (row) has the same number of features (columns) associated with it. What changes areåÊthe values of those features (e.g. 0 or 1 in our assignment). Since the &#34;features&#34; in our caseåÊwere words, <span style=""text-decoration:underline"">the number of features (number of columns of X) is equal to the number of words in our vocabulary</span> (this was probably ~300,000-500,000 in our dataset, depending on your preprocessing). If theåÊword appeared in the article, theåÊvalue for theåÊword is 1 for that article, and if it did not appear, theåÊvalue is 0.åÊ</p>
<p></p>
<p>You are not alone if you find this concept a bit abstract. The biggest hurdle to overcome when learning about ML is getting comfortable thinking about your data as a feature matrix. Once you have internalized this notion, the rest of ML is reasonably straightforward. Please, ask us questions!</p>
<p></p>
<p><strong>False positives and false negativesåÊ</strong></p>
<p>Many of you were confused by the concept of false positives and false negatives. <span style=""text-decoration:underline"">A *false positive* in our case is an article that should have been labeled &#34;not gun related&#34; (0) but was falsely labeled &#34;gun related&#34; (1).</span> These were likely articles with words like &#34;shoot&#34; but which referred to sports or movies instead of violent crimes. A *false negative* isåÊan article that should have been labeled &#34;gun related&#34; (1) but was falsely labeled &#34;not gun related&#34; (0). Its good to think about these different types of errors when you are working on prediction tasks-- often different types of errors have different costs, or one type of error is worse than another.åÊåÊ(E.g. in medicine, tests that produce many false negativesåÊare usually considered worse that ones that produce false positives. I&#39;m sure you can think of some reasons why?)</p>
<p>åÊ</p>
<p><strong>But what now? My life has been so empty since turning in that classifier assignment...åÊ</strong></p>
<p>Many of you have expressed interest in learning more about machine learning, and you should absolutely consider enrolling in <a href=""https://alliance.seas.upenn.edu/~cis520/wiki/"" target=""_blank"">penn&#39;s ML course</a>åÊor <a href=""http://www.seas.upenn.edu/~cis519/fall2015/"" target=""_blank"">penn&#39;s intro ML course</a>. Or check out the <a href=""https://www.coursera.org/learn/machine-learning"" target=""_blank"">Coursera course</a>åÊ(crowdsourcing woot woot!). If you can&#39;t wait that long,åÊyou have all the freedom in the world to build an ML component into your final project, and we are more than happy to help you do so!</p>
<p></p>
<p>Hope everyone hasåÊa relaxing, happy weekend!åÊ</p>
<p></p>
<p>#pin</p>",Take-aways from you classifier assignments,"<p>When I run the classifier, I get the error:åÊ</p>
<p>Reading raw data</p>
<p>Loading training data</p>
<p>Traceback (most recent call last):</p>
<p>åÊ File &#34;predict_unlabelled.py&#34;, line 65, in &lt;module&gt;</p>
<p>åÊ åÊ y, X, texts, dv, le = get_matricies(training_data)</p>
<p>åÊ File &#34;predict_unlabelled.py&#34;, line 31, in get_matricies</p>
<p>åÊ åÊ texts = [d[1] for d in data]</p>
<p>IndexError: list index out of range</p>
<p></p>
<p>Which is an error indicated in part of the code we were given. Does anyone know why this is?åÊ</p>",Error when running classifier,<p>ShouldåÊwe have a single if-and-or statement with all of the keywords we use? Or should we have a series of if statements when checking keywords?åÊ</p>,rule-based classifier,"<p>Can we physically draw a picture of the tree resulting from the classifier? Ours seems like it may be pretty extensive, so it would take quite a while to draw it using draw.io/other online tools...</p>",Large classifier tree...,<p>Resolved!</p>,Error when running classifier template,1
940847290,4/26/2016 17:32:09,false,1969460733,,4/26/2016 17:30:32,false,neodev,0.8889,19625264,DZA,41,Chlef,41.102.7.217,0,,"<p>Hi everyone,åÊ</p>
<p></p>
<p>We&#39;ve had a chance to look through submissions for HW3 and while it&#39;ll be a few days before grades are ready to be sent out, we wanted to offer some feedback and clarification. Most of you had really high accuracies and many of you noted that accuracy was &#34;higher&#34; when you eliminated cross-validation because of overfitting (in the same way scores are higher when the midterm matches the practice test very closely).åÊ</p>
<p></p>
<p>X represented the full set of articles. y represented the labels. There were hundreds of thousands of unigram features, because features corresponded to each word that appeared (repeats did not contribute to # of features -- see &#39;that guy shot that other guy&#39; for an example).åÊ</p>
<p></p>
<p>This week, you are re-training your classifier on the full set of articles that we gave you last week (&#39;articles&#39; is the name of the text file) and then output predictions on the new URLs you collected.åÊYou&#39;ll want to modify the code for your statistical classifier to generate the labels for crowdworkers to check - the file on the assignment page is a great start towards that.åÊ</p>
<p></p>
<p>If you have any questions, please come to office hours sooner rather than later and post to Piazza if your question hasn&#39;t already been asked. Some parts of this assignment just take time to finish -- crawling, waiting for crowd worker judgements, the questionnaire -- and it&#39;s better to leave yourself time to debug.åÊ</p>",Using your classifier in HW4: Becoming a Requester,"<p>Hi all!</p>
<p></p>
<p>We have finished grading your classifier assignments, and you should get your grades shortly if you have not already (look for an email from Kate!). You guys did very well overall. It was not an easy assignment and there wereåÊa lot of new concepts that you had to take in (and new packages you had to install) all at once, so very well done.åÊ</p>
<p></p>
<p>There were twoåÊcommon points of confusion. Please feel free to ask questions here or in OHs if you are having trouble with these concepts, since they are important if you intend to do more ML in the future.</p>
<p></p>
<p><strong>What exactly areåÊX and y? What are their dimensions?åÊ</strong></p>
<p>In short, y is a vector of labels (one label per article). X is a matrix of features (one row per article, one column per feature). (You mightåÊpreferåÊto think of X as aåÊlist of vectors of features).åÊA key point is thatåÊ<span style=""text-decoration:underline"">every article has a value for every feature</span>-- so every article (row) has the same number of features (columns) associated with it. What changes areåÊthe values of those features (e.g. 0 or 1 in our assignment). Since the &#34;features&#34; in our caseåÊwere words, <span style=""text-decoration:underline"">the number of features (number of columns of X) is equal to the number of words in our vocabulary</span> (this was probably ~300,000-500,000 in our dataset, depending on your preprocessing). If theåÊword appeared in the article, theåÊvalue for theåÊword is 1 for that article, and if it did not appear, theåÊvalue is 0.åÊ</p>
<p></p>
<p>You are not alone if you find this concept a bit abstract. The biggest hurdle to overcome when learning about ML is getting comfortable thinking about your data as a feature matrix. Once you have internalized this notion, the rest of ML is reasonably straightforward. Please, ask us questions!</p>
<p></p>
<p><strong>False positives and false negativesåÊ</strong></p>
<p>Many of you were confused by the concept of false positives and false negatives. <span style=""text-decoration:underline"">A *false positive* in our case is an article that should have been labeled &#34;not gun related&#34; (0) but was falsely labeled &#34;gun related&#34; (1).</span> These were likely articles with words like &#34;shoot&#34; but which referred to sports or movies instead of violent crimes. A *false negative* isåÊan article that should have been labeled &#34;gun related&#34; (1) but was falsely labeled &#34;not gun related&#34; (0). Its good to think about these different types of errors when you are working on prediction tasks-- often different types of errors have different costs, or one type of error is worse than another.åÊåÊ(E.g. in medicine, tests that produce many false negativesåÊare usually considered worse that ones that produce false positives. I&#39;m sure you can think of some reasons why?)</p>
<p>åÊ</p>
<p><strong>But what now? My life has been so empty since turning in that classifier assignment...åÊ</strong></p>
<p>Many of you have expressed interest in learning more about machine learning, and you should absolutely consider enrolling in <a href=""https://alliance.seas.upenn.edu/~cis520/wiki/"" target=""_blank"">penn&#39;s ML course</a>åÊor <a href=""http://www.seas.upenn.edu/~cis519/fall2015/"" target=""_blank"">penn&#39;s intro ML course</a>. Or check out the <a href=""https://www.coursera.org/learn/machine-learning"" target=""_blank"">Coursera course</a>åÊ(crowdsourcing woot woot!). If you can&#39;t wait that long,åÊyou have all the freedom in the world to build an ML component into your final project, and we are more than happy to help you do so!</p>
<p></p>
<p>Hope everyone hasåÊa relaxing, happy weekend!åÊ</p>
<p></p>
<p>#pin</p>",Take-aways from you classifier assignments,"<p>When I run the classifier, I get the error:åÊ</p>
<p>Reading raw data</p>
<p>Loading training data</p>
<p>Traceback (most recent call last):</p>
<p>åÊ File &#34;predict_unlabelled.py&#34;, line 65, in &lt;module&gt;</p>
<p>åÊ åÊ y, X, texts, dv, le = get_matricies(training_data)</p>
<p>åÊ File &#34;predict_unlabelled.py&#34;, line 31, in get_matricies</p>
<p>åÊ åÊ texts = [d[1] for d in data]</p>
<p>IndexError: list index out of range</p>
<p></p>
<p>Which is an error indicated in part of the code we were given. Does anyone know why this is?åÊ</p>",Error when running classifier,<p>ShouldåÊwe have a single if-and-or statement with all of the keywords we use? Or should we have a series of if statements when checking keywords?åÊ</p>,rule-based classifier,"<p>Can we physically draw a picture of the tree resulting from the classifier? Ours seems like it may be pretty extensive, so it would take quite a while to draw it using draw.io/other online tools...</p>",Large classifier tree...,<p>Resolved!</p>,Error when running classifier template,1
940847290,4/26/2016 17:32:14,false,1969460772,,4/26/2016 17:15:40,false,clixsense,0.8889,35338593,ITA,14,Cagliari,151.56.132.145,0,,"<p>Hi everyone,åÊ</p>
<p></p>
<p>We&#39;ve had a chance to look through submissions for HW3 and while it&#39;ll be a few days before grades are ready to be sent out, we wanted to offer some feedback and clarification. Most of you had really high accuracies and many of you noted that accuracy was &#34;higher&#34; when you eliminated cross-validation because of overfitting (in the same way scores are higher when the midterm matches the practice test very closely).åÊ</p>
<p></p>
<p>X represented the full set of articles. y represented the labels. There were hundreds of thousands of unigram features, because features corresponded to each word that appeared (repeats did not contribute to # of features -- see &#39;that guy shot that other guy&#39; for an example).åÊ</p>
<p></p>
<p>This week, you are re-training your classifier on the full set of articles that we gave you last week (&#39;articles&#39; is the name of the text file) and then output predictions on the new URLs you collected.åÊYou&#39;ll want to modify the code for your statistical classifier to generate the labels for crowdworkers to check - the file on the assignment page is a great start towards that.åÊ</p>
<p></p>
<p>If you have any questions, please come to office hours sooner rather than later and post to Piazza if your question hasn&#39;t already been asked. Some parts of this assignment just take time to finish -- crawling, waiting for crowd worker judgements, the questionnaire -- and it&#39;s better to leave yourself time to debug.åÊ</p>",Using your classifier in HW4: Becoming a Requester,"<p>Hi all!</p>
<p></p>
<p>We have finished grading your classifier assignments, and you should get your grades shortly if you have not already (look for an email from Kate!). You guys did very well overall. It was not an easy assignment and there wereåÊa lot of new concepts that you had to take in (and new packages you had to install) all at once, so very well done.åÊ</p>
<p></p>
<p>There were twoåÊcommon points of confusion. Please feel free to ask questions here or in OHs if you are having trouble with these concepts, since they are important if you intend to do more ML in the future.</p>
<p></p>
<p><strong>What exactly areåÊX and y? What are their dimensions?åÊ</strong></p>
<p>In short, y is a vector of labels (one label per article). X is a matrix of features (one row per article, one column per feature). (You mightåÊpreferåÊto think of X as aåÊlist of vectors of features).åÊA key point is thatåÊ<span style=""text-decoration:underline"">every article has a value for every feature</span>-- so every article (row) has the same number of features (columns) associated with it. What changes areåÊthe values of those features (e.g. 0 or 1 in our assignment). Since the &#34;features&#34; in our caseåÊwere words, <span style=""text-decoration:underline"">the number of features (number of columns of X) is equal to the number of words in our vocabulary</span> (this was probably ~300,000-500,000 in our dataset, depending on your preprocessing). If theåÊword appeared in the article, theåÊvalue for theåÊword is 1 for that article, and if it did not appear, theåÊvalue is 0.åÊ</p>
<p></p>
<p>You are not alone if you find this concept a bit abstract. The biggest hurdle to overcome when learning about ML is getting comfortable thinking about your data as a feature matrix. Once you have internalized this notion, the rest of ML is reasonably straightforward. Please, ask us questions!</p>
<p></p>
<p><strong>False positives and false negativesåÊ</strong></p>
<p>Many of you were confused by the concept of false positives and false negatives. <span style=""text-decoration:underline"">A *false positive* in our case is an article that should have been labeled &#34;not gun related&#34; (0) but was falsely labeled &#34;gun related&#34; (1).</span> These were likely articles with words like &#34;shoot&#34; but which referred to sports or movies instead of violent crimes. A *false negative* isåÊan article that should have been labeled &#34;gun related&#34; (1) but was falsely labeled &#34;not gun related&#34; (0). Its good to think about these different types of errors when you are working on prediction tasks-- often different types of errors have different costs, or one type of error is worse than another.åÊåÊ(E.g. in medicine, tests that produce many false negativesåÊare usually considered worse that ones that produce false positives. I&#39;m sure you can think of some reasons why?)</p>
<p>åÊ</p>
<p><strong>But what now? My life has been so empty since turning in that classifier assignment...åÊ</strong></p>
<p>Many of you have expressed interest in learning more about machine learning, and you should absolutely consider enrolling in <a href=""https://alliance.seas.upenn.edu/~cis520/wiki/"" target=""_blank"">penn&#39;s ML course</a>åÊor <a href=""http://www.seas.upenn.edu/~cis519/fall2015/"" target=""_blank"">penn&#39;s intro ML course</a>. Or check out the <a href=""https://www.coursera.org/learn/machine-learning"" target=""_blank"">Coursera course</a>åÊ(crowdsourcing woot woot!). If you can&#39;t wait that long,åÊyou have all the freedom in the world to build an ML component into your final project, and we are more than happy to help you do so!</p>
<p></p>
<p>Hope everyone hasåÊa relaxing, happy weekend!åÊ</p>
<p></p>
<p>#pin</p>",Take-aways from you classifier assignments,"<p>When I run the classifier, I get the error:åÊ</p>
<p>Reading raw data</p>
<p>Loading training data</p>
<p>Traceback (most recent call last):</p>
<p>åÊ File &#34;predict_unlabelled.py&#34;, line 65, in &lt;module&gt;</p>
<p>åÊ åÊ y, X, texts, dv, le = get_matricies(training_data)</p>
<p>åÊ File &#34;predict_unlabelled.py&#34;, line 31, in get_matricies</p>
<p>åÊ åÊ texts = [d[1] for d in data]</p>
<p>IndexError: list index out of range</p>
<p></p>
<p>Which is an error indicated in part of the code we were given. Does anyone know why this is?åÊ</p>",Error when running classifier,<p>ShouldåÊwe have a single if-and-or statement with all of the keywords we use? Or should we have a series of if statements when checking keywords?åÊ</p>,rule-based classifier,"<p>Can we physically draw a picture of the tree resulting from the classifier? Ours seems like it may be pretty extensive, so it would take quite a while to draw it using draw.io/other online tools...</p>",Large classifier tree...,<p>Resolved!</p>,Error when running classifier template,1
940847291,4/26/2016 15:59:59,false,1969403524,,4/26/2016 15:58:26,false,elite,1.0,33243069,IND,10,Faridabad,116.203.79.150,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I was just wondering, is there a way for us to see what we answered for our Market Research questionnaires? I didn&#39;t record my answers elsewhere and don&#39;t remember exactly what I submitted and was hoping I could use my answers in narrating my video.</p>",Questionnaire answers,4
940847291,4/26/2016 16:01:46,false,1969404614,,4/26/2016 16:01:11,false,personaly,1.0,33663352,ARG,01,Mar Del Plata,181.168.213.227,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I was just wondering, is there a way for us to see what we answered for our Market Research questionnaires? I didn&#39;t record my answers elsewhere and don&#39;t remember exactly what I submitted and was hoping I could use my answers in narrating my video.</p>",Questionnaire answers,4
940847291,4/26/2016 16:09:03,false,1969408352,,4/26/2016 16:03:36,false,clixsense,0.8889,8057247,PRT,17,Póvoa De Varzim,144.64.25.68,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I was just wondering, is there a way for us to see what we answered for our Market Research questionnaires? I didn&#39;t record my answers elsewhere and don&#39;t remember exactly what I submitted and was hoping I could use my answers in narrating my video.</p>",Questionnaire answers,4
940847291,4/26/2016 16:24:40,false,1969419135,,4/26/2016 16:04:24,false,neodev,0.8889,21971187,TTO,08,Valsayn,190.213.132.190,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I was just wondering, is there a way for us to see what we answered for our Market Research questionnaires? I didn&#39;t record my answers elsewhere and don&#39;t remember exactly what I submitted and was hoping I could use my answers in narrating my video.</p>",Questionnaire answers,4
940847291,4/26/2016 16:40:10,false,1969429582,,4/26/2016 16:37:31,false,neodev,0.7778,32569659,USA,MN,Minneapolis,97.127.88.224,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I was just wondering, is there a way for us to see what we answered for our Market Research questionnaires? I didn&#39;t record my answers elsewhere and don&#39;t remember exactly what I submitted and was hoping I could use my answers in narrating my video.</p>",Questionnaire answers,4
940847292,4/26/2016 15:28:06,false,1969389960,,4/26/2016 15:27:30,false,instagc,0.8889,13581319,USA,IL,Waltonville,208.70.36.12,0,,"After adding my fratures to the features list, and running the program, i still get the same decision tree as we saw initially with just gun. Not sure whats wrong",decision tree with add features,<p>How do I print the tree like we did in class Friday?</p>,Printing Decision Tree,"<p>I&#39;ve installed the graphivz onto my computer, but my program doesn&#39;t generate a diagram.</p>",Unable to generate Decision Tree Diagram,"<p>In our rule-based classifier, if we have a rule with the format &#34;If A in text and B not in text,&#34; should we put &#34;A and not B&#34; in the same box of the decision tree, or should we have separate boxes for A and B?åÊ</p>",Decision Tree,"<p>Hi, I looked over the chapter on decision trees and I am still a tad confused about how they work.</p>
<p></p>
<p>For my rule-based classifier, I used the following format:</p>
<p>if &#34;word1&#34; in text : decision = 1</p>
<p>if &#34;word2&#34; in text : decision = 1</p>
<p>... (rest of decision = 1 words)</p>
<p>if &#34;word10&#34; in text : decision = 0</p>
<p>if &#34;word11&#34; in text : decision = 0</p>
<p></p>
<p>I am still a bit confused about how a decision tree would work for this when the conditions are not interrelated</p>
<p></p>
<p>Would the tree look like this?</p>
<p></p>
<p>åÊåÊåÊ w1åÊ -------| åÊåÊåÊåÊåÊ åÊ åÊåÊ (and so on for w2, w3, etc)</p>
<p>åÊåÊ /åÊåÊåÊ \åÊåÊåÊåÊåÊåÊåÊåÊåÊåÊ | åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊåÊ</p>
<p>w10åÊåÊ w11åÊåÊåÊåÊ |åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ</p>
<p>|åÊåÊåÊåÊåÊåÊåÊåÊåÊåÊ |åÊåÊåÊåÊåÊåÊ yes</p>
<p>noåÊåÊåÊåÊåÊåÊåÊ no</p>",Decision Tree Where Conditions Are Not Dependent On One Another,<p>I keep getting an error message when I save my draw.io file as a .png on Linux VM and try to open the file with any sort of image viewer. Any idea what might be going on?</p>,Could not load decision tree image from draw.io,1
940847292,4/26/2016 15:30:54,false,1969391260,,4/26/2016 15:28:59,false,elite,1.0,30280423,ITA,15,Siracusa,151.54.84.121,0,,"After adding my fratures to the features list, and running the program, i still get the same decision tree as we saw initially with just gun. Not sure whats wrong",decision tree with add features,<p>How do I print the tree like we did in class Friday?</p>,Printing Decision Tree,"<p>I&#39;ve installed the graphivz onto my computer, but my program doesn&#39;t generate a diagram.</p>",Unable to generate Decision Tree Diagram,"<p>In our rule-based classifier, if we have a rule with the format &#34;If A in text and B not in text,&#34; should we put &#34;A and not B&#34; in the same box of the decision tree, or should we have separate boxes for A and B?åÊ</p>",Decision Tree,"<p>Hi, I looked over the chapter on decision trees and I am still a tad confused about how they work.</p>
<p></p>
<p>For my rule-based classifier, I used the following format:</p>
<p>if &#34;word1&#34; in text : decision = 1</p>
<p>if &#34;word2&#34; in text : decision = 1</p>
<p>... (rest of decision = 1 words)</p>
<p>if &#34;word10&#34; in text : decision = 0</p>
<p>if &#34;word11&#34; in text : decision = 0</p>
<p></p>
<p>I am still a bit confused about how a decision tree would work for this when the conditions are not interrelated</p>
<p></p>
<p>Would the tree look like this?</p>
<p></p>
<p>åÊåÊåÊ w1åÊ -------| åÊåÊåÊåÊåÊ åÊ åÊåÊ (and so on for w2, w3, etc)</p>
<p>åÊåÊ /åÊåÊåÊ \åÊåÊåÊåÊåÊåÊåÊåÊåÊåÊ | åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊåÊ</p>
<p>w10åÊåÊ w11åÊåÊåÊåÊ |åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ</p>
<p>|åÊåÊåÊåÊåÊåÊåÊåÊåÊåÊ |åÊåÊåÊåÊåÊåÊ yes</p>
<p>noåÊåÊåÊåÊåÊåÊåÊ no</p>",Decision Tree Where Conditions Are Not Dependent On One Another,<p>I keep getting an error message when I save my draw.io file as a .png on Linux VM and try to open the file with any sort of image viewer. Any idea what might be going on?</p>,Could not load decision tree image from draw.io,1
940847292,4/26/2016 15:35:58,false,1969392978,,4/26/2016 15:30:06,false,clixsense,0.8889,36052512,PHL,F2,Quezon City,49.149.150.150,0,,"After adding my fratures to the features list, and running the program, i still get the same decision tree as we saw initially with just gun. Not sure whats wrong",decision tree with add features,<p>How do I print the tree like we did in class Friday?</p>,Printing Decision Tree,"<p>I&#39;ve installed the graphivz onto my computer, but my program doesn&#39;t generate a diagram.</p>",Unable to generate Decision Tree Diagram,"<p>In our rule-based classifier, if we have a rule with the format &#34;If A in text and B not in text,&#34; should we put &#34;A and not B&#34; in the same box of the decision tree, or should we have separate boxes for A and B?åÊ</p>",Decision Tree,"<p>Hi, I looked over the chapter on decision trees and I am still a tad confused about how they work.</p>
<p></p>
<p>For my rule-based classifier, I used the following format:</p>
<p>if &#34;word1&#34; in text : decision = 1</p>
<p>if &#34;word2&#34; in text : decision = 1</p>
<p>... (rest of decision = 1 words)</p>
<p>if &#34;word10&#34; in text : decision = 0</p>
<p>if &#34;word11&#34; in text : decision = 0</p>
<p></p>
<p>I am still a bit confused about how a decision tree would work for this when the conditions are not interrelated</p>
<p></p>
<p>Would the tree look like this?</p>
<p></p>
<p>åÊåÊåÊ w1åÊ -------| åÊåÊåÊåÊåÊ åÊ åÊåÊ (and so on for w2, w3, etc)</p>
<p>åÊåÊ /åÊåÊåÊ \åÊåÊåÊåÊåÊåÊåÊåÊåÊåÊ | åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊåÊ</p>
<p>w10åÊåÊ w11åÊåÊåÊåÊ |åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ</p>
<p>|åÊåÊåÊåÊåÊåÊåÊåÊåÊåÊ |åÊåÊåÊåÊåÊåÊ yes</p>
<p>noåÊåÊåÊåÊåÊåÊåÊ no</p>",Decision Tree Where Conditions Are Not Dependent On One Another,<p>I keep getting an error message when I save my draw.io file as a .png on Linux VM and try to open the file with any sort of image viewer. Any idea what might be going on?</p>,Could not load decision tree image from draw.io,1
940847292,4/26/2016 15:45:41,false,1969397213,,4/26/2016 15:39:57,false,clixsense,1.0,21875134,GBR,H9,London,87.112.158.81,0,,"After adding my fratures to the features list, and running the program, i still get the same decision tree as we saw initially with just gun. Not sure whats wrong",decision tree with add features,<p>How do I print the tree like we did in class Friday?</p>,Printing Decision Tree,"<p>I&#39;ve installed the graphivz onto my computer, but my program doesn&#39;t generate a diagram.</p>",Unable to generate Decision Tree Diagram,"<p>In our rule-based classifier, if we have a rule with the format &#34;If A in text and B not in text,&#34; should we put &#34;A and not B&#34; in the same box of the decision tree, or should we have separate boxes for A and B?åÊ</p>",Decision Tree,"<p>Hi, I looked over the chapter on decision trees and I am still a tad confused about how they work.</p>
<p></p>
<p>For my rule-based classifier, I used the following format:</p>
<p>if &#34;word1&#34; in text : decision = 1</p>
<p>if &#34;word2&#34; in text : decision = 1</p>
<p>... (rest of decision = 1 words)</p>
<p>if &#34;word10&#34; in text : decision = 0</p>
<p>if &#34;word11&#34; in text : decision = 0</p>
<p></p>
<p>I am still a bit confused about how a decision tree would work for this when the conditions are not interrelated</p>
<p></p>
<p>Would the tree look like this?</p>
<p></p>
<p>åÊåÊåÊ w1åÊ -------| åÊåÊåÊåÊåÊ åÊ åÊåÊ (and so on for w2, w3, etc)</p>
<p>åÊåÊ /åÊåÊåÊ \åÊåÊåÊåÊåÊåÊåÊåÊåÊåÊ | åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊåÊ</p>
<p>w10åÊåÊ w11åÊåÊåÊåÊ |åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ</p>
<p>|åÊåÊåÊåÊåÊåÊåÊåÊåÊåÊ |åÊåÊåÊåÊåÊåÊ yes</p>
<p>noåÊåÊåÊåÊåÊåÊåÊ no</p>",Decision Tree Where Conditions Are Not Dependent On One Another,<p>I keep getting an error message when I save my draw.io file as a .png on Linux VM and try to open the file with any sort of image viewer. Any idea what might be going on?</p>,Could not load decision tree image from draw.io,1
940847292,4/26/2016 15:47:45,false,1969398094,,4/26/2016 15:41:08,false,neodev,0.7778,32569659,USA,MN,Minneapolis,97.127.88.224,0,,"After adding my fratures to the features list, and running the program, i still get the same decision tree as we saw initially with just gun. Not sure whats wrong",decision tree with add features,<p>How do I print the tree like we did in class Friday?</p>,Printing Decision Tree,"<p>I&#39;ve installed the graphivz onto my computer, but my program doesn&#39;t generate a diagram.</p>",Unable to generate Decision Tree Diagram,"<p>In our rule-based classifier, if we have a rule with the format &#34;If A in text and B not in text,&#34; should we put &#34;A and not B&#34; in the same box of the decision tree, or should we have separate boxes for A and B?åÊ</p>",Decision Tree,"<p>Hi, I looked over the chapter on decision trees and I am still a tad confused about how they work.</p>
<p></p>
<p>For my rule-based classifier, I used the following format:</p>
<p>if &#34;word1&#34; in text : decision = 1</p>
<p>if &#34;word2&#34; in text : decision = 1</p>
<p>... (rest of decision = 1 words)</p>
<p>if &#34;word10&#34; in text : decision = 0</p>
<p>if &#34;word11&#34; in text : decision = 0</p>
<p></p>
<p>I am still a bit confused about how a decision tree would work for this when the conditions are not interrelated</p>
<p></p>
<p>Would the tree look like this?</p>
<p></p>
<p>åÊåÊåÊ w1åÊ -------| åÊåÊåÊåÊåÊ åÊ åÊåÊ (and so on for w2, w3, etc)</p>
<p>åÊåÊ /åÊåÊåÊ \åÊåÊåÊåÊåÊåÊåÊåÊåÊåÊ | åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊåÊ</p>
<p>w10åÊåÊ w11åÊåÊåÊåÊ |åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ</p>
<p>|åÊåÊåÊåÊåÊåÊåÊåÊåÊåÊ |åÊåÊåÊåÊåÊåÊ yes</p>
<p>noåÊåÊåÊåÊåÊåÊåÊ no</p>",Decision Tree Where Conditions Are Not Dependent On One Another,<p>I keep getting an error message when I save my draw.io file as a .png on Linux VM and try to open the file with any sort of image viewer. Any idea what might be going on?</p>,Could not load decision tree image from draw.io,1
940847293,4/26/2016 15:48:55,false,1969398738,,4/26/2016 15:46:06,false,neodev,0.8889,21971187,TTO,08,Valsayn,190.213.132.190,0,,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p></p><pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p>When I run the code, I keep getting this error. I have tried looking at my API Key and seeing if I have an extra space in the API key field, but it looks perfectly fine to me.åÊ</p>",Error on running Bing API.py,"<p>I&#39;m getting the following errors when I try and run the python code... I changed the header in the .py file to match what I have. Any suggestions? Thanks!</p>
<p></p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8n8y74iq4ov/ilx2j8u87feg/Screen_Shot_20160317_at_10.08.53_PM.png"" /></p>",read error,4
940847293,4/26/2016 15:50:47,false,1969399562,,4/26/2016 15:49:12,false,elite,0.8889,36575101,IND,07,New Delhi,112.196.144.2,0,,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p></p><pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p>When I run the code, I keep getting this error. I have tried looking at my API Key and seeing if I have an extra space in the API key field, but it looks perfectly fine to me.åÊ</p>",Error on running Bing API.py,"<p>I&#39;m getting the following errors when I try and run the python code... I changed the header in the .py file to match what I have. Any suggestions? Thanks!</p>
<p></p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8n8y74iq4ov/ilx2j8u87feg/Screen_Shot_20160317_at_10.08.53_PM.png"" /></p>",read error,4
940847293,4/26/2016 15:51:11,false,1969399726,,4/26/2016 15:47:56,false,neodev,1.0,28875937,PAK,08,Islamabad,119.153.105.50,0,,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p></p><pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p>When I run the code, I keep getting this error. I have tried looking at my API Key and seeing if I have an extra space in the API key field, but it looks perfectly fine to me.åÊ</p>",Error on running Bing API.py,"<p>I&#39;m getting the following errors when I try and run the python code... I changed the header in the .py file to match what I have. Any suggestions? Thanks!</p>
<p></p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8n8y74iq4ov/ilx2j8u87feg/Screen_Shot_20160317_at_10.08.53_PM.png"" /></p>",read error,4
940847293,4/26/2016 15:53:17,false,1969400602,,4/26/2016 15:51:23,false,neodev,1.0,13396426,VEN,15,Santa Teresa,190.38.163.149,0,,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p></p><pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p>When I run the code, I keep getting this error. I have tried looking at my API Key and seeing if I have an extra space in the API key field, but it looks perfectly fine to me.åÊ</p>",Error on running Bing API.py,"<p>I&#39;m getting the following errors when I try and run the python code... I changed the header in the .py file to match what I have. Any suggestions? Thanks!</p>
<p></p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8n8y74iq4ov/ilx2j8u87feg/Screen_Shot_20160317_at_10.08.53_PM.png"" /></p>",read error,4
940847293,4/26/2016 15:56:46,false,1969402054,,4/26/2016 15:55:02,false,elite,1.0,33243069,IND,10,Faridabad,116.203.79.150,0,,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p></p><pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p>When I run the code, I keep getting this error. I have tried looking at my API Key and seeing if I have an extra space in the API key field, but it looks perfectly fine to me.åÊ</p>",Error on running Bing API.py,"<p>I&#39;m getting the following errors when I try and run the python code... I changed the header in the .py file to match what I have. Any suggestions? Thanks!</p>
<p></p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8n8y74iq4ov/ilx2j8u87feg/Screen_Shot_20160317_at_10.08.53_PM.png"" /></p>",read error,4
940847294,4/26/2016 15:28:37,false,1969390286,,4/26/2016 15:28:07,false,instagc,0.8889,13581319,USA,IL,Waltonville,208.70.36.12,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,,Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>When I parse the reviews, I&#39;m stripping the commas, periods, and parentheses from the words and making them all lower case from the get go, like:</p>
<p></p>
<pre>word = words[x].strip(&#34;,.()&#34;).lower()</pre>
<p>However, this gives me a different answer for the number of ten most common words. That is, instead ofåÊ</p>
<p></p>
<pre>a	623
and	587
the	521
-	389
of	366
but	365
I	251
to	237
with	207
it	196</pre>
<p></p>
<p>I&#39;m getting</p>
<p></p>
<pre>{&#39;very&#39;: 292, &#39;a&#39;: 810, &#39;it&#39;: 245, &#39;-&#39;: 389, &#39;but&#39;: 379, &#39;and&#39;: 591, &#39;i&#39;: 258, &#39;good&#39;: 272, &#39;of&#39;: 368, &#39;the&#39;: 550}</pre>
<p></p>
<p>And I found that I get the right answers if I don&#39;t strip() or lower(). (Interestingly enough, &#39;-&#39; stays the same, because no one ever puts parentheses around or a comma or period around &#39;-&#39;)</p>
<p></p>
<p>Basically all my answers are slightly off for the entire homework because I did some variation of this at each step, andåÊI&#39;m wondering if this would be accepted for homework or if I should go back to figure out what to strip/lower and what not to?</p>",Ten Most Common Words,5
940847294,4/26/2016 15:39:31,false,1969394680,,4/26/2016 15:36:13,false,clixsense,0.8889,36052512,PHL,F2,Quezon City,49.149.150.150,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,,Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>When I parse the reviews, I&#39;m stripping the commas, periods, and parentheses from the words and making them all lower case from the get go, like:</p>
<p></p>
<pre>word = words[x].strip(&#34;,.()&#34;).lower()</pre>
<p>However, this gives me a different answer for the number of ten most common words. That is, instead ofåÊ</p>
<p></p>
<pre>a	623
and	587
the	521
-	389
of	366
but	365
I	251
to	237
with	207
it	196</pre>
<p></p>
<p>I&#39;m getting</p>
<p></p>
<pre>{&#39;very&#39;: 292, &#39;a&#39;: 810, &#39;it&#39;: 245, &#39;-&#39;: 389, &#39;but&#39;: 379, &#39;and&#39;: 591, &#39;i&#39;: 258, &#39;good&#39;: 272, &#39;of&#39;: 368, &#39;the&#39;: 550}</pre>
<p></p>
<p>And I found that I get the right answers if I don&#39;t strip() or lower(). (Interestingly enough, &#39;-&#39; stays the same, because no one ever puts parentheses around or a comma or period around &#39;-&#39;)</p>
<p></p>
<p>Basically all my answers are slightly off for the entire homework because I did some variation of this at each step, andåÊI&#39;m wondering if this would be accepted for homework or if I should go back to figure out what to strip/lower and what not to?</p>",Ten Most Common Words,5
940847294,4/26/2016 15:47:54,false,1969398211,,4/26/2016 15:44:26,false,neodev,1.0,28875937,PAK,05,Karachi,182.180.125.133,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,,Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>When I parse the reviews, I&#39;m stripping the commas, periods, and parentheses from the words and making them all lower case from the get go, like:</p>
<p></p>
<pre>word = words[x].strip(&#34;,.()&#34;).lower()</pre>
<p>However, this gives me a different answer for the number of ten most common words. That is, instead ofåÊ</p>
<p></p>
<pre>a	623
and	587
the	521
-	389
of	366
but	365
I	251
to	237
with	207
it	196</pre>
<p></p>
<p>I&#39;m getting</p>
<p></p>
<pre>{&#39;very&#39;: 292, &#39;a&#39;: 810, &#39;it&#39;: 245, &#39;-&#39;: 389, &#39;but&#39;: 379, &#39;and&#39;: 591, &#39;i&#39;: 258, &#39;good&#39;: 272, &#39;of&#39;: 368, &#39;the&#39;: 550}</pre>
<p></p>
<p>And I found that I get the right answers if I don&#39;t strip() or lower(). (Interestingly enough, &#39;-&#39; stays the same, because no one ever puts parentheses around or a comma or period around &#39;-&#39;)</p>
<p></p>
<p>Basically all my answers are slightly off for the entire homework because I did some variation of this at each step, andåÊI&#39;m wondering if this would be accepted for homework or if I should go back to figure out what to strip/lower and what not to?</p>",Ten Most Common Words,5
940847294,4/26/2016 15:49:11,false,1969398842,,4/26/2016 15:45:52,false,elite,0.8889,36575101,IND,07,New Delhi,112.196.144.2,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,,Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>When I parse the reviews, I&#39;m stripping the commas, periods, and parentheses from the words and making them all lower case from the get go, like:</p>
<p></p>
<pre>word = words[x].strip(&#34;,.()&#34;).lower()</pre>
<p>However, this gives me a different answer for the number of ten most common words. That is, instead ofåÊ</p>
<p></p>
<pre>a	623
and	587
the	521
-	389
of	366
but	365
I	251
to	237
with	207
it	196</pre>
<p></p>
<p>I&#39;m getting</p>
<p></p>
<pre>{&#39;very&#39;: 292, &#39;a&#39;: 810, &#39;it&#39;: 245, &#39;-&#39;: 389, &#39;but&#39;: 379, &#39;and&#39;: 591, &#39;i&#39;: 258, &#39;good&#39;: 272, &#39;of&#39;: 368, &#39;the&#39;: 550}</pre>
<p></p>
<p>And I found that I get the right answers if I don&#39;t strip() or lower(). (Interestingly enough, &#39;-&#39; stays the same, because no one ever puts parentheses around or a comma or period around &#39;-&#39;)</p>
<p></p>
<p>Basically all my answers are slightly off for the entire homework because I did some variation of this at each step, andåÊI&#39;m wondering if this would be accepted for homework or if I should go back to figure out what to strip/lower and what not to?</p>",Ten Most Common Words,5
940847294,4/26/2016 15:56:09,false,1969401805,,4/26/2016 15:45:42,false,clixsense,1.0,21875134,GBR,H9,London,87.112.158.81,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,,Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>When I parse the reviews, I&#39;m stripping the commas, periods, and parentheses from the words and making them all lower case from the get go, like:</p>
<p></p>
<pre>word = words[x].strip(&#34;,.()&#34;).lower()</pre>
<p>However, this gives me a different answer for the number of ten most common words. That is, instead ofåÊ</p>
<p></p>
<pre>a	623
and	587
the	521
-	389
of	366
but	365
I	251
to	237
with	207
it	196</pre>
<p></p>
<p>I&#39;m getting</p>
<p></p>
<pre>{&#39;very&#39;: 292, &#39;a&#39;: 810, &#39;it&#39;: 245, &#39;-&#39;: 389, &#39;but&#39;: 379, &#39;and&#39;: 591, &#39;i&#39;: 258, &#39;good&#39;: 272, &#39;of&#39;: 368, &#39;the&#39;: 550}</pre>
<p></p>
<p>And I found that I get the right answers if I don&#39;t strip() or lower(). (Interestingly enough, &#39;-&#39; stays the same, because no one ever puts parentheses around or a comma or period around &#39;-&#39;)</p>
<p></p>
<p>Basically all my answers are slightly off for the entire homework because I did some variation of this at each step, andåÊI&#39;m wondering if this would be accepted for homework or if I should go back to figure out what to strip/lower and what not to?</p>",Ten Most Common Words,5
940847296,4/26/2016 15:54:11,false,1969401047,,4/26/2016 15:52:34,false,elite,0.8889,36575101,IND,07,New Delhi,112.196.144.2,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"In Part 1: Crawling the Web, step 1 has a code fragment that reads:

 $ tar -xzvf asssignment5.tgz 

This should read:

tar -xzvf assignment5.tgz",Slight typo in homework assignment,4
940847296,4/26/2016 15:56:34,false,1969401958,,4/26/2016 15:53:47,false,neodev,1.0,28875937,PAK,08,Islamabad,119.153.105.50,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"In Part 1: Crawling the Web, step 1 has a code fragment that reads:

 $ tar -xzvf asssignment5.tgz 

This should read:

tar -xzvf assignment5.tgz",Slight typo in homework assignment,4
940847296,4/26/2016 15:58:11,false,1969402626,,4/26/2016 15:56:24,false,neodev,1.0,13396426,VEN,15,Santa Teresa,190.38.163.149,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"In Part 1: Crawling the Web, step 1 has a code fragment that reads:

 $ tar -xzvf asssignment5.tgz 

This should read:

tar -xzvf assignment5.tgz",Slight typo in homework assignment,4
940847296,4/26/2016 16:04:22,false,1969405939,,4/26/2016 15:56:16,false,neodev,0.8889,21971187,TTO,08,Valsayn,190.213.132.190,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"In Part 1: Crawling the Web, step 1 has a code fragment that reads:

 $ tar -xzvf asssignment5.tgz 

This should read:

tar -xzvf assignment5.tgz",Slight typo in homework assignment,4
940847296,4/26/2016 16:19:52,false,1969415129,,4/26/2016 15:56:52,false,neodev,0.7778,32569659,USA,MN,Minneapolis,97.127.88.224,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"In Part 1: Crawling the Web, step 1 has a code fragment that reads:

 $ tar -xzvf asssignment5.tgz 

This should read:

tar -xzvf assignment5.tgz",Slight typo in homework assignment,4
940847297,4/26/2016 15:52:33,false,1969400264,,4/26/2016 15:50:49,false,elite,0.8889,36575101,IND,07,New Delhi,112.196.144.2,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p></p><p>I keep getting this error when I uncomment the diagram tree code:</p>
<p></p>
<p>TypeError: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.</p>
<p></p>
<p>I googled the error and it&#39;s telling me to convert X.todense() but I haven&#39;tåÊseen this error elsewhere on Piazza ..</p>",TypeError: sparse matrix passed,3
940847297,4/26/2016 15:53:45,false,1969400808,,4/26/2016 15:51:13,false,neodev,1.0,28875937,PAK,08,Islamabad,119.153.105.50,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p></p><p>I keep getting this error when I uncomment the diagram tree code:</p>
<p></p>
<p>TypeError: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.</p>
<p></p>
<p>I googled the error and it&#39;s telling me to convert X.todense() but I haven&#39;tåÊseen this error elsewhere on Piazza ..</p>",TypeError: sparse matrix passed,3
940847297,4/26/2016 15:56:15,false,1969401850,,4/26/2016 15:48:57,false,neodev,0.8889,21971187,TTO,08,Valsayn,190.213.132.190,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p></p><p>I keep getting this error when I uncomment the diagram tree code:</p>
<p></p>
<p>TypeError: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.</p>
<p></p>
<p>I googled the error and it&#39;s telling me to convert X.todense() but I haven&#39;tåÊseen this error elsewhere on Piazza ..</p>",TypeError: sparse matrix passed,3
940847297,4/26/2016 15:56:23,false,1969401927,,4/26/2016 15:53:18,false,neodev,1.0,13396426,VEN,15,Santa Teresa,190.38.163.149,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p></p><p>I keep getting this error when I uncomment the diagram tree code:</p>
<p></p>
<p>TypeError: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.</p>
<p></p>
<p>I googled the error and it&#39;s telling me to convert X.todense() but I haven&#39;tåÊseen this error elsewhere on Piazza ..</p>",TypeError: sparse matrix passed,3
940847297,4/26/2016 16:00:16,false,1969403677,,4/26/2016 15:56:11,false,clixsense,1.0,21875134,GBR,H9,London,87.112.158.81,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p></p><p>I keep getting this error when I uncomment the diagram tree code:</p>
<p></p>
<p>TypeError: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.</p>
<p></p>
<p>I googled the error and it&#39;s telling me to convert X.todense() but I haven&#39;tåÊseen this error elsewhere on Piazza ..</p>",TypeError: sparse matrix passed,3
940847298,4/26/2016 17:08:54,false,1969446911,,4/26/2016 17:07:44,false,neodev,1.0,36167043,GBR,G6,Hull,77.86.101.69,0,,"<p>I&#39;ve already started my OH in the GRW bump space so feel free to come on by early if you&#39;d like. I&#39;ll be here till 5 unless it get&#39;s crowded in which case find me near the study space next to the elevators on the 5th floor of Levine.</p>
<p></p>",Starting OH early,"<p>Hey everyone,åÊI&#39;m moving my office hours tonight to Moore 100 instead of the bump space.</p>",OH moved to Moore,"<p>Hi everyone,åÊ</p>
<p></p>
<p>I&#39;m not feeling well, so I&#39;m canceling the office hours I was planning to hold today from 4-6pm. However, Sierra will still be holding her office hours at the same time. I&#39;ll try to hold make-up OH soon.åÊ</p>
<p></p>
<p>Best,</p>
<p>Kate</p>",OH Today,"<p>Hi all,</p>
<p></p>
<p>I have to cancel my office hours today from 12-2. If you needed help on somethingåÊandåÊwere planning to come, send me an email or a private message here and we can set up a time to talk either in person or over video chat today or this weekend.åÊ</p>
<p></p>
<p>Sorry for the inconvenience!</p>
<p>Ellie</p>",Canceling OH for today,Will there be any OH over the weekend since this assignment is due on Monday instead of the usual Friday?,OH over the weekend?,"<p>Hi all,</p>
<p></p>
<p>I will be holding my office hours WednesdayåÊfrom 12-2 right before lecture, instead of on Friday. Anyone still working on the HW 7, please stop by my office (Levine 614). We will be releasing HW 8 tomorrow in lecture, so hopefully we can get everyone caught up soon.</p>
<p></p>
<p>--Ellie</p>",Ellie&#39;s OH on Wednesday,1
940847298,4/26/2016 17:16:52,false,1969451848,,4/26/2016 17:08:45,false,neodev,1.0,33973110,VEN,23,Maracaibo,186.94.238.104,0,,"<p>I&#39;ve already started my OH in the GRW bump space so feel free to come on by early if you&#39;d like. I&#39;ll be here till 5 unless it get&#39;s crowded in which case find me near the study space next to the elevators on the 5th floor of Levine.</p>
<p></p>",Starting OH early,"<p>Hey everyone,åÊI&#39;m moving my office hours tonight to Moore 100 instead of the bump space.</p>",OH moved to Moore,"<p>Hi everyone,åÊ</p>
<p></p>
<p>I&#39;m not feeling well, so I&#39;m canceling the office hours I was planning to hold today from 4-6pm. However, Sierra will still be holding her office hours at the same time. I&#39;ll try to hold make-up OH soon.åÊ</p>
<p></p>
<p>Best,</p>
<p>Kate</p>",OH Today,"<p>Hi all,</p>
<p></p>
<p>I have to cancel my office hours today from 12-2. If you needed help on somethingåÊandåÊwere planning to come, send me an email or a private message here and we can set up a time to talk either in person or over video chat today or this weekend.åÊ</p>
<p></p>
<p>Sorry for the inconvenience!</p>
<p>Ellie</p>",Canceling OH for today,Will there be any OH over the weekend since this assignment is due on Monday instead of the usual Friday?,OH over the weekend?,"<p>Hi all,</p>
<p></p>
<p>I will be holding my office hours WednesdayåÊfrom 12-2 right before lecture, instead of on Friday. Anyone still working on the HW 7, please stop by my office (Levine 614). We will be releasing HW 8 tomorrow in lecture, so hopefully we can get everyone caught up soon.</p>
<p></p>
<p>--Ellie</p>",Ellie&#39;s OH on Wednesday,1
940847298,4/26/2016 17:19:05,false,1969453021,,4/26/2016 17:16:43,false,elite,1.0,25411289,HRV,"","",31.147.119.175,0,,"<p>I&#39;ve already started my OH in the GRW bump space so feel free to come on by early if you&#39;d like. I&#39;ll be here till 5 unless it get&#39;s crowded in which case find me near the study space next to the elevators on the 5th floor of Levine.</p>
<p></p>",Starting OH early,"<p>Hey everyone,åÊI&#39;m moving my office hours tonight to Moore 100 instead of the bump space.</p>",OH moved to Moore,"<p>Hi everyone,åÊ</p>
<p></p>
<p>I&#39;m not feeling well, so I&#39;m canceling the office hours I was planning to hold today from 4-6pm. However, Sierra will still be holding her office hours at the same time. I&#39;ll try to hold make-up OH soon.åÊ</p>
<p></p>
<p>Best,</p>
<p>Kate</p>",OH Today,"<p>Hi all,</p>
<p></p>
<p>I have to cancel my office hours today from 12-2. If you needed help on somethingåÊandåÊwere planning to come, send me an email or a private message here and we can set up a time to talk either in person or over video chat today or this weekend.åÊ</p>
<p></p>
<p>Sorry for the inconvenience!</p>
<p>Ellie</p>",Canceling OH for today,Will there be any OH over the weekend since this assignment is due on Monday instead of the usual Friday?,OH over the weekend?,"<p>Hi all,</p>
<p></p>
<p>I will be holding my office hours WednesdayåÊfrom 12-2 right before lecture, instead of on Friday. Anyone still working on the HW 7, please stop by my office (Levine 614). We will be releasing HW 8 tomorrow in lecture, so hopefully we can get everyone caught up soon.</p>
<p></p>
<p>--Ellie</p>",Ellie&#39;s OH on Wednesday,1
940847298,4/26/2016 17:32:09,false,1969460732,,4/26/2016 17:30:32,false,neodev,0.8889,19625264,DZA,41,Chlef,41.102.7.217,0,,"<p>I&#39;ve already started my OH in the GRW bump space so feel free to come on by early if you&#39;d like. I&#39;ll be here till 5 unless it get&#39;s crowded in which case find me near the study space next to the elevators on the 5th floor of Levine.</p>
<p></p>",Starting OH early,"<p>Hey everyone,åÊI&#39;m moving my office hours tonight to Moore 100 instead of the bump space.</p>",OH moved to Moore,"<p>Hi everyone,åÊ</p>
<p></p>
<p>I&#39;m not feeling well, so I&#39;m canceling the office hours I was planning to hold today from 4-6pm. However, Sierra will still be holding her office hours at the same time. I&#39;ll try to hold make-up OH soon.åÊ</p>
<p></p>
<p>Best,</p>
<p>Kate</p>",OH Today,"<p>Hi all,</p>
<p></p>
<p>I have to cancel my office hours today from 12-2. If you needed help on somethingåÊandåÊwere planning to come, send me an email or a private message here and we can set up a time to talk either in person or over video chat today or this weekend.åÊ</p>
<p></p>
<p>Sorry for the inconvenience!</p>
<p>Ellie</p>",Canceling OH for today,Will there be any OH over the weekend since this assignment is due on Monday instead of the usual Friday?,OH over the weekend?,"<p>Hi all,</p>
<p></p>
<p>I will be holding my office hours WednesdayåÊfrom 12-2 right before lecture, instead of on Friday. Anyone still working on the HW 7, please stop by my office (Levine 614). We will be releasing HW 8 tomorrow in lecture, so hopefully we can get everyone caught up soon.</p>
<p></p>
<p>--Ellie</p>",Ellie&#39;s OH on Wednesday,1
940847298,4/26/2016 17:32:14,false,1969460764,,4/26/2016 17:15:40,false,clixsense,0.8889,35338593,ITA,14,Cagliari,151.56.132.145,0,,"<p>I&#39;ve already started my OH in the GRW bump space so feel free to come on by early if you&#39;d like. I&#39;ll be here till 5 unless it get&#39;s crowded in which case find me near the study space next to the elevators on the 5th floor of Levine.</p>
<p></p>",Starting OH early,"<p>Hey everyone,åÊI&#39;m moving my office hours tonight to Moore 100 instead of the bump space.</p>",OH moved to Moore,"<p>Hi everyone,åÊ</p>
<p></p>
<p>I&#39;m not feeling well, so I&#39;m canceling the office hours I was planning to hold today from 4-6pm. However, Sierra will still be holding her office hours at the same time. I&#39;ll try to hold make-up OH soon.åÊ</p>
<p></p>
<p>Best,</p>
<p>Kate</p>",OH Today,"<p>Hi all,</p>
<p></p>
<p>I have to cancel my office hours today from 12-2. If you needed help on somethingåÊandåÊwere planning to come, send me an email or a private message here and we can set up a time to talk either in person or over video chat today or this weekend.åÊ</p>
<p></p>
<p>Sorry for the inconvenience!</p>
<p>Ellie</p>",Canceling OH for today,Will there be any OH over the weekend since this assignment is due on Monday instead of the usual Friday?,OH over the weekend?,"<p>Hi all,</p>
<p></p>
<p>I will be holding my office hours WednesdayåÊfrom 12-2 right before lecture, instead of on Friday. Anyone still working on the HW 7, please stop by my office (Levine 614). We will be releasing HW 8 tomorrow in lecture, so hopefully we can get everyone caught up soon.</p>
<p></p>
<p>--Ellie</p>",Ellie&#39;s OH on Wednesday,1
940847299,4/26/2016 18:22:36,false,1969487899,,4/26/2016 18:21:16,false,neodev,0.8889,35550011,VEN,07,Valencia,190.204.238.112,0,,"<p>Hi everyone,</p>
<p></p>
<p>I&#39;m looking for a partner toåÊcollaborate with on upcoming assignments and/or the final project. E-mail me at <a href=""mailto:hcutler&#64;seas.upenn.edu"">hcutler&#64;seas.upenn.edu</a>åÊif you&#39;re interested!</p>
<p></p>
<p>Have a great day.</p>
<p></p>
<p>Hannah Cutler</p>
<p>SEAS &#39;17</p>",Looking for a partner for homeworks and/or final project?,"<p>Are we allowed to modify one of the past final project ideas shown to us in class? We have an interesting ideaåÊsimilar to PictureThis and wanted to make sure it&#39;s OK that it&#39;s similar.</p>
<p></p>
<p>Thanks!</p>",Final project idea similar to PictureThis,"<p>FYI, the second part of the final project was released last night:åÊ<a href=""http://crowdsourcing-class.org/final-project-part2.html"">http://crowdsourcing-class.org/final-project-part2.html</a></p>
<p></p>
<p>There are two deliverables. åÊThe first one is due on Saturday before 11:59pm.</p>",Final Project Part 2 has been released,<p>This is the link:åÊhttps://github.com/benpleitner/NETS213-Final-Project</p>,Group Project,"<p>Here is some updated details about the $10,000 prize for the best final project:</p>
<p><a href=""http://crowdsourcing-class.org/project.html"">http://crowdsourcing-class.org/project.html</a></p>
<p></p>
<p>Let me know what you think!</p>","$10,000 prize for the final project","<p>For the NETS 213 final project, do we as students retain ownership of our projects? I know that some schools will hold the ownership rights, and juståÊwanted some clarification on this policy for the NETS 213 course.</p>",Student Ownership of Final Project?,2
940847299,4/26/2016 18:27:26,false,1969490293,,4/26/2016 18:25:35,false,elite,1.0,30128662,BGR,50,Pleven,212.233.177.195,0,,"<p>Hi everyone,</p>
<p></p>
<p>I&#39;m looking for a partner toåÊcollaborate with on upcoming assignments and/or the final project. E-mail me at <a href=""mailto:hcutler&#64;seas.upenn.edu"">hcutler&#64;seas.upenn.edu</a>åÊif you&#39;re interested!</p>
<p></p>
<p>Have a great day.</p>
<p></p>
<p>Hannah Cutler</p>
<p>SEAS &#39;17</p>",Looking for a partner for homeworks and/or final project?,"<p>Are we allowed to modify one of the past final project ideas shown to us in class? We have an interesting ideaåÊsimilar to PictureThis and wanted to make sure it&#39;s OK that it&#39;s similar.</p>
<p></p>
<p>Thanks!</p>",Final project idea similar to PictureThis,"<p>FYI, the second part of the final project was released last night:åÊ<a href=""http://crowdsourcing-class.org/final-project-part2.html"">http://crowdsourcing-class.org/final-project-part2.html</a></p>
<p></p>
<p>There are two deliverables. åÊThe first one is due on Saturday before 11:59pm.</p>",Final Project Part 2 has been released,<p>This is the link:åÊhttps://github.com/benpleitner/NETS213-Final-Project</p>,Group Project,"<p>Here is some updated details about the $10,000 prize for the best final project:</p>
<p><a href=""http://crowdsourcing-class.org/project.html"">http://crowdsourcing-class.org/project.html</a></p>
<p></p>
<p>Let me know what you think!</p>","$10,000 prize for the final project","<p>For the NETS 213 final project, do we as students retain ownership of our projects? I know that some schools will hold the ownership rights, and juståÊwanted some clarification on this policy for the NETS 213 course.</p>",Student Ownership of Final Project?,2
940847299,4/26/2016 18:37:13,false,1969495294,,4/26/2016 18:33:39,false,neodev,1.0,29879245,RUS,69,Smolensk,37.144.124.118,0,,"<p>Hi everyone,</p>
<p></p>
<p>I&#39;m looking for a partner toåÊcollaborate with on upcoming assignments and/or the final project. E-mail me at <a href=""mailto:hcutler&#64;seas.upenn.edu"">hcutler&#64;seas.upenn.edu</a>åÊif you&#39;re interested!</p>
<p></p>
<p>Have a great day.</p>
<p></p>
<p>Hannah Cutler</p>
<p>SEAS &#39;17</p>",Looking for a partner for homeworks and/or final project?,"<p>Are we allowed to modify one of the past final project ideas shown to us in class? We have an interesting ideaåÊsimilar to PictureThis and wanted to make sure it&#39;s OK that it&#39;s similar.</p>
<p></p>
<p>Thanks!</p>",Final project idea similar to PictureThis,"<p>FYI, the second part of the final project was released last night:åÊ<a href=""http://crowdsourcing-class.org/final-project-part2.html"">http://crowdsourcing-class.org/final-project-part2.html</a></p>
<p></p>
<p>There are two deliverables. åÊThe first one is due on Saturday before 11:59pm.</p>",Final Project Part 2 has been released,<p>This is the link:åÊhttps://github.com/benpleitner/NETS213-Final-Project</p>,Group Project,"<p>Here is some updated details about the $10,000 prize for the best final project:</p>
<p><a href=""http://crowdsourcing-class.org/project.html"">http://crowdsourcing-class.org/project.html</a></p>
<p></p>
<p>Let me know what you think!</p>","$10,000 prize for the final project","<p>For the NETS 213 final project, do we as students retain ownership of our projects? I know that some schools will hold the ownership rights, and juståÊwanted some clarification on this policy for the NETS 213 course.</p>",Student Ownership of Final Project?,2
940847299,4/26/2016 19:02:49,false,1969507171,,4/26/2016 18:44:58,false,neodev,1.0,11172894,IND,28,Champdani,117.194.5.117,0,,"<p>Hi everyone,</p>
<p></p>
<p>I&#39;m looking for a partner toåÊcollaborate with on upcoming assignments and/or the final project. E-mail me at <a href=""mailto:hcutler&#64;seas.upenn.edu"">hcutler&#64;seas.upenn.edu</a>åÊif you&#39;re interested!</p>
<p></p>
<p>Have a great day.</p>
<p></p>
<p>Hannah Cutler</p>
<p>SEAS &#39;17</p>",Looking for a partner for homeworks and/or final project?,"<p>Are we allowed to modify one of the past final project ideas shown to us in class? We have an interesting ideaåÊsimilar to PictureThis and wanted to make sure it&#39;s OK that it&#39;s similar.</p>
<p></p>
<p>Thanks!</p>",Final project idea similar to PictureThis,"<p>FYI, the second part of the final project was released last night:åÊ<a href=""http://crowdsourcing-class.org/final-project-part2.html"">http://crowdsourcing-class.org/final-project-part2.html</a></p>
<p></p>
<p>There are two deliverables. åÊThe first one is due on Saturday before 11:59pm.</p>",Final Project Part 2 has been released,<p>This is the link:åÊhttps://github.com/benpleitner/NETS213-Final-Project</p>,Group Project,"<p>Here is some updated details about the $10,000 prize for the best final project:</p>
<p><a href=""http://crowdsourcing-class.org/project.html"">http://crowdsourcing-class.org/project.html</a></p>
<p></p>
<p>Let me know what you think!</p>","$10,000 prize for the final project","<p>For the NETS 213 final project, do we as students retain ownership of our projects? I know that some schools will hold the ownership rights, and juståÊwanted some clarification on this policy for the NETS 213 course.</p>",Student Ownership of Final Project?,2
940847299,4/26/2016 19:25:38,false,1969519498,,4/26/2016 19:25:12,false,tremorgames,1.0,25197223,HRV,15,Split,94.253.234.240,0,,"<p>Hi everyone,</p>
<p></p>
<p>I&#39;m looking for a partner toåÊcollaborate with on upcoming assignments and/or the final project. E-mail me at <a href=""mailto:hcutler&#64;seas.upenn.edu"">hcutler&#64;seas.upenn.edu</a>åÊif you&#39;re interested!</p>
<p></p>
<p>Have a great day.</p>
<p></p>
<p>Hannah Cutler</p>
<p>SEAS &#39;17</p>",Looking for a partner for homeworks and/or final project?,"<p>Are we allowed to modify one of the past final project ideas shown to us in class? We have an interesting ideaåÊsimilar to PictureThis and wanted to make sure it&#39;s OK that it&#39;s similar.</p>
<p></p>
<p>Thanks!</p>",Final project idea similar to PictureThis,"<p>FYI, the second part of the final project was released last night:åÊ<a href=""http://crowdsourcing-class.org/final-project-part2.html"">http://crowdsourcing-class.org/final-project-part2.html</a></p>
<p></p>
<p>There are two deliverables. åÊThe first one is due on Saturday before 11:59pm.</p>",Final Project Part 2 has been released,<p>This is the link:åÊhttps://github.com/benpleitner/NETS213-Final-Project</p>,Group Project,"<p>Here is some updated details about the $10,000 prize for the best final project:</p>
<p><a href=""http://crowdsourcing-class.org/project.html"">http://crowdsourcing-class.org/project.html</a></p>
<p></p>
<p>Let me know what you think!</p>","$10,000 prize for the final project","<p>For the NETS 213 final project, do we as students retain ownership of our projects? I know that some schools will hold the ownership rights, and juståÊwanted some clarification on this policy for the NETS 213 course.</p>",Student Ownership of Final Project?,2
940847300,4/26/2016 15:11:31,false,1969363827,,4/26/2016 15:10:32,false,tremorgames,1.0,32635967,LTU,60,Panevezys,78.63.38.165,0,,"<p>Hi, so I read though the other piazza post about how the statistical analysis takes a long time, but I am still having some trouble with my code.</p>
<p></p>
<p>I have already tried waiting 10-15 minutes and removing all stopwords, but for some reason the statistical analysis refuses to complete in a reasonable amount of time</p>
<p></p>
<p>Below is my code:</p>
<pre>def isStopWord(word):
	with open(&#39;stopwords.txt&#39;, &#39;r&#39;) as stopwords:
		for line in stopwords:
			if word in line:
				return True
		return False

def get_features(X) : 
	features = []
	for x in X : 
		f = {}
		#TODO replace this dummy feature function with a unigram model, like we did in class
		for word in x.split():
			if isStopWord(word):
				continue
			if word in f:
				f[word] = f[word] &#43; 1.0
			else:
				f[word] =  1.0
		features.append(f)
	return features</pre>
<p>Not really sure what to do at this point because of this. Also might this be related to using the VM?</p>",Statistical Analysis Taking A Very Long Time,It takes a really long time. Is that expected?,statistical classification runtime,"<p>My script runs forever on my virtual machine, so I&#39;m running it off of biglab. I got the statistical analysis to work once (took about 2 minutes), but after un-commenting get_top_features and get_misclassified, my script gets killed before finishing the initial statistical analysis. I&#39;ve tried several times and I&#39;m unsure what the issue is; I&#39;ve copied my code below.</p>
<p></p>
<pre>#!/bin/python<br /><br />import os<br />import sys<br />import string<br />import random<br />import operator<br />from sklearn.tree import export_graphviz<br />from sklearn.tree import DecisionTreeClassifier<br />from sklearn.naive_bayes import MultinomialNB<br />from sklearn.linear_model import LogisticRegression<br />from sklearn.preprocessing import LabelEncoder<br />from sklearn.feature_extraction import DictVectorizer<br />from sklearn.cross_validation import train_test_split<br />from sklearn.externals.six import StringIO  <br /><br />#read in raw data from file and return a list of (label, article) tuples<br />def get_data(filename): <br />	data = [line.strip().split(&#39;\t&#39;) for line in open(filename).readlines()]<br />	random.shuffle(data)<br />	return data<br /><br />#this function builds the feature matrix for the Decision Tree.<br />def get_dtree_features(X) :<br />	features = []<br />	#TODO : Add the features you would like to use to train the Decision Tree here.<br />	feature_list = [&#39;gun&#39;, &#39;shooter&#39;, &#39;shot&#39;, &#39;shooting&#39;, &#39;player&#39;, &#39;coach&#39;, &#39;game&#39;, &#39;movie&#39;, &#39;actor&#39;, &#39;film&#39;, &#39;bullet&#39;, &#39;photo&#39;, &#39;model&#39;, &#39;point&#39;, &#39;suspect&#39;, &#39;victim&#39;, &#39;police&#39;, &#39;fired&#39;]<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w in feature_list : <br />				f[w] = 1.0<br />		features.append(f)<br />	return features<br /><br /><br />#this is the main function you care about; pack all the cleverest features you can think of into here.<br />def get_features(X) :<br />	stopwords = open(&#39;stopwords.txt&#39;).read()<br />	features = []<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w not in stopwords:            <br />				if w not in f : <br />					f[w] = 0<br />				f[w] &#43;= 1                <br />		features.append(f)<br />	return features<br /><br />#vectorize feature dictionaries and return feature and label matricies<br />def get_matricies(data, typ=&#34;unigram&#34;) : <br />	dv = DictVectorizer(sparse=True) <br />	le = LabelEncoder()<br />	y = [d[0] for d in data]<br />	texts = [d[1] for d in data]<br />	if typ == &#34;tree&#34;:<br />		X = get_dtree_features(texts)<br />	else :<br />		X = get_features(texts)<br />	#Here we are returning 5 things, the label vector y and feature matrix X, but also the texts from which the features were extracted and the <br />	#objects that were used to encode them. These will come in handy for your analysis, but you can ignore them for the initial parts of the assignment<br />	return le.fit_transform(y), dv.fit_transform(X), texts, dv, le<br /><br />#train and multinomial naive bayes classifier<br />def train_classifier(X, y):<br />	clf = LogisticRegression()<br />	clf.fit(X,y)<br />	return clf <br /><br />#train a Decision Tree classifier<br />def train_dtree_classifier(X, y):<br />	clf = DecisionTreeClassifier(max_depth=10)<br />	clf.fit(X,y)<br />	return clf<br /><br />#test the classifier<br />def test_classifier(clf, X, y):<br />	return clf.score(X,y)<br /><br />#cross validation	<br />def cross_validate(X, y, dv=None, typ=&#34;unigram&#34;, numfolds=5,):<br />	test_accs = []<br />	split = 1.0 / numfolds<br />	for i in range(numfolds):<br />		x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=i)<br />		if typ == &#34;tree&#34; :<br />			clf = train_dtree_classifier(x_train, y_train)<br />		else :<br />			clf = train_classifier(x_train, y_train)<br />		test_acc = test_classifier(clf, x_test, y_test)<br />		test_accs.append(test_acc)<br />		print &#39;Fold %d : %.05f&#39;%(i,test_acc)<br />	test_average = float(sum(test_accs))/ numfolds<br />	if typ == &#34;tree&#34; :<br />		with open(&#34;output.dot&#34;, &#39;w&#39;) as f:<br />			f = export_graphviz(clf, out_file=f, feature_names=dv.get_feature_names(), class_names=[&#39;Non Gun Related&#39;,&#39;Gun Related&#39;])<br />		create_graph(&#34;decision-tree.png&#34;)<br />	print &#39;Test Average : %.05f&#39;%(test_average)<br />	print<br />	return test_average<br /><br />#run a rule based classifier and calculate the accuracy<br />def rule_based_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO add more keywords, see how well they do alone and in combination<br />		if &#34;shooter&#34; in text or &#34;shot&#34; in text or &#34;shooting&#34; in text :<br />			if &#34;player&#34; in text or &#34;coach&#34; in text or &#34;game&#34; in text: <br />				prediction = &#39;0&#39;<br />			elif &#34;movie&#34; in text or &#34;film&#34; in text or &#34;actor&#34; in text:<br />				prediction = &#39;0&#39;<br />			else : prediction = &#39;1&#39;<br />		if &#34;gun&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Rule-based classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#Extra Credit Rule Based Classifier<br />def extra_credit_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO develop your conditional statements here<br />		if &#34;shooting&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Reverse Engineered classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#train and multinomial naive bayes classifier<br />def get_top_features(X, y, dv):<br />	clf = train_classifier(X, y)<br />	#the DictVectorizer object remembers which column number corresponds to which feature, and return the feature names in the correct order<br />	feature_names = dv.get_feature_names() <br /><br />	#The below code will get the weights from the classifier, and print out the weights of the features you are interested in<br />	features = [] #this will be a list of (feature_idx, weight) tuples<br />	for i,w in enumerate(clf.coef_[0]): <br />		features.append((i,w))<br />	#Sort the list by values, with the largest ones first<br />	features = sorted(features, key=lambda e: e[1], reverse=True)<br /><br />        #Print out the feature names and thier weights<br />	for i,w in features:<br />	  print &#39;%s\t%s&#39;%(feature_names[i], w)<br /><br />def get_misclassified_examples(y, X, texts) :<br />	x_train, x_test, y_train, y_test, train_texts, test_texts = train_test_split(X, y, texts)<br />	clf = train_classifier(x_train, y_train)<br /><br />	#TODO: You will have to write some code to call your classifier on each of the test examples, and check whether its prediction was right or wrong<br /><br />def create_graph(file_name) :<br />	os.system(&#34;dot -Tpng output.dot -o &#34; &#43; file_name)<br />	os.unlink(&#34;output.dot&#34;)<br /><br />if __name__ == &#39;__main__&#39; : <br /><br />	raw_data = get_data(sys.argv[1])<br />	<br />	print &#39;\nRule-based classification&#39;<br />	rule_based_classifier(raw_data)<br /><br />################ Decision Tree ################<br /><br /># print &#39;\nDecision Tree classification&#39;<br /># y, X, texts, dv, le = get_matricies(raw_data, &#34;tree&#34;)<br /># cross_validate(X,y,dv,&#34;tree&#34;)<br /><br />################ Statistical Classification ################<br />print &#39;\nStatistical classification&#39;<br />y, X, texts, dv, le = get_matricies(raw_data)<br />cross_validate(X,y)<br /><br />get_top_features(X, y, dv)<br />get_misclassified_examples(y, X, texts)<br />
</pre>
<p>Thanks in advance for any help you can give.</p>",Statistical Analysis &#34;Killed&#34; - even on biglab,"<p>When my program gets to the Statistical Analysis bit, it makes it through all the words and creates wordcount dictionaries that are appended to the list of features. Then it hangs for a bit until the terminal just prints &#34;Killed&#34; and program ends. Any idea what might be causing this? Not enough memory?</p>",Statistical Analysis &#34;Killed&#34;,,,"<p>I&#39;m confused on what exactly we&#39;re supposed to do for part 6. Are the y dimensions the same as the number of articles? Also, how is logistic regression supposed to be used here?</p>",Statistical Analysis Part 6,0
940847300,4/26/2016 15:15:42,false,1969369974,,4/26/2016 15:14:06,false,clixsense,1.0,24287706,TWN,04,Keelung,61.231.195.173,0,,"<p>Hi, so I read though the other piazza post about how the statistical analysis takes a long time, but I am still having some trouble with my code.</p>
<p></p>
<p>I have already tried waiting 10-15 minutes and removing all stopwords, but for some reason the statistical analysis refuses to complete in a reasonable amount of time</p>
<p></p>
<p>Below is my code:</p>
<pre>def isStopWord(word):
	with open(&#39;stopwords.txt&#39;, &#39;r&#39;) as stopwords:
		for line in stopwords:
			if word in line:
				return True
		return False

def get_features(X) : 
	features = []
	for x in X : 
		f = {}
		#TODO replace this dummy feature function with a unigram model, like we did in class
		for word in x.split():
			if isStopWord(word):
				continue
			if word in f:
				f[word] = f[word] &#43; 1.0
			else:
				f[word] =  1.0
		features.append(f)
	return features</pre>
<p>Not really sure what to do at this point because of this. Also might this be related to using the VM?</p>",Statistical Analysis Taking A Very Long Time,It takes a really long time. Is that expected?,statistical classification runtime,"<p>My script runs forever on my virtual machine, so I&#39;m running it off of biglab. I got the statistical analysis to work once (took about 2 minutes), but after un-commenting get_top_features and get_misclassified, my script gets killed before finishing the initial statistical analysis. I&#39;ve tried several times and I&#39;m unsure what the issue is; I&#39;ve copied my code below.</p>
<p></p>
<pre>#!/bin/python<br /><br />import os<br />import sys<br />import string<br />import random<br />import operator<br />from sklearn.tree import export_graphviz<br />from sklearn.tree import DecisionTreeClassifier<br />from sklearn.naive_bayes import MultinomialNB<br />from sklearn.linear_model import LogisticRegression<br />from sklearn.preprocessing import LabelEncoder<br />from sklearn.feature_extraction import DictVectorizer<br />from sklearn.cross_validation import train_test_split<br />from sklearn.externals.six import StringIO  <br /><br />#read in raw data from file and return a list of (label, article) tuples<br />def get_data(filename): <br />	data = [line.strip().split(&#39;\t&#39;) for line in open(filename).readlines()]<br />	random.shuffle(data)<br />	return data<br /><br />#this function builds the feature matrix for the Decision Tree.<br />def get_dtree_features(X) :<br />	features = []<br />	#TODO : Add the features you would like to use to train the Decision Tree here.<br />	feature_list = [&#39;gun&#39;, &#39;shooter&#39;, &#39;shot&#39;, &#39;shooting&#39;, &#39;player&#39;, &#39;coach&#39;, &#39;game&#39;, &#39;movie&#39;, &#39;actor&#39;, &#39;film&#39;, &#39;bullet&#39;, &#39;photo&#39;, &#39;model&#39;, &#39;point&#39;, &#39;suspect&#39;, &#39;victim&#39;, &#39;police&#39;, &#39;fired&#39;]<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w in feature_list : <br />				f[w] = 1.0<br />		features.append(f)<br />	return features<br /><br /><br />#this is the main function you care about; pack all the cleverest features you can think of into here.<br />def get_features(X) :<br />	stopwords = open(&#39;stopwords.txt&#39;).read()<br />	features = []<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w not in stopwords:            <br />				if w not in f : <br />					f[w] = 0<br />				f[w] &#43;= 1                <br />		features.append(f)<br />	return features<br /><br />#vectorize feature dictionaries and return feature and label matricies<br />def get_matricies(data, typ=&#34;unigram&#34;) : <br />	dv = DictVectorizer(sparse=True) <br />	le = LabelEncoder()<br />	y = [d[0] for d in data]<br />	texts = [d[1] for d in data]<br />	if typ == &#34;tree&#34;:<br />		X = get_dtree_features(texts)<br />	else :<br />		X = get_features(texts)<br />	#Here we are returning 5 things, the label vector y and feature matrix X, but also the texts from which the features were extracted and the <br />	#objects that were used to encode them. These will come in handy for your analysis, but you can ignore them for the initial parts of the assignment<br />	return le.fit_transform(y), dv.fit_transform(X), texts, dv, le<br /><br />#train and multinomial naive bayes classifier<br />def train_classifier(X, y):<br />	clf = LogisticRegression()<br />	clf.fit(X,y)<br />	return clf <br /><br />#train a Decision Tree classifier<br />def train_dtree_classifier(X, y):<br />	clf = DecisionTreeClassifier(max_depth=10)<br />	clf.fit(X,y)<br />	return clf<br /><br />#test the classifier<br />def test_classifier(clf, X, y):<br />	return clf.score(X,y)<br /><br />#cross validation	<br />def cross_validate(X, y, dv=None, typ=&#34;unigram&#34;, numfolds=5,):<br />	test_accs = []<br />	split = 1.0 / numfolds<br />	for i in range(numfolds):<br />		x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=i)<br />		if typ == &#34;tree&#34; :<br />			clf = train_dtree_classifier(x_train, y_train)<br />		else :<br />			clf = train_classifier(x_train, y_train)<br />		test_acc = test_classifier(clf, x_test, y_test)<br />		test_accs.append(test_acc)<br />		print &#39;Fold %d : %.05f&#39;%(i,test_acc)<br />	test_average = float(sum(test_accs))/ numfolds<br />	if typ == &#34;tree&#34; :<br />		with open(&#34;output.dot&#34;, &#39;w&#39;) as f:<br />			f = export_graphviz(clf, out_file=f, feature_names=dv.get_feature_names(), class_names=[&#39;Non Gun Related&#39;,&#39;Gun Related&#39;])<br />		create_graph(&#34;decision-tree.png&#34;)<br />	print &#39;Test Average : %.05f&#39;%(test_average)<br />	print<br />	return test_average<br /><br />#run a rule based classifier and calculate the accuracy<br />def rule_based_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO add more keywords, see how well they do alone and in combination<br />		if &#34;shooter&#34; in text or &#34;shot&#34; in text or &#34;shooting&#34; in text :<br />			if &#34;player&#34; in text or &#34;coach&#34; in text or &#34;game&#34; in text: <br />				prediction = &#39;0&#39;<br />			elif &#34;movie&#34; in text or &#34;film&#34; in text or &#34;actor&#34; in text:<br />				prediction = &#39;0&#39;<br />			else : prediction = &#39;1&#39;<br />		if &#34;gun&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Rule-based classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#Extra Credit Rule Based Classifier<br />def extra_credit_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO develop your conditional statements here<br />		if &#34;shooting&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Reverse Engineered classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#train and multinomial naive bayes classifier<br />def get_top_features(X, y, dv):<br />	clf = train_classifier(X, y)<br />	#the DictVectorizer object remembers which column number corresponds to which feature, and return the feature names in the correct order<br />	feature_names = dv.get_feature_names() <br /><br />	#The below code will get the weights from the classifier, and print out the weights of the features you are interested in<br />	features = [] #this will be a list of (feature_idx, weight) tuples<br />	for i,w in enumerate(clf.coef_[0]): <br />		features.append((i,w))<br />	#Sort the list by values, with the largest ones first<br />	features = sorted(features, key=lambda e: e[1], reverse=True)<br /><br />        #Print out the feature names and thier weights<br />	for i,w in features:<br />	  print &#39;%s\t%s&#39;%(feature_names[i], w)<br /><br />def get_misclassified_examples(y, X, texts) :<br />	x_train, x_test, y_train, y_test, train_texts, test_texts = train_test_split(X, y, texts)<br />	clf = train_classifier(x_train, y_train)<br /><br />	#TODO: You will have to write some code to call your classifier on each of the test examples, and check whether its prediction was right or wrong<br /><br />def create_graph(file_name) :<br />	os.system(&#34;dot -Tpng output.dot -o &#34; &#43; file_name)<br />	os.unlink(&#34;output.dot&#34;)<br /><br />if __name__ == &#39;__main__&#39; : <br /><br />	raw_data = get_data(sys.argv[1])<br />	<br />	print &#39;\nRule-based classification&#39;<br />	rule_based_classifier(raw_data)<br /><br />################ Decision Tree ################<br /><br /># print &#39;\nDecision Tree classification&#39;<br /># y, X, texts, dv, le = get_matricies(raw_data, &#34;tree&#34;)<br /># cross_validate(X,y,dv,&#34;tree&#34;)<br /><br />################ Statistical Classification ################<br />print &#39;\nStatistical classification&#39;<br />y, X, texts, dv, le = get_matricies(raw_data)<br />cross_validate(X,y)<br /><br />get_top_features(X, y, dv)<br />get_misclassified_examples(y, X, texts)<br />
</pre>
<p>Thanks in advance for any help you can give.</p>",Statistical Analysis &#34;Killed&#34; - even on biglab,"<p>When my program gets to the Statistical Analysis bit, it makes it through all the words and creates wordcount dictionaries that are appended to the list of features. Then it hangs for a bit until the terminal just prints &#34;Killed&#34; and program ends. Any idea what might be causing this? Not enough memory?</p>",Statistical Analysis &#34;Killed&#34;,,,"<p>I&#39;m confused on what exactly we&#39;re supposed to do for part 6. Are the y dimensions the same as the number of articles? Also, how is logistic regression supposed to be used here?</p>",Statistical Analysis Part 6,0
940847300,4/26/2016 15:19:31,false,1969376281,,4/26/2016 15:17:21,false,clixsense,1.0,7837812,SRB,00,Belgrade,79.101.254.233,0,,"<p>Hi, so I read though the other piazza post about how the statistical analysis takes a long time, but I am still having some trouble with my code.</p>
<p></p>
<p>I have already tried waiting 10-15 minutes and removing all stopwords, but for some reason the statistical analysis refuses to complete in a reasonable amount of time</p>
<p></p>
<p>Below is my code:</p>
<pre>def isStopWord(word):
	with open(&#39;stopwords.txt&#39;, &#39;r&#39;) as stopwords:
		for line in stopwords:
			if word in line:
				return True
		return False

def get_features(X) : 
	features = []
	for x in X : 
		f = {}
		#TODO replace this dummy feature function with a unigram model, like we did in class
		for word in x.split():
			if isStopWord(word):
				continue
			if word in f:
				f[word] = f[word] &#43; 1.0
			else:
				f[word] =  1.0
		features.append(f)
	return features</pre>
<p>Not really sure what to do at this point because of this. Also might this be related to using the VM?</p>",Statistical Analysis Taking A Very Long Time,It takes a really long time. Is that expected?,statistical classification runtime,"<p>My script runs forever on my virtual machine, so I&#39;m running it off of biglab. I got the statistical analysis to work once (took about 2 minutes), but after un-commenting get_top_features and get_misclassified, my script gets killed before finishing the initial statistical analysis. I&#39;ve tried several times and I&#39;m unsure what the issue is; I&#39;ve copied my code below.</p>
<p></p>
<pre>#!/bin/python<br /><br />import os<br />import sys<br />import string<br />import random<br />import operator<br />from sklearn.tree import export_graphviz<br />from sklearn.tree import DecisionTreeClassifier<br />from sklearn.naive_bayes import MultinomialNB<br />from sklearn.linear_model import LogisticRegression<br />from sklearn.preprocessing import LabelEncoder<br />from sklearn.feature_extraction import DictVectorizer<br />from sklearn.cross_validation import train_test_split<br />from sklearn.externals.six import StringIO  <br /><br />#read in raw data from file and return a list of (label, article) tuples<br />def get_data(filename): <br />	data = [line.strip().split(&#39;\t&#39;) for line in open(filename).readlines()]<br />	random.shuffle(data)<br />	return data<br /><br />#this function builds the feature matrix for the Decision Tree.<br />def get_dtree_features(X) :<br />	features = []<br />	#TODO : Add the features you would like to use to train the Decision Tree here.<br />	feature_list = [&#39;gun&#39;, &#39;shooter&#39;, &#39;shot&#39;, &#39;shooting&#39;, &#39;player&#39;, &#39;coach&#39;, &#39;game&#39;, &#39;movie&#39;, &#39;actor&#39;, &#39;film&#39;, &#39;bullet&#39;, &#39;photo&#39;, &#39;model&#39;, &#39;point&#39;, &#39;suspect&#39;, &#39;victim&#39;, &#39;police&#39;, &#39;fired&#39;]<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w in feature_list : <br />				f[w] = 1.0<br />		features.append(f)<br />	return features<br /><br /><br />#this is the main function you care about; pack all the cleverest features you can think of into here.<br />def get_features(X) :<br />	stopwords = open(&#39;stopwords.txt&#39;).read()<br />	features = []<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w not in stopwords:            <br />				if w not in f : <br />					f[w] = 0<br />				f[w] &#43;= 1                <br />		features.append(f)<br />	return features<br /><br />#vectorize feature dictionaries and return feature and label matricies<br />def get_matricies(data, typ=&#34;unigram&#34;) : <br />	dv = DictVectorizer(sparse=True) <br />	le = LabelEncoder()<br />	y = [d[0] for d in data]<br />	texts = [d[1] for d in data]<br />	if typ == &#34;tree&#34;:<br />		X = get_dtree_features(texts)<br />	else :<br />		X = get_features(texts)<br />	#Here we are returning 5 things, the label vector y and feature matrix X, but also the texts from which the features were extracted and the <br />	#objects that were used to encode them. These will come in handy for your analysis, but you can ignore them for the initial parts of the assignment<br />	return le.fit_transform(y), dv.fit_transform(X), texts, dv, le<br /><br />#train and multinomial naive bayes classifier<br />def train_classifier(X, y):<br />	clf = LogisticRegression()<br />	clf.fit(X,y)<br />	return clf <br /><br />#train a Decision Tree classifier<br />def train_dtree_classifier(X, y):<br />	clf = DecisionTreeClassifier(max_depth=10)<br />	clf.fit(X,y)<br />	return clf<br /><br />#test the classifier<br />def test_classifier(clf, X, y):<br />	return clf.score(X,y)<br /><br />#cross validation	<br />def cross_validate(X, y, dv=None, typ=&#34;unigram&#34;, numfolds=5,):<br />	test_accs = []<br />	split = 1.0 / numfolds<br />	for i in range(numfolds):<br />		x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=i)<br />		if typ == &#34;tree&#34; :<br />			clf = train_dtree_classifier(x_train, y_train)<br />		else :<br />			clf = train_classifier(x_train, y_train)<br />		test_acc = test_classifier(clf, x_test, y_test)<br />		test_accs.append(test_acc)<br />		print &#39;Fold %d : %.05f&#39;%(i,test_acc)<br />	test_average = float(sum(test_accs))/ numfolds<br />	if typ == &#34;tree&#34; :<br />		with open(&#34;output.dot&#34;, &#39;w&#39;) as f:<br />			f = export_graphviz(clf, out_file=f, feature_names=dv.get_feature_names(), class_names=[&#39;Non Gun Related&#39;,&#39;Gun Related&#39;])<br />		create_graph(&#34;decision-tree.png&#34;)<br />	print &#39;Test Average : %.05f&#39;%(test_average)<br />	print<br />	return test_average<br /><br />#run a rule based classifier and calculate the accuracy<br />def rule_based_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO add more keywords, see how well they do alone and in combination<br />		if &#34;shooter&#34; in text or &#34;shot&#34; in text or &#34;shooting&#34; in text :<br />			if &#34;player&#34; in text or &#34;coach&#34; in text or &#34;game&#34; in text: <br />				prediction = &#39;0&#39;<br />			elif &#34;movie&#34; in text or &#34;film&#34; in text or &#34;actor&#34; in text:<br />				prediction = &#39;0&#39;<br />			else : prediction = &#39;1&#39;<br />		if &#34;gun&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Rule-based classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#Extra Credit Rule Based Classifier<br />def extra_credit_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO develop your conditional statements here<br />		if &#34;shooting&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Reverse Engineered classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#train and multinomial naive bayes classifier<br />def get_top_features(X, y, dv):<br />	clf = train_classifier(X, y)<br />	#the DictVectorizer object remembers which column number corresponds to which feature, and return the feature names in the correct order<br />	feature_names = dv.get_feature_names() <br /><br />	#The below code will get the weights from the classifier, and print out the weights of the features you are interested in<br />	features = [] #this will be a list of (feature_idx, weight) tuples<br />	for i,w in enumerate(clf.coef_[0]): <br />		features.append((i,w))<br />	#Sort the list by values, with the largest ones first<br />	features = sorted(features, key=lambda e: e[1], reverse=True)<br /><br />        #Print out the feature names and thier weights<br />	for i,w in features:<br />	  print &#39;%s\t%s&#39;%(feature_names[i], w)<br /><br />def get_misclassified_examples(y, X, texts) :<br />	x_train, x_test, y_train, y_test, train_texts, test_texts = train_test_split(X, y, texts)<br />	clf = train_classifier(x_train, y_train)<br /><br />	#TODO: You will have to write some code to call your classifier on each of the test examples, and check whether its prediction was right or wrong<br /><br />def create_graph(file_name) :<br />	os.system(&#34;dot -Tpng output.dot -o &#34; &#43; file_name)<br />	os.unlink(&#34;output.dot&#34;)<br /><br />if __name__ == &#39;__main__&#39; : <br /><br />	raw_data = get_data(sys.argv[1])<br />	<br />	print &#39;\nRule-based classification&#39;<br />	rule_based_classifier(raw_data)<br /><br />################ Decision Tree ################<br /><br /># print &#39;\nDecision Tree classification&#39;<br /># y, X, texts, dv, le = get_matricies(raw_data, &#34;tree&#34;)<br /># cross_validate(X,y,dv,&#34;tree&#34;)<br /><br />################ Statistical Classification ################<br />print &#39;\nStatistical classification&#39;<br />y, X, texts, dv, le = get_matricies(raw_data)<br />cross_validate(X,y)<br /><br />get_top_features(X, y, dv)<br />get_misclassified_examples(y, X, texts)<br />
</pre>
<p>Thanks in advance for any help you can give.</p>",Statistical Analysis &#34;Killed&#34; - even on biglab,"<p>When my program gets to the Statistical Analysis bit, it makes it through all the words and creates wordcount dictionaries that are appended to the list of features. Then it hangs for a bit until the terminal just prints &#34;Killed&#34; and program ends. Any idea what might be causing this? Not enough memory?</p>",Statistical Analysis &#34;Killed&#34;,,,"<p>I&#39;m confused on what exactly we&#39;re supposed to do for part 6. Are the y dimensions the same as the number of articles? Also, how is logistic regression supposed to be used here?</p>",Statistical Analysis Part 6,0
940847300,4/26/2016 15:19:53,false,1969376816,,4/26/2016 15:19:30,false,neodev,1.0,19132694,LKA,36,Colombo,123.231.124.170,0,,"<p>Hi, so I read though the other piazza post about how the statistical analysis takes a long time, but I am still having some trouble with my code.</p>
<p></p>
<p>I have already tried waiting 10-15 minutes and removing all stopwords, but for some reason the statistical analysis refuses to complete in a reasonable amount of time</p>
<p></p>
<p>Below is my code:</p>
<pre>def isStopWord(word):
	with open(&#39;stopwords.txt&#39;, &#39;r&#39;) as stopwords:
		for line in stopwords:
			if word in line:
				return True
		return False

def get_features(X) : 
	features = []
	for x in X : 
		f = {}
		#TODO replace this dummy feature function with a unigram model, like we did in class
		for word in x.split():
			if isStopWord(word):
				continue
			if word in f:
				f[word] = f[word] &#43; 1.0
			else:
				f[word] =  1.0
		features.append(f)
	return features</pre>
<p>Not really sure what to do at this point because of this. Also might this be related to using the VM?</p>",Statistical Analysis Taking A Very Long Time,It takes a really long time. Is that expected?,statistical classification runtime,"<p>My script runs forever on my virtual machine, so I&#39;m running it off of biglab. I got the statistical analysis to work once (took about 2 minutes), but after un-commenting get_top_features and get_misclassified, my script gets killed before finishing the initial statistical analysis. I&#39;ve tried several times and I&#39;m unsure what the issue is; I&#39;ve copied my code below.</p>
<p></p>
<pre>#!/bin/python<br /><br />import os<br />import sys<br />import string<br />import random<br />import operator<br />from sklearn.tree import export_graphviz<br />from sklearn.tree import DecisionTreeClassifier<br />from sklearn.naive_bayes import MultinomialNB<br />from sklearn.linear_model import LogisticRegression<br />from sklearn.preprocessing import LabelEncoder<br />from sklearn.feature_extraction import DictVectorizer<br />from sklearn.cross_validation import train_test_split<br />from sklearn.externals.six import StringIO  <br /><br />#read in raw data from file and return a list of (label, article) tuples<br />def get_data(filename): <br />	data = [line.strip().split(&#39;\t&#39;) for line in open(filename).readlines()]<br />	random.shuffle(data)<br />	return data<br /><br />#this function builds the feature matrix for the Decision Tree.<br />def get_dtree_features(X) :<br />	features = []<br />	#TODO : Add the features you would like to use to train the Decision Tree here.<br />	feature_list = [&#39;gun&#39;, &#39;shooter&#39;, &#39;shot&#39;, &#39;shooting&#39;, &#39;player&#39;, &#39;coach&#39;, &#39;game&#39;, &#39;movie&#39;, &#39;actor&#39;, &#39;film&#39;, &#39;bullet&#39;, &#39;photo&#39;, &#39;model&#39;, &#39;point&#39;, &#39;suspect&#39;, &#39;victim&#39;, &#39;police&#39;, &#39;fired&#39;]<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w in feature_list : <br />				f[w] = 1.0<br />		features.append(f)<br />	return features<br /><br /><br />#this is the main function you care about; pack all the cleverest features you can think of into here.<br />def get_features(X) :<br />	stopwords = open(&#39;stopwords.txt&#39;).read()<br />	features = []<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w not in stopwords:            <br />				if w not in f : <br />					f[w] = 0<br />				f[w] &#43;= 1                <br />		features.append(f)<br />	return features<br /><br />#vectorize feature dictionaries and return feature and label matricies<br />def get_matricies(data, typ=&#34;unigram&#34;) : <br />	dv = DictVectorizer(sparse=True) <br />	le = LabelEncoder()<br />	y = [d[0] for d in data]<br />	texts = [d[1] for d in data]<br />	if typ == &#34;tree&#34;:<br />		X = get_dtree_features(texts)<br />	else :<br />		X = get_features(texts)<br />	#Here we are returning 5 things, the label vector y and feature matrix X, but also the texts from which the features were extracted and the <br />	#objects that were used to encode them. These will come in handy for your analysis, but you can ignore them for the initial parts of the assignment<br />	return le.fit_transform(y), dv.fit_transform(X), texts, dv, le<br /><br />#train and multinomial naive bayes classifier<br />def train_classifier(X, y):<br />	clf = LogisticRegression()<br />	clf.fit(X,y)<br />	return clf <br /><br />#train a Decision Tree classifier<br />def train_dtree_classifier(X, y):<br />	clf = DecisionTreeClassifier(max_depth=10)<br />	clf.fit(X,y)<br />	return clf<br /><br />#test the classifier<br />def test_classifier(clf, X, y):<br />	return clf.score(X,y)<br /><br />#cross validation	<br />def cross_validate(X, y, dv=None, typ=&#34;unigram&#34;, numfolds=5,):<br />	test_accs = []<br />	split = 1.0 / numfolds<br />	for i in range(numfolds):<br />		x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=i)<br />		if typ == &#34;tree&#34; :<br />			clf = train_dtree_classifier(x_train, y_train)<br />		else :<br />			clf = train_classifier(x_train, y_train)<br />		test_acc = test_classifier(clf, x_test, y_test)<br />		test_accs.append(test_acc)<br />		print &#39;Fold %d : %.05f&#39;%(i,test_acc)<br />	test_average = float(sum(test_accs))/ numfolds<br />	if typ == &#34;tree&#34; :<br />		with open(&#34;output.dot&#34;, &#39;w&#39;) as f:<br />			f = export_graphviz(clf, out_file=f, feature_names=dv.get_feature_names(), class_names=[&#39;Non Gun Related&#39;,&#39;Gun Related&#39;])<br />		create_graph(&#34;decision-tree.png&#34;)<br />	print &#39;Test Average : %.05f&#39;%(test_average)<br />	print<br />	return test_average<br /><br />#run a rule based classifier and calculate the accuracy<br />def rule_based_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO add more keywords, see how well they do alone and in combination<br />		if &#34;shooter&#34; in text or &#34;shot&#34; in text or &#34;shooting&#34; in text :<br />			if &#34;player&#34; in text or &#34;coach&#34; in text or &#34;game&#34; in text: <br />				prediction = &#39;0&#39;<br />			elif &#34;movie&#34; in text or &#34;film&#34; in text or &#34;actor&#34; in text:<br />				prediction = &#39;0&#39;<br />			else : prediction = &#39;1&#39;<br />		if &#34;gun&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Rule-based classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#Extra Credit Rule Based Classifier<br />def extra_credit_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO develop your conditional statements here<br />		if &#34;shooting&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Reverse Engineered classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#train and multinomial naive bayes classifier<br />def get_top_features(X, y, dv):<br />	clf = train_classifier(X, y)<br />	#the DictVectorizer object remembers which column number corresponds to which feature, and return the feature names in the correct order<br />	feature_names = dv.get_feature_names() <br /><br />	#The below code will get the weights from the classifier, and print out the weights of the features you are interested in<br />	features = [] #this will be a list of (feature_idx, weight) tuples<br />	for i,w in enumerate(clf.coef_[0]): <br />		features.append((i,w))<br />	#Sort the list by values, with the largest ones first<br />	features = sorted(features, key=lambda e: e[1], reverse=True)<br /><br />        #Print out the feature names and thier weights<br />	for i,w in features:<br />	  print &#39;%s\t%s&#39;%(feature_names[i], w)<br /><br />def get_misclassified_examples(y, X, texts) :<br />	x_train, x_test, y_train, y_test, train_texts, test_texts = train_test_split(X, y, texts)<br />	clf = train_classifier(x_train, y_train)<br /><br />	#TODO: You will have to write some code to call your classifier on each of the test examples, and check whether its prediction was right or wrong<br /><br />def create_graph(file_name) :<br />	os.system(&#34;dot -Tpng output.dot -o &#34; &#43; file_name)<br />	os.unlink(&#34;output.dot&#34;)<br /><br />if __name__ == &#39;__main__&#39; : <br /><br />	raw_data = get_data(sys.argv[1])<br />	<br />	print &#39;\nRule-based classification&#39;<br />	rule_based_classifier(raw_data)<br /><br />################ Decision Tree ################<br /><br /># print &#39;\nDecision Tree classification&#39;<br /># y, X, texts, dv, le = get_matricies(raw_data, &#34;tree&#34;)<br /># cross_validate(X,y,dv,&#34;tree&#34;)<br /><br />################ Statistical Classification ################<br />print &#39;\nStatistical classification&#39;<br />y, X, texts, dv, le = get_matricies(raw_data)<br />cross_validate(X,y)<br /><br />get_top_features(X, y, dv)<br />get_misclassified_examples(y, X, texts)<br />
</pre>
<p>Thanks in advance for any help you can give.</p>",Statistical Analysis &#34;Killed&#34; - even on biglab,"<p>When my program gets to the Statistical Analysis bit, it makes it through all the words and creates wordcount dictionaries that are appended to the list of features. Then it hangs for a bit until the terminal just prints &#34;Killed&#34; and program ends. Any idea what might be causing this? Not enough memory?</p>",Statistical Analysis &#34;Killed&#34;,,,"<p>I&#39;m confused on what exactly we&#39;re supposed to do for part 6. Are the y dimensions the same as the number of articles? Also, how is logistic regression supposed to be used here?</p>",Statistical Analysis Part 6,0
940847300,4/26/2016 15:24:26,false,1969384553,,4/26/2016 15:21:07,false,elite,1.0,30280423,ITA,15,Siracusa,151.54.84.121,0,,"<p>Hi, so I read though the other piazza post about how the statistical analysis takes a long time, but I am still having some trouble with my code.</p>
<p></p>
<p>I have already tried waiting 10-15 minutes and removing all stopwords, but for some reason the statistical analysis refuses to complete in a reasonable amount of time</p>
<p></p>
<p>Below is my code:</p>
<pre>def isStopWord(word):
	with open(&#39;stopwords.txt&#39;, &#39;r&#39;) as stopwords:
		for line in stopwords:
			if word in line:
				return True
		return False

def get_features(X) : 
	features = []
	for x in X : 
		f = {}
		#TODO replace this dummy feature function with a unigram model, like we did in class
		for word in x.split():
			if isStopWord(word):
				continue
			if word in f:
				f[word] = f[word] &#43; 1.0
			else:
				f[word] =  1.0
		features.append(f)
	return features</pre>
<p>Not really sure what to do at this point because of this. Also might this be related to using the VM?</p>",Statistical Analysis Taking A Very Long Time,It takes a really long time. Is that expected?,statistical classification runtime,"<p>My script runs forever on my virtual machine, so I&#39;m running it off of biglab. I got the statistical analysis to work once (took about 2 minutes), but after un-commenting get_top_features and get_misclassified, my script gets killed before finishing the initial statistical analysis. I&#39;ve tried several times and I&#39;m unsure what the issue is; I&#39;ve copied my code below.</p>
<p></p>
<pre>#!/bin/python<br /><br />import os<br />import sys<br />import string<br />import random<br />import operator<br />from sklearn.tree import export_graphviz<br />from sklearn.tree import DecisionTreeClassifier<br />from sklearn.naive_bayes import MultinomialNB<br />from sklearn.linear_model import LogisticRegression<br />from sklearn.preprocessing import LabelEncoder<br />from sklearn.feature_extraction import DictVectorizer<br />from sklearn.cross_validation import train_test_split<br />from sklearn.externals.six import StringIO  <br /><br />#read in raw data from file and return a list of (label, article) tuples<br />def get_data(filename): <br />	data = [line.strip().split(&#39;\t&#39;) for line in open(filename).readlines()]<br />	random.shuffle(data)<br />	return data<br /><br />#this function builds the feature matrix for the Decision Tree.<br />def get_dtree_features(X) :<br />	features = []<br />	#TODO : Add the features you would like to use to train the Decision Tree here.<br />	feature_list = [&#39;gun&#39;, &#39;shooter&#39;, &#39;shot&#39;, &#39;shooting&#39;, &#39;player&#39;, &#39;coach&#39;, &#39;game&#39;, &#39;movie&#39;, &#39;actor&#39;, &#39;film&#39;, &#39;bullet&#39;, &#39;photo&#39;, &#39;model&#39;, &#39;point&#39;, &#39;suspect&#39;, &#39;victim&#39;, &#39;police&#39;, &#39;fired&#39;]<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w in feature_list : <br />				f[w] = 1.0<br />		features.append(f)<br />	return features<br /><br /><br />#this is the main function you care about; pack all the cleverest features you can think of into here.<br />def get_features(X) :<br />	stopwords = open(&#39;stopwords.txt&#39;).read()<br />	features = []<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w not in stopwords:            <br />				if w not in f : <br />					f[w] = 0<br />				f[w] &#43;= 1                <br />		features.append(f)<br />	return features<br /><br />#vectorize feature dictionaries and return feature and label matricies<br />def get_matricies(data, typ=&#34;unigram&#34;) : <br />	dv = DictVectorizer(sparse=True) <br />	le = LabelEncoder()<br />	y = [d[0] for d in data]<br />	texts = [d[1] for d in data]<br />	if typ == &#34;tree&#34;:<br />		X = get_dtree_features(texts)<br />	else :<br />		X = get_features(texts)<br />	#Here we are returning 5 things, the label vector y and feature matrix X, but also the texts from which the features were extracted and the <br />	#objects that were used to encode them. These will come in handy for your analysis, but you can ignore them for the initial parts of the assignment<br />	return le.fit_transform(y), dv.fit_transform(X), texts, dv, le<br /><br />#train and multinomial naive bayes classifier<br />def train_classifier(X, y):<br />	clf = LogisticRegression()<br />	clf.fit(X,y)<br />	return clf <br /><br />#train a Decision Tree classifier<br />def train_dtree_classifier(X, y):<br />	clf = DecisionTreeClassifier(max_depth=10)<br />	clf.fit(X,y)<br />	return clf<br /><br />#test the classifier<br />def test_classifier(clf, X, y):<br />	return clf.score(X,y)<br /><br />#cross validation	<br />def cross_validate(X, y, dv=None, typ=&#34;unigram&#34;, numfolds=5,):<br />	test_accs = []<br />	split = 1.0 / numfolds<br />	for i in range(numfolds):<br />		x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=i)<br />		if typ == &#34;tree&#34; :<br />			clf = train_dtree_classifier(x_train, y_train)<br />		else :<br />			clf = train_classifier(x_train, y_train)<br />		test_acc = test_classifier(clf, x_test, y_test)<br />		test_accs.append(test_acc)<br />		print &#39;Fold %d : %.05f&#39;%(i,test_acc)<br />	test_average = float(sum(test_accs))/ numfolds<br />	if typ == &#34;tree&#34; :<br />		with open(&#34;output.dot&#34;, &#39;w&#39;) as f:<br />			f = export_graphviz(clf, out_file=f, feature_names=dv.get_feature_names(), class_names=[&#39;Non Gun Related&#39;,&#39;Gun Related&#39;])<br />		create_graph(&#34;decision-tree.png&#34;)<br />	print &#39;Test Average : %.05f&#39;%(test_average)<br />	print<br />	return test_average<br /><br />#run a rule based classifier and calculate the accuracy<br />def rule_based_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO add more keywords, see how well they do alone and in combination<br />		if &#34;shooter&#34; in text or &#34;shot&#34; in text or &#34;shooting&#34; in text :<br />			if &#34;player&#34; in text or &#34;coach&#34; in text or &#34;game&#34; in text: <br />				prediction = &#39;0&#39;<br />			elif &#34;movie&#34; in text or &#34;film&#34; in text or &#34;actor&#34; in text:<br />				prediction = &#39;0&#39;<br />			else : prediction = &#39;1&#39;<br />		if &#34;gun&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Rule-based classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#Extra Credit Rule Based Classifier<br />def extra_credit_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO develop your conditional statements here<br />		if &#34;shooting&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Reverse Engineered classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#train and multinomial naive bayes classifier<br />def get_top_features(X, y, dv):<br />	clf = train_classifier(X, y)<br />	#the DictVectorizer object remembers which column number corresponds to which feature, and return the feature names in the correct order<br />	feature_names = dv.get_feature_names() <br /><br />	#The below code will get the weights from the classifier, and print out the weights of the features you are interested in<br />	features = [] #this will be a list of (feature_idx, weight) tuples<br />	for i,w in enumerate(clf.coef_[0]): <br />		features.append((i,w))<br />	#Sort the list by values, with the largest ones first<br />	features = sorted(features, key=lambda e: e[1], reverse=True)<br /><br />        #Print out the feature names and thier weights<br />	for i,w in features:<br />	  print &#39;%s\t%s&#39;%(feature_names[i], w)<br /><br />def get_misclassified_examples(y, X, texts) :<br />	x_train, x_test, y_train, y_test, train_texts, test_texts = train_test_split(X, y, texts)<br />	clf = train_classifier(x_train, y_train)<br /><br />	#TODO: You will have to write some code to call your classifier on each of the test examples, and check whether its prediction was right or wrong<br /><br />def create_graph(file_name) :<br />	os.system(&#34;dot -Tpng output.dot -o &#34; &#43; file_name)<br />	os.unlink(&#34;output.dot&#34;)<br /><br />if __name__ == &#39;__main__&#39; : <br /><br />	raw_data = get_data(sys.argv[1])<br />	<br />	print &#39;\nRule-based classification&#39;<br />	rule_based_classifier(raw_data)<br /><br />################ Decision Tree ################<br /><br /># print &#39;\nDecision Tree classification&#39;<br /># y, X, texts, dv, le = get_matricies(raw_data, &#34;tree&#34;)<br /># cross_validate(X,y,dv,&#34;tree&#34;)<br /><br />################ Statistical Classification ################<br />print &#39;\nStatistical classification&#39;<br />y, X, texts, dv, le = get_matricies(raw_data)<br />cross_validate(X,y)<br /><br />get_top_features(X, y, dv)<br />get_misclassified_examples(y, X, texts)<br />
</pre>
<p>Thanks in advance for any help you can give.</p>",Statistical Analysis &#34;Killed&#34; - even on biglab,"<p>When my program gets to the Statistical Analysis bit, it makes it through all the words and creates wordcount dictionaries that are appended to the list of features. Then it hangs for a bit until the terminal just prints &#34;Killed&#34; and program ends. Any idea what might be causing this? Not enough memory?</p>",Statistical Analysis &#34;Killed&#34;,,,"<p>I&#39;m confused on what exactly we&#39;re supposed to do for part 6. Are the y dimensions the same as the number of articles? Also, how is logistic regression supposed to be used here?</p>",Statistical Analysis Part 6,0
940847301,4/26/2016 16:26:27,false,1969420526,,4/26/2016 16:25:32,false,neodev,1.0,29175140,VEN,25,Caracas,190.72.125.134,0,,"<p>The slides for today&#39;s lecture topics are now online:</p>
<p><a href=""http://crowdsourcing-class.org/slides/machine-learning-part-2.pdf"">Machine Learning - part 2</a></p>
<p><a href=""http://crowdsourcing-class.org/slides/amazon-mechanical-turk.pdf"">The Amazon Mechanical Turk crowdsourcing platform</a></p>
<p></p>
<p>Please post any questions that you have about either topic to this thread.åÊ</p>",Slides for today&#39;s lecture,"<p>If you&#39;d like to discussåÊAl Filries&#39; lecture about MOOC&#39;s, let&#39;s use this discussion thread.</p>
<p></p>
<p>Here&#39;s a link toåÊhis <a href=""https://www.coursera.org/course/modernpoetry"" target=""_blank"">ModPo Coursera course</a>.</p>
<p></p>",Al Filries&#39; lecture,"<p>On Fridays I&#39;m planning on doing more applied hands-on lectures where the TAs and I walk you through the homework assignments and help you get started. åÊHow did you like today&#39;s lecture?</p>
<p></p>
<p>Feel free to leave comments on things you would like to see in the Friday lectures, after you have voted.</p>
<br/> [o] Today&#39;s lecture was good, and helped me understand the assignment
[o] I didn&#39;t find it particularly useful, or I would have preferred a standard non-applied lecture",Poll: Friday hands-on lectures,"<p>Hi everyone,</p>
<p></p>
<p>Just a friendly reminder that you&#39;ll get credit towards your participation grade for attending today&#39;s NETS 213 lecture. åÊThe TAs will be passing out unique participation codes, one per student, as you leave the classroom. åÊBe sure to get one, and then enter it <a href=""https://docs.google.com/forms/d/14UZWosW5_W_-qDNI8KJ_zUGkiyTO9yuwv7yCkCvuZgQ/viewform"" target=""_blank"">here</a>åÊafter class.</p>
<p></p>
<p>--Chris</p>",Reminder: participation credit for attending today&#39;s NETS 213 lecture,<p>There will be a guest lecturer in NETS 213 today. åÊPlease attend the lecture. åÊWe mightåÊtake attendance.åÊ</p>,Guest lecture today,"<p>I have posted the slides from today&#39;s lecture here:åÊ<a href=""http://crowdsourcing-class.org/slides/class-intro.pdf"">Intro Lecture</a>åÊåÊ</p>
<p>TheåÊ<a href=""http://crowdsourcing-class.org/lectures.html"">Lectures</a>åÊpage on the <a href=""http://crowdsourcing-class.org/"" target=""_blank"">course web site</a> also contains a list of recommended readings for each lecture. åÊ</p>
<p></p>
<p>If you have any questions or comments about the lecture, please post them here. åÊ</p>",Intro Lecture,2
940847301,4/26/2016 16:45:07,false,1969432483,,4/26/2016 16:44:04,false,clixsense,1.0,6329782,IDN,10,Sleman,202.67.40.222,0,,"<p>The slides for today&#39;s lecture topics are now online:</p>
<p><a href=""http://crowdsourcing-class.org/slides/machine-learning-part-2.pdf"">Machine Learning - part 2</a></p>
<p><a href=""http://crowdsourcing-class.org/slides/amazon-mechanical-turk.pdf"">The Amazon Mechanical Turk crowdsourcing platform</a></p>
<p></p>
<p>Please post any questions that you have about either topic to this thread.åÊ</p>",Slides for today&#39;s lecture,"<p>If you&#39;d like to discussåÊAl Filries&#39; lecture about MOOC&#39;s, let&#39;s use this discussion thread.</p>
<p></p>
<p>Here&#39;s a link toåÊhis <a href=""https://www.coursera.org/course/modernpoetry"" target=""_blank"">ModPo Coursera course</a>.</p>
<p></p>",Al Filries&#39; lecture,"<p>On Fridays I&#39;m planning on doing more applied hands-on lectures where the TAs and I walk you through the homework assignments and help you get started. åÊHow did you like today&#39;s lecture?</p>
<p></p>
<p>Feel free to leave comments on things you would like to see in the Friday lectures, after you have voted.</p>
<br/> [o] Today&#39;s lecture was good, and helped me understand the assignment
[o] I didn&#39;t find it particularly useful, or I would have preferred a standard non-applied lecture",Poll: Friday hands-on lectures,"<p>Hi everyone,</p>
<p></p>
<p>Just a friendly reminder that you&#39;ll get credit towards your participation grade for attending today&#39;s NETS 213 lecture. åÊThe TAs will be passing out unique participation codes, one per student, as you leave the classroom. åÊBe sure to get one, and then enter it <a href=""https://docs.google.com/forms/d/14UZWosW5_W_-qDNI8KJ_zUGkiyTO9yuwv7yCkCvuZgQ/viewform"" target=""_blank"">here</a>åÊafter class.</p>
<p></p>
<p>--Chris</p>",Reminder: participation credit for attending today&#39;s NETS 213 lecture,<p>There will be a guest lecturer in NETS 213 today. åÊPlease attend the lecture. åÊWe mightåÊtake attendance.åÊ</p>,Guest lecture today,"<p>I have posted the slides from today&#39;s lecture here:åÊ<a href=""http://crowdsourcing-class.org/slides/class-intro.pdf"">Intro Lecture</a>åÊåÊ</p>
<p>TheåÊ<a href=""http://crowdsourcing-class.org/lectures.html"">Lectures</a>åÊpage on the <a href=""http://crowdsourcing-class.org/"" target=""_blank"">course web site</a> also contains a list of recommended readings for each lecture. åÊ</p>
<p></p>
<p>If you have any questions or comments about the lecture, please post them here. åÊ</p>",Intro Lecture,2
940847301,4/26/2016 17:01:22,false,1969442589,,4/26/2016 17:00:04,false,clixsense,1.0,21408115,IDN,07,Semarang,36.79.23.180,1,,"<p>The slides for today&#39;s lecture topics are now online:</p>
<p><a href=""http://crowdsourcing-class.org/slides/machine-learning-part-2.pdf"">Machine Learning - part 2</a></p>
<p><a href=""http://crowdsourcing-class.org/slides/amazon-mechanical-turk.pdf"">The Amazon Mechanical Turk crowdsourcing platform</a></p>
<p></p>
<p>Please post any questions that you have about either topic to this thread.åÊ</p>",Slides for today&#39;s lecture,"<p>If you&#39;d like to discussåÊAl Filries&#39; lecture about MOOC&#39;s, let&#39;s use this discussion thread.</p>
<p></p>
<p>Here&#39;s a link toåÊhis <a href=""https://www.coursera.org/course/modernpoetry"" target=""_blank"">ModPo Coursera course</a>.</p>
<p></p>",Al Filries&#39; lecture,"<p>On Fridays I&#39;m planning on doing more applied hands-on lectures where the TAs and I walk you through the homework assignments and help you get started. åÊHow did you like today&#39;s lecture?</p>
<p></p>
<p>Feel free to leave comments on things you would like to see in the Friday lectures, after you have voted.</p>
<br/> [o] Today&#39;s lecture was good, and helped me understand the assignment
[o] I didn&#39;t find it particularly useful, or I would have preferred a standard non-applied lecture",Poll: Friday hands-on lectures,"<p>Hi everyone,</p>
<p></p>
<p>Just a friendly reminder that you&#39;ll get credit towards your participation grade for attending today&#39;s NETS 213 lecture. åÊThe TAs will be passing out unique participation codes, one per student, as you leave the classroom. åÊBe sure to get one, and then enter it <a href=""https://docs.google.com/forms/d/14UZWosW5_W_-qDNI8KJ_zUGkiyTO9yuwv7yCkCvuZgQ/viewform"" target=""_blank"">here</a>åÊafter class.</p>
<p></p>
<p>--Chris</p>",Reminder: participation credit for attending today&#39;s NETS 213 lecture,<p>There will be a guest lecturer in NETS 213 today. åÊPlease attend the lecture. åÊWe mightåÊtake attendance.åÊ</p>,Guest lecture today,"<p>I have posted the slides from today&#39;s lecture here:åÊ<a href=""http://crowdsourcing-class.org/slides/class-intro.pdf"">Intro Lecture</a>åÊåÊ</p>
<p>TheåÊ<a href=""http://crowdsourcing-class.org/lectures.html"">Lectures</a>åÊpage on the <a href=""http://crowdsourcing-class.org/"" target=""_blank"">course web site</a> also contains a list of recommended readings for each lecture. åÊ</p>
<p></p>
<p>If you have any questions or comments about the lecture, please post them here. åÊ</p>",Intro Lecture,2
940847301,4/26/2016 17:05:47,false,1969445077,,4/26/2016 17:04:18,false,neodev,1.0,36167043,GBR,G6,Hull,77.86.101.69,1,,"<p>The slides for today&#39;s lecture topics are now online:</p>
<p><a href=""http://crowdsourcing-class.org/slides/machine-learning-part-2.pdf"">Machine Learning - part 2</a></p>
<p><a href=""http://crowdsourcing-class.org/slides/amazon-mechanical-turk.pdf"">The Amazon Mechanical Turk crowdsourcing platform</a></p>
<p></p>
<p>Please post any questions that you have about either topic to this thread.åÊ</p>",Slides for today&#39;s lecture,"<p>If you&#39;d like to discussåÊAl Filries&#39; lecture about MOOC&#39;s, let&#39;s use this discussion thread.</p>
<p></p>
<p>Here&#39;s a link toåÊhis <a href=""https://www.coursera.org/course/modernpoetry"" target=""_blank"">ModPo Coursera course</a>.</p>
<p></p>",Al Filries&#39; lecture,"<p>On Fridays I&#39;m planning on doing more applied hands-on lectures where the TAs and I walk you through the homework assignments and help you get started. åÊHow did you like today&#39;s lecture?</p>
<p></p>
<p>Feel free to leave comments on things you would like to see in the Friday lectures, after you have voted.</p>
<br/> [o] Today&#39;s lecture was good, and helped me understand the assignment
[o] I didn&#39;t find it particularly useful, or I would have preferred a standard non-applied lecture",Poll: Friday hands-on lectures,"<p>Hi everyone,</p>
<p></p>
<p>Just a friendly reminder that you&#39;ll get credit towards your participation grade for attending today&#39;s NETS 213 lecture. åÊThe TAs will be passing out unique participation codes, one per student, as you leave the classroom. åÊBe sure to get one, and then enter it <a href=""https://docs.google.com/forms/d/14UZWosW5_W_-qDNI8KJ_zUGkiyTO9yuwv7yCkCvuZgQ/viewform"" target=""_blank"">here</a>åÊafter class.</p>
<p></p>
<p>--Chris</p>",Reminder: participation credit for attending today&#39;s NETS 213 lecture,<p>There will be a guest lecturer in NETS 213 today. åÊPlease attend the lecture. åÊWe mightåÊtake attendance.åÊ</p>,Guest lecture today,"<p>I have posted the slides from today&#39;s lecture here:åÊ<a href=""http://crowdsourcing-class.org/slides/class-intro.pdf"">Intro Lecture</a>åÊåÊ</p>
<p>TheåÊ<a href=""http://crowdsourcing-class.org/lectures.html"">Lectures</a>åÊpage on the <a href=""http://crowdsourcing-class.org/"" target=""_blank"">course web site</a> also contains a list of recommended readings for each lecture. åÊ</p>
<p></p>
<p>If you have any questions or comments about the lecture, please post them here. åÊ</p>",Intro Lecture,2
940847301,4/26/2016 17:34:24,false,1969461914,,4/26/2016 17:33:46,false,clixsense,1.0,30712378,ROU,21,Deva,79.119.241.200,0,,"<p>The slides for today&#39;s lecture topics are now online:</p>
<p><a href=""http://crowdsourcing-class.org/slides/machine-learning-part-2.pdf"">Machine Learning - part 2</a></p>
<p><a href=""http://crowdsourcing-class.org/slides/amazon-mechanical-turk.pdf"">The Amazon Mechanical Turk crowdsourcing platform</a></p>
<p></p>
<p>Please post any questions that you have about either topic to this thread.åÊ</p>",Slides for today&#39;s lecture,"<p>If you&#39;d like to discussåÊAl Filries&#39; lecture about MOOC&#39;s, let&#39;s use this discussion thread.</p>
<p></p>
<p>Here&#39;s a link toåÊhis <a href=""https://www.coursera.org/course/modernpoetry"" target=""_blank"">ModPo Coursera course</a>.</p>
<p></p>",Al Filries&#39; lecture,"<p>On Fridays I&#39;m planning on doing more applied hands-on lectures where the TAs and I walk you through the homework assignments and help you get started. åÊHow did you like today&#39;s lecture?</p>
<p></p>
<p>Feel free to leave comments on things you would like to see in the Friday lectures, after you have voted.</p>
<br/> [o] Today&#39;s lecture was good, and helped me understand the assignment
[o] I didn&#39;t find it particularly useful, or I would have preferred a standard non-applied lecture",Poll: Friday hands-on lectures,"<p>Hi everyone,</p>
<p></p>
<p>Just a friendly reminder that you&#39;ll get credit towards your participation grade for attending today&#39;s NETS 213 lecture. åÊThe TAs will be passing out unique participation codes, one per student, as you leave the classroom. åÊBe sure to get one, and then enter it <a href=""https://docs.google.com/forms/d/14UZWosW5_W_-qDNI8KJ_zUGkiyTO9yuwv7yCkCvuZgQ/viewform"" target=""_blank"">here</a>åÊafter class.</p>
<p></p>
<p>--Chris</p>",Reminder: participation credit for attending today&#39;s NETS 213 lecture,<p>There will be a guest lecturer in NETS 213 today. åÊPlease attend the lecture. åÊWe mightåÊtake attendance.åÊ</p>,Guest lecture today,"<p>I have posted the slides from today&#39;s lecture here:åÊ<a href=""http://crowdsourcing-class.org/slides/class-intro.pdf"">Intro Lecture</a>åÊåÊ</p>
<p>TheåÊ<a href=""http://crowdsourcing-class.org/lectures.html"">Lectures</a>åÊpage on the <a href=""http://crowdsourcing-class.org/"" target=""_blank"">course web site</a> also contains a list of recommended readings for each lecture. åÊ</p>
<p></p>
<p>If you have any questions or comments about the lecture, please post them here. åÊ</p>",Intro Lecture,2
940847302,4/26/2016 17:27:57,false,1969458133,,4/26/2016 17:23:11,false,neodev,1.0,33973110,VEN,23,Maracaibo,186.94.238.104,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I&#39;m editing theåÊpython_crawler.py (step 1 of part 1) and I&#39;ve changed both the URL at the top and the URL at the bottom to link to the Gun Violence Archive. However, when I run it in the same way that I ran the Gun Reports Blog one, I get 0 links. I&#39;m looking for advice onåÊhow to debug this. I haveåÊsome understanding of the command line but not much, and even less understanding of Python, so I am not really sure where to start.</p>",Modifying python_crawler and getting no results,0
940847302,4/26/2016 17:32:16,false,1969460792,,4/26/2016 17:31:58,false,neodev,0.8889,33131546,IDN,04,Jakarta,139.194.89.60,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I&#39;m editing theåÊpython_crawler.py (step 1 of part 1) and I&#39;ve changed both the URL at the top and the URL at the bottom to link to the Gun Violence Archive. However, when I run it in the same way that I ran the Gun Reports Blog one, I get 0 links. I&#39;m looking for advice onåÊhow to debug this. I haveåÊsome understanding of the command line but not much, and even less understanding of Python, so I am not really sure where to start.</p>",Modifying python_crawler and getting no results,0
940847302,4/26/2016 17:35:19,false,1969462336,,4/26/2016 17:34:26,false,clixsense,1.0,30712378,ROU,21,Deva,79.119.241.200,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I&#39;m editing theåÊpython_crawler.py (step 1 of part 1) and I&#39;ve changed both the URL at the top and the URL at the bottom to link to the Gun Violence Archive. However, when I run it in the same way that I ran the Gun Reports Blog one, I get 0 links. I&#39;m looking for advice onåÊhow to debug this. I haveåÊsome understanding of the command line but not much, and even less understanding of Python, so I am not really sure where to start.</p>",Modifying python_crawler and getting no results,0
940847302,4/26/2016 18:27:49,false,1969490433,,4/26/2016 18:04:06,false,clixsense,0.8889,35338593,ITA,14,Cagliari,151.56.132.145,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I&#39;m editing theåÊpython_crawler.py (step 1 of part 1) and I&#39;ve changed both the URL at the top and the URL at the bottom to link to the Gun Violence Archive. However, when I run it in the same way that I ran the Gun Reports Blog one, I get 0 links. I&#39;m looking for advice onåÊhow to debug this. I haveåÊsome understanding of the command line but not much, and even less understanding of Python, so I am not really sure where to start.</p>",Modifying python_crawler and getting no results,0
940847302,4/26/2016 18:41:46,false,1969497898,,4/26/2016 18:39:26,false,neodev,1.0,29879245,RUS,69,Smolensk,37.144.124.118,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I&#39;m editing theåÊpython_crawler.py (step 1 of part 1) and I&#39;ve changed both the URL at the top and the URL at the bottom to link to the Gun Violence Archive. However, when I run it in the same way that I ran the Gun Reports Blog one, I get 0 links. I&#39;m looking for advice onåÊhow to debug this. I haveåÊsome understanding of the command line but not much, and even less understanding of Python, so I am not really sure where to start.</p>",Modifying python_crawler and getting no results,0
940847303,4/26/2016 15:48:55,false,1969398736,,4/26/2016 15:46:06,false,neodev,0.8889,21971187,TTO,08,Valsayn,190.213.132.190,0,,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,<p></p>,How to submit Deliverable 1?,1
940847303,4/26/2016 15:50:47,false,1969399568,,4/26/2016 15:49:12,false,elite,0.8889,36575101,IND,07,New Delhi,112.196.144.2,0,,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,<p></p>,How to submit Deliverable 1?,1
940847303,4/26/2016 15:51:11,false,1969399730,,4/26/2016 15:47:56,false,neodev,1.0,28875937,PAK,08,Islamabad,119.153.105.50,0,,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,<p></p>,How to submit Deliverable 1?,1
940847303,4/26/2016 15:53:17,false,1969400603,,4/26/2016 15:51:23,false,neodev,1.0,13396426,VEN,15,Santa Teresa,190.38.163.149,0,,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,<p></p>,How to submit Deliverable 1?,1
940847303,4/26/2016 15:56:46,false,1969402042,,4/26/2016 15:55:02,false,elite,1.0,33243069,IND,10,Faridabad,116.203.79.150,0,,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,<p></p>,How to submit Deliverable 1?,1
940847304,4/26/2016 17:32:48,false,1969461083,,4/26/2016 17:32:36,false,neodev,0.8889,33131546,IDN,04,Jakarta,139.194.89.60,0,,"After adding my fratures to the features list, and running the program, i still get the same decision tree as we saw initially with just gun. Not sure whats wrong",decision tree with add features,<p>How do I print the tree like we did in class Friday?</p>,Printing Decision Tree,"<p>I&#39;ve installed the graphivz onto my computer, but my program doesn&#39;t generate a diagram.</p>",Unable to generate Decision Tree Diagram,<p>I keep getting an error message when I save my draw.io file as a .png on Linux VM and try to open the file with any sort of image viewer. Any idea what might be going on?</p>,Could not load decision tree image from draw.io,"<p>Hi, I looked over the chapter on decision trees and I am still a tad confused about how they work.</p>
<p></p>
<p>For my rule-based classifier, I used the following format:</p>
<p>if &#34;word1&#34; in text : decision = 1</p>
<p>if &#34;word2&#34; in text : decision = 1</p>
<p>... (rest of decision = 1 words)</p>
<p>if &#34;word10&#34; in text : decision = 0</p>
<p>if &#34;word11&#34; in text : decision = 0</p>
<p></p>
<p>I am still a bit confused about how a decision tree would work for this when the conditions are not interrelated</p>
<p></p>
<p>Would the tree look like this?</p>
<p></p>
<p>åÊåÊåÊ w1åÊ -------| åÊåÊåÊåÊåÊ åÊ åÊåÊ (and so on for w2, w3, etc)</p>
<p>åÊåÊ /åÊåÊåÊ \åÊåÊåÊåÊåÊåÊåÊåÊåÊåÊ | åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊåÊ</p>
<p>w10åÊåÊ w11åÊåÊåÊåÊ |åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ</p>
<p>|åÊåÊåÊåÊåÊåÊåÊåÊåÊåÊ |åÊåÊåÊåÊåÊåÊ yes</p>
<p>noåÊåÊåÊåÊåÊåÊåÊ no</p>",Decision Tree Where Conditions Are Not Dependent On One Another,"<p>In our rule-based classifier, if we have a rule with the format &#34;If A in text and B not in text,&#34; should we put &#34;A and not B&#34; in the same box of the decision tree, or should we have separate boxes for A and B?åÊ</p>",Decision Tree,3
940847304,4/26/2016 17:36:20,false,1969462851,,4/26/2016 17:36:03,false,clixsense,1.0,30712378,ROU,21,Deva,79.119.241.200,0,,"After adding my fratures to the features list, and running the program, i still get the same decision tree as we saw initially with just gun. Not sure whats wrong",decision tree with add features,<p>How do I print the tree like we did in class Friday?</p>,Printing Decision Tree,"<p>I&#39;ve installed the graphivz onto my computer, but my program doesn&#39;t generate a diagram.</p>",Unable to generate Decision Tree Diagram,<p>I keep getting an error message when I save my draw.io file as a .png on Linux VM and try to open the file with any sort of image viewer. Any idea what might be going on?</p>,Could not load decision tree image from draw.io,"<p>Hi, I looked over the chapter on decision trees and I am still a tad confused about how they work.</p>
<p></p>
<p>For my rule-based classifier, I used the following format:</p>
<p>if &#34;word1&#34; in text : decision = 1</p>
<p>if &#34;word2&#34; in text : decision = 1</p>
<p>... (rest of decision = 1 words)</p>
<p>if &#34;word10&#34; in text : decision = 0</p>
<p>if &#34;word11&#34; in text : decision = 0</p>
<p></p>
<p>I am still a bit confused about how a decision tree would work for this when the conditions are not interrelated</p>
<p></p>
<p>Would the tree look like this?</p>
<p></p>
<p>åÊåÊåÊ w1åÊ -------| åÊåÊåÊåÊåÊ åÊ åÊåÊ (and so on for w2, w3, etc)</p>
<p>åÊåÊ /åÊåÊåÊ \åÊåÊåÊåÊåÊåÊåÊåÊåÊåÊ | åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊåÊ</p>
<p>w10åÊåÊ w11åÊåÊåÊåÊ |åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ</p>
<p>|åÊåÊåÊåÊåÊåÊåÊåÊåÊåÊ |åÊåÊåÊåÊåÊåÊ yes</p>
<p>noåÊåÊåÊåÊåÊåÊåÊ no</p>",Decision Tree Where Conditions Are Not Dependent On One Another,"<p>In our rule-based classifier, if we have a rule with the format &#34;If A in text and B not in text,&#34; should we put &#34;A and not B&#34; in the same box of the decision tree, or should we have separate boxes for A and B?åÊ</p>",Decision Tree,3
940847304,4/26/2016 17:39:37,false,1969464624,,4/26/2016 17:38:56,false,neodev,0.8889,33568303,VEN,23,Cabimas,190.77.7.36,0,,"After adding my fratures to the features list, and running the program, i still get the same decision tree as we saw initially with just gun. Not sure whats wrong",decision tree with add features,<p>How do I print the tree like we did in class Friday?</p>,Printing Decision Tree,"<p>I&#39;ve installed the graphivz onto my computer, but my program doesn&#39;t generate a diagram.</p>",Unable to generate Decision Tree Diagram,<p>I keep getting an error message when I save my draw.io file as a .png on Linux VM and try to open the file with any sort of image viewer. Any idea what might be going on?</p>,Could not load decision tree image from draw.io,"<p>Hi, I looked over the chapter on decision trees and I am still a tad confused about how they work.</p>
<p></p>
<p>For my rule-based classifier, I used the following format:</p>
<p>if &#34;word1&#34; in text : decision = 1</p>
<p>if &#34;word2&#34; in text : decision = 1</p>
<p>... (rest of decision = 1 words)</p>
<p>if &#34;word10&#34; in text : decision = 0</p>
<p>if &#34;word11&#34; in text : decision = 0</p>
<p></p>
<p>I am still a bit confused about how a decision tree would work for this when the conditions are not interrelated</p>
<p></p>
<p>Would the tree look like this?</p>
<p></p>
<p>åÊåÊåÊ w1åÊ -------| åÊåÊåÊåÊåÊ åÊ åÊåÊ (and so on for w2, w3, etc)</p>
<p>åÊåÊ /åÊåÊåÊ \åÊåÊåÊåÊåÊåÊåÊåÊåÊåÊ | åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊåÊ</p>
<p>w10åÊåÊ w11åÊåÊåÊåÊ |åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ</p>
<p>|åÊåÊåÊåÊåÊåÊåÊåÊåÊåÊ |åÊåÊåÊåÊåÊåÊ yes</p>
<p>noåÊåÊåÊåÊåÊåÊåÊ no</p>",Decision Tree Where Conditions Are Not Dependent On One Another,"<p>In our rule-based classifier, if we have a rule with the format &#34;If A in text and B not in text,&#34; should we put &#34;A and not B&#34; in the same box of the decision tree, or should we have separate boxes for A and B?åÊ</p>",Decision Tree,3
940847304,4/26/2016 17:49:32,false,1969470400,,4/26/2016 17:45:16,false,clixsense,1.0,35444326,BRA,07,Brasília,177.15.130.106,0,,"After adding my fratures to the features list, and running the program, i still get the same decision tree as we saw initially with just gun. Not sure whats wrong",decision tree with add features,<p>How do I print the tree like we did in class Friday?</p>,Printing Decision Tree,"<p>I&#39;ve installed the graphivz onto my computer, but my program doesn&#39;t generate a diagram.</p>",Unable to generate Decision Tree Diagram,<p>I keep getting an error message when I save my draw.io file as a .png on Linux VM and try to open the file with any sort of image viewer. Any idea what might be going on?</p>,Could not load decision tree image from draw.io,"<p>Hi, I looked over the chapter on decision trees and I am still a tad confused about how they work.</p>
<p></p>
<p>For my rule-based classifier, I used the following format:</p>
<p>if &#34;word1&#34; in text : decision = 1</p>
<p>if &#34;word2&#34; in text : decision = 1</p>
<p>... (rest of decision = 1 words)</p>
<p>if &#34;word10&#34; in text : decision = 0</p>
<p>if &#34;word11&#34; in text : decision = 0</p>
<p></p>
<p>I am still a bit confused about how a decision tree would work for this when the conditions are not interrelated</p>
<p></p>
<p>Would the tree look like this?</p>
<p></p>
<p>åÊåÊåÊ w1åÊ -------| åÊåÊåÊåÊåÊ åÊ åÊåÊ (and so on for w2, w3, etc)</p>
<p>åÊåÊ /åÊåÊåÊ \åÊåÊåÊåÊåÊåÊåÊåÊåÊåÊ | åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊåÊ</p>
<p>w10åÊåÊ w11åÊåÊåÊåÊ |åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ</p>
<p>|åÊåÊåÊåÊåÊåÊåÊåÊåÊåÊ |åÊåÊåÊåÊåÊåÊ yes</p>
<p>noåÊåÊåÊåÊåÊåÊåÊ no</p>",Decision Tree Where Conditions Are Not Dependent On One Another,"<p>In our rule-based classifier, if we have a rule with the format &#34;If A in text and B not in text,&#34; should we put &#34;A and not B&#34; in the same box of the decision tree, or should we have separate boxes for A and B?åÊ</p>",Decision Tree,3
940847304,4/26/2016 18:04:05,false,1969477771,,4/26/2016 17:46:51,false,clixsense,0.8889,35338593,ITA,14,Cagliari,151.56.132.145,0,,"After adding my fratures to the features list, and running the program, i still get the same decision tree as we saw initially with just gun. Not sure whats wrong",decision tree with add features,<p>How do I print the tree like we did in class Friday?</p>,Printing Decision Tree,"<p>I&#39;ve installed the graphivz onto my computer, but my program doesn&#39;t generate a diagram.</p>",Unable to generate Decision Tree Diagram,<p>I keep getting an error message when I save my draw.io file as a .png on Linux VM and try to open the file with any sort of image viewer. Any idea what might be going on?</p>,Could not load decision tree image from draw.io,"<p>Hi, I looked over the chapter on decision trees and I am still a tad confused about how they work.</p>
<p></p>
<p>For my rule-based classifier, I used the following format:</p>
<p>if &#34;word1&#34; in text : decision = 1</p>
<p>if &#34;word2&#34; in text : decision = 1</p>
<p>... (rest of decision = 1 words)</p>
<p>if &#34;word10&#34; in text : decision = 0</p>
<p>if &#34;word11&#34; in text : decision = 0</p>
<p></p>
<p>I am still a bit confused about how a decision tree would work for this when the conditions are not interrelated</p>
<p></p>
<p>Would the tree look like this?</p>
<p></p>
<p>åÊåÊåÊ w1åÊ -------| åÊåÊåÊåÊåÊ åÊ åÊåÊ (and so on for w2, w3, etc)</p>
<p>åÊåÊ /åÊåÊåÊ \åÊåÊåÊåÊåÊåÊåÊåÊåÊåÊ | åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊåÊ</p>
<p>w10åÊåÊ w11åÊåÊåÊåÊ |åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ</p>
<p>|åÊåÊåÊåÊåÊåÊåÊåÊåÊåÊ |åÊåÊåÊåÊåÊåÊ yes</p>
<p>noåÊåÊåÊåÊåÊåÊåÊ no</p>",Decision Tree Where Conditions Are Not Dependent On One Another,"<p>In our rule-based classifier, if we have a rule with the format &#34;If A in text and B not in text,&#34; should we put &#34;A and not B&#34; in the same box of the decision tree, or should we have separate boxes for A and B?åÊ</p>",Decision Tree,3
940847305,4/26/2016 16:26:27,false,1969420531,,4/26/2016 16:25:32,false,neodev,1.0,29175140,VEN,25,Caracas,190.72.125.134,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I modified python_crawler.py in two places: 1) I made the domain <a href=""http://www.gunviolencearchive.org/"">http://www.gunviolencearchive.org/</a>åÊ2) I changed the range to 1,10 and changed the link.åÊ</p>
<p></p>
<p>The terminal prints:åÊ</p>
<p>Crawling from page http://www.gunviolencearchive.org/last-72-hours?page=1/</p>
<p>Crawling from page http://www.gunviolencearchive.org/last-72-hours?page=2/</p>
<p>Crawling from page http://www.gunviolencearchive.org/last-72-hours?page=3/</p>
<p>Crawling from page http://www.gunviolencearchive.org/last-72-hours?page=4/</p>
<p>Crawling from page http://www.gunviolencearchive.org/last-72-hours?page=5/</p>
<p>Crawling from page http://www.gunviolencearchive.org/last-72-hours?page=6/</p>
<p>Crawling from page http://www.gunviolencearchive.org/last-72-hours?page=7/</p>
<p>Crawling from page http://www.gunviolencearchive.org/last-72-hours?page=8/</p>
<p>Crawling from page http://www.gunviolencearchive.org/last-72-hours?page=9/</p>
<p></p>
<p>However, the links are not showing up in gun_report_url.txtåÊ</p>
<p></p>
<p>Any insight as to why this might be?</p>",Having difficulties scraping links using Gun Violence Archive,3
940847305,4/26/2016 16:45:07,false,1969432487,,4/26/2016 16:44:04,false,clixsense,1.0,6329782,IDN,10,Sleman,202.67.40.222,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I modified python_crawler.py in two places: 1) I made the domain <a href=""http://www.gunviolencearchive.org/"">http://www.gunviolencearchive.org/</a>åÊ2) I changed the range to 1,10 and changed the link.åÊ</p>
<p></p>
<p>The terminal prints:åÊ</p>
<p>Crawling from page http://www.gunviolencearchive.org/last-72-hours?page=1/</p>
<p>Crawling from page http://www.gunviolencearchive.org/last-72-hours?page=2/</p>
<p>Crawling from page http://www.gunviolencearchive.org/last-72-hours?page=3/</p>
<p>Crawling from page http://www.gunviolencearchive.org/last-72-hours?page=4/</p>
<p>Crawling from page http://www.gunviolencearchive.org/last-72-hours?page=5/</p>
<p>Crawling from page http://www.gunviolencearchive.org/last-72-hours?page=6/</p>
<p>Crawling from page http://www.gunviolencearchive.org/last-72-hours?page=7/</p>
<p>Crawling from page http://www.gunviolencearchive.org/last-72-hours?page=8/</p>
<p>Crawling from page http://www.gunviolencearchive.org/last-72-hours?page=9/</p>
<p></p>
<p>However, the links are not showing up in gun_report_url.txtåÊ</p>
<p></p>
<p>Any insight as to why this might be?</p>",Having difficulties scraping links using Gun Violence Archive,3
940847305,4/26/2016 17:01:22,false,1969442593,,4/26/2016 17:00:04,false,clixsense,1.0,21408115,IDN,07,Semarang,36.79.23.180,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I modified python_crawler.py in two places: 1) I made the domain <a href=""http://www.gunviolencearchive.org/"">http://www.gunviolencearchive.org/</a>åÊ2) I changed the range to 1,10 and changed the link.åÊ</p>
<p></p>
<p>The terminal prints:åÊ</p>
<p>Crawling from page http://www.gunviolencearchive.org/last-72-hours?page=1/</p>
<p>Crawling from page http://www.gunviolencearchive.org/last-72-hours?page=2/</p>
<p>Crawling from page http://www.gunviolencearchive.org/last-72-hours?page=3/</p>
<p>Crawling from page http://www.gunviolencearchive.org/last-72-hours?page=4/</p>
<p>Crawling from page http://www.gunviolencearchive.org/last-72-hours?page=5/</p>
<p>Crawling from page http://www.gunviolencearchive.org/last-72-hours?page=6/</p>
<p>Crawling from page http://www.gunviolencearchive.org/last-72-hours?page=7/</p>
<p>Crawling from page http://www.gunviolencearchive.org/last-72-hours?page=8/</p>
<p>Crawling from page http://www.gunviolencearchive.org/last-72-hours?page=9/</p>
<p></p>
<p>However, the links are not showing up in gun_report_url.txtåÊ</p>
<p></p>
<p>Any insight as to why this might be?</p>",Having difficulties scraping links using Gun Violence Archive,3
940847305,4/26/2016 17:05:47,false,1969445079,,4/26/2016 17:04:18,false,neodev,1.0,36167043,GBR,G6,Hull,77.86.101.69,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I modified python_crawler.py in two places: 1) I made the domain <a href=""http://www.gunviolencearchive.org/"">http://www.gunviolencearchive.org/</a>åÊ2) I changed the range to 1,10 and changed the link.åÊ</p>
<p></p>
<p>The terminal prints:åÊ</p>
<p>Crawling from page http://www.gunviolencearchive.org/last-72-hours?page=1/</p>
<p>Crawling from page http://www.gunviolencearchive.org/last-72-hours?page=2/</p>
<p>Crawling from page http://www.gunviolencearchive.org/last-72-hours?page=3/</p>
<p>Crawling from page http://www.gunviolencearchive.org/last-72-hours?page=4/</p>
<p>Crawling from page http://www.gunviolencearchive.org/last-72-hours?page=5/</p>
<p>Crawling from page http://www.gunviolencearchive.org/last-72-hours?page=6/</p>
<p>Crawling from page http://www.gunviolencearchive.org/last-72-hours?page=7/</p>
<p>Crawling from page http://www.gunviolencearchive.org/last-72-hours?page=8/</p>
<p>Crawling from page http://www.gunviolencearchive.org/last-72-hours?page=9/</p>
<p></p>
<p>However, the links are not showing up in gun_report_url.txtåÊ</p>
<p></p>
<p>Any insight as to why this might be?</p>",Having difficulties scraping links using Gun Violence Archive,3
940847305,4/26/2016 17:34:24,false,1969461909,,4/26/2016 17:33:46,false,clixsense,1.0,30712378,ROU,21,Deva,79.119.241.200,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I modified python_crawler.py in two places: 1) I made the domain <a href=""http://www.gunviolencearchive.org/"">http://www.gunviolencearchive.org/</a>åÊ2) I changed the range to 1,10 and changed the link.åÊ</p>
<p></p>
<p>The terminal prints:åÊ</p>
<p>Crawling from page http://www.gunviolencearchive.org/last-72-hours?page=1/</p>
<p>Crawling from page http://www.gunviolencearchive.org/last-72-hours?page=2/</p>
<p>Crawling from page http://www.gunviolencearchive.org/last-72-hours?page=3/</p>
<p>Crawling from page http://www.gunviolencearchive.org/last-72-hours?page=4/</p>
<p>Crawling from page http://www.gunviolencearchive.org/last-72-hours?page=5/</p>
<p>Crawling from page http://www.gunviolencearchive.org/last-72-hours?page=6/</p>
<p>Crawling from page http://www.gunviolencearchive.org/last-72-hours?page=7/</p>
<p>Crawling from page http://www.gunviolencearchive.org/last-72-hours?page=8/</p>
<p>Crawling from page http://www.gunviolencearchive.org/last-72-hours?page=9/</p>
<p></p>
<p>However, the links are not showing up in gun_report_url.txtåÊ</p>
<p></p>
<p>Any insight as to why this might be?</p>",Having difficulties scraping links using Gun Violence Archive,3
940847306,4/26/2016 17:08:54,false,1969446910,,4/26/2016 17:07:44,false,neodev,1.0,36167043,GBR,G6,Hull,77.86.101.69,0,,"<p>When I use the command</p>
<pre><code> cat articles_and_urls.txt | cut -f 1 &gt; urls.txt 
 cat articles_and_urls.txt | cut -f 2 &gt; unlabelled_articles.txt </code></pre>
<p></p>
<p>I get this error</p>
<p>cut: stdin: Illegal byte sequence</p>
<p></p>
<p>I tried looking it up but nothing has really helped.</p>",Error preparing input data for part 2,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p>I wanted to resubmit my project to include the .md file. But whenever I try to submit I get this error:</p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8n7u3ifw4mc/il5eikvnqd55/Screen_Shot_20160227_at_12.27.14_PM.png"" /></p>
<p></p>
<p>Is it ok if I email my directory to a TA? I don&#39;t really know how to fix this issue.</p>
<p></p>
<p>Best,</p>
<p>Ben</p>
<p></p>",Error on resubmission,1
940847306,4/26/2016 17:16:52,false,1969451844,,4/26/2016 17:08:45,false,neodev,1.0,33973110,VEN,23,Maracaibo,186.94.238.104,0,,"<p>When I use the command</p>
<pre><code> cat articles_and_urls.txt | cut -f 1 &gt; urls.txt 
 cat articles_and_urls.txt | cut -f 2 &gt; unlabelled_articles.txt </code></pre>
<p></p>
<p>I get this error</p>
<p>cut: stdin: Illegal byte sequence</p>
<p></p>
<p>I tried looking it up but nothing has really helped.</p>",Error preparing input data for part 2,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p>I wanted to resubmit my project to include the .md file. But whenever I try to submit I get this error:</p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8n7u3ifw4mc/il5eikvnqd55/Screen_Shot_20160227_at_12.27.14_PM.png"" /></p>
<p></p>
<p>Is it ok if I email my directory to a TA? I don&#39;t really know how to fix this issue.</p>
<p></p>
<p>Best,</p>
<p>Ben</p>
<p></p>",Error on resubmission,1
940847306,4/26/2016 17:19:05,false,1969453023,,4/26/2016 17:16:43,false,elite,1.0,25411289,HRV,"","",31.147.119.175,0,,"<p>When I use the command</p>
<pre><code> cat articles_and_urls.txt | cut -f 1 &gt; urls.txt 
 cat articles_and_urls.txt | cut -f 2 &gt; unlabelled_articles.txt </code></pre>
<p></p>
<p>I get this error</p>
<p>cut: stdin: Illegal byte sequence</p>
<p></p>
<p>I tried looking it up but nothing has really helped.</p>",Error preparing input data for part 2,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p>I wanted to resubmit my project to include the .md file. But whenever I try to submit I get this error:</p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8n7u3ifw4mc/il5eikvnqd55/Screen_Shot_20160227_at_12.27.14_PM.png"" /></p>
<p></p>
<p>Is it ok if I email my directory to a TA? I don&#39;t really know how to fix this issue.</p>
<p></p>
<p>Best,</p>
<p>Ben</p>
<p></p>",Error on resubmission,1
940847306,4/26/2016 17:32:09,false,1969460738,,4/26/2016 17:30:32,false,neodev,0.8889,19625264,DZA,41,Chlef,41.102.7.217,0,,"<p>When I use the command</p>
<pre><code> cat articles_and_urls.txt | cut -f 1 &gt; urls.txt 
 cat articles_and_urls.txt | cut -f 2 &gt; unlabelled_articles.txt </code></pre>
<p></p>
<p>I get this error</p>
<p>cut: stdin: Illegal byte sequence</p>
<p></p>
<p>I tried looking it up but nothing has really helped.</p>",Error preparing input data for part 2,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p>I wanted to resubmit my project to include the .md file. But whenever I try to submit I get this error:</p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8n7u3ifw4mc/il5eikvnqd55/Screen_Shot_20160227_at_12.27.14_PM.png"" /></p>
<p></p>
<p>Is it ok if I email my directory to a TA? I don&#39;t really know how to fix this issue.</p>
<p></p>
<p>Best,</p>
<p>Ben</p>
<p></p>",Error on resubmission,1
940847306,4/26/2016 17:32:14,false,1969460766,,4/26/2016 17:15:40,false,clixsense,0.8889,35338593,ITA,14,Cagliari,151.56.132.145,0,,"<p>When I use the command</p>
<pre><code> cat articles_and_urls.txt | cut -f 1 &gt; urls.txt 
 cat articles_and_urls.txt | cut -f 2 &gt; unlabelled_articles.txt </code></pre>
<p></p>
<p>I get this error</p>
<p>cut: stdin: Illegal byte sequence</p>
<p></p>
<p>I tried looking it up but nothing has really helped.</p>",Error preparing input data for part 2,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p>I wanted to resubmit my project to include the .md file. But whenever I try to submit I get this error:</p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8n7u3ifw4mc/il5eikvnqd55/Screen_Shot_20160227_at_12.27.14_PM.png"" /></p>
<p></p>
<p>Is it ok if I email my directory to a TA? I don&#39;t really know how to fix this issue.</p>
<p></p>
<p>Best,</p>
<p>Ben</p>
<p></p>",Error on resubmission,1
940847307,4/26/2016 16:03:21,false,1969405344,,4/26/2016 16:02:52,false,personaly,1.0,33663352,ARG,01,Mar Del Plata,181.168.213.227,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>If I am borrowing a friend&#39;s MTurk Account to complete the assignment, should I include their Worker ID in the homework survey or just leave that blank since it&#39;s not technically my account?</p>",Borrowing MTurk Account,4
940847307,4/26/2016 16:24:34,false,1969419059,,4/26/2016 16:23:47,false,neodev,1.0,29175140,VEN,25,Caracas,190.72.125.134,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>If I am borrowing a friend&#39;s MTurk Account to complete the assignment, should I include their Worker ID in the homework survey or just leave that blank since it&#39;s not technically my account?</p>",Borrowing MTurk Account,4
940847307,4/26/2016 16:34:40,false,1969426097,,4/26/2016 16:32:37,false,clixsense,0.8889,8057247,PRT,17,Póvoa De Varzim,144.64.25.68,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>If I am borrowing a friend&#39;s MTurk Account to complete the assignment, should I include their Worker ID in the homework survey or just leave that blank since it&#39;s not technically my account?</p>",Borrowing MTurk Account,4
940847307,4/26/2016 16:41:30,false,1969430409,,4/26/2016 16:40:27,false,clixsense,1.0,6329782,IDN,07,Bekonang,202.67.40.31,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>If I am borrowing a friend&#39;s MTurk Account to complete the assignment, should I include their Worker ID in the homework survey or just leave that blank since it&#39;s not technically my account?</p>",Borrowing MTurk Account,4
940847307,4/26/2016 16:58:38,false,1969440703,,4/26/2016 16:56:43,false,clixsense,1.0,21408115,IDN,07,Semarang,36.79.23.180,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>If I am borrowing a friend&#39;s MTurk Account to complete the assignment, should I include their Worker ID in the homework survey or just leave that blank since it&#39;s not technically my account?</p>",Borrowing MTurk Account,4
940847308,4/26/2016 16:25:31,false,1969419715,,4/26/2016 16:24:35,false,neodev,1.0,29175140,VEN,25,Caracas,190.72.125.134,0,,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>Hi! I&#39;m trying to turn in the homework, but it says that classifier is not a current project for submission. I&#39;m using the below code to try and turn it in by sshing into eniac:</p>
<p></p>
<p>turnin -c nets213 -p classifier -v assignment4</p>
<p></p>
<p>Is this correct?</p>",Turning in,5
940847308,4/26/2016 16:44:00,false,1969431830,,4/26/2016 16:41:32,false,clixsense,1.0,6329782,IDN,10,Sleman,202.67.40.222,0,,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>Hi! I&#39;m trying to turn in the homework, but it says that classifier is not a current project for submission. I&#39;m using the below code to try and turn it in by sshing into eniac:</p>
<p></p>
<p>turnin -c nets213 -p classifier -v assignment4</p>
<p></p>
<p>Is this correct?</p>",Turning in,5
940847308,4/26/2016 17:00:02,false,1969441671,,4/26/2016 16:58:39,false,clixsense,1.0,21408115,IDN,07,Semarang,36.79.23.180,0,,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>Hi! I&#39;m trying to turn in the homework, but it says that classifier is not a current project for submission. I&#39;m using the below code to try and turn it in by sshing into eniac:</p>
<p></p>
<p>turnin -c nets213 -p classifier -v assignment4</p>
<p></p>
<p>Is this correct?</p>",Turning in,5
940847308,4/26/2016 17:04:17,false,1969444195,,4/26/2016 17:02:14,false,neodev,1.0,36167043,GBR,G6,Hull,77.86.101.69,0,,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>Hi! I&#39;m trying to turn in the homework, but it says that classifier is not a current project for submission. I&#39;m using the below code to try and turn it in by sshing into eniac:</p>
<p></p>
<p>turnin -c nets213 -p classifier -v assignment4</p>
<p></p>
<p>Is this correct?</p>",Turning in,5
940847308,4/26/2016 17:47:57,false,1969469596,,4/26/2016 17:41:51,false,neodev,0.8889,19625264,DZA,41,Chlef,41.102.7.217,0,,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>Hi! I&#39;m trying to turn in the homework, but it says that classifier is not a current project for submission. I&#39;m using the below code to try and turn it in by sshing into eniac:</p>
<p></p>
<p>turnin -c nets213 -p classifier -v assignment4</p>
<p></p>
<p>Is this correct?</p>",Turning in,5
940847309,4/26/2016 15:54:11,false,1969401049,,4/26/2016 15:52:34,false,elite,0.8889,36575101,IND,07,New Delhi,112.196.144.2,0,,"<p>Hi Everyone!</p>
<p></p>
<p>Sorry class got so rushed-- I was hoping to walk through more of the bootcamp. Hopefully you were able to get a good start. We have back to back office hours now and office hoursåÊfrom noon until 8 tomorrow, so come get help early!åÊ</p>
<p></p>
<p>Attached is the iPython notebook from class today, which takes care of question 1 for you. The relevant code is also below. You should be able to recycle it in some shape or form for most of the other questions in the assignment.åÊ</p>
<p></p>
<pre>#This line reads in full text of the file, the splits it up using the newline character (&#39;\n&#39;)
#The return value (stored in wine) is a list of strings, each one corresponding to a line in the file
#Fun fact: This is the same as calling open(&#39;data/wine.txt&#39;).read().split(&#39;\n&#39;)
wine = open(&#39;data/wine.txt&#39;).readlines()

ratings = {} #dictionary keep track of how many times each star value is seen
for line in wine:
    review_and_rating = line.strip().split(&#39;\t&#39;) #split each line into two strings, using the tab character
    stars = review_and_rating[1]
    if stars not in ratings: #check if the key is in the dictionary. If not, add it and give it a count of 0
        print(&#34;adding&#34;, stars, &#34;to ratings&#34;)
        ratings[stars] = 0
    ratings[stars] &#43;= 1 #increment the count by 1
    
for key in ratings:
    print(key, &#34;appeared&#34;, ratings[key], &#34;times&#34;)</pre>
<p>Please ask questions!</p>
<p></p>
<p><a href=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/h6sr8zhfyuw71v/ijxaiooi4tz/IPythonBootcamp.ipynb"" target=""_blank"">IPythonBootcamp.ipynb</a></p>",Python Bootcamp code from class today,"<p>My partner and I are having trouble finding the title for our articles. We managed to get the url (given), and the date (changed the xpath to //entry//url), but we cannot access the title. When we try to look at subchildren, it&#39;s empty. We&#39;ve also tried looking at stackoverflow links provided in the previous piazza post, but were unable to get the title. Is there a good way to go about getting the title of these articles?</p>",XML calls in python,"<p>Hi, I&#39;m not sure if I submitted HW3 correctly. Is there any way I can get a confirmation that my file was received?åÊ</p>",Check Python Bootcamp Submission,"<p>Hi Pr. Callison-Burch,</p>
<p></p>
<p>In lecture today you mentioned that our attendance would be taken in Python bootcamp, and that we&#39;d receive a grade based on it.</p>
<p></p>
<p>I already have significant experience with python; specifically, I&#39;ve been writing Python for over six years, and have already taken several courses that use Python for the assignments (CIS 391, and CIS 419, among others). Is there any chance I can avoid getting penalized without attending the bootcamp?åÊFrom what I understand, this is to ensure that we know enough Python to be able to competently do the homework assignments and the final project; I think that I already have sufficient experience to be able to do so.</p>
<p></p>
<p>Thanks for taking the time to read this!</p>
<p>Sam</p>",Can I get a waiver for attending Python bootcamp?,"<p>Will our output be compared character by character against the sample? For example, does our spacing and line breaks have to be the exact same? Does the order of the output have to be the exact same?</p>",Grading python bootcamp,"<p>I downloaded the newest version of Python (3.5) on Windows but when I go to the command prompt and type in &#34;python --version&#34; as the instructions require, I get a response saying &#34; &#39;python&#39; is not recognized as an internal or external command, operable program or batch file.&#34; However, I am able to open the Python application with no issue. What might be the issue here? Thanks for the help!</p>",Python on Windows,0
940847309,4/26/2016 15:56:34,false,1969401961,,4/26/2016 15:53:47,false,neodev,1.0,28875937,PAK,08,Islamabad,119.153.105.50,0,,"<p>Hi Everyone!</p>
<p></p>
<p>Sorry class got so rushed-- I was hoping to walk through more of the bootcamp. Hopefully you were able to get a good start. We have back to back office hours now and office hoursåÊfrom noon until 8 tomorrow, so come get help early!åÊ</p>
<p></p>
<p>Attached is the iPython notebook from class today, which takes care of question 1 for you. The relevant code is also below. You should be able to recycle it in some shape or form for most of the other questions in the assignment.åÊ</p>
<p></p>
<pre>#This line reads in full text of the file, the splits it up using the newline character (&#39;\n&#39;)
#The return value (stored in wine) is a list of strings, each one corresponding to a line in the file
#Fun fact: This is the same as calling open(&#39;data/wine.txt&#39;).read().split(&#39;\n&#39;)
wine = open(&#39;data/wine.txt&#39;).readlines()

ratings = {} #dictionary keep track of how many times each star value is seen
for line in wine:
    review_and_rating = line.strip().split(&#39;\t&#39;) #split each line into two strings, using the tab character
    stars = review_and_rating[1]
    if stars not in ratings: #check if the key is in the dictionary. If not, add it and give it a count of 0
        print(&#34;adding&#34;, stars, &#34;to ratings&#34;)
        ratings[stars] = 0
    ratings[stars] &#43;= 1 #increment the count by 1
    
for key in ratings:
    print(key, &#34;appeared&#34;, ratings[key], &#34;times&#34;)</pre>
<p>Please ask questions!</p>
<p></p>
<p><a href=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/h6sr8zhfyuw71v/ijxaiooi4tz/IPythonBootcamp.ipynb"" target=""_blank"">IPythonBootcamp.ipynb</a></p>",Python Bootcamp code from class today,"<p>My partner and I are having trouble finding the title for our articles. We managed to get the url (given), and the date (changed the xpath to //entry//url), but we cannot access the title. When we try to look at subchildren, it&#39;s empty. We&#39;ve also tried looking at stackoverflow links provided in the previous piazza post, but were unable to get the title. Is there a good way to go about getting the title of these articles?</p>",XML calls in python,"<p>Hi, I&#39;m not sure if I submitted HW3 correctly. Is there any way I can get a confirmation that my file was received?åÊ</p>",Check Python Bootcamp Submission,"<p>Hi Pr. Callison-Burch,</p>
<p></p>
<p>In lecture today you mentioned that our attendance would be taken in Python bootcamp, and that we&#39;d receive a grade based on it.</p>
<p></p>
<p>I already have significant experience with python; specifically, I&#39;ve been writing Python for over six years, and have already taken several courses that use Python for the assignments (CIS 391, and CIS 419, among others). Is there any chance I can avoid getting penalized without attending the bootcamp?åÊFrom what I understand, this is to ensure that we know enough Python to be able to competently do the homework assignments and the final project; I think that I already have sufficient experience to be able to do so.</p>
<p></p>
<p>Thanks for taking the time to read this!</p>
<p>Sam</p>",Can I get a waiver for attending Python bootcamp?,"<p>Will our output be compared character by character against the sample? For example, does our spacing and line breaks have to be the exact same? Does the order of the output have to be the exact same?</p>",Grading python bootcamp,"<p>I downloaded the newest version of Python (3.5) on Windows but when I go to the command prompt and type in &#34;python --version&#34; as the instructions require, I get a response saying &#34; &#39;python&#39; is not recognized as an internal or external command, operable program or batch file.&#34; However, I am able to open the Python application with no issue. What might be the issue here? Thanks for the help!</p>",Python on Windows,0
940847309,4/26/2016 15:58:11,false,1969402628,,4/26/2016 15:56:24,false,neodev,1.0,13396426,VEN,15,Santa Teresa,190.38.163.149,0,,"<p>Hi Everyone!</p>
<p></p>
<p>Sorry class got so rushed-- I was hoping to walk through more of the bootcamp. Hopefully you were able to get a good start. We have back to back office hours now and office hoursåÊfrom noon until 8 tomorrow, so come get help early!åÊ</p>
<p></p>
<p>Attached is the iPython notebook from class today, which takes care of question 1 for you. The relevant code is also below. You should be able to recycle it in some shape or form for most of the other questions in the assignment.åÊ</p>
<p></p>
<pre>#This line reads in full text of the file, the splits it up using the newline character (&#39;\n&#39;)
#The return value (stored in wine) is a list of strings, each one corresponding to a line in the file
#Fun fact: This is the same as calling open(&#39;data/wine.txt&#39;).read().split(&#39;\n&#39;)
wine = open(&#39;data/wine.txt&#39;).readlines()

ratings = {} #dictionary keep track of how many times each star value is seen
for line in wine:
    review_and_rating = line.strip().split(&#39;\t&#39;) #split each line into two strings, using the tab character
    stars = review_and_rating[1]
    if stars not in ratings: #check if the key is in the dictionary. If not, add it and give it a count of 0
        print(&#34;adding&#34;, stars, &#34;to ratings&#34;)
        ratings[stars] = 0
    ratings[stars] &#43;= 1 #increment the count by 1
    
for key in ratings:
    print(key, &#34;appeared&#34;, ratings[key], &#34;times&#34;)</pre>
<p>Please ask questions!</p>
<p></p>
<p><a href=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/h6sr8zhfyuw71v/ijxaiooi4tz/IPythonBootcamp.ipynb"" target=""_blank"">IPythonBootcamp.ipynb</a></p>",Python Bootcamp code from class today,"<p>My partner and I are having trouble finding the title for our articles. We managed to get the url (given), and the date (changed the xpath to //entry//url), but we cannot access the title. When we try to look at subchildren, it&#39;s empty. We&#39;ve also tried looking at stackoverflow links provided in the previous piazza post, but were unable to get the title. Is there a good way to go about getting the title of these articles?</p>",XML calls in python,"<p>Hi, I&#39;m not sure if I submitted HW3 correctly. Is there any way I can get a confirmation that my file was received?åÊ</p>",Check Python Bootcamp Submission,"<p>Hi Pr. Callison-Burch,</p>
<p></p>
<p>In lecture today you mentioned that our attendance would be taken in Python bootcamp, and that we&#39;d receive a grade based on it.</p>
<p></p>
<p>I already have significant experience with python; specifically, I&#39;ve been writing Python for over six years, and have already taken several courses that use Python for the assignments (CIS 391, and CIS 419, among others). Is there any chance I can avoid getting penalized without attending the bootcamp?åÊFrom what I understand, this is to ensure that we know enough Python to be able to competently do the homework assignments and the final project; I think that I already have sufficient experience to be able to do so.</p>
<p></p>
<p>Thanks for taking the time to read this!</p>
<p>Sam</p>",Can I get a waiver for attending Python bootcamp?,"<p>Will our output be compared character by character against the sample? For example, does our spacing and line breaks have to be the exact same? Does the order of the output have to be the exact same?</p>",Grading python bootcamp,"<p>I downloaded the newest version of Python (3.5) on Windows but when I go to the command prompt and type in &#34;python --version&#34; as the instructions require, I get a response saying &#34; &#39;python&#39; is not recognized as an internal or external command, operable program or batch file.&#34; However, I am able to open the Python application with no issue. What might be the issue here? Thanks for the help!</p>",Python on Windows,0
940847309,4/26/2016 16:04:22,false,1969405943,,4/26/2016 15:56:16,false,neodev,0.8889,21971187,TTO,08,Valsayn,190.213.132.190,0,,"<p>Hi Everyone!</p>
<p></p>
<p>Sorry class got so rushed-- I was hoping to walk through more of the bootcamp. Hopefully you were able to get a good start. We have back to back office hours now and office hoursåÊfrom noon until 8 tomorrow, so come get help early!åÊ</p>
<p></p>
<p>Attached is the iPython notebook from class today, which takes care of question 1 for you. The relevant code is also below. You should be able to recycle it in some shape or form for most of the other questions in the assignment.åÊ</p>
<p></p>
<pre>#This line reads in full text of the file, the splits it up using the newline character (&#39;\n&#39;)
#The return value (stored in wine) is a list of strings, each one corresponding to a line in the file
#Fun fact: This is the same as calling open(&#39;data/wine.txt&#39;).read().split(&#39;\n&#39;)
wine = open(&#39;data/wine.txt&#39;).readlines()

ratings = {} #dictionary keep track of how many times each star value is seen
for line in wine:
    review_and_rating = line.strip().split(&#39;\t&#39;) #split each line into two strings, using the tab character
    stars = review_and_rating[1]
    if stars not in ratings: #check if the key is in the dictionary. If not, add it and give it a count of 0
        print(&#34;adding&#34;, stars, &#34;to ratings&#34;)
        ratings[stars] = 0
    ratings[stars] &#43;= 1 #increment the count by 1
    
for key in ratings:
    print(key, &#34;appeared&#34;, ratings[key], &#34;times&#34;)</pre>
<p>Please ask questions!</p>
<p></p>
<p><a href=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/h6sr8zhfyuw71v/ijxaiooi4tz/IPythonBootcamp.ipynb"" target=""_blank"">IPythonBootcamp.ipynb</a></p>",Python Bootcamp code from class today,"<p>My partner and I are having trouble finding the title for our articles. We managed to get the url (given), and the date (changed the xpath to //entry//url), but we cannot access the title. When we try to look at subchildren, it&#39;s empty. We&#39;ve also tried looking at stackoverflow links provided in the previous piazza post, but were unable to get the title. Is there a good way to go about getting the title of these articles?</p>",XML calls in python,"<p>Hi, I&#39;m not sure if I submitted HW3 correctly. Is there any way I can get a confirmation that my file was received?åÊ</p>",Check Python Bootcamp Submission,"<p>Hi Pr. Callison-Burch,</p>
<p></p>
<p>In lecture today you mentioned that our attendance would be taken in Python bootcamp, and that we&#39;d receive a grade based on it.</p>
<p></p>
<p>I already have significant experience with python; specifically, I&#39;ve been writing Python for over six years, and have already taken several courses that use Python for the assignments (CIS 391, and CIS 419, among others). Is there any chance I can avoid getting penalized without attending the bootcamp?åÊFrom what I understand, this is to ensure that we know enough Python to be able to competently do the homework assignments and the final project; I think that I already have sufficient experience to be able to do so.</p>
<p></p>
<p>Thanks for taking the time to read this!</p>
<p>Sam</p>",Can I get a waiver for attending Python bootcamp?,"<p>Will our output be compared character by character against the sample? For example, does our spacing and line breaks have to be the exact same? Does the order of the output have to be the exact same?</p>",Grading python bootcamp,"<p>I downloaded the newest version of Python (3.5) on Windows but when I go to the command prompt and type in &#34;python --version&#34; as the instructions require, I get a response saying &#34; &#39;python&#39; is not recognized as an internal or external command, operable program or batch file.&#34; However, I am able to open the Python application with no issue. What might be the issue here? Thanks for the help!</p>",Python on Windows,0
940847309,4/26/2016 16:19:52,false,1969415127,,4/26/2016 15:56:52,false,neodev,0.7778,32569659,USA,MN,Minneapolis,97.127.88.224,"1
4",,"<p>Hi Everyone!</p>
<p></p>
<p>Sorry class got so rushed-- I was hoping to walk through more of the bootcamp. Hopefully you were able to get a good start. We have back to back office hours now and office hoursåÊfrom noon until 8 tomorrow, so come get help early!åÊ</p>
<p></p>
<p>Attached is the iPython notebook from class today, which takes care of question 1 for you. The relevant code is also below. You should be able to recycle it in some shape or form for most of the other questions in the assignment.åÊ</p>
<p></p>
<pre>#This line reads in full text of the file, the splits it up using the newline character (&#39;\n&#39;)
#The return value (stored in wine) is a list of strings, each one corresponding to a line in the file
#Fun fact: This is the same as calling open(&#39;data/wine.txt&#39;).read().split(&#39;\n&#39;)
wine = open(&#39;data/wine.txt&#39;).readlines()

ratings = {} #dictionary keep track of how many times each star value is seen
for line in wine:
    review_and_rating = line.strip().split(&#39;\t&#39;) #split each line into two strings, using the tab character
    stars = review_and_rating[1]
    if stars not in ratings: #check if the key is in the dictionary. If not, add it and give it a count of 0
        print(&#34;adding&#34;, stars, &#34;to ratings&#34;)
        ratings[stars] = 0
    ratings[stars] &#43;= 1 #increment the count by 1
    
for key in ratings:
    print(key, &#34;appeared&#34;, ratings[key], &#34;times&#34;)</pre>
<p>Please ask questions!</p>
<p></p>
<p><a href=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/h6sr8zhfyuw71v/ijxaiooi4tz/IPythonBootcamp.ipynb"" target=""_blank"">IPythonBootcamp.ipynb</a></p>",Python Bootcamp code from class today,"<p>My partner and I are having trouble finding the title for our articles. We managed to get the url (given), and the date (changed the xpath to //entry//url), but we cannot access the title. When we try to look at subchildren, it&#39;s empty. We&#39;ve also tried looking at stackoverflow links provided in the previous piazza post, but were unable to get the title. Is there a good way to go about getting the title of these articles?</p>",XML calls in python,"<p>Hi, I&#39;m not sure if I submitted HW3 correctly. Is there any way I can get a confirmation that my file was received?åÊ</p>",Check Python Bootcamp Submission,"<p>Hi Pr. Callison-Burch,</p>
<p></p>
<p>In lecture today you mentioned that our attendance would be taken in Python bootcamp, and that we&#39;d receive a grade based on it.</p>
<p></p>
<p>I already have significant experience with python; specifically, I&#39;ve been writing Python for over six years, and have already taken several courses that use Python for the assignments (CIS 391, and CIS 419, among others). Is there any chance I can avoid getting penalized without attending the bootcamp?åÊFrom what I understand, this is to ensure that we know enough Python to be able to competently do the homework assignments and the final project; I think that I already have sufficient experience to be able to do so.</p>
<p></p>
<p>Thanks for taking the time to read this!</p>
<p>Sam</p>",Can I get a waiver for attending Python bootcamp?,"<p>Will our output be compared character by character against the sample? For example, does our spacing and line breaks have to be the exact same? Does the order of the output have to be the exact same?</p>",Grading python bootcamp,"<p>I downloaded the newest version of Python (3.5) on Windows but when I go to the command prompt and type in &#34;python --version&#34; as the instructions require, I get a response saying &#34; &#39;python&#39; is not recognized as an internal or external command, operable program or batch file.&#34; However, I am able to open the Python application with no issue. What might be the issue here? Thanks for the help!</p>",Python on Windows,0
940847310,4/26/2016 15:59:59,false,1969403527,,4/26/2016 15:58:26,false,elite,1.0,33243069,IND,10,Faridabad,116.203.79.150,0,,"<p>What&#39;s a rough number for an expected kappa value for urls? The example shown gives a value of 0.969854, but running the kappa on my results for the majority and gold files is not nearly as accurate<code></code></p>",Rough number for kappa value,"<p>For the output that should look likeåÊ</p>
<pre>-	17<br />lovely	13<br />lovely.	11<br />bare	9<br />really	8<br />fine	7<br />fruit.	6<br />nose.	6<br />quite	6<br />long.	5</pre>
<p>I am getting</p>
<pre>-	17<br />lovely	13<br />lovely.	11<br />bare	9<br />really	8<br />fine	7<br />fruit.	6<br />quite	6<br />nose.	6<br />touch	5</pre>
<p>touch occurs the same number of times as long., the only difference I think is the way my dictionary is ordered and thus the word I choose for that position. Is this ok?</p>",Same number of occurences for a given word,"<p>Hi i&#39;m confused by the following statement in the homework:</p>
<p></p>
<p><strong>5. Gather the positive predictionsåÊ</strong>&#34;You now have three parallel files, each with the same number of lines in it:åÊ<code>articles.txt</code>,åÊ<code>urls.txt</code>, andåÊ<code>classifier_predictions.txt&#34;</code></p>
<p></p>
<p>I can understand whyåÊ<code>urls.txt</code>, andåÊ<code>classifier_predictions.txtåÊ</code>have the same number of lines (because they describe the articles that we found), but I don&#39;t understand howåÊarticles.txt would be the same size. I thought these were the 70,000åÊtraining data articles?</p>","number of lines in articles.txt, urls.txt, and classifier_predictions.txt","<p>I am an international student and don&#39;t have a SSN. Is it okay to use an account created by someone, who is not in the class, but has a SSN?</p>",Social Security Number,"<p>I was just wondering if we should be getting some crazy numbers for the time to annotate 1 million articles. Like for the open-ended hit, it took 8 hours to get all 100 results. Meaning that for 1m articles, it would 80,000 hours. Is that okay, or am I doing the wrong math?</p>","Crazy numbers for estimating time spent for 1,000,000 articles","<p>Would it be possible to verify the number of occurrences of the word &#34;red&#34;, and the word &#34;white&#34;?</p>
<p></p>
<p>I seem to be only coming across 8 instances of &#34;red&#34; and 13 instances of &#34;white&#34;, without any filtering, whereas the answer key seems to indicate that there are at least 11 instances of &#34;red&#34; and 16 instances of &#34;white&#34;.</p>",Number of &#34;red&#34; lines and &#34;white&#34; lines,0
940847310,4/26/2016 16:01:46,false,1969404624,,4/26/2016 16:01:11,false,personaly,1.0,33663352,ARG,01,Mar Del Plata,181.168.213.227,0,,"<p>What&#39;s a rough number for an expected kappa value for urls? The example shown gives a value of 0.969854, but running the kappa on my results for the majority and gold files is not nearly as accurate<code></code></p>",Rough number for kappa value,"<p>For the output that should look likeåÊ</p>
<pre>-	17<br />lovely	13<br />lovely.	11<br />bare	9<br />really	8<br />fine	7<br />fruit.	6<br />nose.	6<br />quite	6<br />long.	5</pre>
<p>I am getting</p>
<pre>-	17<br />lovely	13<br />lovely.	11<br />bare	9<br />really	8<br />fine	7<br />fruit.	6<br />quite	6<br />nose.	6<br />touch	5</pre>
<p>touch occurs the same number of times as long., the only difference I think is the way my dictionary is ordered and thus the word I choose for that position. Is this ok?</p>",Same number of occurences for a given word,"<p>Hi i&#39;m confused by the following statement in the homework:</p>
<p></p>
<p><strong>5. Gather the positive predictionsåÊ</strong>&#34;You now have three parallel files, each with the same number of lines in it:åÊ<code>articles.txt</code>,åÊ<code>urls.txt</code>, andåÊ<code>classifier_predictions.txt&#34;</code></p>
<p></p>
<p>I can understand whyåÊ<code>urls.txt</code>, andåÊ<code>classifier_predictions.txtåÊ</code>have the same number of lines (because they describe the articles that we found), but I don&#39;t understand howåÊarticles.txt would be the same size. I thought these were the 70,000åÊtraining data articles?</p>","number of lines in articles.txt, urls.txt, and classifier_predictions.txt","<p>I am an international student and don&#39;t have a SSN. Is it okay to use an account created by someone, who is not in the class, but has a SSN?</p>",Social Security Number,"<p>I was just wondering if we should be getting some crazy numbers for the time to annotate 1 million articles. Like for the open-ended hit, it took 8 hours to get all 100 results. Meaning that for 1m articles, it would 80,000 hours. Is that okay, or am I doing the wrong math?</p>","Crazy numbers for estimating time spent for 1,000,000 articles","<p>Would it be possible to verify the number of occurrences of the word &#34;red&#34;, and the word &#34;white&#34;?</p>
<p></p>
<p>I seem to be only coming across 8 instances of &#34;red&#34; and 13 instances of &#34;white&#34;, without any filtering, whereas the answer key seems to indicate that there are at least 11 instances of &#34;red&#34; and 16 instances of &#34;white&#34;.</p>",Number of &#34;red&#34; lines and &#34;white&#34; lines,0
940847310,4/26/2016 16:09:03,false,1969408355,,4/26/2016 16:03:36,false,clixsense,0.8889,8057247,PRT,17,Póvoa De Varzim,144.64.25.68,0,,"<p>What&#39;s a rough number for an expected kappa value for urls? The example shown gives a value of 0.969854, but running the kappa on my results for the majority and gold files is not nearly as accurate<code></code></p>",Rough number for kappa value,"<p>For the output that should look likeåÊ</p>
<pre>-	17<br />lovely	13<br />lovely.	11<br />bare	9<br />really	8<br />fine	7<br />fruit.	6<br />nose.	6<br />quite	6<br />long.	5</pre>
<p>I am getting</p>
<pre>-	17<br />lovely	13<br />lovely.	11<br />bare	9<br />really	8<br />fine	7<br />fruit.	6<br />quite	6<br />nose.	6<br />touch	5</pre>
<p>touch occurs the same number of times as long., the only difference I think is the way my dictionary is ordered and thus the word I choose for that position. Is this ok?</p>",Same number of occurences for a given word,"<p>Hi i&#39;m confused by the following statement in the homework:</p>
<p></p>
<p><strong>5. Gather the positive predictionsåÊ</strong>&#34;You now have three parallel files, each with the same number of lines in it:åÊ<code>articles.txt</code>,åÊ<code>urls.txt</code>, andåÊ<code>classifier_predictions.txt&#34;</code></p>
<p></p>
<p>I can understand whyåÊ<code>urls.txt</code>, andåÊ<code>classifier_predictions.txtåÊ</code>have the same number of lines (because they describe the articles that we found), but I don&#39;t understand howåÊarticles.txt would be the same size. I thought these were the 70,000åÊtraining data articles?</p>","number of lines in articles.txt, urls.txt, and classifier_predictions.txt","<p>I am an international student and don&#39;t have a SSN. Is it okay to use an account created by someone, who is not in the class, but has a SSN?</p>",Social Security Number,"<p>I was just wondering if we should be getting some crazy numbers for the time to annotate 1 million articles. Like for the open-ended hit, it took 8 hours to get all 100 results. Meaning that for 1m articles, it would 80,000 hours. Is that okay, or am I doing the wrong math?</p>","Crazy numbers for estimating time spent for 1,000,000 articles","<p>Would it be possible to verify the number of occurrences of the word &#34;red&#34;, and the word &#34;white&#34;?</p>
<p></p>
<p>I seem to be only coming across 8 instances of &#34;red&#34; and 13 instances of &#34;white&#34;, without any filtering, whereas the answer key seems to indicate that there are at least 11 instances of &#34;red&#34; and 16 instances of &#34;white&#34;.</p>",Number of &#34;red&#34; lines and &#34;white&#34; lines,0
940847310,4/26/2016 16:24:40,false,1969419122,,4/26/2016 16:04:24,false,neodev,0.8889,21971187,TTO,08,Valsayn,190.213.132.190,0,,"<p>What&#39;s a rough number for an expected kappa value for urls? The example shown gives a value of 0.969854, but running the kappa on my results for the majority and gold files is not nearly as accurate<code></code></p>",Rough number for kappa value,"<p>For the output that should look likeåÊ</p>
<pre>-	17<br />lovely	13<br />lovely.	11<br />bare	9<br />really	8<br />fine	7<br />fruit.	6<br />nose.	6<br />quite	6<br />long.	5</pre>
<p>I am getting</p>
<pre>-	17<br />lovely	13<br />lovely.	11<br />bare	9<br />really	8<br />fine	7<br />fruit.	6<br />quite	6<br />nose.	6<br />touch	5</pre>
<p>touch occurs the same number of times as long., the only difference I think is the way my dictionary is ordered and thus the word I choose for that position. Is this ok?</p>",Same number of occurences for a given word,"<p>Hi i&#39;m confused by the following statement in the homework:</p>
<p></p>
<p><strong>5. Gather the positive predictionsåÊ</strong>&#34;You now have three parallel files, each with the same number of lines in it:åÊ<code>articles.txt</code>,åÊ<code>urls.txt</code>, andåÊ<code>classifier_predictions.txt&#34;</code></p>
<p></p>
<p>I can understand whyåÊ<code>urls.txt</code>, andåÊ<code>classifier_predictions.txtåÊ</code>have the same number of lines (because they describe the articles that we found), but I don&#39;t understand howåÊarticles.txt would be the same size. I thought these were the 70,000åÊtraining data articles?</p>","number of lines in articles.txt, urls.txt, and classifier_predictions.txt","<p>I am an international student and don&#39;t have a SSN. Is it okay to use an account created by someone, who is not in the class, but has a SSN?</p>",Social Security Number,"<p>I was just wondering if we should be getting some crazy numbers for the time to annotate 1 million articles. Like for the open-ended hit, it took 8 hours to get all 100 results. Meaning that for 1m articles, it would 80,000 hours. Is that okay, or am I doing the wrong math?</p>","Crazy numbers for estimating time spent for 1,000,000 articles","<p>Would it be possible to verify the number of occurrences of the word &#34;red&#34;, and the word &#34;white&#34;?</p>
<p></p>
<p>I seem to be only coming across 8 instances of &#34;red&#34; and 13 instances of &#34;white&#34;, without any filtering, whereas the answer key seems to indicate that there are at least 11 instances of &#34;red&#34; and 16 instances of &#34;white&#34;.</p>",Number of &#34;red&#34; lines and &#34;white&#34; lines,0
940847310,4/26/2016 16:40:10,false,1969429585,,4/26/2016 16:37:31,false,neodev,0.7778,32569659,USA,MN,Minneapolis,97.127.88.224,0,,"<p>What&#39;s a rough number for an expected kappa value for urls? The example shown gives a value of 0.969854, but running the kappa on my results for the majority and gold files is not nearly as accurate<code></code></p>",Rough number for kappa value,"<p>For the output that should look likeåÊ</p>
<pre>-	17<br />lovely	13<br />lovely.	11<br />bare	9<br />really	8<br />fine	7<br />fruit.	6<br />nose.	6<br />quite	6<br />long.	5</pre>
<p>I am getting</p>
<pre>-	17<br />lovely	13<br />lovely.	11<br />bare	9<br />really	8<br />fine	7<br />fruit.	6<br />quite	6<br />nose.	6<br />touch	5</pre>
<p>touch occurs the same number of times as long., the only difference I think is the way my dictionary is ordered and thus the word I choose for that position. Is this ok?</p>",Same number of occurences for a given word,"<p>Hi i&#39;m confused by the following statement in the homework:</p>
<p></p>
<p><strong>5. Gather the positive predictionsåÊ</strong>&#34;You now have three parallel files, each with the same number of lines in it:åÊ<code>articles.txt</code>,åÊ<code>urls.txt</code>, andåÊ<code>classifier_predictions.txt&#34;</code></p>
<p></p>
<p>I can understand whyåÊ<code>urls.txt</code>, andåÊ<code>classifier_predictions.txtåÊ</code>have the same number of lines (because they describe the articles that we found), but I don&#39;t understand howåÊarticles.txt would be the same size. I thought these were the 70,000åÊtraining data articles?</p>","number of lines in articles.txt, urls.txt, and classifier_predictions.txt","<p>I am an international student and don&#39;t have a SSN. Is it okay to use an account created by someone, who is not in the class, but has a SSN?</p>",Social Security Number,"<p>I was just wondering if we should be getting some crazy numbers for the time to annotate 1 million articles. Like for the open-ended hit, it took 8 hours to get all 100 results. Meaning that for 1m articles, it would 80,000 hours. Is that okay, or am I doing the wrong math?</p>","Crazy numbers for estimating time spent for 1,000,000 articles","<p>Would it be possible to verify the number of occurrences of the word &#34;red&#34;, and the word &#34;white&#34;?</p>
<p></p>
<p>I seem to be only coming across 8 instances of &#34;red&#34; and 13 instances of &#34;white&#34;, without any filtering, whereas the answer key seems to indicate that there are at least 11 instances of &#34;red&#34; and 16 instances of &#34;white&#34;.</p>",Number of &#34;red&#34; lines and &#34;white&#34; lines,0
940847311,4/26/2016 17:32:34,false,1969460926,,4/26/2016 17:32:18,false,neodev,0.8889,33131546,IDN,04,Jakarta,139.194.89.60,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>Hi! The survey dropdown to choose which project you&#39;re reviewing is blank ÛÒ how should we specify the projects?</p>,Survey dropdown,4
940847311,4/26/2016 17:36:01,false,1969462742,,4/26/2016 17:35:20,false,clixsense,1.0,30712378,ROU,21,Deva,79.119.241.200,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>Hi! The survey dropdown to choose which project you&#39;re reviewing is blank ÛÒ how should we specify the projects?</p>,Survey dropdown,4
940847311,4/26/2016 17:38:55,false,1969464253,,4/26/2016 17:36:08,false,neodev,0.8889,33568303,VEN,23,Cabimas,190.77.7.36,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>Hi! The survey dropdown to choose which project you&#39;re reviewing is blank ÛÒ how should we specify the projects?</p>,Survey dropdown,4
940847311,4/26/2016 17:45:14,false,1969468162,,4/26/2016 17:39:40,false,clixsense,1.0,35444326,BRA,07,Brasília,177.15.130.106,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>Hi! The survey dropdown to choose which project you&#39;re reviewing is blank ÛÒ how should we specify the projects?</p>,Survey dropdown,4
940847311,4/26/2016 18:19:09,false,1969486213,,4/26/2016 18:16:24,false,neodev,0.8889,35550011,VEN,07,Valencia,190.204.238.112,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>Hi! The survey dropdown to choose which project you&#39;re reviewing is blank ÛÒ how should we specify the projects?</p>,Survey dropdown,4
940847312,4/26/2016 15:52:33,false,1969400281,,4/26/2016 15:50:49,false,elite,0.8889,36575101,IND,07,New Delhi,112.196.144.2,0,,"<script type=""text/javascript"">PA.load(""/dashboard/tips_tricks"", null, function(data){ $('#' + 'questionText').html(data);});</script>",Tips & Tricks for a successful class,<p>Hello! Just curious: is there any benefit in coming to class tomorrow if you already have a full team? Or is the team-matchmakingåÊthe only activity planned for tomorrow?</p>,Wednesday&#39;s Class,"<p>The course selection period ends today. åÊIf you were shopping NETS 213 and do not intend to take it, please drop it today so that one of the 10 students on the waitlist can take your place.</p>
<p></p>
<p></p>",Course selection period ends today,"<p>Just wondering, will we have class on the Friday before Spring Break?</p>",Class on Friday Before Spring Break,"<p>We are going to use Wednesday&#39;s NETS 213 class periodåÊto form teams for the final projects. åÊIf you do not yet have a team, or if your team has fewer than 3 team members, then you are required to attend tomorrow&#39;s class and form a team. åÊIf your team only has 3 members then you&#39;re encouraged to go and see if you can pick up 1 or 2 new teammates.åÊ</p>
<p></p>
<p>I&#39;m traveling for a DARPA PI meeting, so the TAs will run the session. åÊYou&#39;ll be asked to introduce yourselves (names, degree, year, etc), and say oneåÊproject idea that you have (since the first team assignment is to brainstorm 3 ideas). åÊYou should pair up with people by the end of class. åÊDon&#39;tåÊleaveåÊwithout joiningåÊa team.åÊ</p>
<p></p>
<p></p>",Find a team during Wednesday&#39;s class,"<p>Hi,</p>
<p></p>
<p>I emailed the course staff, but I&#39;m not sure if my email went through. I just wanted to write to you to let you know what I woke up with a stomach bug today and so I don&#39;t think I will be able to attend class. If there is attendance taken in class, will I be excused from it?</p>
<p></p>
<p>Best,</p>
<p>Ben</p>",Class Attendance Today,4
940847312,4/26/2016 15:53:45,false,1969400805,,4/26/2016 15:51:13,false,neodev,1.0,28875937,PAK,08,Islamabad,119.153.105.50,0,,"<script type=""text/javascript"">PA.load(""/dashboard/tips_tricks"", null, function(data){ $('#' + 'questionText').html(data);});</script>",Tips & Tricks for a successful class,<p>Hello! Just curious: is there any benefit in coming to class tomorrow if you already have a full team? Or is the team-matchmakingåÊthe only activity planned for tomorrow?</p>,Wednesday&#39;s Class,"<p>The course selection period ends today. åÊIf you were shopping NETS 213 and do not intend to take it, please drop it today so that one of the 10 students on the waitlist can take your place.</p>
<p></p>
<p></p>",Course selection period ends today,"<p>Just wondering, will we have class on the Friday before Spring Break?</p>",Class on Friday Before Spring Break,"<p>We are going to use Wednesday&#39;s NETS 213 class periodåÊto form teams for the final projects. åÊIf you do not yet have a team, or if your team has fewer than 3 team members, then you are required to attend tomorrow&#39;s class and form a team. åÊIf your team only has 3 members then you&#39;re encouraged to go and see if you can pick up 1 or 2 new teammates.åÊ</p>
<p></p>
<p>I&#39;m traveling for a DARPA PI meeting, so the TAs will run the session. åÊYou&#39;ll be asked to introduce yourselves (names, degree, year, etc), and say oneåÊproject idea that you have (since the first team assignment is to brainstorm 3 ideas). åÊYou should pair up with people by the end of class. åÊDon&#39;tåÊleaveåÊwithout joiningåÊa team.åÊ</p>
<p></p>
<p></p>",Find a team during Wednesday&#39;s class,"<p>Hi,</p>
<p></p>
<p>I emailed the course staff, but I&#39;m not sure if my email went through. I just wanted to write to you to let you know what I woke up with a stomach bug today and so I don&#39;t think I will be able to attend class. If there is attendance taken in class, will I be excused from it?</p>
<p></p>
<p>Best,</p>
<p>Ben</p>",Class Attendance Today,4
940847312,4/26/2016 15:56:15,false,1969401846,,4/26/2016 15:48:57,false,neodev,0.8889,21971187,TTO,08,Valsayn,190.213.132.190,0,,"<script type=""text/javascript"">PA.load(""/dashboard/tips_tricks"", null, function(data){ $('#' + 'questionText').html(data);});</script>",Tips & Tricks for a successful class,<p>Hello! Just curious: is there any benefit in coming to class tomorrow if you already have a full team? Or is the team-matchmakingåÊthe only activity planned for tomorrow?</p>,Wednesday&#39;s Class,"<p>The course selection period ends today. åÊIf you were shopping NETS 213 and do not intend to take it, please drop it today so that one of the 10 students on the waitlist can take your place.</p>
<p></p>
<p></p>",Course selection period ends today,"<p>Just wondering, will we have class on the Friday before Spring Break?</p>",Class on Friday Before Spring Break,"<p>We are going to use Wednesday&#39;s NETS 213 class periodåÊto form teams for the final projects. åÊIf you do not yet have a team, or if your team has fewer than 3 team members, then you are required to attend tomorrow&#39;s class and form a team. åÊIf your team only has 3 members then you&#39;re encouraged to go and see if you can pick up 1 or 2 new teammates.åÊ</p>
<p></p>
<p>I&#39;m traveling for a DARPA PI meeting, so the TAs will run the session. åÊYou&#39;ll be asked to introduce yourselves (names, degree, year, etc), and say oneåÊproject idea that you have (since the first team assignment is to brainstorm 3 ideas). åÊYou should pair up with people by the end of class. åÊDon&#39;tåÊleaveåÊwithout joiningåÊa team.åÊ</p>
<p></p>
<p></p>",Find a team during Wednesday&#39;s class,"<p>Hi,</p>
<p></p>
<p>I emailed the course staff, but I&#39;m not sure if my email went through. I just wanted to write to you to let you know what I woke up with a stomach bug today and so I don&#39;t think I will be able to attend class. If there is attendance taken in class, will I be excused from it?</p>
<p></p>
<p>Best,</p>
<p>Ben</p>",Class Attendance Today,4
940847312,4/26/2016 15:56:23,false,1969401925,,4/26/2016 15:53:18,false,neodev,1.0,13396426,VEN,15,Santa Teresa,190.38.163.149,0,,"<script type=""text/javascript"">PA.load(""/dashboard/tips_tricks"", null, function(data){ $('#' + 'questionText').html(data);});</script>",Tips & Tricks for a successful class,<p>Hello! Just curious: is there any benefit in coming to class tomorrow if you already have a full team? Or is the team-matchmakingåÊthe only activity planned for tomorrow?</p>,Wednesday&#39;s Class,"<p>The course selection period ends today. åÊIf you were shopping NETS 213 and do not intend to take it, please drop it today so that one of the 10 students on the waitlist can take your place.</p>
<p></p>
<p></p>",Course selection period ends today,"<p>Just wondering, will we have class on the Friday before Spring Break?</p>",Class on Friday Before Spring Break,"<p>We are going to use Wednesday&#39;s NETS 213 class periodåÊto form teams for the final projects. åÊIf you do not yet have a team, or if your team has fewer than 3 team members, then you are required to attend tomorrow&#39;s class and form a team. åÊIf your team only has 3 members then you&#39;re encouraged to go and see if you can pick up 1 or 2 new teammates.åÊ</p>
<p></p>
<p>I&#39;m traveling for a DARPA PI meeting, so the TAs will run the session. åÊYou&#39;ll be asked to introduce yourselves (names, degree, year, etc), and say oneåÊproject idea that you have (since the first team assignment is to brainstorm 3 ideas). åÊYou should pair up with people by the end of class. åÊDon&#39;tåÊleaveåÊwithout joiningåÊa team.åÊ</p>
<p></p>
<p></p>",Find a team during Wednesday&#39;s class,"<p>Hi,</p>
<p></p>
<p>I emailed the course staff, but I&#39;m not sure if my email went through. I just wanted to write to you to let you know what I woke up with a stomach bug today and so I don&#39;t think I will be able to attend class. If there is attendance taken in class, will I be excused from it?</p>
<p></p>
<p>Best,</p>
<p>Ben</p>",Class Attendance Today,4
940847312,4/26/2016 16:00:16,false,1969403679,,4/26/2016 15:56:11,false,clixsense,1.0,21875134,GBR,H9,London,87.112.158.81,0,,"<script type=""text/javascript"">PA.load(""/dashboard/tips_tricks"", null, function(data){ $('#' + 'questionText').html(data);});</script>",Tips & Tricks for a successful class,<p>Hello! Just curious: is there any benefit in coming to class tomorrow if you already have a full team? Or is the team-matchmakingåÊthe only activity planned for tomorrow?</p>,Wednesday&#39;s Class,"<p>The course selection period ends today. åÊIf you were shopping NETS 213 and do not intend to take it, please drop it today so that one of the 10 students on the waitlist can take your place.</p>
<p></p>
<p></p>",Course selection period ends today,"<p>Just wondering, will we have class on the Friday before Spring Break?</p>",Class on Friday Before Spring Break,"<p>We are going to use Wednesday&#39;s NETS 213 class periodåÊto form teams for the final projects. åÊIf you do not yet have a team, or if your team has fewer than 3 team members, then you are required to attend tomorrow&#39;s class and form a team. åÊIf your team only has 3 members then you&#39;re encouraged to go and see if you can pick up 1 or 2 new teammates.åÊ</p>
<p></p>
<p>I&#39;m traveling for a DARPA PI meeting, so the TAs will run the session. åÊYou&#39;ll be asked to introduce yourselves (names, degree, year, etc), and say oneåÊproject idea that you have (since the first team assignment is to brainstorm 3 ideas). åÊYou should pair up with people by the end of class. åÊDon&#39;tåÊleaveåÊwithout joiningåÊa team.åÊ</p>
<p></p>
<p></p>",Find a team during Wednesday&#39;s class,"<p>Hi,</p>
<p></p>
<p>I emailed the course staff, but I&#39;m not sure if my email went through. I just wanted to write to you to let you know what I woke up with a stomach bug today and so I don&#39;t think I will be able to attend class. If there is attendance taken in class, will I be excused from it?</p>
<p></p>
<p>Best,</p>
<p>Ben</p>",Class Attendance Today,4
940847313,4/26/2016 15:27:29,false,1969389365,,4/26/2016 15:26:56,false,instagc,0.8889,13581319,USA,IL,Waltonville,208.70.36.12,0,,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"<p>When I runåÊ</p>
<p>$ cat bing_urls.txt | python get_clean_text.py &gt; articles_and_urls.txt</p>
<p></p>
<p>I keep on getting this error</p>
<p></p>
<p>urllib2.URLError: &lt;urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:590)&gt;</p>
<p></p>
<p>What should I do?</p>",Error scraping url texts,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,1
940847313,4/26/2016 15:28:58,false,1969390420,,4/26/2016 15:27:17,false,elite,1.0,30280423,ITA,15,Siracusa,151.54.84.121,0,,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"<p>When I runåÊ</p>
<p>$ cat bing_urls.txt | python get_clean_text.py &gt; articles_and_urls.txt</p>
<p></p>
<p>I keep on getting this error</p>
<p></p>
<p>urllib2.URLError: &lt;urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:590)&gt;</p>
<p></p>
<p>What should I do?</p>",Error scraping url texts,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,1
940847313,4/26/2016 15:30:05,false,1969390868,,4/26/2016 15:26:19,false,clixsense,0.8889,36052512,PHL,F2,Quezon City,49.149.150.150,0,,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"<p>When I runåÊ</p>
<p>$ cat bing_urls.txt | python get_clean_text.py &gt; articles_and_urls.txt</p>
<p></p>
<p>I keep on getting this error</p>
<p></p>
<p>urllib2.URLError: &lt;urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:590)&gt;</p>
<p></p>
<p>What should I do?</p>",Error scraping url texts,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,1
940847313,4/26/2016 15:51:22,false,1969399815,,4/26/2016 15:36:03,false,neodev,1.0,13396426,VEN,15,Santa Teresa,190.38.163.149,0,,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"<p>When I runåÊ</p>
<p>$ cat bing_urls.txt | python get_clean_text.py &gt; articles_and_urls.txt</p>
<p></p>
<p>I keep on getting this error</p>
<p></p>
<p>urllib2.URLError: &lt;urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:590)&gt;</p>
<p></p>
<p>What should I do?</p>",Error scraping url texts,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,1
940847313,4/26/2016 15:58:24,false,1969402712,,4/26/2016 15:56:48,false,elite,1.0,33243069,IND,10,Faridabad,116.203.79.150,0,,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"<p>When I runåÊ</p>
<p>$ cat bing_urls.txt | python get_clean_text.py &gt; articles_and_urls.txt</p>
<p></p>
<p>I keep on getting this error</p>
<p></p>
<p>urllib2.URLError: &lt;urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:590)&gt;</p>
<p></p>
<p>What should I do?</p>",Error scraping url texts,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,1
940847314,4/26/2016 15:27:29,false,1969389362,,4/26/2016 15:26:56,false,instagc,0.8889,13581319,USA,IL,Waltonville,208.70.36.12,0,,"<p>I got .5 points off &#34;for not using the crawler to collect URLs in addition to the Bing API&#34; -- my collected URLs included URLs from the Gun Violence Archive, and I collected the rest using the Bing API.åÊ</p>
<p></p>
<p>The instructions didn&#39;t specify that we necessarily needed to use other methods to get 2500 URLs. The homework says to &#34;turn in a list of at least 2,500 urls, including some crawled from the Gun Violence Archive and some obtained using the Bing API.&#34;åÊ</p>
<p></p>
<p>Am I understanding correctly why I got the points off? If so, I&#39;m confused why I got points off given the instructions. Thank you!åÊ</p>",Question about grade,"<p>I&#39;m having an issue in the last part of the assignment creating the questions that require information from the spreadsheet. For example, for the question &#34;In what city did the incident take place?&#34; I am trying to add {{city_1}} as an option in the question so that the information from the spreadsheet will populate that field, but I get this error: &#34;&#39;{{city_1}}&#39; will show up in your results as &#39;city_1&#39;, which already exists as a header in your uploaded data.&#34; How can I achieve what is supposed to happen, instead of getting this error?</p>
<p></p>
<p>Thanks!</p>",Crowdflower question creation,<p>I&#39;m working on one of the iterativeåÊimage annotation assignment...I&#39;m having trouble understanding how to implement the iterative component to designing a task. Are there any hints/advice on where to start?åÊ</p>,Iterative questions on CrowdFlower,<p>Crowdflower recommends 13 Test Questions. Is this number ok?</p>,How many test questions on Crowdflower?,"<p>My partner and I are signed up for the Netflix Prize problem but we are running into questions like &#34;what company are you profiling?&#34; Should we be focusing more on Netflix or the Netflix Prize itself?</p>
<p></p>
<p>Thanks!</p>",Question about survey,<p>What exactly is meant by the words in the &#34;red&#34; reviews that DO NOT appear in the &#34;white&#34; reviews? Does this mean we should not count a word that appears in a red review and at least oneåÊwhite review? Or is this simply a check for reviews that have both &#34;red&#34; and &#34;white&#34; in them (and thus we can&#39;t classify the words as either red or white)?åÊ</p>,Questions 9 &amp; 10: Red and White reviews,4
940847314,4/26/2016 15:28:58,false,1969390412,,4/26/2016 15:27:17,false,elite,1.0,30280423,ITA,15,Siracusa,151.54.84.121,0,,"<p>I got .5 points off &#34;for not using the crawler to collect URLs in addition to the Bing API&#34; -- my collected URLs included URLs from the Gun Violence Archive, and I collected the rest using the Bing API.åÊ</p>
<p></p>
<p>The instructions didn&#39;t specify that we necessarily needed to use other methods to get 2500 URLs. The homework says to &#34;turn in a list of at least 2,500 urls, including some crawled from the Gun Violence Archive and some obtained using the Bing API.&#34;åÊ</p>
<p></p>
<p>Am I understanding correctly why I got the points off? If so, I&#39;m confused why I got points off given the instructions. Thank you!åÊ</p>",Question about grade,"<p>I&#39;m having an issue in the last part of the assignment creating the questions that require information from the spreadsheet. For example, for the question &#34;In what city did the incident take place?&#34; I am trying to add {{city_1}} as an option in the question so that the information from the spreadsheet will populate that field, but I get this error: &#34;&#39;{{city_1}}&#39; will show up in your results as &#39;city_1&#39;, which already exists as a header in your uploaded data.&#34; How can I achieve what is supposed to happen, instead of getting this error?</p>
<p></p>
<p>Thanks!</p>",Crowdflower question creation,<p>I&#39;m working on one of the iterativeåÊimage annotation assignment...I&#39;m having trouble understanding how to implement the iterative component to designing a task. Are there any hints/advice on where to start?åÊ</p>,Iterative questions on CrowdFlower,<p>Crowdflower recommends 13 Test Questions. Is this number ok?</p>,How many test questions on Crowdflower?,"<p>My partner and I are signed up for the Netflix Prize problem but we are running into questions like &#34;what company are you profiling?&#34; Should we be focusing more on Netflix or the Netflix Prize itself?</p>
<p></p>
<p>Thanks!</p>",Question about survey,<p>What exactly is meant by the words in the &#34;red&#34; reviews that DO NOT appear in the &#34;white&#34; reviews? Does this mean we should not count a word that appears in a red review and at least oneåÊwhite review? Or is this simply a check for reviews that have both &#34;red&#34; and &#34;white&#34; in them (and thus we can&#39;t classify the words as either red or white)?åÊ</p>,Questions 9 &amp; 10: Red and White reviews,4
940847314,4/26/2016 15:30:05,false,1969390866,,4/26/2016 15:26:19,false,clixsense,0.8889,36052512,PHL,F2,Quezon City,49.149.150.150,0,,"<p>I got .5 points off &#34;for not using the crawler to collect URLs in addition to the Bing API&#34; -- my collected URLs included URLs from the Gun Violence Archive, and I collected the rest using the Bing API.åÊ</p>
<p></p>
<p>The instructions didn&#39;t specify that we necessarily needed to use other methods to get 2500 URLs. The homework says to &#34;turn in a list of at least 2,500 urls, including some crawled from the Gun Violence Archive and some obtained using the Bing API.&#34;åÊ</p>
<p></p>
<p>Am I understanding correctly why I got the points off? If so, I&#39;m confused why I got points off given the instructions. Thank you!åÊ</p>",Question about grade,"<p>I&#39;m having an issue in the last part of the assignment creating the questions that require information from the spreadsheet. For example, for the question &#34;In what city did the incident take place?&#34; I am trying to add {{city_1}} as an option in the question so that the information from the spreadsheet will populate that field, but I get this error: &#34;&#39;{{city_1}}&#39; will show up in your results as &#39;city_1&#39;, which already exists as a header in your uploaded data.&#34; How can I achieve what is supposed to happen, instead of getting this error?</p>
<p></p>
<p>Thanks!</p>",Crowdflower question creation,<p>I&#39;m working on one of the iterativeåÊimage annotation assignment...I&#39;m having trouble understanding how to implement the iterative component to designing a task. Are there any hints/advice on where to start?åÊ</p>,Iterative questions on CrowdFlower,<p>Crowdflower recommends 13 Test Questions. Is this number ok?</p>,How many test questions on Crowdflower?,"<p>My partner and I are signed up for the Netflix Prize problem but we are running into questions like &#34;what company are you profiling?&#34; Should we be focusing more on Netflix or the Netflix Prize itself?</p>
<p></p>
<p>Thanks!</p>",Question about survey,<p>What exactly is meant by the words in the &#34;red&#34; reviews that DO NOT appear in the &#34;white&#34; reviews? Does this mean we should not count a word that appears in a red review and at least oneåÊwhite review? Or is this simply a check for reviews that have both &#34;red&#34; and &#34;white&#34; in them (and thus we can&#39;t classify the words as either red or white)?åÊ</p>,Questions 9 &amp; 10: Red and White reviews,4
940847314,4/26/2016 15:51:22,false,1969399819,,4/26/2016 15:36:03,false,neodev,1.0,13396426,VEN,15,Santa Teresa,190.38.163.149,0,,"<p>I got .5 points off &#34;for not using the crawler to collect URLs in addition to the Bing API&#34; -- my collected URLs included URLs from the Gun Violence Archive, and I collected the rest using the Bing API.åÊ</p>
<p></p>
<p>The instructions didn&#39;t specify that we necessarily needed to use other methods to get 2500 URLs. The homework says to &#34;turn in a list of at least 2,500 urls, including some crawled from the Gun Violence Archive and some obtained using the Bing API.&#34;åÊ</p>
<p></p>
<p>Am I understanding correctly why I got the points off? If so, I&#39;m confused why I got points off given the instructions. Thank you!åÊ</p>",Question about grade,"<p>I&#39;m having an issue in the last part of the assignment creating the questions that require information from the spreadsheet. For example, for the question &#34;In what city did the incident take place?&#34; I am trying to add {{city_1}} as an option in the question so that the information from the spreadsheet will populate that field, but I get this error: &#34;&#39;{{city_1}}&#39; will show up in your results as &#39;city_1&#39;, which already exists as a header in your uploaded data.&#34; How can I achieve what is supposed to happen, instead of getting this error?</p>
<p></p>
<p>Thanks!</p>",Crowdflower question creation,<p>I&#39;m working on one of the iterativeåÊimage annotation assignment...I&#39;m having trouble understanding how to implement the iterative component to designing a task. Are there any hints/advice on where to start?åÊ</p>,Iterative questions on CrowdFlower,<p>Crowdflower recommends 13 Test Questions. Is this number ok?</p>,How many test questions on Crowdflower?,"<p>My partner and I are signed up for the Netflix Prize problem but we are running into questions like &#34;what company are you profiling?&#34; Should we be focusing more on Netflix or the Netflix Prize itself?</p>
<p></p>
<p>Thanks!</p>",Question about survey,<p>What exactly is meant by the words in the &#34;red&#34; reviews that DO NOT appear in the &#34;white&#34; reviews? Does this mean we should not count a word that appears in a red review and at least oneåÊwhite review? Or is this simply a check for reviews that have both &#34;red&#34; and &#34;white&#34; in them (and thus we can&#39;t classify the words as either red or white)?åÊ</p>,Questions 9 &amp; 10: Red and White reviews,4
940847314,4/26/2016 15:58:24,false,1969402715,,4/26/2016 15:56:48,false,elite,1.0,33243069,IND,10,Faridabad,116.203.79.150,0,,"<p>I got .5 points off &#34;for not using the crawler to collect URLs in addition to the Bing API&#34; -- my collected URLs included URLs from the Gun Violence Archive, and I collected the rest using the Bing API.åÊ</p>
<p></p>
<p>The instructions didn&#39;t specify that we necessarily needed to use other methods to get 2500 URLs. The homework says to &#34;turn in a list of at least 2,500 urls, including some crawled from the Gun Violence Archive and some obtained using the Bing API.&#34;åÊ</p>
<p></p>
<p>Am I understanding correctly why I got the points off? If so, I&#39;m confused why I got points off given the instructions. Thank you!åÊ</p>",Question about grade,"<p>I&#39;m having an issue in the last part of the assignment creating the questions that require information from the spreadsheet. For example, for the question &#34;In what city did the incident take place?&#34; I am trying to add {{city_1}} as an option in the question so that the information from the spreadsheet will populate that field, but I get this error: &#34;&#39;{{city_1}}&#39; will show up in your results as &#39;city_1&#39;, which already exists as a header in your uploaded data.&#34; How can I achieve what is supposed to happen, instead of getting this error?</p>
<p></p>
<p>Thanks!</p>",Crowdflower question creation,<p>I&#39;m working on one of the iterativeåÊimage annotation assignment...I&#39;m having trouble understanding how to implement the iterative component to designing a task. Are there any hints/advice on where to start?åÊ</p>,Iterative questions on CrowdFlower,<p>Crowdflower recommends 13 Test Questions. Is this number ok?</p>,How many test questions on Crowdflower?,"<p>My partner and I are signed up for the Netflix Prize problem but we are running into questions like &#34;what company are you profiling?&#34; Should we be focusing more on Netflix or the Netflix Prize itself?</p>
<p></p>
<p>Thanks!</p>",Question about survey,<p>What exactly is meant by the words in the &#34;red&#34; reviews that DO NOT appear in the &#34;white&#34; reviews? Does this mean we should not count a word that appears in a red review and at least oneåÊwhite review? Or is this simply a check for reviews that have both &#34;red&#34; and &#34;white&#34; in them (and thus we can&#39;t classify the words as either red or white)?åÊ</p>,Questions 9 &amp; 10: Red and White reviews,4
940847315,4/26/2016 17:32:34,false,1969460924,,4/26/2016 17:32:18,false,neodev,0.8889,33131546,IDN,04,Jakarta,139.194.89.60,0,,"<p>When I runåÊ</p>
<p>$ cat bing_urls.txt | python get_clean_text.py &gt; articles_and_urls.txt</p>
<p></p>
<p>I keep on getting this error</p>
<p></p>
<p>urllib2.URLError: &lt;urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:590)&gt;</p>
<p></p>
<p>What should I do?</p>",Error scraping url texts,"<p>A High School Is Actually Not ManhattanÛªs Saddest Spot, a Researcher SaysåÊ<a href=""http://nyti.ms/165bok9"" target=""_blank"">http://nyti.ms/165bok9</a>åÊvia &#64;nytmetro</p>
<p></p>
<blockquote>
<p>...In August, the group, New England Complex Systems Institute, released a study assessing the moods of New Yorkers based on their Twitter posts. The lead researcher on the study, Prof. Yaneer Bar-Yam, told media outlets that Hunter High School, an elite public school on the Upper East Side, had the highest percentage of ÛÏnegative sentimentÛ posts of any place in Manhattan.</p>
<p></p>
<p>This was determined, he said, by a computer program that sorted geo-tagged posts into negative or positive sentiment designations, based upon their language and emoticons...</p>
</blockquote>","Text sentiment analysis gone awry, featuring my high school","<p>So you know how you can view the job and THEN decide to accept or refuse the job? ( I am currently using crowdflower to post this task)</p>
<p></p>
<p>I am somewhat worried that if I just do something like this, where &#34;hidden text here&#34; is shown only after 5 seconds,</p>
<p></p>
<pre>&lt;p id=&#34;showtext&#34; style=&#34;display: block;&#34;&gt;&lt;strong&gt;hidden text here&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&#34;text/javascript&#34;&gt; setTimeout(function(){åÊåÊåÊåÊ document.getElementById(&#39;showtext&#39;).style.display = &#39;block&#39;; },5 * 1000); &lt;/script&gt;</pre>
<p>then it will show this text during the preview of the job (before they accept). How can I control it so that the JS function only runs after they accept the job?</p>
<p></p>",How do I make it display text after they accept job?,"<p>I think I did the extract_text portion correctly, but I have a couple questions just to be sure, because the output is a little strange:åÊ</p>
<p></p>
<p>1) 5 of the 100 links gave Errors and were not outputted, so my text file only has 95 urls. Is that fine?åÊ</p>
<p></p>
<p>2) Should the text portions be surrounded by &lt;p&gt; &lt;/p&gt; tags? All of them are, which I assume is what we want, but some text does not look useful at all (e.g., theåÊtextåÊfor &#34;http://segazine.com/police-say-shooting-near-city-market-injured-5-not-3/&#34;åÊis simplyåÊ&lt;p&gt;Savannah&lt;/p&gt;)</p>
<p></p>
<p>Any clarification would help - thanks!</p>",extract_text.py - how should the text look?,"<p>For the blurry text recognition example we went over in class, which is in one of the 5 research papers, how exactly does the parallel process work? Does every workeråÊliterally start from scratch and try to fill in the words?åÊ</p>
<p></p>
<p>The paper mentions that &#34;multiple turkersåÊdeciphered this entire passage almost perfectly in the parallel process.&#34;åÊWhen I try to decipher the passages, I feel like I&#39;d be lucky if I can even get 1 out of 30 words correct. Is the researcher using some type of hybrid parallel-iterative here to aid the turkers?åÊ</p>",Blurry Text Recognition - Clarification on Parallel Process,"<p>I&#39;ve noticed that this article has hidden tabs, which are causing &#34;too many values to unpack&#34; errors when its clean_text is run on convert_to_csv.py. It seems that in extract_text.py, the code provided in the get_text(url) function is already stripping the text returned by the Alchemy API. And yet it appears that this processing is not sufficient. How should we go about removing tabs found within an article? In this one, there are parts where there are just 5 tabs back to back.</p>
<p></p>
<p><a href=""http://www.10tv.com/content/stories/2015/11/20/los-angeles-ap-16-year-old-charged-as-adult-in-california-officers-killing.html"">http://www.10tv.com/content/stories/2015/11/20/los-angeles-ap-16-year-old-charged-as-adult-in-california-officers-killing.html</a></p>",Hidden Tabs in Article Text,4
940847315,4/26/2016 17:36:01,false,1969462735,,4/26/2016 17:35:20,false,clixsense,1.0,30712378,ROU,21,Deva,79.119.241.200,0,,"<p>When I runåÊ</p>
<p>$ cat bing_urls.txt | python get_clean_text.py &gt; articles_and_urls.txt</p>
<p></p>
<p>I keep on getting this error</p>
<p></p>
<p>urllib2.URLError: &lt;urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:590)&gt;</p>
<p></p>
<p>What should I do?</p>",Error scraping url texts,"<p>A High School Is Actually Not ManhattanÛªs Saddest Spot, a Researcher SaysåÊ<a href=""http://nyti.ms/165bok9"" target=""_blank"">http://nyti.ms/165bok9</a>åÊvia &#64;nytmetro</p>
<p></p>
<blockquote>
<p>...In August, the group, New England Complex Systems Institute, released a study assessing the moods of New Yorkers based on their Twitter posts. The lead researcher on the study, Prof. Yaneer Bar-Yam, told media outlets that Hunter High School, an elite public school on the Upper East Side, had the highest percentage of ÛÏnegative sentimentÛ posts of any place in Manhattan.</p>
<p></p>
<p>This was determined, he said, by a computer program that sorted geo-tagged posts into negative or positive sentiment designations, based upon their language and emoticons...</p>
</blockquote>","Text sentiment analysis gone awry, featuring my high school","<p>So you know how you can view the job and THEN decide to accept or refuse the job? ( I am currently using crowdflower to post this task)</p>
<p></p>
<p>I am somewhat worried that if I just do something like this, where &#34;hidden text here&#34; is shown only after 5 seconds,</p>
<p></p>
<pre>&lt;p id=&#34;showtext&#34; style=&#34;display: block;&#34;&gt;&lt;strong&gt;hidden text here&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&#34;text/javascript&#34;&gt; setTimeout(function(){åÊåÊåÊåÊ document.getElementById(&#39;showtext&#39;).style.display = &#39;block&#39;; },5 * 1000); &lt;/script&gt;</pre>
<p>then it will show this text during the preview of the job (before they accept). How can I control it so that the JS function only runs after they accept the job?</p>
<p></p>",How do I make it display text after they accept job?,"<p>I think I did the extract_text portion correctly, but I have a couple questions just to be sure, because the output is a little strange:åÊ</p>
<p></p>
<p>1) 5 of the 100 links gave Errors and were not outputted, so my text file only has 95 urls. Is that fine?åÊ</p>
<p></p>
<p>2) Should the text portions be surrounded by &lt;p&gt; &lt;/p&gt; tags? All of them are, which I assume is what we want, but some text does not look useful at all (e.g., theåÊtextåÊfor &#34;http://segazine.com/police-say-shooting-near-city-market-injured-5-not-3/&#34;åÊis simplyåÊ&lt;p&gt;Savannah&lt;/p&gt;)</p>
<p></p>
<p>Any clarification would help - thanks!</p>",extract_text.py - how should the text look?,"<p>For the blurry text recognition example we went over in class, which is in one of the 5 research papers, how exactly does the parallel process work? Does every workeråÊliterally start from scratch and try to fill in the words?åÊ</p>
<p></p>
<p>The paper mentions that &#34;multiple turkersåÊdeciphered this entire passage almost perfectly in the parallel process.&#34;åÊWhen I try to decipher the passages, I feel like I&#39;d be lucky if I can even get 1 out of 30 words correct. Is the researcher using some type of hybrid parallel-iterative here to aid the turkers?åÊ</p>",Blurry Text Recognition - Clarification on Parallel Process,"<p>I&#39;ve noticed that this article has hidden tabs, which are causing &#34;too many values to unpack&#34; errors when its clean_text is run on convert_to_csv.py. It seems that in extract_text.py, the code provided in the get_text(url) function is already stripping the text returned by the Alchemy API. And yet it appears that this processing is not sufficient. How should we go about removing tabs found within an article? In this one, there are parts where there are just 5 tabs back to back.</p>
<p></p>
<p><a href=""http://www.10tv.com/content/stories/2015/11/20/los-angeles-ap-16-year-old-charged-as-adult-in-california-officers-killing.html"">http://www.10tv.com/content/stories/2015/11/20/los-angeles-ap-16-year-old-charged-as-adult-in-california-officers-killing.html</a></p>",Hidden Tabs in Article Text,4
940847315,4/26/2016 17:38:55,false,1969464251,,4/26/2016 17:36:08,false,neodev,0.8889,33568303,VEN,23,Cabimas,190.77.7.36,0,,"<p>When I runåÊ</p>
<p>$ cat bing_urls.txt | python get_clean_text.py &gt; articles_and_urls.txt</p>
<p></p>
<p>I keep on getting this error</p>
<p></p>
<p>urllib2.URLError: &lt;urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:590)&gt;</p>
<p></p>
<p>What should I do?</p>",Error scraping url texts,"<p>A High School Is Actually Not ManhattanÛªs Saddest Spot, a Researcher SaysåÊ<a href=""http://nyti.ms/165bok9"" target=""_blank"">http://nyti.ms/165bok9</a>åÊvia &#64;nytmetro</p>
<p></p>
<blockquote>
<p>...In August, the group, New England Complex Systems Institute, released a study assessing the moods of New Yorkers based on their Twitter posts. The lead researcher on the study, Prof. Yaneer Bar-Yam, told media outlets that Hunter High School, an elite public school on the Upper East Side, had the highest percentage of ÛÏnegative sentimentÛ posts of any place in Manhattan.</p>
<p></p>
<p>This was determined, he said, by a computer program that sorted geo-tagged posts into negative or positive sentiment designations, based upon their language and emoticons...</p>
</blockquote>","Text sentiment analysis gone awry, featuring my high school","<p>So you know how you can view the job and THEN decide to accept or refuse the job? ( I am currently using crowdflower to post this task)</p>
<p></p>
<p>I am somewhat worried that if I just do something like this, where &#34;hidden text here&#34; is shown only after 5 seconds,</p>
<p></p>
<pre>&lt;p id=&#34;showtext&#34; style=&#34;display: block;&#34;&gt;&lt;strong&gt;hidden text here&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&#34;text/javascript&#34;&gt; setTimeout(function(){åÊåÊåÊåÊ document.getElementById(&#39;showtext&#39;).style.display = &#39;block&#39;; },5 * 1000); &lt;/script&gt;</pre>
<p>then it will show this text during the preview of the job (before they accept). How can I control it so that the JS function only runs after they accept the job?</p>
<p></p>",How do I make it display text after they accept job?,"<p>I think I did the extract_text portion correctly, but I have a couple questions just to be sure, because the output is a little strange:åÊ</p>
<p></p>
<p>1) 5 of the 100 links gave Errors and were not outputted, so my text file only has 95 urls. Is that fine?åÊ</p>
<p></p>
<p>2) Should the text portions be surrounded by &lt;p&gt; &lt;/p&gt; tags? All of them are, which I assume is what we want, but some text does not look useful at all (e.g., theåÊtextåÊfor &#34;http://segazine.com/police-say-shooting-near-city-market-injured-5-not-3/&#34;åÊis simplyåÊ&lt;p&gt;Savannah&lt;/p&gt;)</p>
<p></p>
<p>Any clarification would help - thanks!</p>",extract_text.py - how should the text look?,"<p>For the blurry text recognition example we went over in class, which is in one of the 5 research papers, how exactly does the parallel process work? Does every workeråÊliterally start from scratch and try to fill in the words?åÊ</p>
<p></p>
<p>The paper mentions that &#34;multiple turkersåÊdeciphered this entire passage almost perfectly in the parallel process.&#34;åÊWhen I try to decipher the passages, I feel like I&#39;d be lucky if I can even get 1 out of 30 words correct. Is the researcher using some type of hybrid parallel-iterative here to aid the turkers?åÊ</p>",Blurry Text Recognition - Clarification on Parallel Process,"<p>I&#39;ve noticed that this article has hidden tabs, which are causing &#34;too many values to unpack&#34; errors when its clean_text is run on convert_to_csv.py. It seems that in extract_text.py, the code provided in the get_text(url) function is already stripping the text returned by the Alchemy API. And yet it appears that this processing is not sufficient. How should we go about removing tabs found within an article? In this one, there are parts where there are just 5 tabs back to back.</p>
<p></p>
<p><a href=""http://www.10tv.com/content/stories/2015/11/20/los-angeles-ap-16-year-old-charged-as-adult-in-california-officers-killing.html"">http://www.10tv.com/content/stories/2015/11/20/los-angeles-ap-16-year-old-charged-as-adult-in-california-officers-killing.html</a></p>",Hidden Tabs in Article Text,4
940847315,4/26/2016 17:45:14,false,1969468163,,4/26/2016 17:39:40,false,clixsense,1.0,35444326,BRA,07,Brasília,177.15.130.106,0,,"<p>When I runåÊ</p>
<p>$ cat bing_urls.txt | python get_clean_text.py &gt; articles_and_urls.txt</p>
<p></p>
<p>I keep on getting this error</p>
<p></p>
<p>urllib2.URLError: &lt;urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:590)&gt;</p>
<p></p>
<p>What should I do?</p>",Error scraping url texts,"<p>A High School Is Actually Not ManhattanÛªs Saddest Spot, a Researcher SaysåÊ<a href=""http://nyti.ms/165bok9"" target=""_blank"">http://nyti.ms/165bok9</a>åÊvia &#64;nytmetro</p>
<p></p>
<blockquote>
<p>...In August, the group, New England Complex Systems Institute, released a study assessing the moods of New Yorkers based on their Twitter posts. The lead researcher on the study, Prof. Yaneer Bar-Yam, told media outlets that Hunter High School, an elite public school on the Upper East Side, had the highest percentage of ÛÏnegative sentimentÛ posts of any place in Manhattan.</p>
<p></p>
<p>This was determined, he said, by a computer program that sorted geo-tagged posts into negative or positive sentiment designations, based upon their language and emoticons...</p>
</blockquote>","Text sentiment analysis gone awry, featuring my high school","<p>So you know how you can view the job and THEN decide to accept or refuse the job? ( I am currently using crowdflower to post this task)</p>
<p></p>
<p>I am somewhat worried that if I just do something like this, where &#34;hidden text here&#34; is shown only after 5 seconds,</p>
<p></p>
<pre>&lt;p id=&#34;showtext&#34; style=&#34;display: block;&#34;&gt;&lt;strong&gt;hidden text here&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&#34;text/javascript&#34;&gt; setTimeout(function(){åÊåÊåÊåÊ document.getElementById(&#39;showtext&#39;).style.display = &#39;block&#39;; },5 * 1000); &lt;/script&gt;</pre>
<p>then it will show this text during the preview of the job (before they accept). How can I control it so that the JS function only runs after they accept the job?</p>
<p></p>",How do I make it display text after they accept job?,"<p>I think I did the extract_text portion correctly, but I have a couple questions just to be sure, because the output is a little strange:åÊ</p>
<p></p>
<p>1) 5 of the 100 links gave Errors and were not outputted, so my text file only has 95 urls. Is that fine?åÊ</p>
<p></p>
<p>2) Should the text portions be surrounded by &lt;p&gt; &lt;/p&gt; tags? All of them are, which I assume is what we want, but some text does not look useful at all (e.g., theåÊtextåÊfor &#34;http://segazine.com/police-say-shooting-near-city-market-injured-5-not-3/&#34;åÊis simplyåÊ&lt;p&gt;Savannah&lt;/p&gt;)</p>
<p></p>
<p>Any clarification would help - thanks!</p>",extract_text.py - how should the text look?,"<p>For the blurry text recognition example we went over in class, which is in one of the 5 research papers, how exactly does the parallel process work? Does every workeråÊliterally start from scratch and try to fill in the words?åÊ</p>
<p></p>
<p>The paper mentions that &#34;multiple turkersåÊdeciphered this entire passage almost perfectly in the parallel process.&#34;åÊWhen I try to decipher the passages, I feel like I&#39;d be lucky if I can even get 1 out of 30 words correct. Is the researcher using some type of hybrid parallel-iterative here to aid the turkers?åÊ</p>",Blurry Text Recognition - Clarification on Parallel Process,"<p>I&#39;ve noticed that this article has hidden tabs, which are causing &#34;too many values to unpack&#34; errors when its clean_text is run on convert_to_csv.py. It seems that in extract_text.py, the code provided in the get_text(url) function is already stripping the text returned by the Alchemy API. And yet it appears that this processing is not sufficient. How should we go about removing tabs found within an article? In this one, there are parts where there are just 5 tabs back to back.</p>
<p></p>
<p><a href=""http://www.10tv.com/content/stories/2015/11/20/los-angeles-ap-16-year-old-charged-as-adult-in-california-officers-killing.html"">http://www.10tv.com/content/stories/2015/11/20/los-angeles-ap-16-year-old-charged-as-adult-in-california-officers-killing.html</a></p>",Hidden Tabs in Article Text,4
940847315,4/26/2016 18:19:09,false,1969486223,,4/26/2016 18:16:24,false,neodev,0.8889,35550011,VEN,07,Valencia,190.204.238.112,0,,"<p>When I runåÊ</p>
<p>$ cat bing_urls.txt | python get_clean_text.py &gt; articles_and_urls.txt</p>
<p></p>
<p>I keep on getting this error</p>
<p></p>
<p>urllib2.URLError: &lt;urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:590)&gt;</p>
<p></p>
<p>What should I do?</p>",Error scraping url texts,"<p>A High School Is Actually Not ManhattanÛªs Saddest Spot, a Researcher SaysåÊ<a href=""http://nyti.ms/165bok9"" target=""_blank"">http://nyti.ms/165bok9</a>åÊvia &#64;nytmetro</p>
<p></p>
<blockquote>
<p>...In August, the group, New England Complex Systems Institute, released a study assessing the moods of New Yorkers based on their Twitter posts. The lead researcher on the study, Prof. Yaneer Bar-Yam, told media outlets that Hunter High School, an elite public school on the Upper East Side, had the highest percentage of ÛÏnegative sentimentÛ posts of any place in Manhattan.</p>
<p></p>
<p>This was determined, he said, by a computer program that sorted geo-tagged posts into negative or positive sentiment designations, based upon their language and emoticons...</p>
</blockquote>","Text sentiment analysis gone awry, featuring my high school","<p>So you know how you can view the job and THEN decide to accept or refuse the job? ( I am currently using crowdflower to post this task)</p>
<p></p>
<p>I am somewhat worried that if I just do something like this, where &#34;hidden text here&#34; is shown only after 5 seconds,</p>
<p></p>
<pre>&lt;p id=&#34;showtext&#34; style=&#34;display: block;&#34;&gt;&lt;strong&gt;hidden text here&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&#34;text/javascript&#34;&gt; setTimeout(function(){åÊåÊåÊåÊ document.getElementById(&#39;showtext&#39;).style.display = &#39;block&#39;; },5 * 1000); &lt;/script&gt;</pre>
<p>then it will show this text during the preview of the job (before they accept). How can I control it so that the JS function only runs after they accept the job?</p>
<p></p>",How do I make it display text after they accept job?,"<p>I think I did the extract_text portion correctly, but I have a couple questions just to be sure, because the output is a little strange:åÊ</p>
<p></p>
<p>1) 5 of the 100 links gave Errors and were not outputted, so my text file only has 95 urls. Is that fine?åÊ</p>
<p></p>
<p>2) Should the text portions be surrounded by &lt;p&gt; &lt;/p&gt; tags? All of them are, which I assume is what we want, but some text does not look useful at all (e.g., theåÊtextåÊfor &#34;http://segazine.com/police-say-shooting-near-city-market-injured-5-not-3/&#34;åÊis simplyåÊ&lt;p&gt;Savannah&lt;/p&gt;)</p>
<p></p>
<p>Any clarification would help - thanks!</p>",extract_text.py - how should the text look?,"<p>For the blurry text recognition example we went over in class, which is in one of the 5 research papers, how exactly does the parallel process work? Does every workeråÊliterally start from scratch and try to fill in the words?åÊ</p>
<p></p>
<p>The paper mentions that &#34;multiple turkersåÊdeciphered this entire passage almost perfectly in the parallel process.&#34;åÊWhen I try to decipher the passages, I feel like I&#39;d be lucky if I can even get 1 out of 30 words correct. Is the researcher using some type of hybrid parallel-iterative here to aid the turkers?åÊ</p>",Blurry Text Recognition - Clarification on Parallel Process,"<p>I&#39;ve noticed that this article has hidden tabs, which are causing &#34;too many values to unpack&#34; errors when its clean_text is run on convert_to_csv.py. It seems that in extract_text.py, the code provided in the get_text(url) function is already stripping the text returned by the Alchemy API. And yet it appears that this processing is not sufficient. How should we go about removing tabs found within an article? In this one, there are parts where there are just 5 tabs back to back.</p>
<p></p>
<p><a href=""http://www.10tv.com/content/stories/2015/11/20/los-angeles-ap-16-year-old-charged-as-adult-in-california-officers-killing.html"">http://www.10tv.com/content/stories/2015/11/20/los-angeles-ap-16-year-old-charged-as-adult-in-california-officers-killing.html</a></p>",Hidden Tabs in Article Text,4
940847316,4/26/2016 16:25:31,false,1969419712,,4/26/2016 16:24:35,false,neodev,1.0,29175140,VEN,25,Caracas,190.72.125.134,0,,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>Is there a preferred way of doing inline images with Markdown for our reports? Right now we&#39;re considering either linking to an image hosted online or just in the local directory, although we aren&#39;t too sure how to check and verify the formatting is correct when it&#39;s viewed.</p>",Markdown images,1
940847316,4/26/2016 16:44:00,false,1969431826,,4/26/2016 16:41:32,false,clixsense,1.0,6329782,IDN,10,Sleman,202.67.40.222,0,,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>Is there a preferred way of doing inline images with Markdown for our reports? Right now we&#39;re considering either linking to an image hosted online or just in the local directory, although we aren&#39;t too sure how to check and verify the formatting is correct when it&#39;s viewed.</p>",Markdown images,1
940847316,4/26/2016 17:00:02,false,1969441668,,4/26/2016 16:58:39,false,clixsense,1.0,21408115,IDN,07,Semarang,36.79.23.180,0,,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>Is there a preferred way of doing inline images with Markdown for our reports? Right now we&#39;re considering either linking to an image hosted online or just in the local directory, although we aren&#39;t too sure how to check and verify the formatting is correct when it&#39;s viewed.</p>",Markdown images,1
940847316,4/26/2016 17:04:17,false,1969444194,,4/26/2016 17:02:14,false,neodev,1.0,36167043,GBR,G6,Hull,77.86.101.69,0,,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>Is there a preferred way of doing inline images with Markdown for our reports? Right now we&#39;re considering either linking to an image hosted online or just in the local directory, although we aren&#39;t too sure how to check and verify the formatting is correct when it&#39;s viewed.</p>",Markdown images,1
940847316,4/26/2016 17:47:57,false,1969469599,,4/26/2016 17:41:51,false,neodev,0.8889,19625264,DZA,41,Chlef,41.102.7.217,0,,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>Is there a preferred way of doing inline images with Markdown for our reports? Right now we&#39;re considering either linking to an image hosted online or just in the local directory, although we aren&#39;t too sure how to check and verify the formatting is correct when it&#39;s viewed.</p>",Markdown images,1
940847317,4/26/2016 15:11:59,false,1969364461,,4/26/2016 15:11:46,false,tremorgames,1.0,32635967,LTU,60,Panevezys,78.63.38.165,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>Currently, we are looking at replicating Study 1åÊfrom &#34;Crowds in Two Seconds&#34; butåÊwe wanted to confirm if that was enough or also had to do Study 2 since they are part of the same objective / are similar experiments. Is it ok to just do Study 1?</p>
<p>Thanks!</p>",Is it ok to pick one (Retention study 1) or should we do both since they are proving similar points,4
940847317,4/26/2016 15:20:28,false,1969377830,,4/26/2016 15:19:15,false,clixsense,1.0,24287706,TWN,04,Keelung,61.231.195.173,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>Currently, we are looking at replicating Study 1åÊfrom &#34;Crowds in Two Seconds&#34; butåÊwe wanted to confirm if that was enough or also had to do Study 2 since they are part of the same objective / are similar experiments. Is it ok to just do Study 1?</p>
<p>Thanks!</p>",Is it ok to pick one (Retention study 1) or should we do both since they are proving similar points,4
940847317,4/26/2016 15:20:29,false,1969377845,,4/26/2016 15:20:10,false,neodev,1.0,19132694,LKA,36,Colombo,123.231.124.170,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>Currently, we are looking at replicating Study 1åÊfrom &#34;Crowds in Two Seconds&#34; butåÊwe wanted to confirm if that was enough or also had to do Study 2 since they are part of the same objective / are similar experiments. Is it ok to just do Study 1?</p>
<p>Thanks!</p>",Is it ok to pick one (Retention study 1) or should we do both since they are proving similar points,4
940847317,4/26/2016 15:24:23,false,1969384425,,4/26/2016 15:21:38,false,clixsense,1.0,7837812,SRB,00,Belgrade,79.101.254.233,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>Currently, we are looking at replicating Study 1åÊfrom &#34;Crowds in Two Seconds&#34; butåÊwe wanted to confirm if that was enough or also had to do Study 2 since they are part of the same objective / are similar experiments. Is it ok to just do Study 1?</p>
<p>Thanks!</p>",Is it ok to pick one (Retention study 1) or should we do both since they are proving similar points,4
940847317,4/26/2016 15:27:16,false,1969389142,,4/26/2016 15:24:27,false,elite,1.0,30280423,ITA,15,Siracusa,151.54.84.121,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>Currently, we are looking at replicating Study 1åÊfrom &#34;Crowds in Two Seconds&#34; butåÊwe wanted to confirm if that was enough or also had to do Study 2 since they are part of the same objective / are similar experiments. Is it ok to just do Study 1?</p>
<p>Thanks!</p>",Is it ok to pick one (Retention study 1) or should we do both since they are proving similar points,4
940847318,4/26/2016 17:23:10,false,1969455460,,4/26/2016 17:16:53,false,neodev,1.0,33973110,VEN,23,Maracaibo,186.94.238.104,0,,"<p>When I try to submit the homework, it does this marvelous thing where it tells me that net213 is an invalid config file.</p>
<p></p>
<p>What to do now? Somewhat concerned.</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hqh2r7yc6p926r/ikaaipxq9b3g/image.png"" /></p>",Invalid config file,"<p>InåÊthe bing_api.py, I am trying to print out the xml response that we receive.</p>
<p>Any ideas how to do this?</p>",Print out xml file,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>Right now, my data is organized as url,textåÊåÊåÊåÊ where all of the commas in the text column have a backslash to escape them</p>
<p></p>
<p>I am getting the following error:</p>
<div>File upload failed</div>
<div>One of your uploaded headers was blank. This can sometimes happen when a row contains more columns than defined in your header row.</div>
<div></div>
<div>I narrowed down my csv file to two lines, but I am still getting the same error even though everything seems fine</div>
<pre>url,text
http://44news.wevv.com/mom-recovering-son-accidentally-shoots/,&lt;p&gt;A post Thanksgiving dinner shopping trip took a turn for the worse after a 15-year old accidentally shot his mother with her own gun.&lt;/p&gt; &lt;p&gt;It happened Thursday night just before 10pm\, in the parking lot outside the east side Target store.&lt;/p&gt; &lt;p&gt;According to Evansville Police Stacey Craven was shot in the back when her teenage son fired a handgun inside their van.&lt;/p&gt; &lt;p&gt;Police believe the 15 year old removed the gun from under a seat. He pulled the trigger\, but thought the gun had an external safety that would prevent the gun from firing.&lt;/p&gt; &lt;p&gt;Police say the gun had a trigger assembly safety and discharged when the teen pulled the trigger.&lt;/p&gt; &lt;p&gt;Craven was was taken to the hospital for treatment. Her injuries are not believed to be life threatening.&lt;/p&gt; &lt;p&gt;The shooting appears to be an accident and no charges have been filed.&lt;/p&gt; &lt;p&gt;From the Evansville Police Department:&lt;/p&gt; &lt;p&gt;This event is a reminder that all guns should be secured in a manner that prevents untrained individuals from accessing them. It is also a reminder that all guns should be treated as though they are loaded\, muzzles should be pointed in a safe direction\, and you should not put your finger on the trigger unless you INTEND to shoot.&lt;/p&gt; &lt;p&gt;comments&lt;/p&gt;</pre>
<p>What could be the cause of this?</p>",Having trouble making a csv file out of gun-violence-urls-and-text,4
940847318,4/26/2016 17:24:11,false,1969455980,,4/26/2016 17:22:14,false,elite,1.0,25411289,HRV,"","",31.147.119.175,0,,"<p>When I try to submit the homework, it does this marvelous thing where it tells me that net213 is an invalid config file.</p>
<p></p>
<p>What to do now? Somewhat concerned.</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hqh2r7yc6p926r/ikaaipxq9b3g/image.png"" /></p>",Invalid config file,"<p>InåÊthe bing_api.py, I am trying to print out the xml response that we receive.</p>
<p>Any ideas how to do this?</p>",Print out xml file,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>Right now, my data is organized as url,textåÊåÊåÊåÊ where all of the commas in the text column have a backslash to escape them</p>
<p></p>
<p>I am getting the following error:</p>
<div>File upload failed</div>
<div>One of your uploaded headers was blank. This can sometimes happen when a row contains more columns than defined in your header row.</div>
<div></div>
<div>I narrowed down my csv file to two lines, but I am still getting the same error even though everything seems fine</div>
<pre>url,text
http://44news.wevv.com/mom-recovering-son-accidentally-shoots/,&lt;p&gt;A post Thanksgiving dinner shopping trip took a turn for the worse after a 15-year old accidentally shot his mother with her own gun.&lt;/p&gt; &lt;p&gt;It happened Thursday night just before 10pm\, in the parking lot outside the east side Target store.&lt;/p&gt; &lt;p&gt;According to Evansville Police Stacey Craven was shot in the back when her teenage son fired a handgun inside their van.&lt;/p&gt; &lt;p&gt;Police believe the 15 year old removed the gun from under a seat. He pulled the trigger\, but thought the gun had an external safety that would prevent the gun from firing.&lt;/p&gt; &lt;p&gt;Police say the gun had a trigger assembly safety and discharged when the teen pulled the trigger.&lt;/p&gt; &lt;p&gt;Craven was was taken to the hospital for treatment. Her injuries are not believed to be life threatening.&lt;/p&gt; &lt;p&gt;The shooting appears to be an accident and no charges have been filed.&lt;/p&gt; &lt;p&gt;From the Evansville Police Department:&lt;/p&gt; &lt;p&gt;This event is a reminder that all guns should be secured in a manner that prevents untrained individuals from accessing them. It is also a reminder that all guns should be treated as though they are loaded\, muzzles should be pointed in a safe direction\, and you should not put your finger on the trigger unless you INTEND to shoot.&lt;/p&gt; &lt;p&gt;comments&lt;/p&gt;</pre>
<p>What could be the cause of this?</p>",Having trouble making a csv file out of gun-violence-urls-and-text,4
940847318,4/26/2016 17:31:56,false,1969460621,,4/26/2016 17:31:26,false,neodev,0.8889,33131546,IDN,04,Jakarta,139.194.89.60,0,,"<p>When I try to submit the homework, it does this marvelous thing where it tells me that net213 is an invalid config file.</p>
<p></p>
<p>What to do now? Somewhat concerned.</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hqh2r7yc6p926r/ikaaipxq9b3g/image.png"" /></p>",Invalid config file,"<p>InåÊthe bing_api.py, I am trying to print out the xml response that we receive.</p>
<p>Any ideas how to do this?</p>",Print out xml file,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>Right now, my data is organized as url,textåÊåÊåÊåÊ where all of the commas in the text column have a backslash to escape them</p>
<p></p>
<p>I am getting the following error:</p>
<div>File upload failed</div>
<div>One of your uploaded headers was blank. This can sometimes happen when a row contains more columns than defined in your header row.</div>
<div></div>
<div>I narrowed down my csv file to two lines, but I am still getting the same error even though everything seems fine</div>
<pre>url,text
http://44news.wevv.com/mom-recovering-son-accidentally-shoots/,&lt;p&gt;A post Thanksgiving dinner shopping trip took a turn for the worse after a 15-year old accidentally shot his mother with her own gun.&lt;/p&gt; &lt;p&gt;It happened Thursday night just before 10pm\, in the parking lot outside the east side Target store.&lt;/p&gt; &lt;p&gt;According to Evansville Police Stacey Craven was shot in the back when her teenage son fired a handgun inside their van.&lt;/p&gt; &lt;p&gt;Police believe the 15 year old removed the gun from under a seat. He pulled the trigger\, but thought the gun had an external safety that would prevent the gun from firing.&lt;/p&gt; &lt;p&gt;Police say the gun had a trigger assembly safety and discharged when the teen pulled the trigger.&lt;/p&gt; &lt;p&gt;Craven was was taken to the hospital for treatment. Her injuries are not believed to be life threatening.&lt;/p&gt; &lt;p&gt;The shooting appears to be an accident and no charges have been filed.&lt;/p&gt; &lt;p&gt;From the Evansville Police Department:&lt;/p&gt; &lt;p&gt;This event is a reminder that all guns should be secured in a manner that prevents untrained individuals from accessing them. It is also a reminder that all guns should be treated as though they are loaded\, muzzles should be pointed in a safe direction\, and you should not put your finger on the trigger unless you INTEND to shoot.&lt;/p&gt; &lt;p&gt;comments&lt;/p&gt;</pre>
<p>What could be the cause of this?</p>",Having trouble making a csv file out of gun-violence-urls-and-text,4
940847318,4/26/2016 17:41:50,false,1969466057,,4/26/2016 17:32:10,false,neodev,0.8889,19625264,DZA,41,Chlef,41.102.7.217,0,,"<p>When I try to submit the homework, it does this marvelous thing where it tells me that net213 is an invalid config file.</p>
<p></p>
<p>What to do now? Somewhat concerned.</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hqh2r7yc6p926r/ikaaipxq9b3g/image.png"" /></p>",Invalid config file,"<p>InåÊthe bing_api.py, I am trying to print out the xml response that we receive.</p>
<p>Any ideas how to do this?</p>",Print out xml file,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>Right now, my data is organized as url,textåÊåÊåÊåÊ where all of the commas in the text column have a backslash to escape them</p>
<p></p>
<p>I am getting the following error:</p>
<div>File upload failed</div>
<div>One of your uploaded headers was blank. This can sometimes happen when a row contains more columns than defined in your header row.</div>
<div></div>
<div>I narrowed down my csv file to two lines, but I am still getting the same error even though everything seems fine</div>
<pre>url,text
http://44news.wevv.com/mom-recovering-son-accidentally-shoots/,&lt;p&gt;A post Thanksgiving dinner shopping trip took a turn for the worse after a 15-year old accidentally shot his mother with her own gun.&lt;/p&gt; &lt;p&gt;It happened Thursday night just before 10pm\, in the parking lot outside the east side Target store.&lt;/p&gt; &lt;p&gt;According to Evansville Police Stacey Craven was shot in the back when her teenage son fired a handgun inside their van.&lt;/p&gt; &lt;p&gt;Police believe the 15 year old removed the gun from under a seat. He pulled the trigger\, but thought the gun had an external safety that would prevent the gun from firing.&lt;/p&gt; &lt;p&gt;Police say the gun had a trigger assembly safety and discharged when the teen pulled the trigger.&lt;/p&gt; &lt;p&gt;Craven was was taken to the hospital for treatment. Her injuries are not believed to be life threatening.&lt;/p&gt; &lt;p&gt;The shooting appears to be an accident and no charges have been filed.&lt;/p&gt; &lt;p&gt;From the Evansville Police Department:&lt;/p&gt; &lt;p&gt;This event is a reminder that all guns should be secured in a manner that prevents untrained individuals from accessing them. It is also a reminder that all guns should be treated as though they are loaded\, muzzles should be pointed in a safe direction\, and you should not put your finger on the trigger unless you INTEND to shoot.&lt;/p&gt; &lt;p&gt;comments&lt;/p&gt;</pre>
<p>What could be the cause of this?</p>",Having trouble making a csv file out of gun-violence-urls-and-text,4
940847318,4/26/2016 17:46:50,false,1969469061,,4/26/2016 17:32:15,false,clixsense,0.8889,35338593,ITA,14,Cagliari,151.56.132.145,0,,"<p>When I try to submit the homework, it does this marvelous thing where it tells me that net213 is an invalid config file.</p>
<p></p>
<p>What to do now? Somewhat concerned.</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hqh2r7yc6p926r/ikaaipxq9b3g/image.png"" /></p>",Invalid config file,"<p>InåÊthe bing_api.py, I am trying to print out the xml response that we receive.</p>
<p>Any ideas how to do this?</p>",Print out xml file,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>Right now, my data is organized as url,textåÊåÊåÊåÊ where all of the commas in the text column have a backslash to escape them</p>
<p></p>
<p>I am getting the following error:</p>
<div>File upload failed</div>
<div>One of your uploaded headers was blank. This can sometimes happen when a row contains more columns than defined in your header row.</div>
<div></div>
<div>I narrowed down my csv file to two lines, but I am still getting the same error even though everything seems fine</div>
<pre>url,text
http://44news.wevv.com/mom-recovering-son-accidentally-shoots/,&lt;p&gt;A post Thanksgiving dinner shopping trip took a turn for the worse after a 15-year old accidentally shot his mother with her own gun.&lt;/p&gt; &lt;p&gt;It happened Thursday night just before 10pm\, in the parking lot outside the east side Target store.&lt;/p&gt; &lt;p&gt;According to Evansville Police Stacey Craven was shot in the back when her teenage son fired a handgun inside their van.&lt;/p&gt; &lt;p&gt;Police believe the 15 year old removed the gun from under a seat. He pulled the trigger\, but thought the gun had an external safety that would prevent the gun from firing.&lt;/p&gt; &lt;p&gt;Police say the gun had a trigger assembly safety and discharged when the teen pulled the trigger.&lt;/p&gt; &lt;p&gt;Craven was was taken to the hospital for treatment. Her injuries are not believed to be life threatening.&lt;/p&gt; &lt;p&gt;The shooting appears to be an accident and no charges have been filed.&lt;/p&gt; &lt;p&gt;From the Evansville Police Department:&lt;/p&gt; &lt;p&gt;This event is a reminder that all guns should be secured in a manner that prevents untrained individuals from accessing them. It is also a reminder that all guns should be treated as though they are loaded\, muzzles should be pointed in a safe direction\, and you should not put your finger on the trigger unless you INTEND to shoot.&lt;/p&gt; &lt;p&gt;comments&lt;/p&gt;</pre>
<p>What could be the cause of this?</p>",Having trouble making a csv file out of gun-violence-urls-and-text,4
940847319,4/26/2016 16:25:31,false,1969419714,,4/26/2016 16:24:35,false,neodev,1.0,29175140,VEN,25,Caracas,190.72.125.134,0,,"<p></p><div>Please register to be a worker on Amazon Mechanical Turk TODAY atåÊ<a href=""https://www.mturk.com/mturk/welcome"">https://www.mturk.com/mturk/welcome</a>åÊ</div>
<div>You will need an account to for the <a href=""http://crowdsourcing-class.org/assignment1.html"" target=""_blank"">first homework assignment</a>. åÊWe will be doing a walk through on Friday, and it would be great if you have your account activated by then. åÊTypically it takes Amazon several days to activate workers accounts, so please create yours ASAP.</div>
<div></div>
<div>Note that all foreign students, and some domestic students had problems creating Mechanical Turk accounts last year. åÊIf you have a problem, please try to sign up to be a Crowd Flower worker (also called a &#34;contributor&#34;). You can sign up here: <a href=""https://elite.crowdflower.com/?view=register"">https://elite.crowdflower.com/?view=register</a><a href=""https://elite.crowdflower.com/?view=register"" target=""_blank""></a></div>
<div></div>
<div>--CCB</div>",TODO: Register as a Mechanical Turk worker,"<p>I just realized that, for some of my test questions, a lot of workers complained that the answers I gave were wrong or lacked concrete instructions. Some of them asked me to &#34;make corrections&#34; to the test questions and &#34;fix their accuracy&#34;. How would I go about doing this?åÊ</p>
<p></p>
<p>Looking back, maybe my answers to the test questions were incorrect.</p>",Fixing Worker&#39;s accuracy,<p>One of the workers in our csv seems to have answered no test questions and doesn&#39;t appear on the weighted majority worker quality set. Should this worker just be given a weight of 0? åÊ</p>,Weighted majority worker weight,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hljibjyn5s85mp/ijmy2ryhm0ve/Screenshot_20160120_09.45.32.png"" />FYI, I had my developers add a &#34;My hourly rate&#34; feature to the Track section of the Analytics page on <a href=""http://crowd-workers.com/analytics"" target=""_blank"">crowd-workers.com</a>. åÊYou can view it by first reloading your web page, then clicking on the Display button, and selecting theåÊ&#34;MyåÊhourlyåÊrate&#34; option. åÊ Screenshot is attached.</p>
<p></p>
<p></p>",&#34;My hourly rate&#34; now available on Crowd Workers,"<p>To clarify this calculation, I understand that you shouldåÊmultiply the following: (accuracy when labelåÊis porn = P,P)(% of URLs which are porn) &#43; (accuracy when label is not porn = NP, NP)(% of URLs which are not porn). Just to clarify this further,åÊif we are calculating worker quality for iteration 2, do we take the % of URLs that are porn and not porn from the true labels we calculated in iteration 2 or the ones we calculated iteration 1. I guess I am confused because we calculated the URL true labels andåÊmajority vote of iteration 2 based on the worker qualities of iteration 2. Correct me if I am wrong. Thanks!</p>
<p></p>
<p>Also other people are stating that we can add the top left corner (P,P) &#43; bottom right corner values (NP,NP) and then divide by 2. However, I don&#39;t believe those values are the same.</p>
<p>åÊ</p>",Reducing Worker Quality to One Value,<p>Changed the price paid in &#34;Settings&#34; but I don&#39;t think it applies to past workers. Giving a bonus to each contributor feelsåÊtoo manual.</p>,I underpaid workers by accident - any way to retroactively increase payment?,3
940847319,4/26/2016 16:44:00,false,1969431831,,4/26/2016 16:41:32,false,clixsense,1.0,6329782,IDN,10,Sleman,202.67.40.222,0,,"<p></p><div>Please register to be a worker on Amazon Mechanical Turk TODAY atåÊ<a href=""https://www.mturk.com/mturk/welcome"">https://www.mturk.com/mturk/welcome</a>åÊ</div>
<div>You will need an account to for the <a href=""http://crowdsourcing-class.org/assignment1.html"" target=""_blank"">first homework assignment</a>. åÊWe will be doing a walk through on Friday, and it would be great if you have your account activated by then. åÊTypically it takes Amazon several days to activate workers accounts, so please create yours ASAP.</div>
<div></div>
<div>Note that all foreign students, and some domestic students had problems creating Mechanical Turk accounts last year. åÊIf you have a problem, please try to sign up to be a Crowd Flower worker (also called a &#34;contributor&#34;). You can sign up here: <a href=""https://elite.crowdflower.com/?view=register"">https://elite.crowdflower.com/?view=register</a><a href=""https://elite.crowdflower.com/?view=register"" target=""_blank""></a></div>
<div></div>
<div>--CCB</div>",TODO: Register as a Mechanical Turk worker,"<p>I just realized that, for some of my test questions, a lot of workers complained that the answers I gave were wrong or lacked concrete instructions. Some of them asked me to &#34;make corrections&#34; to the test questions and &#34;fix their accuracy&#34;. How would I go about doing this?åÊ</p>
<p></p>
<p>Looking back, maybe my answers to the test questions were incorrect.</p>",Fixing Worker&#39;s accuracy,<p>One of the workers in our csv seems to have answered no test questions and doesn&#39;t appear on the weighted majority worker quality set. Should this worker just be given a weight of 0? åÊ</p>,Weighted majority worker weight,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hljibjyn5s85mp/ijmy2ryhm0ve/Screenshot_20160120_09.45.32.png"" />FYI, I had my developers add a &#34;My hourly rate&#34; feature to the Track section of the Analytics page on <a href=""http://crowd-workers.com/analytics"" target=""_blank"">crowd-workers.com</a>. åÊYou can view it by first reloading your web page, then clicking on the Display button, and selecting theåÊ&#34;MyåÊhourlyåÊrate&#34; option. åÊ Screenshot is attached.</p>
<p></p>
<p></p>",&#34;My hourly rate&#34; now available on Crowd Workers,"<p>To clarify this calculation, I understand that you shouldåÊmultiply the following: (accuracy when labelåÊis porn = P,P)(% of URLs which are porn) &#43; (accuracy when label is not porn = NP, NP)(% of URLs which are not porn). Just to clarify this further,åÊif we are calculating worker quality for iteration 2, do we take the % of URLs that are porn and not porn from the true labels we calculated in iteration 2 or the ones we calculated iteration 1. I guess I am confused because we calculated the URL true labels andåÊmajority vote of iteration 2 based on the worker qualities of iteration 2. Correct me if I am wrong. Thanks!</p>
<p></p>
<p>Also other people are stating that we can add the top left corner (P,P) &#43; bottom right corner values (NP,NP) and then divide by 2. However, I don&#39;t believe those values are the same.</p>
<p>åÊ</p>",Reducing Worker Quality to One Value,<p>Changed the price paid in &#34;Settings&#34; but I don&#39;t think it applies to past workers. Giving a bonus to each contributor feelsåÊtoo manual.</p>,I underpaid workers by accident - any way to retroactively increase payment?,3
940847319,4/26/2016 17:00:02,false,1969441675,,4/26/2016 16:58:39,false,clixsense,1.0,21408115,IDN,07,Semarang,36.79.23.180,0,,"<p></p><div>Please register to be a worker on Amazon Mechanical Turk TODAY atåÊ<a href=""https://www.mturk.com/mturk/welcome"">https://www.mturk.com/mturk/welcome</a>åÊ</div>
<div>You will need an account to for the <a href=""http://crowdsourcing-class.org/assignment1.html"" target=""_blank"">first homework assignment</a>. åÊWe will be doing a walk through on Friday, and it would be great if you have your account activated by then. åÊTypically it takes Amazon several days to activate workers accounts, so please create yours ASAP.</div>
<div></div>
<div>Note that all foreign students, and some domestic students had problems creating Mechanical Turk accounts last year. åÊIf you have a problem, please try to sign up to be a Crowd Flower worker (also called a &#34;contributor&#34;). You can sign up here: <a href=""https://elite.crowdflower.com/?view=register"">https://elite.crowdflower.com/?view=register</a><a href=""https://elite.crowdflower.com/?view=register"" target=""_blank""></a></div>
<div></div>
<div>--CCB</div>",TODO: Register as a Mechanical Turk worker,"<p>I just realized that, for some of my test questions, a lot of workers complained that the answers I gave were wrong or lacked concrete instructions. Some of them asked me to &#34;make corrections&#34; to the test questions and &#34;fix their accuracy&#34;. How would I go about doing this?åÊ</p>
<p></p>
<p>Looking back, maybe my answers to the test questions were incorrect.</p>",Fixing Worker&#39;s accuracy,<p>One of the workers in our csv seems to have answered no test questions and doesn&#39;t appear on the weighted majority worker quality set. Should this worker just be given a weight of 0? åÊ</p>,Weighted majority worker weight,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hljibjyn5s85mp/ijmy2ryhm0ve/Screenshot_20160120_09.45.32.png"" />FYI, I had my developers add a &#34;My hourly rate&#34; feature to the Track section of the Analytics page on <a href=""http://crowd-workers.com/analytics"" target=""_blank"">crowd-workers.com</a>. åÊYou can view it by first reloading your web page, then clicking on the Display button, and selecting theåÊ&#34;MyåÊhourlyåÊrate&#34; option. åÊ Screenshot is attached.</p>
<p></p>
<p></p>",&#34;My hourly rate&#34; now available on Crowd Workers,"<p>To clarify this calculation, I understand that you shouldåÊmultiply the following: (accuracy when labelåÊis porn = P,P)(% of URLs which are porn) &#43; (accuracy when label is not porn = NP, NP)(% of URLs which are not porn). Just to clarify this further,åÊif we are calculating worker quality for iteration 2, do we take the % of URLs that are porn and not porn from the true labels we calculated in iteration 2 or the ones we calculated iteration 1. I guess I am confused because we calculated the URL true labels andåÊmajority vote of iteration 2 based on the worker qualities of iteration 2. Correct me if I am wrong. Thanks!</p>
<p></p>
<p>Also other people are stating that we can add the top left corner (P,P) &#43; bottom right corner values (NP,NP) and then divide by 2. However, I don&#39;t believe those values are the same.</p>
<p>åÊ</p>",Reducing Worker Quality to One Value,<p>Changed the price paid in &#34;Settings&#34; but I don&#39;t think it applies to past workers. Giving a bonus to each contributor feelsåÊtoo manual.</p>,I underpaid workers by accident - any way to retroactively increase payment?,3
940847319,4/26/2016 17:04:17,false,1969444193,,4/26/2016 17:02:14,false,neodev,1.0,36167043,GBR,G6,Hull,77.86.101.69,0,,"<p></p><div>Please register to be a worker on Amazon Mechanical Turk TODAY atåÊ<a href=""https://www.mturk.com/mturk/welcome"">https://www.mturk.com/mturk/welcome</a>åÊ</div>
<div>You will need an account to for the <a href=""http://crowdsourcing-class.org/assignment1.html"" target=""_blank"">first homework assignment</a>. åÊWe will be doing a walk through on Friday, and it would be great if you have your account activated by then. åÊTypically it takes Amazon several days to activate workers accounts, so please create yours ASAP.</div>
<div></div>
<div>Note that all foreign students, and some domestic students had problems creating Mechanical Turk accounts last year. åÊIf you have a problem, please try to sign up to be a Crowd Flower worker (also called a &#34;contributor&#34;). You can sign up here: <a href=""https://elite.crowdflower.com/?view=register"">https://elite.crowdflower.com/?view=register</a><a href=""https://elite.crowdflower.com/?view=register"" target=""_blank""></a></div>
<div></div>
<div>--CCB</div>",TODO: Register as a Mechanical Turk worker,"<p>I just realized that, for some of my test questions, a lot of workers complained that the answers I gave were wrong or lacked concrete instructions. Some of them asked me to &#34;make corrections&#34; to the test questions and &#34;fix their accuracy&#34;. How would I go about doing this?åÊ</p>
<p></p>
<p>Looking back, maybe my answers to the test questions were incorrect.</p>",Fixing Worker&#39;s accuracy,<p>One of the workers in our csv seems to have answered no test questions and doesn&#39;t appear on the weighted majority worker quality set. Should this worker just be given a weight of 0? åÊ</p>,Weighted majority worker weight,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hljibjyn5s85mp/ijmy2ryhm0ve/Screenshot_20160120_09.45.32.png"" />FYI, I had my developers add a &#34;My hourly rate&#34; feature to the Track section of the Analytics page on <a href=""http://crowd-workers.com/analytics"" target=""_blank"">crowd-workers.com</a>. åÊYou can view it by first reloading your web page, then clicking on the Display button, and selecting theåÊ&#34;MyåÊhourlyåÊrate&#34; option. åÊ Screenshot is attached.</p>
<p></p>
<p></p>",&#34;My hourly rate&#34; now available on Crowd Workers,"<p>To clarify this calculation, I understand that you shouldåÊmultiply the following: (accuracy when labelåÊis porn = P,P)(% of URLs which are porn) &#43; (accuracy when label is not porn = NP, NP)(% of URLs which are not porn). Just to clarify this further,åÊif we are calculating worker quality for iteration 2, do we take the % of URLs that are porn and not porn from the true labels we calculated in iteration 2 or the ones we calculated iteration 1. I guess I am confused because we calculated the URL true labels andåÊmajority vote of iteration 2 based on the worker qualities of iteration 2. Correct me if I am wrong. Thanks!</p>
<p></p>
<p>Also other people are stating that we can add the top left corner (P,P) &#43; bottom right corner values (NP,NP) and then divide by 2. However, I don&#39;t believe those values are the same.</p>
<p>åÊ</p>",Reducing Worker Quality to One Value,<p>Changed the price paid in &#34;Settings&#34; but I don&#39;t think it applies to past workers. Giving a bonus to each contributor feelsåÊtoo manual.</p>,I underpaid workers by accident - any way to retroactively increase payment?,3
940847319,4/26/2016 17:47:57,false,1969469601,,4/26/2016 17:41:51,false,neodev,0.8889,19625264,DZA,41,Chlef,41.102.7.217,0,,"<p></p><div>Please register to be a worker on Amazon Mechanical Turk TODAY atåÊ<a href=""https://www.mturk.com/mturk/welcome"">https://www.mturk.com/mturk/welcome</a>åÊ</div>
<div>You will need an account to for the <a href=""http://crowdsourcing-class.org/assignment1.html"" target=""_blank"">first homework assignment</a>. åÊWe will be doing a walk through on Friday, and it would be great if you have your account activated by then. åÊTypically it takes Amazon several days to activate workers accounts, so please create yours ASAP.</div>
<div></div>
<div>Note that all foreign students, and some domestic students had problems creating Mechanical Turk accounts last year. åÊIf you have a problem, please try to sign up to be a Crowd Flower worker (also called a &#34;contributor&#34;). You can sign up here: <a href=""https://elite.crowdflower.com/?view=register"">https://elite.crowdflower.com/?view=register</a><a href=""https://elite.crowdflower.com/?view=register"" target=""_blank""></a></div>
<div></div>
<div>--CCB</div>",TODO: Register as a Mechanical Turk worker,"<p>I just realized that, for some of my test questions, a lot of workers complained that the answers I gave were wrong or lacked concrete instructions. Some of them asked me to &#34;make corrections&#34; to the test questions and &#34;fix their accuracy&#34;. How would I go about doing this?åÊ</p>
<p></p>
<p>Looking back, maybe my answers to the test questions were incorrect.</p>",Fixing Worker&#39;s accuracy,<p>One of the workers in our csv seems to have answered no test questions and doesn&#39;t appear on the weighted majority worker quality set. Should this worker just be given a weight of 0? åÊ</p>,Weighted majority worker weight,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hljibjyn5s85mp/ijmy2ryhm0ve/Screenshot_20160120_09.45.32.png"" />FYI, I had my developers add a &#34;My hourly rate&#34; feature to the Track section of the Analytics page on <a href=""http://crowd-workers.com/analytics"" target=""_blank"">crowd-workers.com</a>. åÊYou can view it by first reloading your web page, then clicking on the Display button, and selecting theåÊ&#34;MyåÊhourlyåÊrate&#34; option. åÊ Screenshot is attached.</p>
<p></p>
<p></p>",&#34;My hourly rate&#34; now available on Crowd Workers,"<p>To clarify this calculation, I understand that you shouldåÊmultiply the following: (accuracy when labelåÊis porn = P,P)(% of URLs which are porn) &#43; (accuracy when label is not porn = NP, NP)(% of URLs which are not porn). Just to clarify this further,åÊif we are calculating worker quality for iteration 2, do we take the % of URLs that are porn and not porn from the true labels we calculated in iteration 2 or the ones we calculated iteration 1. I guess I am confused because we calculated the URL true labels andåÊmajority vote of iteration 2 based on the worker qualities of iteration 2. Correct me if I am wrong. Thanks!</p>
<p></p>
<p>Also other people are stating that we can add the top left corner (P,P) &#43; bottom right corner values (NP,NP) and then divide by 2. However, I don&#39;t believe those values are the same.</p>
<p>åÊ</p>",Reducing Worker Quality to One Value,<p>Changed the price paid in &#34;Settings&#34; but I don&#39;t think it applies to past workers. Giving a bonus to each contributor feelsåÊtoo manual.</p>,I underpaid workers by accident - any way to retroactively increase payment?,3
940847320,4/26/2016 15:48:55,false,1969398739,,4/26/2016 15:46:06,false,neodev,0.8889,21971187,TTO,08,Valsayn,190.213.132.190,0,,"<p>Hi everyone,</p>
<p></p>
<p>I&#39;m looking for a partner toåÊcollaborate with on upcoming assignments and/or the final project. E-mail me at <a href=""mailto:hcutler&#64;seas.upenn.edu"">hcutler&#64;seas.upenn.edu</a>åÊif you&#39;re interested!</p>
<p></p>
<p>Have a great day.</p>
<p></p>
<p>Hannah Cutler</p>
<p>SEAS &#39;17</p>",Looking for a partner for homeworks and/or final project?,"<p>Are we allowed to modify one of the past final project ideas shown to us in class? We have an interesting ideaåÊsimilar to PictureThis and wanted to make sure it&#39;s OK that it&#39;s similar.</p>
<p></p>
<p>Thanks!</p>",Final project idea similar to PictureThis,"<p>FYI, the second part of the final project was released last night:åÊ<a href=""http://crowdsourcing-class.org/final-project-part2.html"">http://crowdsourcing-class.org/final-project-part2.html</a></p>
<p></p>
<p>There are two deliverables. åÊThe first one is due on Saturday before 11:59pm.</p>",Final Project Part 2 has been released,"<p>For the NETS 213 final project, do we as students retain ownership of our projects? I know that some schools will hold the ownership rights, and juståÊwanted some clarification on this policy for the NETS 213 course.</p>",Student Ownership of Final Project?,"<p>Here is some updated details about the $10,000 prize for the best final project:</p>
<p><a href=""http://crowdsourcing-class.org/project.html"">http://crowdsourcing-class.org/project.html</a></p>
<p></p>
<p>Let me know what you think!</p>","$10,000 prize for the final project",<p>This is the link:åÊhttps://github.com/benpleitner/NETS213-Final-Project</p>,Group Project,4
940847320,4/26/2016 15:50:47,false,1969399574,,4/26/2016 15:49:12,false,elite,0.8889,36575101,IND,07,New Delhi,112.196.144.2,0,,"<p>Hi everyone,</p>
<p></p>
<p>I&#39;m looking for a partner toåÊcollaborate with on upcoming assignments and/or the final project. E-mail me at <a href=""mailto:hcutler&#64;seas.upenn.edu"">hcutler&#64;seas.upenn.edu</a>åÊif you&#39;re interested!</p>
<p></p>
<p>Have a great day.</p>
<p></p>
<p>Hannah Cutler</p>
<p>SEAS &#39;17</p>",Looking for a partner for homeworks and/or final project?,"<p>Are we allowed to modify one of the past final project ideas shown to us in class? We have an interesting ideaåÊsimilar to PictureThis and wanted to make sure it&#39;s OK that it&#39;s similar.</p>
<p></p>
<p>Thanks!</p>",Final project idea similar to PictureThis,"<p>FYI, the second part of the final project was released last night:åÊ<a href=""http://crowdsourcing-class.org/final-project-part2.html"">http://crowdsourcing-class.org/final-project-part2.html</a></p>
<p></p>
<p>There are two deliverables. åÊThe first one is due on Saturday before 11:59pm.</p>",Final Project Part 2 has been released,"<p>For the NETS 213 final project, do we as students retain ownership of our projects? I know that some schools will hold the ownership rights, and juståÊwanted some clarification on this policy for the NETS 213 course.</p>",Student Ownership of Final Project?,"<p>Here is some updated details about the $10,000 prize for the best final project:</p>
<p><a href=""http://crowdsourcing-class.org/project.html"">http://crowdsourcing-class.org/project.html</a></p>
<p></p>
<p>Let me know what you think!</p>","$10,000 prize for the final project",<p>This is the link:åÊhttps://github.com/benpleitner/NETS213-Final-Project</p>,Group Project,4
940847320,4/26/2016 15:51:11,false,1969399732,,4/26/2016 15:47:56,false,neodev,1.0,28875937,PAK,08,Islamabad,119.153.105.50,0,,"<p>Hi everyone,</p>
<p></p>
<p>I&#39;m looking for a partner toåÊcollaborate with on upcoming assignments and/or the final project. E-mail me at <a href=""mailto:hcutler&#64;seas.upenn.edu"">hcutler&#64;seas.upenn.edu</a>åÊif you&#39;re interested!</p>
<p></p>
<p>Have a great day.</p>
<p></p>
<p>Hannah Cutler</p>
<p>SEAS &#39;17</p>",Looking for a partner for homeworks and/or final project?,"<p>Are we allowed to modify one of the past final project ideas shown to us in class? We have an interesting ideaåÊsimilar to PictureThis and wanted to make sure it&#39;s OK that it&#39;s similar.</p>
<p></p>
<p>Thanks!</p>",Final project idea similar to PictureThis,"<p>FYI, the second part of the final project was released last night:åÊ<a href=""http://crowdsourcing-class.org/final-project-part2.html"">http://crowdsourcing-class.org/final-project-part2.html</a></p>
<p></p>
<p>There are two deliverables. åÊThe first one is due on Saturday before 11:59pm.</p>",Final Project Part 2 has been released,"<p>For the NETS 213 final project, do we as students retain ownership of our projects? I know that some schools will hold the ownership rights, and juståÊwanted some clarification on this policy for the NETS 213 course.</p>",Student Ownership of Final Project?,"<p>Here is some updated details about the $10,000 prize for the best final project:</p>
<p><a href=""http://crowdsourcing-class.org/project.html"">http://crowdsourcing-class.org/project.html</a></p>
<p></p>
<p>Let me know what you think!</p>","$10,000 prize for the final project",<p>This is the link:åÊhttps://github.com/benpleitner/NETS213-Final-Project</p>,Group Project,4
940847320,4/26/2016 15:53:17,false,1969400605,,4/26/2016 15:51:23,false,neodev,1.0,13396426,VEN,15,Santa Teresa,190.38.163.149,0,,"<p>Hi everyone,</p>
<p></p>
<p>I&#39;m looking for a partner toåÊcollaborate with on upcoming assignments and/or the final project. E-mail me at <a href=""mailto:hcutler&#64;seas.upenn.edu"">hcutler&#64;seas.upenn.edu</a>åÊif you&#39;re interested!</p>
<p></p>
<p>Have a great day.</p>
<p></p>
<p>Hannah Cutler</p>
<p>SEAS &#39;17</p>",Looking for a partner for homeworks and/or final project?,"<p>Are we allowed to modify one of the past final project ideas shown to us in class? We have an interesting ideaåÊsimilar to PictureThis and wanted to make sure it&#39;s OK that it&#39;s similar.</p>
<p></p>
<p>Thanks!</p>",Final project idea similar to PictureThis,"<p>FYI, the second part of the final project was released last night:åÊ<a href=""http://crowdsourcing-class.org/final-project-part2.html"">http://crowdsourcing-class.org/final-project-part2.html</a></p>
<p></p>
<p>There are two deliverables. åÊThe first one is due on Saturday before 11:59pm.</p>",Final Project Part 2 has been released,"<p>For the NETS 213 final project, do we as students retain ownership of our projects? I know that some schools will hold the ownership rights, and juståÊwanted some clarification on this policy for the NETS 213 course.</p>",Student Ownership of Final Project?,"<p>Here is some updated details about the $10,000 prize for the best final project:</p>
<p><a href=""http://crowdsourcing-class.org/project.html"">http://crowdsourcing-class.org/project.html</a></p>
<p></p>
<p>Let me know what you think!</p>","$10,000 prize for the final project",<p>This is the link:åÊhttps://github.com/benpleitner/NETS213-Final-Project</p>,Group Project,4
940847320,4/26/2016 15:56:46,false,1969402056,,4/26/2016 15:55:02,false,elite,1.0,33243069,IND,10,Faridabad,116.203.79.150,0,,"<p>Hi everyone,</p>
<p></p>
<p>I&#39;m looking for a partner toåÊcollaborate with on upcoming assignments and/or the final project. E-mail me at <a href=""mailto:hcutler&#64;seas.upenn.edu"">hcutler&#64;seas.upenn.edu</a>åÊif you&#39;re interested!</p>
<p></p>
<p>Have a great day.</p>
<p></p>
<p>Hannah Cutler</p>
<p>SEAS &#39;17</p>",Looking for a partner for homeworks and/or final project?,"<p>Are we allowed to modify one of the past final project ideas shown to us in class? We have an interesting ideaåÊsimilar to PictureThis and wanted to make sure it&#39;s OK that it&#39;s similar.</p>
<p></p>
<p>Thanks!</p>",Final project idea similar to PictureThis,"<p>FYI, the second part of the final project was released last night:åÊ<a href=""http://crowdsourcing-class.org/final-project-part2.html"">http://crowdsourcing-class.org/final-project-part2.html</a></p>
<p></p>
<p>There are two deliverables. åÊThe first one is due on Saturday before 11:59pm.</p>",Final Project Part 2 has been released,"<p>For the NETS 213 final project, do we as students retain ownership of our projects? I know that some schools will hold the ownership rights, and juståÊwanted some clarification on this policy for the NETS 213 course.</p>",Student Ownership of Final Project?,"<p>Here is some updated details about the $10,000 prize for the best final project:</p>
<p><a href=""http://crowdsourcing-class.org/project.html"">http://crowdsourcing-class.org/project.html</a></p>
<p></p>
<p>Let me know what you think!</p>","$10,000 prize for the final project",<p>This is the link:åÊhttps://github.com/benpleitner/NETS213-Final-Project</p>,Group Project,4
940847321,4/26/2016 15:11:31,false,1969363825,,4/26/2016 15:10:32,false,tremorgames,1.0,32635967,LTU,60,Panevezys,78.63.38.165,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"There&#39;s a to do comment in the function get_misclassified_examples, but nothing mentioned about it in the hw page. Are we expected to do this part?",misclassified examples,2
940847321,4/26/2016 15:15:42,false,1969369976,,4/26/2016 15:14:06,false,clixsense,1.0,24287706,TWN,04,Keelung,61.231.195.173,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"There&#39;s a to do comment in the function get_misclassified_examples, but nothing mentioned about it in the hw page. Are we expected to do this part?",misclassified examples,2
940847321,4/26/2016 15:19:31,false,1969376282,,4/26/2016 15:17:21,false,clixsense,1.0,7837812,SRB,00,Belgrade,79.101.254.233,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"There&#39;s a to do comment in the function get_misclassified_examples, but nothing mentioned about it in the hw page. Are we expected to do this part?",misclassified examples,2
940847321,4/26/2016 15:19:53,false,1969376823,,4/26/2016 15:19:30,false,neodev,1.0,19132694,LKA,36,Colombo,123.231.124.170,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"There&#39;s a to do comment in the function get_misclassified_examples, but nothing mentioned about it in the hw page. Are we expected to do this part?",misclassified examples,2
940847321,4/26/2016 15:24:26,false,1969384551,,4/26/2016 15:21:07,false,elite,1.0,30280423,ITA,15,Siracusa,151.54.84.121,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"There&#39;s a to do comment in the function get_misclassified_examples, but nothing mentioned about it in the hw page. Are we expected to do this part?",misclassified examples,2
940847322,4/26/2016 15:11:45,false,1969364126,,4/26/2016 15:11:33,false,tremorgames,1.0,32635967,LTU,60,Panevezys,78.63.38.165,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,<p>One of the video links for the peer review assignment we are supposed to do is broken (iStockPhoto). What should I do?</p>,Video link broken,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>If we have a password for the Vimeo Video, where should we write that on the project idea survey? Would it be ok to write it in parentheses next to the link?</p>",Vimeo Password,2
940847322,4/26/2016 15:19:14,false,1969375926,,4/26/2016 15:15:44,false,clixsense,1.0,24287706,TWN,04,Keelung,61.231.195.173,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,<p>One of the video links for the peer review assignment we are supposed to do is broken (iStockPhoto). What should I do?</p>,Video link broken,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>If we have a password for the Vimeo Video, where should we write that on the project idea survey? Would it be ok to write it in parentheses next to the link?</p>",Vimeo Password,2
940847322,4/26/2016 15:20:09,false,1969377312,,4/26/2016 15:19:55,false,neodev,1.0,19132694,LKA,36,Colombo,123.231.124.170,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,<p>One of the video links for the peer review assignment we are supposed to do is broken (iStockPhoto). What should I do?</p>,Video link broken,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>If we have a password for the Vimeo Video, where should we write that on the project idea survey? Would it be ok to write it in parentheses next to the link?</p>",Vimeo Password,2
940847322,4/26/2016 15:21:37,false,1969379722,,4/26/2016 15:19:32,false,clixsense,1.0,7837812,SRB,00,Belgrade,79.101.254.233,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,<p>One of the video links for the peer review assignment we are supposed to do is broken (iStockPhoto). What should I do?</p>,Video link broken,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>If we have a password for the Vimeo Video, where should we write that on the project idea survey? Would it be ok to write it in parentheses next to the link?</p>",Vimeo Password,2
940847322,4/26/2016 15:26:18,false,1969387710,,4/26/2016 15:21:33,false,clixsense,0.8889,36052512,PHL,F2,Quezon City,49.149.150.150,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,<p>One of the video links for the peer review assignment we are supposed to do is broken (iStockPhoto). What should I do?</p>,Video link broken,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>If we have a password for the Vimeo Video, where should we write that on the project idea survey? Would it be ok to write it in parentheses next to the link?</p>",Vimeo Password,2
940847323,4/26/2016 15:11:59,false,1969364455,,4/26/2016 15:11:46,false,tremorgames,1.0,32635967,LTU,60,Panevezys,78.63.38.165,0,,"<p>I&#39;ll be flying out for an interview this Thursday so I can&#39;tåÊhold 12-2 OH that day.</p>
<p>Instead I&#39;ll be in the 5th floor GRW bump space today from 6-8 if anyone has questions.</p>
<p></p>
<p>Sorry about the short notice.</p>
<p></p>
<p>-Jim</p>",Change in Office Hours,"<p>Hi everyone,</p>
<p></p>
<p>I will be holding virtual office hours today from 4-6.åÊIf you want help during that time, sign up by puttingåÊyour name and gmail address in this spreadsheet, and I will call you on Google Hangouts when its your turn.åÊ</p>
<p></p>
<p>https://docs.google.com/spreadsheets/d/1enes4IDPyfWzub-1utWUSrrc9a_iNU760HARY2ymqYc/edit?usp=sharing</p>
<p></p>
<p>This will be an experiment. If it works well, itåÊwill allow me to hold office hoursåÊregularly on Thursdays (when demand is probably higher) as opposed to Friday right before class (which is probably a very stressful time to be coming to OH). Plus, you don&#39;t have to leave the comfort of your dorm room. So wins all around. :-)åÊ</p>
<p></p>
<p>--Ellie</p>
<p></p>
<p><br /></p>",Ellie&#39;s Office Hours today,"Hey everyone,<div><br /></div><div>I&#39;ll have office hours starting today at 2:15 PM in the bump space.</div>",Office Hours,"<p>Hey All,</p>
<p></p>
<p>So there have been a lot of questions on Piazza regarding errors and incompatibility issues. I&#39;ve outlined a few suggestions below to make your life easier with regards getting through them.</p>
<p></p>
<p><strong>1. Using Piazza.</strong></p>
<p></p>
<p>Remotely debugging system issues are pretty challenging. To ensure that we can get an answer to you faster - please specify (a) The System you&#39;re using (Mac, Windows, Linux etc.) and how many bits (b) Which installer you used (Pip vs. Conda) (c) If you&#39;re running the VM or not.åÊ</p>
<p></p>
<p><strong>2. Search through Piazza</strong></p>
<p></p>
<p>More often than not the answer is already there on Piazza. Try and look through previous questions (especially ones endorsed by an instructor) to see if that solves your problem.</p>
<p></p>
<p><strong>3. Using Stack Overflow</strong></p>
<p></p>
<p>The best way to learn cool things and debug issues is to use the power of Crowdsourcing! Stack Overflow will definitely have the answer to your questions. As you take more advanced classes you&#39;ll need StackOverflow to try and debug more complicated issues so it will be super useful to get a head start right now. If you&#39;re ever in doubt of what StackOverflow is asking you to do - feel free to post the link and ask on Piazza.</p>
<p></p>
<p><strong>4. Using the VM</strong></p>
<p><strong></strong></p>
<p></p>
<p>Sometimes it makes more sense to just use a different platform which is pre-configured instead of trying to figure out why it doesn&#39;t work on your system. So I&#39;d suggest to everyone to use the VM. It&#39;s also super useful to get a hang of Linux (it&#39;s a rite of passage of sorts for any CS student).åÊ</p>
<p></p>
<p><strong>5. Office Hours</strong></p>
<p><strong></strong></p>
<p>Chances are if the above 3 haven&#39;t helped, Office Hours are the place to go. There are plenty spread throughout the week and more often than not you&#39;ll have at least one friendly NETS 213 TA hanging around Engineering - So swing by Office hours!åÊ</p>
<p></p>
<p>#pin</p>
<p></p>",Using Office Hours and Piazza,"<p>Hi all,</p>
<p></p>
<p>I&#39;m moving my office hours from 5-7 to 6-8 tonight. Sorry for any inconvenience.åÊ</p>
<p></p>
<p>-Ross</p>",Office Hours moved,<p>Just a note that I&#39;m moving my Thursday office hours up by an hour to 5PM. I&#39;ll update it on the calendar as well.</p>,Moving office hours up,1
940847323,4/26/2016 15:20:28,false,1969377829,,4/26/2016 15:19:15,false,clixsense,1.0,24287706,TWN,04,Keelung,61.231.195.173,0,,"<p>I&#39;ll be flying out for an interview this Thursday so I can&#39;tåÊhold 12-2 OH that day.</p>
<p>Instead I&#39;ll be in the 5th floor GRW bump space today from 6-8 if anyone has questions.</p>
<p></p>
<p>Sorry about the short notice.</p>
<p></p>
<p>-Jim</p>",Change in Office Hours,"<p>Hi everyone,</p>
<p></p>
<p>I will be holding virtual office hours today from 4-6.åÊIf you want help during that time, sign up by puttingåÊyour name and gmail address in this spreadsheet, and I will call you on Google Hangouts when its your turn.åÊ</p>
<p></p>
<p>https://docs.google.com/spreadsheets/d/1enes4IDPyfWzub-1utWUSrrc9a_iNU760HARY2ymqYc/edit?usp=sharing</p>
<p></p>
<p>This will be an experiment. If it works well, itåÊwill allow me to hold office hoursåÊregularly on Thursdays (when demand is probably higher) as opposed to Friday right before class (which is probably a very stressful time to be coming to OH). Plus, you don&#39;t have to leave the comfort of your dorm room. So wins all around. :-)åÊ</p>
<p></p>
<p>--Ellie</p>
<p></p>
<p><br /></p>",Ellie&#39;s Office Hours today,"Hey everyone,<div><br /></div><div>I&#39;ll have office hours starting today at 2:15 PM in the bump space.</div>",Office Hours,"<p>Hey All,</p>
<p></p>
<p>So there have been a lot of questions on Piazza regarding errors and incompatibility issues. I&#39;ve outlined a few suggestions below to make your life easier with regards getting through them.</p>
<p></p>
<p><strong>1. Using Piazza.</strong></p>
<p></p>
<p>Remotely debugging system issues are pretty challenging. To ensure that we can get an answer to you faster - please specify (a) The System you&#39;re using (Mac, Windows, Linux etc.) and how many bits (b) Which installer you used (Pip vs. Conda) (c) If you&#39;re running the VM or not.åÊ</p>
<p></p>
<p><strong>2. Search through Piazza</strong></p>
<p></p>
<p>More often than not the answer is already there on Piazza. Try and look through previous questions (especially ones endorsed by an instructor) to see if that solves your problem.</p>
<p></p>
<p><strong>3. Using Stack Overflow</strong></p>
<p></p>
<p>The best way to learn cool things and debug issues is to use the power of Crowdsourcing! Stack Overflow will definitely have the answer to your questions. As you take more advanced classes you&#39;ll need StackOverflow to try and debug more complicated issues so it will be super useful to get a head start right now. If you&#39;re ever in doubt of what StackOverflow is asking you to do - feel free to post the link and ask on Piazza.</p>
<p></p>
<p><strong>4. Using the VM</strong></p>
<p><strong></strong></p>
<p></p>
<p>Sometimes it makes more sense to just use a different platform which is pre-configured instead of trying to figure out why it doesn&#39;t work on your system. So I&#39;d suggest to everyone to use the VM. It&#39;s also super useful to get a hang of Linux (it&#39;s a rite of passage of sorts for any CS student).åÊ</p>
<p></p>
<p><strong>5. Office Hours</strong></p>
<p><strong></strong></p>
<p>Chances are if the above 3 haven&#39;t helped, Office Hours are the place to go. There are plenty spread throughout the week and more often than not you&#39;ll have at least one friendly NETS 213 TA hanging around Engineering - So swing by Office hours!åÊ</p>
<p></p>
<p>#pin</p>
<p></p>",Using Office Hours and Piazza,"<p>Hi all,</p>
<p></p>
<p>I&#39;m moving my office hours from 5-7 to 6-8 tonight. Sorry for any inconvenience.åÊ</p>
<p></p>
<p>-Ross</p>",Office Hours moved,<p>Just a note that I&#39;m moving my Thursday office hours up by an hour to 5PM. I&#39;ll update it on the calendar as well.</p>,Moving office hours up,1
940847323,4/26/2016 15:20:29,false,1969377840,,4/26/2016 15:20:10,false,neodev,1.0,19132694,LKA,36,Colombo,123.231.124.170,0,,"<p>I&#39;ll be flying out for an interview this Thursday so I can&#39;tåÊhold 12-2 OH that day.</p>
<p>Instead I&#39;ll be in the 5th floor GRW bump space today from 6-8 if anyone has questions.</p>
<p></p>
<p>Sorry about the short notice.</p>
<p></p>
<p>-Jim</p>",Change in Office Hours,"<p>Hi everyone,</p>
<p></p>
<p>I will be holding virtual office hours today from 4-6.åÊIf you want help during that time, sign up by puttingåÊyour name and gmail address in this spreadsheet, and I will call you on Google Hangouts when its your turn.åÊ</p>
<p></p>
<p>https://docs.google.com/spreadsheets/d/1enes4IDPyfWzub-1utWUSrrc9a_iNU760HARY2ymqYc/edit?usp=sharing</p>
<p></p>
<p>This will be an experiment. If it works well, itåÊwill allow me to hold office hoursåÊregularly on Thursdays (when demand is probably higher) as opposed to Friday right before class (which is probably a very stressful time to be coming to OH). Plus, you don&#39;t have to leave the comfort of your dorm room. So wins all around. :-)åÊ</p>
<p></p>
<p>--Ellie</p>
<p></p>
<p><br /></p>",Ellie&#39;s Office Hours today,"Hey everyone,<div><br /></div><div>I&#39;ll have office hours starting today at 2:15 PM in the bump space.</div>",Office Hours,"<p>Hey All,</p>
<p></p>
<p>So there have been a lot of questions on Piazza regarding errors and incompatibility issues. I&#39;ve outlined a few suggestions below to make your life easier with regards getting through them.</p>
<p></p>
<p><strong>1. Using Piazza.</strong></p>
<p></p>
<p>Remotely debugging system issues are pretty challenging. To ensure that we can get an answer to you faster - please specify (a) The System you&#39;re using (Mac, Windows, Linux etc.) and how many bits (b) Which installer you used (Pip vs. Conda) (c) If you&#39;re running the VM or not.åÊ</p>
<p></p>
<p><strong>2. Search through Piazza</strong></p>
<p></p>
<p>More often than not the answer is already there on Piazza. Try and look through previous questions (especially ones endorsed by an instructor) to see if that solves your problem.</p>
<p></p>
<p><strong>3. Using Stack Overflow</strong></p>
<p></p>
<p>The best way to learn cool things and debug issues is to use the power of Crowdsourcing! Stack Overflow will definitely have the answer to your questions. As you take more advanced classes you&#39;ll need StackOverflow to try and debug more complicated issues so it will be super useful to get a head start right now. If you&#39;re ever in doubt of what StackOverflow is asking you to do - feel free to post the link and ask on Piazza.</p>
<p></p>
<p><strong>4. Using the VM</strong></p>
<p><strong></strong></p>
<p></p>
<p>Sometimes it makes more sense to just use a different platform which is pre-configured instead of trying to figure out why it doesn&#39;t work on your system. So I&#39;d suggest to everyone to use the VM. It&#39;s also super useful to get a hang of Linux (it&#39;s a rite of passage of sorts for any CS student).åÊ</p>
<p></p>
<p><strong>5. Office Hours</strong></p>
<p><strong></strong></p>
<p>Chances are if the above 3 haven&#39;t helped, Office Hours are the place to go. There are plenty spread throughout the week and more often than not you&#39;ll have at least one friendly NETS 213 TA hanging around Engineering - So swing by Office hours!åÊ</p>
<p></p>
<p>#pin</p>
<p></p>",Using Office Hours and Piazza,"<p>Hi all,</p>
<p></p>
<p>I&#39;m moving my office hours from 5-7 to 6-8 tonight. Sorry for any inconvenience.åÊ</p>
<p></p>
<p>-Ross</p>",Office Hours moved,<p>Just a note that I&#39;m moving my Thursday office hours up by an hour to 5PM. I&#39;ll update it on the calendar as well.</p>,Moving office hours up,1
940847323,4/26/2016 15:24:23,false,1969384421,,4/26/2016 15:21:38,false,clixsense,1.0,7837812,SRB,00,Belgrade,79.101.254.233,0,,"<p>I&#39;ll be flying out for an interview this Thursday so I can&#39;tåÊhold 12-2 OH that day.</p>
<p>Instead I&#39;ll be in the 5th floor GRW bump space today from 6-8 if anyone has questions.</p>
<p></p>
<p>Sorry about the short notice.</p>
<p></p>
<p>-Jim</p>",Change in Office Hours,"<p>Hi everyone,</p>
<p></p>
<p>I will be holding virtual office hours today from 4-6.åÊIf you want help during that time, sign up by puttingåÊyour name and gmail address in this spreadsheet, and I will call you on Google Hangouts when its your turn.åÊ</p>
<p></p>
<p>https://docs.google.com/spreadsheets/d/1enes4IDPyfWzub-1utWUSrrc9a_iNU760HARY2ymqYc/edit?usp=sharing</p>
<p></p>
<p>This will be an experiment. If it works well, itåÊwill allow me to hold office hoursåÊregularly on Thursdays (when demand is probably higher) as opposed to Friday right before class (which is probably a very stressful time to be coming to OH). Plus, you don&#39;t have to leave the comfort of your dorm room. So wins all around. :-)åÊ</p>
<p></p>
<p>--Ellie</p>
<p></p>
<p><br /></p>",Ellie&#39;s Office Hours today,"Hey everyone,<div><br /></div><div>I&#39;ll have office hours starting today at 2:15 PM in the bump space.</div>",Office Hours,"<p>Hey All,</p>
<p></p>
<p>So there have been a lot of questions on Piazza regarding errors and incompatibility issues. I&#39;ve outlined a few suggestions below to make your life easier with regards getting through them.</p>
<p></p>
<p><strong>1. Using Piazza.</strong></p>
<p></p>
<p>Remotely debugging system issues are pretty challenging. To ensure that we can get an answer to you faster - please specify (a) The System you&#39;re using (Mac, Windows, Linux etc.) and how many bits (b) Which installer you used (Pip vs. Conda) (c) If you&#39;re running the VM or not.åÊ</p>
<p></p>
<p><strong>2. Search through Piazza</strong></p>
<p></p>
<p>More often than not the answer is already there on Piazza. Try and look through previous questions (especially ones endorsed by an instructor) to see if that solves your problem.</p>
<p></p>
<p><strong>3. Using Stack Overflow</strong></p>
<p></p>
<p>The best way to learn cool things and debug issues is to use the power of Crowdsourcing! Stack Overflow will definitely have the answer to your questions. As you take more advanced classes you&#39;ll need StackOverflow to try and debug more complicated issues so it will be super useful to get a head start right now. If you&#39;re ever in doubt of what StackOverflow is asking you to do - feel free to post the link and ask on Piazza.</p>
<p></p>
<p><strong>4. Using the VM</strong></p>
<p><strong></strong></p>
<p></p>
<p>Sometimes it makes more sense to just use a different platform which is pre-configured instead of trying to figure out why it doesn&#39;t work on your system. So I&#39;d suggest to everyone to use the VM. It&#39;s also super useful to get a hang of Linux (it&#39;s a rite of passage of sorts for any CS student).åÊ</p>
<p></p>
<p><strong>5. Office Hours</strong></p>
<p><strong></strong></p>
<p>Chances are if the above 3 haven&#39;t helped, Office Hours are the place to go. There are plenty spread throughout the week and more often than not you&#39;ll have at least one friendly NETS 213 TA hanging around Engineering - So swing by Office hours!åÊ</p>
<p></p>
<p>#pin</p>
<p></p>",Using Office Hours and Piazza,"<p>Hi all,</p>
<p></p>
<p>I&#39;m moving my office hours from 5-7 to 6-8 tonight. Sorry for any inconvenience.åÊ</p>
<p></p>
<p>-Ross</p>",Office Hours moved,<p>Just a note that I&#39;m moving my Thursday office hours up by an hour to 5PM. I&#39;ll update it on the calendar as well.</p>,Moving office hours up,1
940847323,4/26/2016 15:27:16,false,1969389143,,4/26/2016 15:24:27,false,elite,1.0,30280423,ITA,15,Siracusa,151.54.84.121,0,,"<p>I&#39;ll be flying out for an interview this Thursday so I can&#39;tåÊhold 12-2 OH that day.</p>
<p>Instead I&#39;ll be in the 5th floor GRW bump space today from 6-8 if anyone has questions.</p>
<p></p>
<p>Sorry about the short notice.</p>
<p></p>
<p>-Jim</p>",Change in Office Hours,"<p>Hi everyone,</p>
<p></p>
<p>I will be holding virtual office hours today from 4-6.åÊIf you want help during that time, sign up by puttingåÊyour name and gmail address in this spreadsheet, and I will call you on Google Hangouts when its your turn.åÊ</p>
<p></p>
<p>https://docs.google.com/spreadsheets/d/1enes4IDPyfWzub-1utWUSrrc9a_iNU760HARY2ymqYc/edit?usp=sharing</p>
<p></p>
<p>This will be an experiment. If it works well, itåÊwill allow me to hold office hoursåÊregularly on Thursdays (when demand is probably higher) as opposed to Friday right before class (which is probably a very stressful time to be coming to OH). Plus, you don&#39;t have to leave the comfort of your dorm room. So wins all around. :-)åÊ</p>
<p></p>
<p>--Ellie</p>
<p></p>
<p><br /></p>",Ellie&#39;s Office Hours today,"Hey everyone,<div><br /></div><div>I&#39;ll have office hours starting today at 2:15 PM in the bump space.</div>",Office Hours,"<p>Hey All,</p>
<p></p>
<p>So there have been a lot of questions on Piazza regarding errors and incompatibility issues. I&#39;ve outlined a few suggestions below to make your life easier with regards getting through them.</p>
<p></p>
<p><strong>1. Using Piazza.</strong></p>
<p></p>
<p>Remotely debugging system issues are pretty challenging. To ensure that we can get an answer to you faster - please specify (a) The System you&#39;re using (Mac, Windows, Linux etc.) and how many bits (b) Which installer you used (Pip vs. Conda) (c) If you&#39;re running the VM or not.åÊ</p>
<p></p>
<p><strong>2. Search through Piazza</strong></p>
<p></p>
<p>More often than not the answer is already there on Piazza. Try and look through previous questions (especially ones endorsed by an instructor) to see if that solves your problem.</p>
<p></p>
<p><strong>3. Using Stack Overflow</strong></p>
<p></p>
<p>The best way to learn cool things and debug issues is to use the power of Crowdsourcing! Stack Overflow will definitely have the answer to your questions. As you take more advanced classes you&#39;ll need StackOverflow to try and debug more complicated issues so it will be super useful to get a head start right now. If you&#39;re ever in doubt of what StackOverflow is asking you to do - feel free to post the link and ask on Piazza.</p>
<p></p>
<p><strong>4. Using the VM</strong></p>
<p><strong></strong></p>
<p></p>
<p>Sometimes it makes more sense to just use a different platform which is pre-configured instead of trying to figure out why it doesn&#39;t work on your system. So I&#39;d suggest to everyone to use the VM. It&#39;s also super useful to get a hang of Linux (it&#39;s a rite of passage of sorts for any CS student).åÊ</p>
<p></p>
<p><strong>5. Office Hours</strong></p>
<p><strong></strong></p>
<p>Chances are if the above 3 haven&#39;t helped, Office Hours are the place to go. There are plenty spread throughout the week and more often than not you&#39;ll have at least one friendly NETS 213 TA hanging around Engineering - So swing by Office hours!åÊ</p>
<p></p>
<p>#pin</p>
<p></p>",Using Office Hours and Piazza,"<p>Hi all,</p>
<p></p>
<p>I&#39;m moving my office hours from 5-7 to 6-8 tonight. Sorry for any inconvenience.åÊ</p>
<p></p>
<p>-Ross</p>",Office Hours moved,<p>Just a note that I&#39;m moving my Thursday office hours up by an hour to 5PM. I&#39;ll update it on the calendar as well.</p>,Moving office hours up,1
940847324,4/26/2016 17:02:29,false,1969443188,,4/26/2016 17:01:24,false,clixsense,1.0,21408115,IDN,07,Semarang,36.79.23.180,0,,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>This was probably the hardest part of the previous assignment, so if you&#39;re interested in how to do it correctly, here is the way we did it.åÊ</p>
<p></p>
<p><a href=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hl4caa4nz74ul/im89pglbq3af/EM_answer_key.pdf"" target=""_blank"">EM_answer_key.pdf</a></p>
<p></p>
<p>#pin</p>",EM answer key,5
940847324,4/26/2016 17:07:43,false,1969446211,,4/26/2016 17:05:48,false,neodev,1.0,36167043,GBR,G6,Hull,77.86.101.69,0,,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>This was probably the hardest part of the previous assignment, so if you&#39;re interested in how to do it correctly, here is the way we did it.åÊ</p>
<p></p>
<p><a href=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hl4caa4nz74ul/im89pglbq3af/EM_answer_key.pdf"" target=""_blank"">EM_answer_key.pdf</a></p>
<p></p>
<p>#pin</p>",EM answer key,5
940847324,4/26/2016 17:08:43,false,1969446811,,4/26/2016 17:06:43,false,neodev,1.0,33973110,VEN,23,Maracaibo,186.94.238.104,0,,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>This was probably the hardest part of the previous assignment, so if you&#39;re interested in how to do it correctly, here is the way we did it.åÊ</p>
<p></p>
<p><a href=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hl4caa4nz74ul/im89pglbq3af/EM_answer_key.pdf"" target=""_blank"">EM_answer_key.pdf</a></p>
<p></p>
<p>#pin</p>",EM answer key,5
940847324,4/26/2016 17:16:41,false,1969451766,,4/26/2016 17:14:02,false,elite,1.0,25411289,HRV,"","",31.147.119.175,0,,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>This was probably the hardest part of the previous assignment, so if you&#39;re interested in how to do it correctly, here is the way we did it.åÊ</p>
<p></p>
<p><a href=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hl4caa4nz74ul/im89pglbq3af/EM_answer_key.pdf"" target=""_blank"">EM_answer_key.pdf</a></p>
<p></p>
<p>#pin</p>",EM answer key,5
940847324,4/26/2016 17:30:30,false,1969459774,,4/26/2016 17:11:11,false,neodev,0.8889,19625264,DZA,41,Chlef,41.102.7.217,0,,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>This was probably the hardest part of the previous assignment, so if you&#39;re interested in how to do it correctly, here is the way we did it.åÊ</p>
<p></p>
<p><a href=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hl4caa4nz74ul/im89pglbq3af/EM_answer_key.pdf"" target=""_blank"">EM_answer_key.pdf</a></p>
<p></p>
<p>#pin</p>",EM answer key,5
940847325,4/26/2016 15:11:31,false,1969363824,,4/26/2016 15:10:32,false,tremorgames,1.0,32635967,LTU,60,Panevezys,78.63.38.165,0,,<p>This is the link:åÊhttps://github.com/benpleitner/NETS213-Final-Project</p>,Group Project,"<p>Hello,</p>
<p></p>
<p>How many projects are we supposed to review for the peer grading due tomorrow?</p>
<p>The form only has 3 projects, but I was emailed with 6 projects</p>",# of projects to review,"<p>My group is scheduled to meet withåÊDhrupad tomorrow. However, we haven&#39;t completed steps 1 and 2 of Deliverable 2 because we were under the impression that it was due on Friday. Should we reschedule our meeting to give us more time to work on this deliverable or should we discuss what we currently have and keep our meeting tomorrow?</p>",Project Meeting,,,,,"<p>Can we switch to a new final project idea? Though the peer reviews we received made it seem possible, we don&#39;t think we can fully implement the three ideas we chose and we don&#39;t think the ideas were that exciting. Our new idea that we&#39;re excited about is to let designers submit multiple versions of a design and have the crowd give them feedback on which one is &#39;better&#39;. All of us have encountered this issue in the past and we think this would be a great solution. We also are excited about theåÊword cloud aspect, where we provide the most common words in the comments that areåÊ<span style=""text-decoration:underline"">not</span> basic english words (the, it, a, is, etc.). Here&#39;s the flowchart:</p>
<p><br /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjtf4eayda2xx/imjnm7shajrb/Untitled_Diagram_1.png"" /></p>",Switching Project Ideas,1
940847325,4/26/2016 15:15:42,false,1969369977,,4/26/2016 15:14:06,false,clixsense,1.0,24287706,TWN,04,Keelung,61.231.195.173,0,,<p>This is the link:åÊhttps://github.com/benpleitner/NETS213-Final-Project</p>,Group Project,"<p>Hello,</p>
<p></p>
<p>How many projects are we supposed to review for the peer grading due tomorrow?</p>
<p>The form only has 3 projects, but I was emailed with 6 projects</p>",# of projects to review,"<p>My group is scheduled to meet withåÊDhrupad tomorrow. However, we haven&#39;t completed steps 1 and 2 of Deliverable 2 because we were under the impression that it was due on Friday. Should we reschedule our meeting to give us more time to work on this deliverable or should we discuss what we currently have and keep our meeting tomorrow?</p>",Project Meeting,,,,,"<p>Can we switch to a new final project idea? Though the peer reviews we received made it seem possible, we don&#39;t think we can fully implement the three ideas we chose and we don&#39;t think the ideas were that exciting. Our new idea that we&#39;re excited about is to let designers submit multiple versions of a design and have the crowd give them feedback on which one is &#39;better&#39;. All of us have encountered this issue in the past and we think this would be a great solution. We also are excited about theåÊword cloud aspect, where we provide the most common words in the comments that areåÊ<span style=""text-decoration:underline"">not</span> basic english words (the, it, a, is, etc.). Here&#39;s the flowchart:</p>
<p><br /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjtf4eayda2xx/imjnm7shajrb/Untitled_Diagram_1.png"" /></p>",Switching Project Ideas,1
940847325,4/26/2016 15:19:31,false,1969376279,,4/26/2016 15:17:21,false,clixsense,1.0,7837812,SRB,00,Belgrade,79.101.254.233,0,,<p>This is the link:åÊhttps://github.com/benpleitner/NETS213-Final-Project</p>,Group Project,"<p>Hello,</p>
<p></p>
<p>How many projects are we supposed to review for the peer grading due tomorrow?</p>
<p>The form only has 3 projects, but I was emailed with 6 projects</p>",# of projects to review,"<p>My group is scheduled to meet withåÊDhrupad tomorrow. However, we haven&#39;t completed steps 1 and 2 of Deliverable 2 because we were under the impression that it was due on Friday. Should we reschedule our meeting to give us more time to work on this deliverable or should we discuss what we currently have and keep our meeting tomorrow?</p>",Project Meeting,,,,,"<p>Can we switch to a new final project idea? Though the peer reviews we received made it seem possible, we don&#39;t think we can fully implement the three ideas we chose and we don&#39;t think the ideas were that exciting. Our new idea that we&#39;re excited about is to let designers submit multiple versions of a design and have the crowd give them feedback on which one is &#39;better&#39;. All of us have encountered this issue in the past and we think this would be a great solution. We also are excited about theåÊword cloud aspect, where we provide the most common words in the comments that areåÊ<span style=""text-decoration:underline"">not</span> basic english words (the, it, a, is, etc.). Here&#39;s the flowchart:</p>
<p><br /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjtf4eayda2xx/imjnm7shajrb/Untitled_Diagram_1.png"" /></p>",Switching Project Ideas,1
940847325,4/26/2016 15:19:53,false,1969376820,,4/26/2016 15:19:30,false,neodev,1.0,19132694,LKA,36,Colombo,123.231.124.170,0,,<p>This is the link:åÊhttps://github.com/benpleitner/NETS213-Final-Project</p>,Group Project,"<p>Hello,</p>
<p></p>
<p>How many projects are we supposed to review for the peer grading due tomorrow?</p>
<p>The form only has 3 projects, but I was emailed with 6 projects</p>",# of projects to review,"<p>My group is scheduled to meet withåÊDhrupad tomorrow. However, we haven&#39;t completed steps 1 and 2 of Deliverable 2 because we were under the impression that it was due on Friday. Should we reschedule our meeting to give us more time to work on this deliverable or should we discuss what we currently have and keep our meeting tomorrow?</p>",Project Meeting,,,,,"<p>Can we switch to a new final project idea? Though the peer reviews we received made it seem possible, we don&#39;t think we can fully implement the three ideas we chose and we don&#39;t think the ideas were that exciting. Our new idea that we&#39;re excited about is to let designers submit multiple versions of a design and have the crowd give them feedback on which one is &#39;better&#39;. All of us have encountered this issue in the past and we think this would be a great solution. We also are excited about theåÊword cloud aspect, where we provide the most common words in the comments that areåÊ<span style=""text-decoration:underline"">not</span> basic english words (the, it, a, is, etc.). Here&#39;s the flowchart:</p>
<p><br /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjtf4eayda2xx/imjnm7shajrb/Untitled_Diagram_1.png"" /></p>",Switching Project Ideas,1
940847325,4/26/2016 15:24:26,false,1969384554,,4/26/2016 15:21:07,false,elite,1.0,30280423,ITA,15,Siracusa,151.54.84.121,0,,<p>This is the link:åÊhttps://github.com/benpleitner/NETS213-Final-Project</p>,Group Project,"<p>Hello,</p>
<p></p>
<p>How many projects are we supposed to review for the peer grading due tomorrow?</p>
<p>The form only has 3 projects, but I was emailed with 6 projects</p>",# of projects to review,"<p>My group is scheduled to meet withåÊDhrupad tomorrow. However, we haven&#39;t completed steps 1 and 2 of Deliverable 2 because we were under the impression that it was due on Friday. Should we reschedule our meeting to give us more time to work on this deliverable or should we discuss what we currently have and keep our meeting tomorrow?</p>",Project Meeting,,,,,"<p>Can we switch to a new final project idea? Though the peer reviews we received made it seem possible, we don&#39;t think we can fully implement the three ideas we chose and we don&#39;t think the ideas were that exciting. Our new idea that we&#39;re excited about is to let designers submit multiple versions of a design and have the crowd give them feedback on which one is &#39;better&#39;. All of us have encountered this issue in the past and we think this would be a great solution. We also are excited about theåÊword cloud aspect, where we provide the most common words in the comments that areåÊ<span style=""text-decoration:underline"">not</span> basic english words (the, it, a, is, etc.). Here&#39;s the flowchart:</p>
<p><br /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjtf4eayda2xx/imjnm7shajrb/Untitled_Diagram_1.png"" /></p>",Switching Project Ideas,1
940847326,4/26/2016 17:23:10,false,1969455463,,4/26/2016 17:16:53,false,neodev,1.0,33973110,VEN,23,Maracaibo,186.94.238.104,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I&#39;m attempting to make it to Katie&#39;s office hours that end at 11:50 but wanted to post this question here if I don&#39;t make it.</p>
<p></p>
<p>My partner and I have been trying to get the &#34;No More Rows Remaining&#34; problem resolved on Crowdflower for about a day now. åÊWe have tried using both our accounts and get that error, and I even made another account and we added it to the team (as per someone&#39;s suggestion on Piazza), but still get that error when trying to launch. åÊI submitted a ticket, but Crowdflower still hasn&#39;t responded.</p>
<p></p>",No More Rows Remaining,2
940847326,4/26/2016 17:24:11,false,1969455990,,4/26/2016 17:22:14,false,elite,1.0,25411289,HRV,"","",31.147.119.175,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I&#39;m attempting to make it to Katie&#39;s office hours that end at 11:50 but wanted to post this question here if I don&#39;t make it.</p>
<p></p>
<p>My partner and I have been trying to get the &#34;No More Rows Remaining&#34; problem resolved on Crowdflower for about a day now. åÊWe have tried using both our accounts and get that error, and I even made another account and we added it to the team (as per someone&#39;s suggestion on Piazza), but still get that error when trying to launch. åÊI submitted a ticket, but Crowdflower still hasn&#39;t responded.</p>
<p></p>",No More Rows Remaining,2
940847326,4/26/2016 17:31:56,false,1969460623,,4/26/2016 17:31:26,false,neodev,0.8889,33131546,IDN,04,Jakarta,139.194.89.60,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I&#39;m attempting to make it to Katie&#39;s office hours that end at 11:50 but wanted to post this question here if I don&#39;t make it.</p>
<p></p>
<p>My partner and I have been trying to get the &#34;No More Rows Remaining&#34; problem resolved on Crowdflower for about a day now. åÊWe have tried using both our accounts and get that error, and I even made another account and we added it to the team (as per someone&#39;s suggestion on Piazza), but still get that error when trying to launch. åÊI submitted a ticket, but Crowdflower still hasn&#39;t responded.</p>
<p></p>",No More Rows Remaining,2
940847326,4/26/2016 17:41:50,false,1969466063,,4/26/2016 17:32:10,false,neodev,0.8889,19625264,DZA,41,Chlef,41.102.7.217,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I&#39;m attempting to make it to Katie&#39;s office hours that end at 11:50 but wanted to post this question here if I don&#39;t make it.</p>
<p></p>
<p>My partner and I have been trying to get the &#34;No More Rows Remaining&#34; problem resolved on Crowdflower for about a day now. åÊWe have tried using both our accounts and get that error, and I even made another account and we added it to the team (as per someone&#39;s suggestion on Piazza), but still get that error when trying to launch. åÊI submitted a ticket, but Crowdflower still hasn&#39;t responded.</p>
<p></p>",No More Rows Remaining,2
940847326,4/26/2016 17:46:50,false,1969469062,,4/26/2016 17:32:15,false,clixsense,0.8889,35338593,ITA,14,Cagliari,151.56.132.145,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I&#39;m attempting to make it to Katie&#39;s office hours that end at 11:50 but wanted to post this question here if I don&#39;t make it.</p>
<p></p>
<p>My partner and I have been trying to get the &#34;No More Rows Remaining&#34; problem resolved on Crowdflower for about a day now. åÊWe have tried using both our accounts and get that error, and I even made another account and we added it to the team (as per someone&#39;s suggestion on Piazza), but still get that error when trying to launch. åÊI submitted a ticket, but Crowdflower still hasn&#39;t responded.</p>
<p></p>",No More Rows Remaining,2
940847327,4/26/2016 16:02:51,false,1969405075,,4/26/2016 16:02:22,false,personaly,1.0,33663352,ARG,01,Mar Del Plata,181.168.213.227,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>Hi,</p>
<p></p>
<p>I&#39;m having trouble with scipy.stats.kendalltau function here.</p>
<p>It does return valid float values when I compare majority vote result and crowdflower result.</p>
<p>However, whenever weighted vote result is involved, it returns nan for both tau and p-value.</p>
<p></p>
<p>My weighted vote result for worker quality is actually 1.0 for all the workers. This seemed to cause nan return value since as soon as I modify one of the values to be non-1.0, the function returns float values.</p>
<p></p>
<p>Could I answer as nan for the questionnaire in this case?</p>
<p></p>
<p>Thanks</p>",scipy.stats.kendalltau returing nan as results,4
940847327,4/26/2016 16:18:19,false,1969413583,,4/26/2016 16:15:37,false,elite,1.0,30128662,BGR,50,Pleven,212.233.177.195,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>Hi,</p>
<p></p>
<p>I&#39;m having trouble with scipy.stats.kendalltau function here.</p>
<p>It does return valid float values when I compare majority vote result and crowdflower result.</p>
<p>However, whenever weighted vote result is involved, it returns nan for both tau and p-value.</p>
<p></p>
<p>My weighted vote result for worker quality is actually 1.0 for all the workers. This seemed to cause nan return value since as soon as I modify one of the values to be non-1.0, the function returns float values.</p>
<p></p>
<p>Could I answer as nan for the questionnaire in this case?</p>
<p></p>
<p>Thanks</p>",scipy.stats.kendalltau returing nan as results,4
940847327,4/26/2016 16:23:45,false,1969418316,,4/26/2016 16:22:19,false,neodev,1.0,29175140,VEN,25,Caracas,190.72.125.134,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>Hi,</p>
<p></p>
<p>I&#39;m having trouble with scipy.stats.kendalltau function here.</p>
<p>It does return valid float values when I compare majority vote result and crowdflower result.</p>
<p>However, whenever weighted vote result is involved, it returns nan for both tau and p-value.</p>
<p></p>
<p>My weighted vote result for worker quality is actually 1.0 for all the workers. This seemed to cause nan return value since as soon as I modify one of the values to be non-1.0, the function returns float values.</p>
<p></p>
<p>Could I answer as nan for the questionnaire in this case?</p>
<p></p>
<p>Thanks</p>",scipy.stats.kendalltau returing nan as results,4
940847327,4/26/2016 16:31:08,false,1969424240,,4/26/2016 16:19:53,false,neodev,0.7778,32569659,USA,MN,Minneapolis,97.127.88.224,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>Hi,</p>
<p></p>
<p>I&#39;m having trouble with scipy.stats.kendalltau function here.</p>
<p>It does return valid float values when I compare majority vote result and crowdflower result.</p>
<p>However, whenever weighted vote result is involved, it returns nan for both tau and p-value.</p>
<p></p>
<p>My weighted vote result for worker quality is actually 1.0 for all the workers. This seemed to cause nan return value since as soon as I modify one of the values to be non-1.0, the function returns float values.</p>
<p></p>
<p>Could I answer as nan for the questionnaire in this case?</p>
<p></p>
<p>Thanks</p>",scipy.stats.kendalltau returing nan as results,4
940847327,4/26/2016 16:32:36,false,1969425035,,4/26/2016 16:13:00,false,clixsense,0.8889,8057247,PRT,17,Póvoa De Varzim,144.64.25.68,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>Hi,</p>
<p></p>
<p>I&#39;m having trouble with scipy.stats.kendalltau function here.</p>
<p>It does return valid float values when I compare majority vote result and crowdflower result.</p>
<p>However, whenever weighted vote result is involved, it returns nan for both tau and p-value.</p>
<p></p>
<p>My weighted vote result for worker quality is actually 1.0 for all the workers. This seemed to cause nan return value since as soon as I modify one of the values to be non-1.0, the function returns float values.</p>
<p></p>
<p>Could I answer as nan for the questionnaire in this case?</p>
<p></p>
<p>Thanks</p>",scipy.stats.kendalltau returing nan as results,4
940847328,4/26/2016 16:02:51,false,1969405074,,4/26/2016 16:02:22,false,personaly,1.0,33663352,ARG,01,Mar Del Plata,181.168.213.227,0,,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"I had about 3200 urls when I ran the get_clean_text.py on the biglab machines, but when I counted the number of lines of the file obtained after running BeautifulSoup, I only have 1379 lines (i.e articles and urls). Is this normal, or should I scrape the urls again? I believe a lot of urls&#39; articles didn&#39;t get scraped because of exception handling in get_clean_text.py",Number of articles and urls is significantly lesser than urls,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,2
940847328,4/26/2016 16:18:19,false,1969413577,,4/26/2016 16:15:37,false,elite,1.0,30128662,BGR,50,Pleven,212.233.177.195,0,,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"I had about 3200 urls when I ran the get_clean_text.py on the biglab machines, but when I counted the number of lines of the file obtained after running BeautifulSoup, I only have 1379 lines (i.e articles and urls). Is this normal, or should I scrape the urls again? I believe a lot of urls&#39; articles didn&#39;t get scraped because of exception handling in get_clean_text.py",Number of articles and urls is significantly lesser than urls,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,2
940847328,4/26/2016 16:23:45,false,1969418304,,4/26/2016 16:22:19,false,neodev,1.0,29175140,VEN,25,Caracas,190.72.125.134,0,,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"I had about 3200 urls when I ran the get_clean_text.py on the biglab machines, but when I counted the number of lines of the file obtained after running BeautifulSoup, I only have 1379 lines (i.e articles and urls). Is this normal, or should I scrape the urls again? I believe a lot of urls&#39; articles didn&#39;t get scraped because of exception handling in get_clean_text.py",Number of articles and urls is significantly lesser than urls,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,2
940847328,4/26/2016 16:31:08,false,1969424237,,4/26/2016 16:19:53,false,neodev,0.7778,32569659,USA,MN,Minneapolis,97.127.88.224,0,,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"I had about 3200 urls when I ran the get_clean_text.py on the biglab machines, but when I counted the number of lines of the file obtained after running BeautifulSoup, I only have 1379 lines (i.e articles and urls). Is this normal, or should I scrape the urls again? I believe a lot of urls&#39; articles didn&#39;t get scraped because of exception handling in get_clean_text.py",Number of articles and urls is significantly lesser than urls,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,2
940847328,4/26/2016 16:32:36,false,1969425039,,4/26/2016 16:13:00,false,clixsense,0.8889,8057247,PRT,17,Póvoa De Varzim,144.64.25.68,0,,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"I had about 3200 urls when I ran the get_clean_text.py on the biglab machines, but when I counted the number of lines of the file obtained after running BeautifulSoup, I only have 1379 lines (i.e articles and urls). Is this normal, or should I scrape the urls again? I believe a lot of urls&#39; articles didn&#39;t get scraped because of exception handling in get_clean_text.py",Number of articles and urls is significantly lesser than urls,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,2
940847329,4/26/2016 17:08:54,false,1969446905,,4/26/2016 17:07:44,false,neodev,1.0,36167043,GBR,G6,Hull,77.86.101.69,0,,"<p>RESOLVED:</p>
<p></p>
<p>CCB had this answer in response to the &#34;dot command not found&#34; post:</p>
<p></p>
<blockquote>
<p>I believe that those packages are just for outputting things to graphviz format from python. åÊYou also need to install graphviz itself by downloading itåÊ<a href=""http://graphviz.org/Download.php"" target=""_blank"">here</a>.</p>
<p></p>
</blockquote>",Is anyone else having issues accessing Graphviz website?,"<p>As mentioned in a previous piazza post, I&#39;m getting a strange error when running extract_text.py in which I receive the following output:</p>
<p></p>
<p>ERROR \t url</p>
<p>ERROR \t url</p>
<p>ERROR \t url</p>
<p></p>
<p>for all my URLs. I&#39;m not sure what the problem could be, but I think it&#39;s a bad API call.</p>
<p></p>
<p>my code is as follows:</p>
<pre>def construct_api_call(url):
  #TODO modify this method to build a text extraction API call for the specified URL
  requeststr = &#39;http://access.alchemyapi.com/calls/url/URLGetText?apikey=[9245f4cc6f60f2e37fd4c7a2901b2bba66a85dec]&amp;url=[&#39; &#43; url.strip() &#43; &#39;]&amp;outputMode=json&#39;
  return requeststr</pre>
<p>and also</p>
<pre>for url in sys.stdin : 
  url = url.strip()
  txt = get_text(url)
  #TODO process each line of input, and output the url and clean text (separated by a tab)
  #HINT: make sure you check the return value of get_text(). If there is an error with the request, get_text 
  #will return None, and you should not print the line.
  if txt == None:
      continue
  else:
    print url &#43; &#39;\t&#39; &#43; txt &#43; &#39;\n&#39;</pre>
<p>Any help would be appreciated. I haven&#39;t been able to figure out the source of the problem.</p>",Issue with extract_text.py,"<p>Hi,</p>
<p>I had an issue with installing ipython using pip and could just get it done through anaconda.</p>
<p></p>
<p>In case any of you guys are having same issue, here&#39;s the link to download anaconda and instructions</p>
<p></p>
<p><a href=""https://www.continuum.io/downloads"">https://www.continuum.io/downloads</a></p>
<p></p>
<p>download whatever fits you and then run the command as the instruction says.</p>
<p></p>
<p>Then you could follow the &#34;in case you are using anaconda&#34; part in iPython Notebook installation guide from class web&#39;s resource page.</p>",Issue with ipython[notebook] installation,"<p>I&#39;m going through the iPython install guide, and am running into a snag where when I run</p>
<pre>sudo pip install virtualenv </pre>
<p>I get an error message saying &#34;The directory &#39;/Users/oldgracearnold/Library/Caches/pip/http&#39; or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo&#39;s -H flag.&#34;åÊ</p>
<p>I&#39;ve also tried running the above command with an -H flag, to no avail.</p>
<p></p>
<p>I&#39;ve done a bit of searching and this StackOverflow post describes similar symptoms to what I&#39;m having:åÊ</p>
<p><a href=""http://stackoverflow.com/questions/27870003/pip-install-please-check-the-permissions-and-owner-of-that-directory"">http://stackoverflow.com/questions/27870003/pip-install-please-check-the-permissions-and-owner-of-that-directory</a></p>
<p>In particular, my command line used to be &#34;oldgracearnold$&#34; and now it&#39;s &#34;Graces-MBP:~ oldgracearnold$&#34;, which must have happened sometime during my install process? (I first installed Python 3, then decided to update my Python 2 from 2.5 to 2.7)åÊ</p>
<p></p>
<p>Anyway, because of all this, I haven&#39;t been able to set up my virtualenv and install iPython notebook. Any thoughts on what the problem could be?</p>
<p></p>",Issue with setting up virtual environment,"<p>When running bing_api.py after entering my primary account key, I get the following output</p>
<p></p>
<pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p></p>
<p>How can I fix this?</p>",bing_api.py Authorization Issue,"<p>I am confused about my score on the training the classifier hw3 assignment. The results say that I did not submit code? Perhaps there was an error with the submission system, but I worked very hard on the assignment, and I am positive that I ran turnin before the deadline. Why was my code not found, andåÊwhat should I do to remedy this situation?</p>
<div>
<div></div>
</div>",HW3 Score Issue,0
940847329,4/26/2016 17:16:52,false,1969451846,,4/26/2016 17:08:45,false,neodev,1.0,33973110,VEN,23,Maracaibo,186.94.238.104,0,,"<p>RESOLVED:</p>
<p></p>
<p>CCB had this answer in response to the &#34;dot command not found&#34; post:</p>
<p></p>
<blockquote>
<p>I believe that those packages are just for outputting things to graphviz format from python. åÊYou also need to install graphviz itself by downloading itåÊ<a href=""http://graphviz.org/Download.php"" target=""_blank"">here</a>.</p>
<p></p>
</blockquote>",Is anyone else having issues accessing Graphviz website?,"<p>As mentioned in a previous piazza post, I&#39;m getting a strange error when running extract_text.py in which I receive the following output:</p>
<p></p>
<p>ERROR \t url</p>
<p>ERROR \t url</p>
<p>ERROR \t url</p>
<p></p>
<p>for all my URLs. I&#39;m not sure what the problem could be, but I think it&#39;s a bad API call.</p>
<p></p>
<p>my code is as follows:</p>
<pre>def construct_api_call(url):
  #TODO modify this method to build a text extraction API call for the specified URL
  requeststr = &#39;http://access.alchemyapi.com/calls/url/URLGetText?apikey=[9245f4cc6f60f2e37fd4c7a2901b2bba66a85dec]&amp;url=[&#39; &#43; url.strip() &#43; &#39;]&amp;outputMode=json&#39;
  return requeststr</pre>
<p>and also</p>
<pre>for url in sys.stdin : 
  url = url.strip()
  txt = get_text(url)
  #TODO process each line of input, and output the url and clean text (separated by a tab)
  #HINT: make sure you check the return value of get_text(). If there is an error with the request, get_text 
  #will return None, and you should not print the line.
  if txt == None:
      continue
  else:
    print url &#43; &#39;\t&#39; &#43; txt &#43; &#39;\n&#39;</pre>
<p>Any help would be appreciated. I haven&#39;t been able to figure out the source of the problem.</p>",Issue with extract_text.py,"<p>Hi,</p>
<p>I had an issue with installing ipython using pip and could just get it done through anaconda.</p>
<p></p>
<p>In case any of you guys are having same issue, here&#39;s the link to download anaconda and instructions</p>
<p></p>
<p><a href=""https://www.continuum.io/downloads"">https://www.continuum.io/downloads</a></p>
<p></p>
<p>download whatever fits you and then run the command as the instruction says.</p>
<p></p>
<p>Then you could follow the &#34;in case you are using anaconda&#34; part in iPython Notebook installation guide from class web&#39;s resource page.</p>",Issue with ipython[notebook] installation,"<p>I&#39;m going through the iPython install guide, and am running into a snag where when I run</p>
<pre>sudo pip install virtualenv </pre>
<p>I get an error message saying &#34;The directory &#39;/Users/oldgracearnold/Library/Caches/pip/http&#39; or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo&#39;s -H flag.&#34;åÊ</p>
<p>I&#39;ve also tried running the above command with an -H flag, to no avail.</p>
<p></p>
<p>I&#39;ve done a bit of searching and this StackOverflow post describes similar symptoms to what I&#39;m having:åÊ</p>
<p><a href=""http://stackoverflow.com/questions/27870003/pip-install-please-check-the-permissions-and-owner-of-that-directory"">http://stackoverflow.com/questions/27870003/pip-install-please-check-the-permissions-and-owner-of-that-directory</a></p>
<p>In particular, my command line used to be &#34;oldgracearnold$&#34; and now it&#39;s &#34;Graces-MBP:~ oldgracearnold$&#34;, which must have happened sometime during my install process? (I first installed Python 3, then decided to update my Python 2 from 2.5 to 2.7)åÊ</p>
<p></p>
<p>Anyway, because of all this, I haven&#39;t been able to set up my virtualenv and install iPython notebook. Any thoughts on what the problem could be?</p>
<p></p>",Issue with setting up virtual environment,"<p>When running bing_api.py after entering my primary account key, I get the following output</p>
<p></p>
<pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p></p>
<p>How can I fix this?</p>",bing_api.py Authorization Issue,"<p>I am confused about my score on the training the classifier hw3 assignment. The results say that I did not submit code? Perhaps there was an error with the submission system, but I worked very hard on the assignment, and I am positive that I ran turnin before the deadline. Why was my code not found, andåÊwhat should I do to remedy this situation?</p>
<div>
<div></div>
</div>",HW3 Score Issue,0
940847329,4/26/2016 17:19:05,false,1969453024,,4/26/2016 17:16:43,false,elite,1.0,25411289,HRV,"","",31.147.119.175,0,,"<p>RESOLVED:</p>
<p></p>
<p>CCB had this answer in response to the &#34;dot command not found&#34; post:</p>
<p></p>
<blockquote>
<p>I believe that those packages are just for outputting things to graphviz format from python. åÊYou also need to install graphviz itself by downloading itåÊ<a href=""http://graphviz.org/Download.php"" target=""_blank"">here</a>.</p>
<p></p>
</blockquote>",Is anyone else having issues accessing Graphviz website?,"<p>As mentioned in a previous piazza post, I&#39;m getting a strange error when running extract_text.py in which I receive the following output:</p>
<p></p>
<p>ERROR \t url</p>
<p>ERROR \t url</p>
<p>ERROR \t url</p>
<p></p>
<p>for all my URLs. I&#39;m not sure what the problem could be, but I think it&#39;s a bad API call.</p>
<p></p>
<p>my code is as follows:</p>
<pre>def construct_api_call(url):
  #TODO modify this method to build a text extraction API call for the specified URL
  requeststr = &#39;http://access.alchemyapi.com/calls/url/URLGetText?apikey=[9245f4cc6f60f2e37fd4c7a2901b2bba66a85dec]&amp;url=[&#39; &#43; url.strip() &#43; &#39;]&amp;outputMode=json&#39;
  return requeststr</pre>
<p>and also</p>
<pre>for url in sys.stdin : 
  url = url.strip()
  txt = get_text(url)
  #TODO process each line of input, and output the url and clean text (separated by a tab)
  #HINT: make sure you check the return value of get_text(). If there is an error with the request, get_text 
  #will return None, and you should not print the line.
  if txt == None:
      continue
  else:
    print url &#43; &#39;\t&#39; &#43; txt &#43; &#39;\n&#39;</pre>
<p>Any help would be appreciated. I haven&#39;t been able to figure out the source of the problem.</p>",Issue with extract_text.py,"<p>Hi,</p>
<p>I had an issue with installing ipython using pip and could just get it done through anaconda.</p>
<p></p>
<p>In case any of you guys are having same issue, here&#39;s the link to download anaconda and instructions</p>
<p></p>
<p><a href=""https://www.continuum.io/downloads"">https://www.continuum.io/downloads</a></p>
<p></p>
<p>download whatever fits you and then run the command as the instruction says.</p>
<p></p>
<p>Then you could follow the &#34;in case you are using anaconda&#34; part in iPython Notebook installation guide from class web&#39;s resource page.</p>",Issue with ipython[notebook] installation,"<p>I&#39;m going through the iPython install guide, and am running into a snag where when I run</p>
<pre>sudo pip install virtualenv </pre>
<p>I get an error message saying &#34;The directory &#39;/Users/oldgracearnold/Library/Caches/pip/http&#39; or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo&#39;s -H flag.&#34;åÊ</p>
<p>I&#39;ve also tried running the above command with an -H flag, to no avail.</p>
<p></p>
<p>I&#39;ve done a bit of searching and this StackOverflow post describes similar symptoms to what I&#39;m having:åÊ</p>
<p><a href=""http://stackoverflow.com/questions/27870003/pip-install-please-check-the-permissions-and-owner-of-that-directory"">http://stackoverflow.com/questions/27870003/pip-install-please-check-the-permissions-and-owner-of-that-directory</a></p>
<p>In particular, my command line used to be &#34;oldgracearnold$&#34; and now it&#39;s &#34;Graces-MBP:~ oldgracearnold$&#34;, which must have happened sometime during my install process? (I first installed Python 3, then decided to update my Python 2 from 2.5 to 2.7)åÊ</p>
<p></p>
<p>Anyway, because of all this, I haven&#39;t been able to set up my virtualenv and install iPython notebook. Any thoughts on what the problem could be?</p>
<p></p>",Issue with setting up virtual environment,"<p>When running bing_api.py after entering my primary account key, I get the following output</p>
<p></p>
<pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p></p>
<p>How can I fix this?</p>",bing_api.py Authorization Issue,"<p>I am confused about my score on the training the classifier hw3 assignment. The results say that I did not submit code? Perhaps there was an error with the submission system, but I worked very hard on the assignment, and I am positive that I ran turnin before the deadline. Why was my code not found, andåÊwhat should I do to remedy this situation?</p>
<div>
<div></div>
</div>",HW3 Score Issue,0
940847329,4/26/2016 17:32:09,false,1969460737,,4/26/2016 17:30:32,false,neodev,0.8889,19625264,DZA,41,Chlef,41.102.7.217,0,,"<p>RESOLVED:</p>
<p></p>
<p>CCB had this answer in response to the &#34;dot command not found&#34; post:</p>
<p></p>
<blockquote>
<p>I believe that those packages are just for outputting things to graphviz format from python. åÊYou also need to install graphviz itself by downloading itåÊ<a href=""http://graphviz.org/Download.php"" target=""_blank"">here</a>.</p>
<p></p>
</blockquote>",Is anyone else having issues accessing Graphviz website?,"<p>As mentioned in a previous piazza post, I&#39;m getting a strange error when running extract_text.py in which I receive the following output:</p>
<p></p>
<p>ERROR \t url</p>
<p>ERROR \t url</p>
<p>ERROR \t url</p>
<p></p>
<p>for all my URLs. I&#39;m not sure what the problem could be, but I think it&#39;s a bad API call.</p>
<p></p>
<p>my code is as follows:</p>
<pre>def construct_api_call(url):
  #TODO modify this method to build a text extraction API call for the specified URL
  requeststr = &#39;http://access.alchemyapi.com/calls/url/URLGetText?apikey=[9245f4cc6f60f2e37fd4c7a2901b2bba66a85dec]&amp;url=[&#39; &#43; url.strip() &#43; &#39;]&amp;outputMode=json&#39;
  return requeststr</pre>
<p>and also</p>
<pre>for url in sys.stdin : 
  url = url.strip()
  txt = get_text(url)
  #TODO process each line of input, and output the url and clean text (separated by a tab)
  #HINT: make sure you check the return value of get_text(). If there is an error with the request, get_text 
  #will return None, and you should not print the line.
  if txt == None:
      continue
  else:
    print url &#43; &#39;\t&#39; &#43; txt &#43; &#39;\n&#39;</pre>
<p>Any help would be appreciated. I haven&#39;t been able to figure out the source of the problem.</p>",Issue with extract_text.py,"<p>Hi,</p>
<p>I had an issue with installing ipython using pip and could just get it done through anaconda.</p>
<p></p>
<p>In case any of you guys are having same issue, here&#39;s the link to download anaconda and instructions</p>
<p></p>
<p><a href=""https://www.continuum.io/downloads"">https://www.continuum.io/downloads</a></p>
<p></p>
<p>download whatever fits you and then run the command as the instruction says.</p>
<p></p>
<p>Then you could follow the &#34;in case you are using anaconda&#34; part in iPython Notebook installation guide from class web&#39;s resource page.</p>",Issue with ipython[notebook] installation,"<p>I&#39;m going through the iPython install guide, and am running into a snag where when I run</p>
<pre>sudo pip install virtualenv </pre>
<p>I get an error message saying &#34;The directory &#39;/Users/oldgracearnold/Library/Caches/pip/http&#39; or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo&#39;s -H flag.&#34;åÊ</p>
<p>I&#39;ve also tried running the above command with an -H flag, to no avail.</p>
<p></p>
<p>I&#39;ve done a bit of searching and this StackOverflow post describes similar symptoms to what I&#39;m having:åÊ</p>
<p><a href=""http://stackoverflow.com/questions/27870003/pip-install-please-check-the-permissions-and-owner-of-that-directory"">http://stackoverflow.com/questions/27870003/pip-install-please-check-the-permissions-and-owner-of-that-directory</a></p>
<p>In particular, my command line used to be &#34;oldgracearnold$&#34; and now it&#39;s &#34;Graces-MBP:~ oldgracearnold$&#34;, which must have happened sometime during my install process? (I first installed Python 3, then decided to update my Python 2 from 2.5 to 2.7)åÊ</p>
<p></p>
<p>Anyway, because of all this, I haven&#39;t been able to set up my virtualenv and install iPython notebook. Any thoughts on what the problem could be?</p>
<p></p>",Issue with setting up virtual environment,"<p>When running bing_api.py after entering my primary account key, I get the following output</p>
<p></p>
<pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p></p>
<p>How can I fix this?</p>",bing_api.py Authorization Issue,"<p>I am confused about my score on the training the classifier hw3 assignment. The results say that I did not submit code? Perhaps there was an error with the submission system, but I worked very hard on the assignment, and I am positive that I ran turnin before the deadline. Why was my code not found, andåÊwhat should I do to remedy this situation?</p>
<div>
<div></div>
</div>",HW3 Score Issue,0
940847329,4/26/2016 17:32:14,false,1969460763,,4/26/2016 17:15:40,false,clixsense,0.8889,35338593,ITA,14,Cagliari,151.56.132.145,0,,"<p>RESOLVED:</p>
<p></p>
<p>CCB had this answer in response to the &#34;dot command not found&#34; post:</p>
<p></p>
<blockquote>
<p>I believe that those packages are just for outputting things to graphviz format from python. åÊYou also need to install graphviz itself by downloading itåÊ<a href=""http://graphviz.org/Download.php"" target=""_blank"">here</a>.</p>
<p></p>
</blockquote>",Is anyone else having issues accessing Graphviz website?,"<p>As mentioned in a previous piazza post, I&#39;m getting a strange error when running extract_text.py in which I receive the following output:</p>
<p></p>
<p>ERROR \t url</p>
<p>ERROR \t url</p>
<p>ERROR \t url</p>
<p></p>
<p>for all my URLs. I&#39;m not sure what the problem could be, but I think it&#39;s a bad API call.</p>
<p></p>
<p>my code is as follows:</p>
<pre>def construct_api_call(url):
  #TODO modify this method to build a text extraction API call for the specified URL
  requeststr = &#39;http://access.alchemyapi.com/calls/url/URLGetText?apikey=[9245f4cc6f60f2e37fd4c7a2901b2bba66a85dec]&amp;url=[&#39; &#43; url.strip() &#43; &#39;]&amp;outputMode=json&#39;
  return requeststr</pre>
<p>and also</p>
<pre>for url in sys.stdin : 
  url = url.strip()
  txt = get_text(url)
  #TODO process each line of input, and output the url and clean text (separated by a tab)
  #HINT: make sure you check the return value of get_text(). If there is an error with the request, get_text 
  #will return None, and you should not print the line.
  if txt == None:
      continue
  else:
    print url &#43; &#39;\t&#39; &#43; txt &#43; &#39;\n&#39;</pre>
<p>Any help would be appreciated. I haven&#39;t been able to figure out the source of the problem.</p>",Issue with extract_text.py,"<p>Hi,</p>
<p>I had an issue with installing ipython using pip and could just get it done through anaconda.</p>
<p></p>
<p>In case any of you guys are having same issue, here&#39;s the link to download anaconda and instructions</p>
<p></p>
<p><a href=""https://www.continuum.io/downloads"">https://www.continuum.io/downloads</a></p>
<p></p>
<p>download whatever fits you and then run the command as the instruction says.</p>
<p></p>
<p>Then you could follow the &#34;in case you are using anaconda&#34; part in iPython Notebook installation guide from class web&#39;s resource page.</p>",Issue with ipython[notebook] installation,"<p>I&#39;m going through the iPython install guide, and am running into a snag where when I run</p>
<pre>sudo pip install virtualenv </pre>
<p>I get an error message saying &#34;The directory &#39;/Users/oldgracearnold/Library/Caches/pip/http&#39; or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo&#39;s -H flag.&#34;åÊ</p>
<p>I&#39;ve also tried running the above command with an -H flag, to no avail.</p>
<p></p>
<p>I&#39;ve done a bit of searching and this StackOverflow post describes similar symptoms to what I&#39;m having:åÊ</p>
<p><a href=""http://stackoverflow.com/questions/27870003/pip-install-please-check-the-permissions-and-owner-of-that-directory"">http://stackoverflow.com/questions/27870003/pip-install-please-check-the-permissions-and-owner-of-that-directory</a></p>
<p>In particular, my command line used to be &#34;oldgracearnold$&#34; and now it&#39;s &#34;Graces-MBP:~ oldgracearnold$&#34;, which must have happened sometime during my install process? (I first installed Python 3, then decided to update my Python 2 from 2.5 to 2.7)åÊ</p>
<p></p>
<p>Anyway, because of all this, I haven&#39;t been able to set up my virtualenv and install iPython notebook. Any thoughts on what the problem could be?</p>
<p></p>",Issue with setting up virtual environment,"<p>When running bing_api.py after entering my primary account key, I get the following output</p>
<p></p>
<pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p></p>
<p>How can I fix this?</p>",bing_api.py Authorization Issue,"<p>I am confused about my score on the training the classifier hw3 assignment. The results say that I did not submit code? Perhaps there was an error with the submission system, but I worked very hard on the assignment, and I am positive that I ran turnin before the deadline. Why was my code not found, andåÊwhat should I do to remedy this situation?</p>
<div>
<div></div>
</div>",HW3 Score Issue,0
940847330,4/26/2016 15:11:59,false,1969364456,,4/26/2016 15:11:46,false,tremorgames,1.0,32635967,LTU,60,Panevezys,78.63.38.165,0,,"<p>Just out of curiosity, what are the technical specs foråÊbiglab? Is it a cluster computer? Supercomputer? 16GB RAM? 128GB RAM?åÊ</p>",What are biglab&#39;s specs?,"<p>So if you spend a lot of yesterday&#39;s class obsessing over package incompatibilities or the fact that Windows was causing you undue misery, read on!</p>
<p></p>
<p>To ensure people don&#39;t have to worry about platform issues, we&#39;ve created a Linux (Ubuntu specifically) Virtual Machine that everyone can run and use for the course. The necessary packages such as iPython, virtualenv etc have already been pre-installed.</p>
<p></p>
<p>Instructions to download and run the VM can be found atåÊ<a href=""http://crowdsourcing-class.org/vm-instructions.html"">http://crowdsourcing-class.org/vm-instructions.html</a>. The link has also been provided under the Resources page on the course website.åÊ</p>
<p></p>
<p>This is the first time we&#39;re doing this so please use Piazza / Office Hours liberally if you run into any issues. Try and run Assignment 3 (Python Bootcamp) on the VM to make sure everything is working.åÊ</p>
<p></p>
<p></p>",Installing and Running a Linux Virtual Machine,"<p>How do we install Beautiful Soup on Biglab? I am asked for &#34;root&#39;s password&#34; when I use sudo pip install beautifulsoup4. I am currently running everything on my laptop, but it is very slow.åÊ</p>
<p></p>
<p></p>",Beautiful Soup installation,"Is there a way to install new Python libraries using pip on the BigLab machines? I&#39;m trying but it doesn&#39;t seem to work, and I can&#39;t use sudo either. I keep getting a Permission denied error when I use pip.",Installing new Python libraries on BigLab through SSH,<p>Is there a way to move my files on Biglab back to my laptop?åÊ</p>,Biglab,"<p></p><p>I tried the stuff listed in <a href=""https://www.piazza.com/class/ijblb017ius5zp?cid=185"">&#64;185</a> but it&#39;s still not working. It doesn&#39;t recognize pip as a command in biglab unless I use sudo before it. What is the root password for biglab?<br /></p>",Installing Beautiful Soup on Biglab,1
940847330,4/26/2016 15:20:29,false,1969377838,,4/26/2016 15:20:10,false,neodev,1.0,19132694,LKA,36,Colombo,123.231.124.170,0,,"<p>Just out of curiosity, what are the technical specs foråÊbiglab? Is it a cluster computer? Supercomputer? 16GB RAM? 128GB RAM?åÊ</p>",What are biglab&#39;s specs?,"<p>So if you spend a lot of yesterday&#39;s class obsessing over package incompatibilities or the fact that Windows was causing you undue misery, read on!</p>
<p></p>
<p>To ensure people don&#39;t have to worry about platform issues, we&#39;ve created a Linux (Ubuntu specifically) Virtual Machine that everyone can run and use for the course. The necessary packages such as iPython, virtualenv etc have already been pre-installed.</p>
<p></p>
<p>Instructions to download and run the VM can be found atåÊ<a href=""http://crowdsourcing-class.org/vm-instructions.html"">http://crowdsourcing-class.org/vm-instructions.html</a>. The link has also been provided under the Resources page on the course website.åÊ</p>
<p></p>
<p>This is the first time we&#39;re doing this so please use Piazza / Office Hours liberally if you run into any issues. Try and run Assignment 3 (Python Bootcamp) on the VM to make sure everything is working.åÊ</p>
<p></p>
<p></p>",Installing and Running a Linux Virtual Machine,"<p>How do we install Beautiful Soup on Biglab? I am asked for &#34;root&#39;s password&#34; when I use sudo pip install beautifulsoup4. I am currently running everything on my laptop, but it is very slow.åÊ</p>
<p></p>
<p></p>",Beautiful Soup installation,"Is there a way to install new Python libraries using pip on the BigLab machines? I&#39;m trying but it doesn&#39;t seem to work, and I can&#39;t use sudo either. I keep getting a Permission denied error when I use pip.",Installing new Python libraries on BigLab through SSH,<p>Is there a way to move my files on Biglab back to my laptop?åÊ</p>,Biglab,"<p></p><p>I tried the stuff listed in <a href=""https://www.piazza.com/class/ijblb017ius5zp?cid=185"">&#64;185</a> but it&#39;s still not working. It doesn&#39;t recognize pip as a command in biglab unless I use sudo before it. What is the root password for biglab?<br /></p>",Installing Beautiful Soup on Biglab,1
940847330,4/26/2016 15:20:28,false,1969377839,,4/26/2016 15:19:15,false,clixsense,1.0,24287706,TWN,04,Keelung,61.231.195.173,0,,"<p>Just out of curiosity, what are the technical specs foråÊbiglab? Is it a cluster computer? Supercomputer? 16GB RAM? 128GB RAM?åÊ</p>",What are biglab&#39;s specs?,"<p>So if you spend a lot of yesterday&#39;s class obsessing over package incompatibilities or the fact that Windows was causing you undue misery, read on!</p>
<p></p>
<p>To ensure people don&#39;t have to worry about platform issues, we&#39;ve created a Linux (Ubuntu specifically) Virtual Machine that everyone can run and use for the course. The necessary packages such as iPython, virtualenv etc have already been pre-installed.</p>
<p></p>
<p>Instructions to download and run the VM can be found atåÊ<a href=""http://crowdsourcing-class.org/vm-instructions.html"">http://crowdsourcing-class.org/vm-instructions.html</a>. The link has also been provided under the Resources page on the course website.åÊ</p>
<p></p>
<p>This is the first time we&#39;re doing this so please use Piazza / Office Hours liberally if you run into any issues. Try and run Assignment 3 (Python Bootcamp) on the VM to make sure everything is working.åÊ</p>
<p></p>
<p></p>",Installing and Running a Linux Virtual Machine,"<p>How do we install Beautiful Soup on Biglab? I am asked for &#34;root&#39;s password&#34; when I use sudo pip install beautifulsoup4. I am currently running everything on my laptop, but it is very slow.åÊ</p>
<p></p>
<p></p>",Beautiful Soup installation,"Is there a way to install new Python libraries using pip on the BigLab machines? I&#39;m trying but it doesn&#39;t seem to work, and I can&#39;t use sudo either. I keep getting a Permission denied error when I use pip.",Installing new Python libraries on BigLab through SSH,<p>Is there a way to move my files on Biglab back to my laptop?åÊ</p>,Biglab,"<p></p><p>I tried the stuff listed in <a href=""https://www.piazza.com/class/ijblb017ius5zp?cid=185"">&#64;185</a> but it&#39;s still not working. It doesn&#39;t recognize pip as a command in biglab unless I use sudo before it. What is the root password for biglab?<br /></p>",Installing Beautiful Soup on Biglab,1
940847330,4/26/2016 15:24:23,false,1969384427,,4/26/2016 15:21:38,false,clixsense,1.0,7837812,SRB,00,Belgrade,79.101.254.233,0,,"<p>Just out of curiosity, what are the technical specs foråÊbiglab? Is it a cluster computer? Supercomputer? 16GB RAM? 128GB RAM?åÊ</p>",What are biglab&#39;s specs?,"<p>So if you spend a lot of yesterday&#39;s class obsessing over package incompatibilities or the fact that Windows was causing you undue misery, read on!</p>
<p></p>
<p>To ensure people don&#39;t have to worry about platform issues, we&#39;ve created a Linux (Ubuntu specifically) Virtual Machine that everyone can run and use for the course. The necessary packages such as iPython, virtualenv etc have already been pre-installed.</p>
<p></p>
<p>Instructions to download and run the VM can be found atåÊ<a href=""http://crowdsourcing-class.org/vm-instructions.html"">http://crowdsourcing-class.org/vm-instructions.html</a>. The link has also been provided under the Resources page on the course website.åÊ</p>
<p></p>
<p>This is the first time we&#39;re doing this so please use Piazza / Office Hours liberally if you run into any issues. Try and run Assignment 3 (Python Bootcamp) on the VM to make sure everything is working.åÊ</p>
<p></p>
<p></p>",Installing and Running a Linux Virtual Machine,"<p>How do we install Beautiful Soup on Biglab? I am asked for &#34;root&#39;s password&#34; when I use sudo pip install beautifulsoup4. I am currently running everything on my laptop, but it is very slow.åÊ</p>
<p></p>
<p></p>",Beautiful Soup installation,"Is there a way to install new Python libraries using pip on the BigLab machines? I&#39;m trying but it doesn&#39;t seem to work, and I can&#39;t use sudo either. I keep getting a Permission denied error when I use pip.",Installing new Python libraries on BigLab through SSH,<p>Is there a way to move my files on Biglab back to my laptop?åÊ</p>,Biglab,"<p></p><p>I tried the stuff listed in <a href=""https://www.piazza.com/class/ijblb017ius5zp?cid=185"">&#64;185</a> but it&#39;s still not working. It doesn&#39;t recognize pip as a command in biglab unless I use sudo before it. What is the root password for biglab?<br /></p>",Installing Beautiful Soup on Biglab,1
940847330,4/26/2016 15:27:16,false,1969389139,,4/26/2016 15:24:27,false,elite,1.0,30280423,ITA,15,Siracusa,151.54.84.121,0,,"<p>Just out of curiosity, what are the technical specs foråÊbiglab? Is it a cluster computer? Supercomputer? 16GB RAM? 128GB RAM?åÊ</p>",What are biglab&#39;s specs?,"<p>So if you spend a lot of yesterday&#39;s class obsessing over package incompatibilities or the fact that Windows was causing you undue misery, read on!</p>
<p></p>
<p>To ensure people don&#39;t have to worry about platform issues, we&#39;ve created a Linux (Ubuntu specifically) Virtual Machine that everyone can run and use for the course. The necessary packages such as iPython, virtualenv etc have already been pre-installed.</p>
<p></p>
<p>Instructions to download and run the VM can be found atåÊ<a href=""http://crowdsourcing-class.org/vm-instructions.html"">http://crowdsourcing-class.org/vm-instructions.html</a>. The link has also been provided under the Resources page on the course website.åÊ</p>
<p></p>
<p>This is the first time we&#39;re doing this so please use Piazza / Office Hours liberally if you run into any issues. Try and run Assignment 3 (Python Bootcamp) on the VM to make sure everything is working.åÊ</p>
<p></p>
<p></p>",Installing and Running a Linux Virtual Machine,"<p>How do we install Beautiful Soup on Biglab? I am asked for &#34;root&#39;s password&#34; when I use sudo pip install beautifulsoup4. I am currently running everything on my laptop, but it is very slow.åÊ</p>
<p></p>
<p></p>",Beautiful Soup installation,"Is there a way to install new Python libraries using pip on the BigLab machines? I&#39;m trying but it doesn&#39;t seem to work, and I can&#39;t use sudo either. I keep getting a Permission denied error when I use pip.",Installing new Python libraries on BigLab through SSH,<p>Is there a way to move my files on Biglab back to my laptop?åÊ</p>,Biglab,"<p></p><p>I tried the stuff listed in <a href=""https://www.piazza.com/class/ijblb017ius5zp?cid=185"">&#64;185</a> but it&#39;s still not working. It doesn&#39;t recognize pip as a command in biglab unless I use sudo before it. What is the root password for biglab?<br /></p>",Installing Beautiful Soup on Biglab,1
940847331,4/26/2016 18:22:36,false,1969487897,,4/26/2016 18:21:16,false,neodev,0.8889,35550011,VEN,07,Valencia,190.204.238.112,0,,"<p>Hello, I&#39;m a little confused as to how late days work for groups. Is it that if the group wants to use a late day, all x members of the group must use their late day or is it that only one person of the group uses one late day. This question is only geared for regular homework assignments. I understand that there is a separate late day policy for the final group project. Thanks so much for the help!</p>",Late days for groups,"<p>This is a friendly reminder that <a href=""http://crowdsourcing-class.org/assignment1.html"" target=""_blank"">homework 1</a> is due tomorrow before class. åÊ</p>
<p></p>
<p>I would also like to draw your attention to the course&#39;s late day policy (given on the <a href=""http://crowdsourcing-class.org"" target=""_blank"">course web page</a>).</p>
<blockquote>
<p dir=""ltr"">Everyone can have 5 free late days without penalty. After you have used your free late days, you will lose 20% per day (or fraction thereof) that your assignment is submitted late. The final project will have its own late day policy.</p>
</blockquote>
<p>You do not need to ask permission to use a late day. åÊNote that I have a strict policy of not granting additional late days foråÊanyåÊreason, because I do not want to adjudicate what constitutes a fair excuse. åÊSo if you exhaust your 5 free late days early, please do not ask for more.</p>",Reminder: HW1 is due tomorrow by 2pm (and late day policy),<p>What if we need to use a late day for Friday?</p>,How do late days work for the final project?,<p>I just submitted my assignment for HW2. Does this count as two or three late days?</p>,Late days,"<p>If we submit something late, do we have to specify we are using a late day? Or is it automatically detected? Also, if we submit something after the class but the same day that it is due, is it still a late day?</p>",Late Day,<p></p>,Confirming: separate late day policy for the final group project?,2
940847331,4/26/2016 18:27:26,false,1969490296,,4/26/2016 18:25:35,false,elite,1.0,30128662,BGR,50,Pleven,212.233.177.195,0,,"<p>Hello, I&#39;m a little confused as to how late days work for groups. Is it that if the group wants to use a late day, all x members of the group must use their late day or is it that only one person of the group uses one late day. This question is only geared for regular homework assignments. I understand that there is a separate late day policy for the final group project. Thanks so much for the help!</p>",Late days for groups,"<p>This is a friendly reminder that <a href=""http://crowdsourcing-class.org/assignment1.html"" target=""_blank"">homework 1</a> is due tomorrow before class. åÊ</p>
<p></p>
<p>I would also like to draw your attention to the course&#39;s late day policy (given on the <a href=""http://crowdsourcing-class.org"" target=""_blank"">course web page</a>).</p>
<blockquote>
<p dir=""ltr"">Everyone can have 5 free late days without penalty. After you have used your free late days, you will lose 20% per day (or fraction thereof) that your assignment is submitted late. The final project will have its own late day policy.</p>
</blockquote>
<p>You do not need to ask permission to use a late day. åÊNote that I have a strict policy of not granting additional late days foråÊanyåÊreason, because I do not want to adjudicate what constitutes a fair excuse. åÊSo if you exhaust your 5 free late days early, please do not ask for more.</p>",Reminder: HW1 is due tomorrow by 2pm (and late day policy),<p>What if we need to use a late day for Friday?</p>,How do late days work for the final project?,<p>I just submitted my assignment for HW2. Does this count as two or three late days?</p>,Late days,"<p>If we submit something late, do we have to specify we are using a late day? Or is it automatically detected? Also, if we submit something after the class but the same day that it is due, is it still a late day?</p>",Late Day,<p></p>,Confirming: separate late day policy for the final group project?,2
940847331,4/26/2016 18:37:13,false,1969495295,,4/26/2016 18:33:39,false,neodev,1.0,29879245,RUS,69,Smolensk,37.144.124.118,0,,"<p>Hello, I&#39;m a little confused as to how late days work for groups. Is it that if the group wants to use a late day, all x members of the group must use their late day or is it that only one person of the group uses one late day. This question is only geared for regular homework assignments. I understand that there is a separate late day policy for the final group project. Thanks so much for the help!</p>",Late days for groups,"<p>This is a friendly reminder that <a href=""http://crowdsourcing-class.org/assignment1.html"" target=""_blank"">homework 1</a> is due tomorrow before class. åÊ</p>
<p></p>
<p>I would also like to draw your attention to the course&#39;s late day policy (given on the <a href=""http://crowdsourcing-class.org"" target=""_blank"">course web page</a>).</p>
<blockquote>
<p dir=""ltr"">Everyone can have 5 free late days without penalty. After you have used your free late days, you will lose 20% per day (or fraction thereof) that your assignment is submitted late. The final project will have its own late day policy.</p>
</blockquote>
<p>You do not need to ask permission to use a late day. åÊNote that I have a strict policy of not granting additional late days foråÊanyåÊreason, because I do not want to adjudicate what constitutes a fair excuse. åÊSo if you exhaust your 5 free late days early, please do not ask for more.</p>",Reminder: HW1 is due tomorrow by 2pm (and late day policy),<p>What if we need to use a late day for Friday?</p>,How do late days work for the final project?,<p>I just submitted my assignment for HW2. Does this count as two or three late days?</p>,Late days,"<p>If we submit something late, do we have to specify we are using a late day? Or is it automatically detected? Also, if we submit something after the class but the same day that it is due, is it still a late day?</p>",Late Day,<p></p>,Confirming: separate late day policy for the final group project?,2
940847331,4/26/2016 19:02:49,false,1969507180,,4/26/2016 18:44:58,false,neodev,1.0,11172894,IND,28,Champdani,117.194.5.117,0,,"<p>Hello, I&#39;m a little confused as to how late days work for groups. Is it that if the group wants to use a late day, all x members of the group must use their late day or is it that only one person of the group uses one late day. This question is only geared for regular homework assignments. I understand that there is a separate late day policy for the final group project. Thanks so much for the help!</p>",Late days for groups,"<p>This is a friendly reminder that <a href=""http://crowdsourcing-class.org/assignment1.html"" target=""_blank"">homework 1</a> is due tomorrow before class. åÊ</p>
<p></p>
<p>I would also like to draw your attention to the course&#39;s late day policy (given on the <a href=""http://crowdsourcing-class.org"" target=""_blank"">course web page</a>).</p>
<blockquote>
<p dir=""ltr"">Everyone can have 5 free late days without penalty. After you have used your free late days, you will lose 20% per day (or fraction thereof) that your assignment is submitted late. The final project will have its own late day policy.</p>
</blockquote>
<p>You do not need to ask permission to use a late day. åÊNote that I have a strict policy of not granting additional late days foråÊanyåÊreason, because I do not want to adjudicate what constitutes a fair excuse. åÊSo if you exhaust your 5 free late days early, please do not ask for more.</p>",Reminder: HW1 is due tomorrow by 2pm (and late day policy),<p>What if we need to use a late day for Friday?</p>,How do late days work for the final project?,<p>I just submitted my assignment for HW2. Does this count as two or three late days?</p>,Late days,"<p>If we submit something late, do we have to specify we are using a late day? Or is it automatically detected? Also, if we submit something after the class but the same day that it is due, is it still a late day?</p>",Late Day,<p></p>,Confirming: separate late day policy for the final group project?,2
940847331,4/26/2016 19:25:38,false,1969519495,,4/26/2016 19:25:12,false,tremorgames,1.0,25197223,HRV,15,Split,94.253.234.240,0,,"<p>Hello, I&#39;m a little confused as to how late days work for groups. Is it that if the group wants to use a late day, all x members of the group must use their late day or is it that only one person of the group uses one late day. This question is only geared for regular homework assignments. I understand that there is a separate late day policy for the final group project. Thanks so much for the help!</p>",Late days for groups,"<p>This is a friendly reminder that <a href=""http://crowdsourcing-class.org/assignment1.html"" target=""_blank"">homework 1</a> is due tomorrow before class. åÊ</p>
<p></p>
<p>I would also like to draw your attention to the course&#39;s late day policy (given on the <a href=""http://crowdsourcing-class.org"" target=""_blank"">course web page</a>).</p>
<blockquote>
<p dir=""ltr"">Everyone can have 5 free late days without penalty. After you have used your free late days, you will lose 20% per day (or fraction thereof) that your assignment is submitted late. The final project will have its own late day policy.</p>
</blockquote>
<p>You do not need to ask permission to use a late day. åÊNote that I have a strict policy of not granting additional late days foråÊanyåÊreason, because I do not want to adjudicate what constitutes a fair excuse. åÊSo if you exhaust your 5 free late days early, please do not ask for more.</p>",Reminder: HW1 is due tomorrow by 2pm (and late day policy),<p>What if we need to use a late day for Friday?</p>,How do late days work for the final project?,<p>I just submitted my assignment for HW2. Does this count as two or three late days?</p>,Late days,"<p>If we submit something late, do we have to specify we are using a late day? Or is it automatically detected? Also, if we submit something after the class but the same day that it is due, is it still a late day?</p>",Late Day,<p></p>,Confirming: separate late day policy for the final group project?,2
940847332,4/26/2016 17:41:58,false,1969466126,,4/26/2016 17:40:30,false,neodev,0.8889,33568303,VEN,23,Cabimas,190.77.7.36,0,,"<p>Hi everyone,</p>
<p></p>
<p>I&#39;m looking for a partner toåÊcollaborate with on upcoming assignments and/or the final project. E-mail me at <a href=""mailto:hcutler&#64;seas.upenn.edu"">hcutler&#64;seas.upenn.edu</a>åÊif you&#39;re interested!</p>
<p></p>
<p>Have a great day.</p>
<p></p>
<p>Hannah Cutler</p>
<p>SEAS &#39;17</p>",Looking for a partner for homeworks and/or final project?,"<p>Are we allowed to modify one of the past final project ideas shown to us in class? We have an interesting ideaåÊsimilar to PictureThis and wanted to make sure it&#39;s OK that it&#39;s similar.</p>
<p></p>
<p>Thanks!</p>",Final project idea similar to PictureThis,"<p>FYI, the second part of the final project was released last night:åÊ<a href=""http://crowdsourcing-class.org/final-project-part2.html"">http://crowdsourcing-class.org/final-project-part2.html</a></p>
<p></p>
<p>There are two deliverables. åÊThe first one is due on Saturday before 11:59pm.</p>",Final Project Part 2 has been released,<p>This is the link:åÊhttps://github.com/benpleitner/NETS213-Final-Project</p>,Group Project,"<p>For the NETS 213 final project, do we as students retain ownership of our projects? I know that some schools will hold the ownership rights, and juståÊwanted some clarification on this policy for the NETS 213 course.</p>",Student Ownership of Final Project?,"<p>Hey guys!</p>
<p></p>
<p>If you&#39;re looking for a team and need somewhere to put your idea or background out there : add follow ups to the post below. You can also use the team finding post &#64;5</p>
<p></p>
<p>Good luck!</p>
<p></p>",Forming teams for the final project,5
940847332,4/26/2016 17:51:16,false,1969471393,,4/26/2016 17:50:22,false,clixsense,1.0,35444326,BRA,07,Brasília,177.15.130.106,0,,"<p>Hi everyone,</p>
<p></p>
<p>I&#39;m looking for a partner toåÊcollaborate with on upcoming assignments and/or the final project. E-mail me at <a href=""mailto:hcutler&#64;seas.upenn.edu"">hcutler&#64;seas.upenn.edu</a>åÊif you&#39;re interested!</p>
<p></p>
<p>Have a great day.</p>
<p></p>
<p>Hannah Cutler</p>
<p>SEAS &#39;17</p>",Looking for a partner for homeworks and/or final project?,"<p>Are we allowed to modify one of the past final project ideas shown to us in class? We have an interesting ideaåÊsimilar to PictureThis and wanted to make sure it&#39;s OK that it&#39;s similar.</p>
<p></p>
<p>Thanks!</p>",Final project idea similar to PictureThis,"<p>FYI, the second part of the final project was released last night:åÊ<a href=""http://crowdsourcing-class.org/final-project-part2.html"">http://crowdsourcing-class.org/final-project-part2.html</a></p>
<p></p>
<p>There are two deliverables. åÊThe first one is due on Saturday before 11:59pm.</p>",Final Project Part 2 has been released,<p>This is the link:åÊhttps://github.com/benpleitner/NETS213-Final-Project</p>,Group Project,"<p>For the NETS 213 final project, do we as students retain ownership of our projects? I know that some schools will hold the ownership rights, and juståÊwanted some clarification on this policy for the NETS 213 course.</p>",Student Ownership of Final Project?,"<p>Hey guys!</p>
<p></p>
<p>If you&#39;re looking for a team and need somewhere to put your idea or background out there : add follow ups to the post below. You can also use the team finding post &#64;5</p>
<p></p>
<p>Good luck!</p>
<p></p>",Forming teams for the final project,5
940847332,4/26/2016 18:21:14,false,1969487259,,4/26/2016 18:20:20,false,neodev,0.8889,35550011,VEN,07,Valencia,190.204.238.112,0,,"<p>Hi everyone,</p>
<p></p>
<p>I&#39;m looking for a partner toåÊcollaborate with on upcoming assignments and/or the final project. E-mail me at <a href=""mailto:hcutler&#64;seas.upenn.edu"">hcutler&#64;seas.upenn.edu</a>åÊif you&#39;re interested!</p>
<p></p>
<p>Have a great day.</p>
<p></p>
<p>Hannah Cutler</p>
<p>SEAS &#39;17</p>",Looking for a partner for homeworks and/or final project?,"<p>Are we allowed to modify one of the past final project ideas shown to us in class? We have an interesting ideaåÊsimilar to PictureThis and wanted to make sure it&#39;s OK that it&#39;s similar.</p>
<p></p>
<p>Thanks!</p>",Final project idea similar to PictureThis,"<p>FYI, the second part of the final project was released last night:åÊ<a href=""http://crowdsourcing-class.org/final-project-part2.html"">http://crowdsourcing-class.org/final-project-part2.html</a></p>
<p></p>
<p>There are two deliverables. åÊThe first one is due on Saturday before 11:59pm.</p>",Final Project Part 2 has been released,<p>This is the link:åÊhttps://github.com/benpleitner/NETS213-Final-Project</p>,Group Project,"<p>For the NETS 213 final project, do we as students retain ownership of our projects? I know that some schools will hold the ownership rights, and juståÊwanted some clarification on this policy for the NETS 213 course.</p>",Student Ownership of Final Project?,"<p>Hey guys!</p>
<p></p>
<p>If you&#39;re looking for a team and need somewhere to put your idea or background out there : add follow ups to the post below. You can also use the team finding post &#64;5</p>
<p></p>
<p>Good luck!</p>
<p></p>",Forming teams for the final project,5
940847332,4/26/2016 18:25:34,false,1969489421,,4/26/2016 18:24:15,false,elite,1.0,30128662,BGR,50,Pleven,212.233.177.195,0,,"<p>Hi everyone,</p>
<p></p>
<p>I&#39;m looking for a partner toåÊcollaborate with on upcoming assignments and/or the final project. E-mail me at <a href=""mailto:hcutler&#64;seas.upenn.edu"">hcutler&#64;seas.upenn.edu</a>åÊif you&#39;re interested!</p>
<p></p>
<p>Have a great day.</p>
<p></p>
<p>Hannah Cutler</p>
<p>SEAS &#39;17</p>",Looking for a partner for homeworks and/or final project?,"<p>Are we allowed to modify one of the past final project ideas shown to us in class? We have an interesting ideaåÊsimilar to PictureThis and wanted to make sure it&#39;s OK that it&#39;s similar.</p>
<p></p>
<p>Thanks!</p>",Final project idea similar to PictureThis,"<p>FYI, the second part of the final project was released last night:åÊ<a href=""http://crowdsourcing-class.org/final-project-part2.html"">http://crowdsourcing-class.org/final-project-part2.html</a></p>
<p></p>
<p>There are two deliverables. åÊThe first one is due on Saturday before 11:59pm.</p>",Final Project Part 2 has been released,<p>This is the link:åÊhttps://github.com/benpleitner/NETS213-Final-Project</p>,Group Project,"<p>For the NETS 213 final project, do we as students retain ownership of our projects? I know that some schools will hold the ownership rights, and juståÊwanted some clarification on this policy for the NETS 213 course.</p>",Student Ownership of Final Project?,"<p>Hey guys!</p>
<p></p>
<p>If you&#39;re looking for a team and need somewhere to put your idea or background out there : add follow ups to the post below. You can also use the team finding post &#64;5</p>
<p></p>
<p>Good luck!</p>
<p></p>",Forming teams for the final project,5
940847332,4/26/2016 18:56:00,false,1969504142,,4/26/2016 18:55:19,false,neodev,1.0,29879245,RUS,69,Smolensk,37.144.124.118,0,,"<p>Hi everyone,</p>
<p></p>
<p>I&#39;m looking for a partner toåÊcollaborate with on upcoming assignments and/or the final project. E-mail me at <a href=""mailto:hcutler&#64;seas.upenn.edu"">hcutler&#64;seas.upenn.edu</a>åÊif you&#39;re interested!</p>
<p></p>
<p>Have a great day.</p>
<p></p>
<p>Hannah Cutler</p>
<p>SEAS &#39;17</p>",Looking for a partner for homeworks and/or final project?,"<p>Are we allowed to modify one of the past final project ideas shown to us in class? We have an interesting ideaåÊsimilar to PictureThis and wanted to make sure it&#39;s OK that it&#39;s similar.</p>
<p></p>
<p>Thanks!</p>",Final project idea similar to PictureThis,"<p>FYI, the second part of the final project was released last night:åÊ<a href=""http://crowdsourcing-class.org/final-project-part2.html"">http://crowdsourcing-class.org/final-project-part2.html</a></p>
<p></p>
<p>There are two deliverables. åÊThe first one is due on Saturday before 11:59pm.</p>",Final Project Part 2 has been released,<p>This is the link:åÊhttps://github.com/benpleitner/NETS213-Final-Project</p>,Group Project,"<p>For the NETS 213 final project, do we as students retain ownership of our projects? I know that some schools will hold the ownership rights, and juståÊwanted some clarification on this policy for the NETS 213 course.</p>",Student Ownership of Final Project?,"<p>Hey guys!</p>
<p></p>
<p>If you&#39;re looking for a team and need somewhere to put your idea or background out there : add follow ups to the post below. You can also use the team finding post &#64;5</p>
<p></p>
<p>Good luck!</p>
<p></p>",Forming teams for the final project,5
940847333,4/26/2016 16:26:27,false,1969420529,,4/26/2016 16:25:32,false,neodev,1.0,29175140,VEN,25,Caracas,190.72.125.134,0,,"<p>When I use the command</p>
<pre><code> cat articles_and_urls.txt | cut -f 1 &gt; urls.txt 
 cat articles_and_urls.txt | cut -f 2 &gt; unlabelled_articles.txt </code></pre>
<p></p>
<p>I get this error</p>
<p>cut: stdin: Illegal byte sequence</p>
<p></p>
<p>I tried looking it up but nothing has really helped.</p>",Error preparing input data for part 2,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hlicfcesNVC/im1ikvuvripg/Screen_Shot_20160321_at_12.50.01_AM.png"" /></p>
<p></p>
<p>I&#39;m getting this error - the aggregated csv for CrowdFlower for this link predicts that theåÊurlåÊisåÊbroken. How mightåÊthis be resolved? Thanks!åÊ</p>",Error with kappa.py,2
940847333,4/26/2016 16:45:07,false,1969432486,,4/26/2016 16:44:04,false,clixsense,1.0,6329782,IDN,10,Sleman,202.67.40.222,0,,"<p>When I use the command</p>
<pre><code> cat articles_and_urls.txt | cut -f 1 &gt; urls.txt 
 cat articles_and_urls.txt | cut -f 2 &gt; unlabelled_articles.txt </code></pre>
<p></p>
<p>I get this error</p>
<p>cut: stdin: Illegal byte sequence</p>
<p></p>
<p>I tried looking it up but nothing has really helped.</p>",Error preparing input data for part 2,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hlicfcesNVC/im1ikvuvripg/Screen_Shot_20160321_at_12.50.01_AM.png"" /></p>
<p></p>
<p>I&#39;m getting this error - the aggregated csv for CrowdFlower for this link predicts that theåÊurlåÊisåÊbroken. How mightåÊthis be resolved? Thanks!åÊ</p>",Error with kappa.py,2
940847333,4/26/2016 17:01:22,false,1969442590,,4/26/2016 17:00:04,false,clixsense,1.0,21408115,IDN,07,Semarang,36.79.23.180,0,,"<p>When I use the command</p>
<pre><code> cat articles_and_urls.txt | cut -f 1 &gt; urls.txt 
 cat articles_and_urls.txt | cut -f 2 &gt; unlabelled_articles.txt </code></pre>
<p></p>
<p>I get this error</p>
<p>cut: stdin: Illegal byte sequence</p>
<p></p>
<p>I tried looking it up but nothing has really helped.</p>",Error preparing input data for part 2,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hlicfcesNVC/im1ikvuvripg/Screen_Shot_20160321_at_12.50.01_AM.png"" /></p>
<p></p>
<p>I&#39;m getting this error - the aggregated csv for CrowdFlower for this link predicts that theåÊurlåÊisåÊbroken. How mightåÊthis be resolved? Thanks!åÊ</p>",Error with kappa.py,2
940847333,4/26/2016 17:05:47,false,1969445076,,4/26/2016 17:04:18,false,neodev,1.0,36167043,GBR,G6,Hull,77.86.101.69,0,,"<p>When I use the command</p>
<pre><code> cat articles_and_urls.txt | cut -f 1 &gt; urls.txt 
 cat articles_and_urls.txt | cut -f 2 &gt; unlabelled_articles.txt </code></pre>
<p></p>
<p>I get this error</p>
<p>cut: stdin: Illegal byte sequence</p>
<p></p>
<p>I tried looking it up but nothing has really helped.</p>",Error preparing input data for part 2,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hlicfcesNVC/im1ikvuvripg/Screen_Shot_20160321_at_12.50.01_AM.png"" /></p>
<p></p>
<p>I&#39;m getting this error - the aggregated csv for CrowdFlower for this link predicts that theåÊurlåÊisåÊbroken. How mightåÊthis be resolved? Thanks!åÊ</p>",Error with kappa.py,2
940847333,4/26/2016 17:34:24,false,1969461911,,4/26/2016 17:33:46,false,clixsense,1.0,30712378,ROU,21,Deva,79.119.241.200,0,,"<p>When I use the command</p>
<pre><code> cat articles_and_urls.txt | cut -f 1 &gt; urls.txt 
 cat articles_and_urls.txt | cut -f 2 &gt; unlabelled_articles.txt </code></pre>
<p></p>
<p>I get this error</p>
<p>cut: stdin: Illegal byte sequence</p>
<p></p>
<p>I tried looking it up but nothing has really helped.</p>",Error preparing input data for part 2,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hlicfcesNVC/im1ikvuvripg/Screen_Shot_20160321_at_12.50.01_AM.png"" /></p>
<p></p>
<p>I&#39;m getting this error - the aggregated csv for CrowdFlower for this link predicts that theåÊurlåÊisåÊbroken. How mightåÊthis be resolved? Thanks!åÊ</p>",Error with kappa.py,2
940847334,4/26/2016 15:12:15,false,1969364817,,4/26/2016 15:12:00,false,tremorgames,1.0,32635967,LTU,60,Panevezys,78.63.38.165,0,,"<p>Is there a way for us to see which files we turned in for a specific assignment, like #5?</p>",turnin,"<p>Hi Everyone,</p>
<p></p>
<p>Many of you had trouble with turnin for the bootcamp, and sent us email submissions. As a general policy, we cannot accept submissions via email. For this assignment, we will not penalize late submissions, so if you have not yet submitted via turnin, or you are still having trouble using scp/ssh, reach out to us and let&#39;s get it figured out.åÊ</p>
<p></p>
<p>For future assignments, we will only consider an assignment to be submitted if it is submitted via turnin before the deadline. Keep in mind our late-day policy: you are allowed 5 total late days during the semester with no penalty. After that, we deduct one point per day late.</p>
<p></p>
<p>Everyone have a wonderful weekend!</p>
<p>Ellie</p>",Late submissions and turnin,"<p>Hi everyone,</p>
<p>åÊ</p>
<p>For your python bootcamp, and for many other assignments this semester, you will be using &#39;turnin&#39;, which is a program on eniac. This requires using the &#39;ssh&#39; and &#39;scp&#39; commands from your terminal. Instructions on how to do this are below, but please come to office hours if this whole process looks foreign to you. It will take about 30 seconds to show you in person, versus much longer than 30 seconds of playing guess-and-check on your own. :)åÊ</p>
<p></p>
<p>You will be turning in your entire iPython notebook. Once you haveåÊyour notebook copied over to the eniac machines and sshed yourself over to the eniac machines (instructions below), you will run.</p>
<p>åÊ</p>
<pre>turnin -c nets213 -p python-bootcamp -v IPythonBootcamp.ipynb</pre>
<p>åÊ</p>
<p>Here is the process, from the beginning. The lines beginning with &#39;$&#39; or &#39;~&gt;&#39; are things you will need to type. Lines beginning with &#39;#&#39; are comments, explaining the commands below. Lines that have no special characters at the show the output you should expect to see.</p>
<p></p>
<pre>#Copy your iPython file (which right now is on your computer) to your account on the eniac. You can do this using the &#39;scp&#39; or &#39;secure copy&#39; command, like below.
$ scp IPythonBootcamp.ipynb epavlick&#64;eniac.seas.upenn.edu:               
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
IPythonBootcamp.ipynb           

#Log on to the  eniac machines (using your usual penn username/password). You should see your command prompt change to say something like &#64;plus or &#64;minus, like below.
$ ssh epavlick&#64;eniac.seas.upenn.edu
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
Last login: Wed Jan 27 14:46:24 2016 from 10.251.134.177
epavlick&#64;plus:~&gt; 

#Make sure your file was copied over successfully. You can check by listing the contents of the current directory using the &#39;ls&#39; command. Hopefully you will see many things listed, and your IPython file should be one of them.
epavlick&#64;plus:~&gt; ls 
Desktop  Documents  Downloads  IPythonBootcamp.ipynb

#You can keep things organized by making a new folder for this assignment using the &#34;mkdir&#34; command, and moving your file into it using the &#34;mv&#34; command. See below.
epavlick&#64;plus:~&gt; mkdir python-bootcamp-submission
epavlick&#64;plus:~&gt; mv IPythonBootcamp.ipynb python-bootcamp-submission/

#To see the list of open projects for our class, type the following. Currently, only the bootcamp and the test project are open.
epavlick&#64;plus:~&gt; turnin -l -c nets213
Current projects for nets213:
python-bootcamp  on (alternate)
turnin-test      off
gun-classifier   off
crowdflower      off
crowdie          off
quality          off
the-end          off
test-turnin      on (current)

# To submit your bootcamp to turnin, use the following command. The 
epavlick&#64;plus:~&gt; turnin -c nets213 -p python-bootcamp -v python-bootcamp-submission/
drwxr-xr-x epavlick/epavlick 0 2016-01-27 15:53 python-bootcamp-submission/
-rw-r----- epavlick/epavlick 16606 2016-01-27 15:48 python-bootcamp-submission/IPythonBootcamp.ipynb
Compressing submitted files... please wait
Your files have been submitted to nets213, python-bootcamp for grading.
</pre>
<p>åÊåÊ</p>
<p>If you want to practice, please try submitting to the test project (called test-turnin), just to make sure everything is working smoothly. You can just submit a blank file, or you are welcome to go on a meaningless rant and submit that. Whatever makes you happy.åÊ</p>
<p></p>
<p>Please let usåÊknow if you have any problems or questions.</p>
<p></p>
<p>Ellie</p>",Submitting assignments via turnin,"When using the $ scp assignment4 pennkey&#64;eniac.seas.upenn.edu command, I am unable to copy over the directory to my account. Not sure what else to do.",Unable to turnin Homework 4,,,"<p>Are the submissions for the latest homework open yet?</p>
<p>I tried to submit and received the error turnin: submissions for -crowdie have been turned off.</p>",HW8 turnin turned off?,0
940847334,4/26/2016 15:20:55,false,1969378546,,4/26/2016 15:20:31,false,neodev,1.0,19132694,LKA,36,Colombo,123.231.124.170,0,,"<p>Is there a way for us to see which files we turned in for a specific assignment, like #5?</p>",turnin,"<p>Hi Everyone,</p>
<p></p>
<p>Many of you had trouble with turnin for the bootcamp, and sent us email submissions. As a general policy, we cannot accept submissions via email. For this assignment, we will not penalize late submissions, so if you have not yet submitted via turnin, or you are still having trouble using scp/ssh, reach out to us and let&#39;s get it figured out.åÊ</p>
<p></p>
<p>For future assignments, we will only consider an assignment to be submitted if it is submitted via turnin before the deadline. Keep in mind our late-day policy: you are allowed 5 total late days during the semester with no penalty. After that, we deduct one point per day late.</p>
<p></p>
<p>Everyone have a wonderful weekend!</p>
<p>Ellie</p>",Late submissions and turnin,"<p>Hi everyone,</p>
<p>åÊ</p>
<p>For your python bootcamp, and for many other assignments this semester, you will be using &#39;turnin&#39;, which is a program on eniac. This requires using the &#39;ssh&#39; and &#39;scp&#39; commands from your terminal. Instructions on how to do this are below, but please come to office hours if this whole process looks foreign to you. It will take about 30 seconds to show you in person, versus much longer than 30 seconds of playing guess-and-check on your own. :)åÊ</p>
<p></p>
<p>You will be turning in your entire iPython notebook. Once you haveåÊyour notebook copied over to the eniac machines and sshed yourself over to the eniac machines (instructions below), you will run.</p>
<p>åÊ</p>
<pre>turnin -c nets213 -p python-bootcamp -v IPythonBootcamp.ipynb</pre>
<p>åÊ</p>
<p>Here is the process, from the beginning. The lines beginning with &#39;$&#39; or &#39;~&gt;&#39; are things you will need to type. Lines beginning with &#39;#&#39; are comments, explaining the commands below. Lines that have no special characters at the show the output you should expect to see.</p>
<p></p>
<pre>#Copy your iPython file (which right now is on your computer) to your account on the eniac. You can do this using the &#39;scp&#39; or &#39;secure copy&#39; command, like below.
$ scp IPythonBootcamp.ipynb epavlick&#64;eniac.seas.upenn.edu:               
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
IPythonBootcamp.ipynb           

#Log on to the  eniac machines (using your usual penn username/password). You should see your command prompt change to say something like &#64;plus or &#64;minus, like below.
$ ssh epavlick&#64;eniac.seas.upenn.edu
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
Last login: Wed Jan 27 14:46:24 2016 from 10.251.134.177
epavlick&#64;plus:~&gt; 

#Make sure your file was copied over successfully. You can check by listing the contents of the current directory using the &#39;ls&#39; command. Hopefully you will see many things listed, and your IPython file should be one of them.
epavlick&#64;plus:~&gt; ls 
Desktop  Documents  Downloads  IPythonBootcamp.ipynb

#You can keep things organized by making a new folder for this assignment using the &#34;mkdir&#34; command, and moving your file into it using the &#34;mv&#34; command. See below.
epavlick&#64;plus:~&gt; mkdir python-bootcamp-submission
epavlick&#64;plus:~&gt; mv IPythonBootcamp.ipynb python-bootcamp-submission/

#To see the list of open projects for our class, type the following. Currently, only the bootcamp and the test project are open.
epavlick&#64;plus:~&gt; turnin -l -c nets213
Current projects for nets213:
python-bootcamp  on (alternate)
turnin-test      off
gun-classifier   off
crowdflower      off
crowdie          off
quality          off
the-end          off
test-turnin      on (current)

# To submit your bootcamp to turnin, use the following command. The 
epavlick&#64;plus:~&gt; turnin -c nets213 -p python-bootcamp -v python-bootcamp-submission/
drwxr-xr-x epavlick/epavlick 0 2016-01-27 15:53 python-bootcamp-submission/
-rw-r----- epavlick/epavlick 16606 2016-01-27 15:48 python-bootcamp-submission/IPythonBootcamp.ipynb
Compressing submitted files... please wait
Your files have been submitted to nets213, python-bootcamp for grading.
</pre>
<p>åÊåÊ</p>
<p>If you want to practice, please try submitting to the test project (called test-turnin), just to make sure everything is working smoothly. You can just submit a blank file, or you are welcome to go on a meaningless rant and submit that. Whatever makes you happy.åÊ</p>
<p></p>
<p>Please let usåÊknow if you have any problems or questions.</p>
<p></p>
<p>Ellie</p>",Submitting assignments via turnin,"When using the $ scp assignment4 pennkey&#64;eniac.seas.upenn.edu command, I am unable to copy over the directory to my account. Not sure what else to do.",Unable to turnin Homework 4,,,"<p>Are the submissions for the latest homework open yet?</p>
<p>I tried to submit and received the error turnin: submissions for -crowdie have been turned off.</p>",HW8 turnin turned off?,0
940847334,4/26/2016 15:23:37,false,1969383212,,4/26/2016 15:20:30,false,clixsense,1.0,24287706,TWN,04,Keelung,61.231.195.173,0,,"<p>Is there a way for us to see which files we turned in for a specific assignment, like #5?</p>",turnin,"<p>Hi Everyone,</p>
<p></p>
<p>Many of you had trouble with turnin for the bootcamp, and sent us email submissions. As a general policy, we cannot accept submissions via email. For this assignment, we will not penalize late submissions, so if you have not yet submitted via turnin, or you are still having trouble using scp/ssh, reach out to us and let&#39;s get it figured out.åÊ</p>
<p></p>
<p>For future assignments, we will only consider an assignment to be submitted if it is submitted via turnin before the deadline. Keep in mind our late-day policy: you are allowed 5 total late days during the semester with no penalty. After that, we deduct one point per day late.</p>
<p></p>
<p>Everyone have a wonderful weekend!</p>
<p>Ellie</p>",Late submissions and turnin,"<p>Hi everyone,</p>
<p>åÊ</p>
<p>For your python bootcamp, and for many other assignments this semester, you will be using &#39;turnin&#39;, which is a program on eniac. This requires using the &#39;ssh&#39; and &#39;scp&#39; commands from your terminal. Instructions on how to do this are below, but please come to office hours if this whole process looks foreign to you. It will take about 30 seconds to show you in person, versus much longer than 30 seconds of playing guess-and-check on your own. :)åÊ</p>
<p></p>
<p>You will be turning in your entire iPython notebook. Once you haveåÊyour notebook copied over to the eniac machines and sshed yourself over to the eniac machines (instructions below), you will run.</p>
<p>åÊ</p>
<pre>turnin -c nets213 -p python-bootcamp -v IPythonBootcamp.ipynb</pre>
<p>åÊ</p>
<p>Here is the process, from the beginning. The lines beginning with &#39;$&#39; or &#39;~&gt;&#39; are things you will need to type. Lines beginning with &#39;#&#39; are comments, explaining the commands below. Lines that have no special characters at the show the output you should expect to see.</p>
<p></p>
<pre>#Copy your iPython file (which right now is on your computer) to your account on the eniac. You can do this using the &#39;scp&#39; or &#39;secure copy&#39; command, like below.
$ scp IPythonBootcamp.ipynb epavlick&#64;eniac.seas.upenn.edu:               
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
IPythonBootcamp.ipynb           

#Log on to the  eniac machines (using your usual penn username/password). You should see your command prompt change to say something like &#64;plus or &#64;minus, like below.
$ ssh epavlick&#64;eniac.seas.upenn.edu
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
Last login: Wed Jan 27 14:46:24 2016 from 10.251.134.177
epavlick&#64;plus:~&gt; 

#Make sure your file was copied over successfully. You can check by listing the contents of the current directory using the &#39;ls&#39; command. Hopefully you will see many things listed, and your IPython file should be one of them.
epavlick&#64;plus:~&gt; ls 
Desktop  Documents  Downloads  IPythonBootcamp.ipynb

#You can keep things organized by making a new folder for this assignment using the &#34;mkdir&#34; command, and moving your file into it using the &#34;mv&#34; command. See below.
epavlick&#64;plus:~&gt; mkdir python-bootcamp-submission
epavlick&#64;plus:~&gt; mv IPythonBootcamp.ipynb python-bootcamp-submission/

#To see the list of open projects for our class, type the following. Currently, only the bootcamp and the test project are open.
epavlick&#64;plus:~&gt; turnin -l -c nets213
Current projects for nets213:
python-bootcamp  on (alternate)
turnin-test      off
gun-classifier   off
crowdflower      off
crowdie          off
quality          off
the-end          off
test-turnin      on (current)

# To submit your bootcamp to turnin, use the following command. The 
epavlick&#64;plus:~&gt; turnin -c nets213 -p python-bootcamp -v python-bootcamp-submission/
drwxr-xr-x epavlick/epavlick 0 2016-01-27 15:53 python-bootcamp-submission/
-rw-r----- epavlick/epavlick 16606 2016-01-27 15:48 python-bootcamp-submission/IPythonBootcamp.ipynb
Compressing submitted files... please wait
Your files have been submitted to nets213, python-bootcamp for grading.
</pre>
<p>åÊåÊ</p>
<p>If you want to practice, please try submitting to the test project (called test-turnin), just to make sure everything is working smoothly. You can just submit a blank file, or you are welcome to go on a meaningless rant and submit that. Whatever makes you happy.åÊ</p>
<p></p>
<p>Please let usåÊknow if you have any problems or questions.</p>
<p></p>
<p>Ellie</p>",Submitting assignments via turnin,"When using the $ scp assignment4 pennkey&#64;eniac.seas.upenn.edu command, I am unable to copy over the directory to my account. Not sure what else to do.",Unable to turnin Homework 4,,,"<p>Are the submissions for the latest homework open yet?</p>
<p>I tried to submit and received the error turnin: submissions for -crowdie have been turned off.</p>",HW8 turnin turned off?,0
940847334,4/26/2016 15:26:30,false,1969388023,,4/26/2016 15:24:24,false,clixsense,1.0,7837812,SRB,00,Belgrade,79.101.254.233,0,,"<p>Is there a way for us to see which files we turned in for a specific assignment, like #5?</p>",turnin,"<p>Hi Everyone,</p>
<p></p>
<p>Many of you had trouble with turnin for the bootcamp, and sent us email submissions. As a general policy, we cannot accept submissions via email. For this assignment, we will not penalize late submissions, so if you have not yet submitted via turnin, or you are still having trouble using scp/ssh, reach out to us and let&#39;s get it figured out.åÊ</p>
<p></p>
<p>For future assignments, we will only consider an assignment to be submitted if it is submitted via turnin before the deadline. Keep in mind our late-day policy: you are allowed 5 total late days during the semester with no penalty. After that, we deduct one point per day late.</p>
<p></p>
<p>Everyone have a wonderful weekend!</p>
<p>Ellie</p>",Late submissions and turnin,"<p>Hi everyone,</p>
<p>åÊ</p>
<p>For your python bootcamp, and for many other assignments this semester, you will be using &#39;turnin&#39;, which is a program on eniac. This requires using the &#39;ssh&#39; and &#39;scp&#39; commands from your terminal. Instructions on how to do this are below, but please come to office hours if this whole process looks foreign to you. It will take about 30 seconds to show you in person, versus much longer than 30 seconds of playing guess-and-check on your own. :)åÊ</p>
<p></p>
<p>You will be turning in your entire iPython notebook. Once you haveåÊyour notebook copied over to the eniac machines and sshed yourself over to the eniac machines (instructions below), you will run.</p>
<p>åÊ</p>
<pre>turnin -c nets213 -p python-bootcamp -v IPythonBootcamp.ipynb</pre>
<p>åÊ</p>
<p>Here is the process, from the beginning. The lines beginning with &#39;$&#39; or &#39;~&gt;&#39; are things you will need to type. Lines beginning with &#39;#&#39; are comments, explaining the commands below. Lines that have no special characters at the show the output you should expect to see.</p>
<p></p>
<pre>#Copy your iPython file (which right now is on your computer) to your account on the eniac. You can do this using the &#39;scp&#39; or &#39;secure copy&#39; command, like below.
$ scp IPythonBootcamp.ipynb epavlick&#64;eniac.seas.upenn.edu:               
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
IPythonBootcamp.ipynb           

#Log on to the  eniac machines (using your usual penn username/password). You should see your command prompt change to say something like &#64;plus or &#64;minus, like below.
$ ssh epavlick&#64;eniac.seas.upenn.edu
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
Last login: Wed Jan 27 14:46:24 2016 from 10.251.134.177
epavlick&#64;plus:~&gt; 

#Make sure your file was copied over successfully. You can check by listing the contents of the current directory using the &#39;ls&#39; command. Hopefully you will see many things listed, and your IPython file should be one of them.
epavlick&#64;plus:~&gt; ls 
Desktop  Documents  Downloads  IPythonBootcamp.ipynb

#You can keep things organized by making a new folder for this assignment using the &#34;mkdir&#34; command, and moving your file into it using the &#34;mv&#34; command. See below.
epavlick&#64;plus:~&gt; mkdir python-bootcamp-submission
epavlick&#64;plus:~&gt; mv IPythonBootcamp.ipynb python-bootcamp-submission/

#To see the list of open projects for our class, type the following. Currently, only the bootcamp and the test project are open.
epavlick&#64;plus:~&gt; turnin -l -c nets213
Current projects for nets213:
python-bootcamp  on (alternate)
turnin-test      off
gun-classifier   off
crowdflower      off
crowdie          off
quality          off
the-end          off
test-turnin      on (current)

# To submit your bootcamp to turnin, use the following command. The 
epavlick&#64;plus:~&gt; turnin -c nets213 -p python-bootcamp -v python-bootcamp-submission/
drwxr-xr-x epavlick/epavlick 0 2016-01-27 15:53 python-bootcamp-submission/
-rw-r----- epavlick/epavlick 16606 2016-01-27 15:48 python-bootcamp-submission/IPythonBootcamp.ipynb
Compressing submitted files... please wait
Your files have been submitted to nets213, python-bootcamp for grading.
</pre>
<p>åÊåÊ</p>
<p>If you want to practice, please try submitting to the test project (called test-turnin), just to make sure everything is working smoothly. You can just submit a blank file, or you are welcome to go on a meaningless rant and submit that. Whatever makes you happy.åÊ</p>
<p></p>
<p>Please let usåÊknow if you have any problems or questions.</p>
<p></p>
<p>Ellie</p>",Submitting assignments via turnin,"When using the $ scp assignment4 pennkey&#64;eniac.seas.upenn.edu command, I am unable to copy over the directory to my account. Not sure what else to do.",Unable to turnin Homework 4,,,"<p>Are the submissions for the latest homework open yet?</p>
<p>I tried to submit and received the error turnin: submissions for -crowdie have been turned off.</p>",HW8 turnin turned off?,0
940847334,4/26/2016 15:26:56,false,1969388621,,4/26/2016 15:26:14,false,instagc,0.8889,13581319,USA,IL,Waltonville,208.70.36.12,0,,"<p>Is there a way for us to see which files we turned in for a specific assignment, like #5?</p>",turnin,"<p>Hi Everyone,</p>
<p></p>
<p>Many of you had trouble with turnin for the bootcamp, and sent us email submissions. As a general policy, we cannot accept submissions via email. For this assignment, we will not penalize late submissions, so if you have not yet submitted via turnin, or you are still having trouble using scp/ssh, reach out to us and let&#39;s get it figured out.åÊ</p>
<p></p>
<p>For future assignments, we will only consider an assignment to be submitted if it is submitted via turnin before the deadline. Keep in mind our late-day policy: you are allowed 5 total late days during the semester with no penalty. After that, we deduct one point per day late.</p>
<p></p>
<p>Everyone have a wonderful weekend!</p>
<p>Ellie</p>",Late submissions and turnin,"<p>Hi everyone,</p>
<p>åÊ</p>
<p>For your python bootcamp, and for many other assignments this semester, you will be using &#39;turnin&#39;, which is a program on eniac. This requires using the &#39;ssh&#39; and &#39;scp&#39; commands from your terminal. Instructions on how to do this are below, but please come to office hours if this whole process looks foreign to you. It will take about 30 seconds to show you in person, versus much longer than 30 seconds of playing guess-and-check on your own. :)åÊ</p>
<p></p>
<p>You will be turning in your entire iPython notebook. Once you haveåÊyour notebook copied over to the eniac machines and sshed yourself over to the eniac machines (instructions below), you will run.</p>
<p>åÊ</p>
<pre>turnin -c nets213 -p python-bootcamp -v IPythonBootcamp.ipynb</pre>
<p>åÊ</p>
<p>Here is the process, from the beginning. The lines beginning with &#39;$&#39; or &#39;~&gt;&#39; are things you will need to type. Lines beginning with &#39;#&#39; are comments, explaining the commands below. Lines that have no special characters at the show the output you should expect to see.</p>
<p></p>
<pre>#Copy your iPython file (which right now is on your computer) to your account on the eniac. You can do this using the &#39;scp&#39; or &#39;secure copy&#39; command, like below.
$ scp IPythonBootcamp.ipynb epavlick&#64;eniac.seas.upenn.edu:               
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
IPythonBootcamp.ipynb           

#Log on to the  eniac machines (using your usual penn username/password). You should see your command prompt change to say something like &#64;plus or &#64;minus, like below.
$ ssh epavlick&#64;eniac.seas.upenn.edu
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
Last login: Wed Jan 27 14:46:24 2016 from 10.251.134.177
epavlick&#64;plus:~&gt; 

#Make sure your file was copied over successfully. You can check by listing the contents of the current directory using the &#39;ls&#39; command. Hopefully you will see many things listed, and your IPython file should be one of them.
epavlick&#64;plus:~&gt; ls 
Desktop  Documents  Downloads  IPythonBootcamp.ipynb

#You can keep things organized by making a new folder for this assignment using the &#34;mkdir&#34; command, and moving your file into it using the &#34;mv&#34; command. See below.
epavlick&#64;plus:~&gt; mkdir python-bootcamp-submission
epavlick&#64;plus:~&gt; mv IPythonBootcamp.ipynb python-bootcamp-submission/

#To see the list of open projects for our class, type the following. Currently, only the bootcamp and the test project are open.
epavlick&#64;plus:~&gt; turnin -l -c nets213
Current projects for nets213:
python-bootcamp  on (alternate)
turnin-test      off
gun-classifier   off
crowdflower      off
crowdie          off
quality          off
the-end          off
test-turnin      on (current)

# To submit your bootcamp to turnin, use the following command. The 
epavlick&#64;plus:~&gt; turnin -c nets213 -p python-bootcamp -v python-bootcamp-submission/
drwxr-xr-x epavlick/epavlick 0 2016-01-27 15:53 python-bootcamp-submission/
-rw-r----- epavlick/epavlick 16606 2016-01-27 15:48 python-bootcamp-submission/IPythonBootcamp.ipynb
Compressing submitted files... please wait
Your files have been submitted to nets213, python-bootcamp for grading.
</pre>
<p>åÊåÊ</p>
<p>If you want to practice, please try submitting to the test project (called test-turnin), just to make sure everything is working smoothly. You can just submit a blank file, or you are welcome to go on a meaningless rant and submit that. Whatever makes you happy.åÊ</p>
<p></p>
<p>Please let usåÊknow if you have any problems or questions.</p>
<p></p>
<p>Ellie</p>",Submitting assignments via turnin,"When using the $ scp assignment4 pennkey&#64;eniac.seas.upenn.edu command, I am unable to copy over the directory to my account. Not sure what else to do.",Unable to turnin Homework 4,,,"<p>Are the submissions for the latest homework open yet?</p>
<p>I tried to submit and received the error turnin: submissions for -crowdie have been turned off.</p>",HW8 turnin turned off?,0
940847335,4/26/2016 16:01:37,false,1969404545,,4/26/2016 16:00:00,false,elite,1.0,33243069,IND,10,Faridabad,116.203.79.150,0,,<p>How can we findåÊhourly wage through CrowdFlower? Do we need to estimate it or there is a section that gives a precise number?</p>,Hourly Wage on CrowdFlower,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,"<p>Based on the instructions, the sample.txt file that we upload to CloudFlower should contain 500 positively labelled articles. In the screenshot at step 4 however the urls have a mix of both 0 and 1 labels. Which one is correct? And more generally, why are we having only the positive articles being labelled by crowdworkers?</p>",CrowdFlower &#34;sample.txt&#34; Clarification,<p>My crowdflower account doesn&#39;t have any funds yet ÛÓ is it possible to share accounts just for this assignment?</p>,sharing crowdflower,No one has completed my request in the last 5 hours and it is still only 20% completed. Is there any possible way to update my task such that it becomes more popular to contributors?,No More Contributors in CrowdFlower,"<p>I&#39;m having this issue where I can&#39;t even log into crowdflower as a contributor. I&#39;ve set up a job before and received results, but I&#39;m unable to do so now. I&#39;m getting this error:åÊ &#34;You signed up for a Contributor account and are attempting to access the Customer platform.&#34;</p>",Can&#39;t log into Crowdflower as Customer,5
940847335,4/26/2016 16:02:20,false,1969404854,,4/26/2016 16:01:47,false,personaly,1.0,33663352,ARG,01,Mar Del Plata,181.168.213.227,0,,<p>How can we findåÊhourly wage through CrowdFlower? Do we need to estimate it or there is a section that gives a precise number?</p>,Hourly Wage on CrowdFlower,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,"<p>Based on the instructions, the sample.txt file that we upload to CloudFlower should contain 500 positively labelled articles. In the screenshot at step 4 however the urls have a mix of both 0 and 1 labels. Which one is correct? And more generally, why are we having only the positive articles being labelled by crowdworkers?</p>",CrowdFlower &#34;sample.txt&#34; Clarification,<p>My crowdflower account doesn&#39;t have any funds yet ÛÓ is it possible to share accounts just for this assignment?</p>,sharing crowdflower,No one has completed my request in the last 5 hours and it is still only 20% completed. Is there any possible way to update my task such that it becomes more popular to contributors?,No More Contributors in CrowdFlower,"<p>I&#39;m having this issue where I can&#39;t even log into crowdflower as a contributor. I&#39;ve set up a job before and received results, but I&#39;m unable to do so now. I&#39;m getting this error:åÊ &#34;You signed up for a Contributor account and are attempting to access the Customer platform.&#34;</p>",Can&#39;t log into Crowdflower as Customer,5
940847335,4/26/2016 16:12:58,false,1969410165,,4/26/2016 16:10:21,false,clixsense,0.8889,8057247,PRT,17,Póvoa De Varzim,144.64.25.68,0,,<p>How can we findåÊhourly wage through CrowdFlower? Do we need to estimate it or there is a section that gives a precise number?</p>,Hourly Wage on CrowdFlower,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,"<p>Based on the instructions, the sample.txt file that we upload to CloudFlower should contain 500 positively labelled articles. In the screenshot at step 4 however the urls have a mix of both 0 and 1 labels. Which one is correct? And more generally, why are we having only the positive articles being labelled by crowdworkers?</p>",CrowdFlower &#34;sample.txt&#34; Clarification,<p>My crowdflower account doesn&#39;t have any funds yet ÛÓ is it possible to share accounts just for this assignment?</p>,sharing crowdflower,No one has completed my request in the last 5 hours and it is still only 20% completed. Is there any possible way to update my task such that it becomes more popular to contributors?,No More Contributors in CrowdFlower,"<p>I&#39;m having this issue where I can&#39;t even log into crowdflower as a contributor. I&#39;ve set up a job before and received results, but I&#39;m unable to do so now. I&#39;m getting this error:åÊ &#34;You signed up for a Contributor account and are attempting to access the Customer platform.&#34;</p>",Can&#39;t log into Crowdflower as Customer,5
940847335,4/26/2016 16:40:25,false,1969429699,,4/26/2016 16:39:20,false,clixsense,1.0,6329782,IDN,07,Bekonang,202.67.40.31,0,,<p>How can we findåÊhourly wage through CrowdFlower? Do we need to estimate it or there is a section that gives a precise number?</p>,Hourly Wage on CrowdFlower,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,"<p>Based on the instructions, the sample.txt file that we upload to CloudFlower should contain 500 positively labelled articles. In the screenshot at step 4 however the urls have a mix of both 0 and 1 labels. Which one is correct? And more generally, why are we having only the positive articles being labelled by crowdworkers?</p>",CrowdFlower &#34;sample.txt&#34; Clarification,<p>My crowdflower account doesn&#39;t have any funds yet ÛÓ is it possible to share accounts just for this assignment?</p>,sharing crowdflower,No one has completed my request in the last 5 hours and it is still only 20% completed. Is there any possible way to update my task such that it becomes more popular to contributors?,No More Contributors in CrowdFlower,"<p>I&#39;m having this issue where I can&#39;t even log into crowdflower as a contributor. I&#39;ve set up a job before and received results, but I&#39;m unable to do so now. I&#39;m getting this error:åÊ &#34;You signed up for a Contributor account and are attempting to access the Customer platform.&#34;</p>",Can&#39;t log into Crowdflower as Customer,5
940847335,4/26/2016 17:22:13,false,1969454940,,4/26/2016 17:19:06,false,elite,1.0,25411289,HRV,"","",31.147.119.175,0,,<p>How can we findåÊhourly wage through CrowdFlower? Do we need to estimate it or there is a section that gives a precise number?</p>,Hourly Wage on CrowdFlower,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,"<p>Based on the instructions, the sample.txt file that we upload to CloudFlower should contain 500 positively labelled articles. In the screenshot at step 4 however the urls have a mix of both 0 and 1 labels. Which one is correct? And more generally, why are we having only the positive articles being labelled by crowdworkers?</p>",CrowdFlower &#34;sample.txt&#34; Clarification,<p>My crowdflower account doesn&#39;t have any funds yet ÛÓ is it possible to share accounts just for this assignment?</p>,sharing crowdflower,No one has completed my request in the last 5 hours and it is still only 20% completed. Is there any possible way to update my task such that it becomes more popular to contributors?,No More Contributors in CrowdFlower,"<p>I&#39;m having this issue where I can&#39;t even log into crowdflower as a contributor. I&#39;ve set up a job before and received results, but I&#39;m unable to do so now. I&#39;m getting this error:åÊ &#34;You signed up for a Contributor account and are attempting to access the Customer platform.&#34;</p>",Can&#39;t log into Crowdflower as Customer,5
940847336,4/26/2016 17:32:48,false,1969461089,,4/26/2016 17:32:36,false,neodev,0.8889,33131546,IDN,04,Jakarta,139.194.89.60,0,,"When I try to scrape the violence archive, I get: &#34;<p>Thanks for your interest in our statistics! Unfortunately, we must limit result sets for the general public. If you would like more information or the full result set, please send an email with your name, organization, project and needs toåÊ<a href=""https://www.piazza.com/cdn-cgi/l/email-protection#f89196898d918a81b89f8e99d68d8bd69b9795"" target=""_blank"">[emailåÊprotected]</a></p>",Scraping violence archive,"<p>Where exactly on the gun violence page are we supposed to crawl? I modified my code to look like:</p>
<p></p>
<pre>   for i in xrange(1, 11):<br />     crawl(&#39;http://gunviolencearchive.org/last-72-hours?page=%s&#39;%i, domain=DOMAIN)</pre>
<p>With a domain restriction of gunviolencearchive.org, but I&#39;m repeatedly running my crawler and it&#39;s not returning and websites (which seems to be accurate - there don&#39;t seem to be many hyperlinks). The same thing happens when I just tell the crawler to crawl the home page. Am I doing something wrong?</p>",Crawling Gun Violence Archive,"<p>should we just copy the output from scraping the gun violence archive and append it to the beginning of all the urls that we got from querying through the bing api? could you please clarify what you mean by:</p>
<p></p>
<p>&#34;These urls must include those that you collect from the Gun Violence Archive in Step 4, and the Bing API urls you collect using the three queries in Step 7. The remaining urls can come from whereever you want&#34;</p>
<p></p>
<p>This is all one big txt file?åÊ</p>",copying output from gun violence archive &amp; bing api ping results,"<p>I am creating my cloudflower task and want to give the most descriptive definition of gun violence as possible.</p>
<p></p>
<p>Are we only interested in articles that mention a specific act of gun violence? For example how should we classify articles that discuss gun violence statistics or gun crime as a whole.</p>
<p>If a gun was involved in the article but no one was injured, should this be considered gun violence?</p>",Definition of Gun Violence,"<p>Should articles related to gun violence, but not describing a particular incident of gun violence, be labelled as related or not?</p>
<p>For example</p>
<ul><li>Gun control legislation (<a href=""http://abcnews.go.com/ABC_Univision/Politics/chicagos-gun-violence-role-national-gun-debate/story?id=18365373"">http://abcnews.go.com/ABC_Univision/Politics/chicagos-gun-violence-role-national-gun-debate/story?id=18365373</a>)</li><li>Statistics on gun violence (<a href=""http://www.nbcnews.com/health/terrible-tally-500-children-dead-gunshots-every-year-7-500-8C11469222"">http://www.nbcnews.com/health/terrible-tally-500-children-dead-gunshots-every-year-7-500-8C11469222</a>)</li><li>Article that refers to another incident of gun violence (<a href=""http://www.kjct8.com/content/news/CIRT-team-called-in-to-investigate-the-officer-involved-shooting--368258921.html"">http://www.kjct8.com/content/news/CIRT-team-called-in-to-investigate-the-officer-involved-shooting--368258921.html</a>)</li></ul>
<p></p>
<p>I have classified these as not related, and am using it as a test question on CrowdFlower, but is causing me problems as many &#34;miss&#34; this question.</p>",Classification for Gun Violence Related Articles,"<p>For the gun violence archive URLs, should we keep the pages that have the gunviolencearchive domain? Most of these pages do have interesting facts about the incident, but I&#39;m assuming by articles you wanted the source material? (For example, foråÊ<a href=""http://www.gunviolencearchive.org/incident/500593,"">http://www.gunviolencearchive.org/incident/500593,</a>åÊwe&#39;d write the linkåÊ<a href=""http://abc7chicago.com/news/2-teens-found-fatally-shot-in-gary/1189625/"" target=""_new"">http://abc7chicago.com/news/2-teens-found-fatally-shot-in-gary/1189625/</a>åÊto the newly created textfile).</p>
<p></p>
<p>Or should we include the gunviolencearchive page as well?</p>",Gun Violence Archive URLs,1
940847336,4/26/2016 17:36:20,false,1969462852,,4/26/2016 17:36:03,false,clixsense,1.0,30712378,ROU,21,Deva,79.119.241.200,0,,"When I try to scrape the violence archive, I get: &#34;<p>Thanks for your interest in our statistics! Unfortunately, we must limit result sets for the general public. If you would like more information or the full result set, please send an email with your name, organization, project and needs toåÊ<a href=""https://www.piazza.com/cdn-cgi/l/email-protection#f89196898d918a81b89f8e99d68d8bd69b9795"" target=""_blank"">[emailåÊprotected]</a></p>",Scraping violence archive,"<p>Where exactly on the gun violence page are we supposed to crawl? I modified my code to look like:</p>
<p></p>
<pre>   for i in xrange(1, 11):<br />     crawl(&#39;http://gunviolencearchive.org/last-72-hours?page=%s&#39;%i, domain=DOMAIN)</pre>
<p>With a domain restriction of gunviolencearchive.org, but I&#39;m repeatedly running my crawler and it&#39;s not returning and websites (which seems to be accurate - there don&#39;t seem to be many hyperlinks). The same thing happens when I just tell the crawler to crawl the home page. Am I doing something wrong?</p>",Crawling Gun Violence Archive,"<p>should we just copy the output from scraping the gun violence archive and append it to the beginning of all the urls that we got from querying through the bing api? could you please clarify what you mean by:</p>
<p></p>
<p>&#34;These urls must include those that you collect from the Gun Violence Archive in Step 4, and the Bing API urls you collect using the three queries in Step 7. The remaining urls can come from whereever you want&#34;</p>
<p></p>
<p>This is all one big txt file?åÊ</p>",copying output from gun violence archive &amp; bing api ping results,"<p>I am creating my cloudflower task and want to give the most descriptive definition of gun violence as possible.</p>
<p></p>
<p>Are we only interested in articles that mention a specific act of gun violence? For example how should we classify articles that discuss gun violence statistics or gun crime as a whole.</p>
<p>If a gun was involved in the article but no one was injured, should this be considered gun violence?</p>",Definition of Gun Violence,"<p>Should articles related to gun violence, but not describing a particular incident of gun violence, be labelled as related or not?</p>
<p>For example</p>
<ul><li>Gun control legislation (<a href=""http://abcnews.go.com/ABC_Univision/Politics/chicagos-gun-violence-role-national-gun-debate/story?id=18365373"">http://abcnews.go.com/ABC_Univision/Politics/chicagos-gun-violence-role-national-gun-debate/story?id=18365373</a>)</li><li>Statistics on gun violence (<a href=""http://www.nbcnews.com/health/terrible-tally-500-children-dead-gunshots-every-year-7-500-8C11469222"">http://www.nbcnews.com/health/terrible-tally-500-children-dead-gunshots-every-year-7-500-8C11469222</a>)</li><li>Article that refers to another incident of gun violence (<a href=""http://www.kjct8.com/content/news/CIRT-team-called-in-to-investigate-the-officer-involved-shooting--368258921.html"">http://www.kjct8.com/content/news/CIRT-team-called-in-to-investigate-the-officer-involved-shooting--368258921.html</a>)</li></ul>
<p></p>
<p>I have classified these as not related, and am using it as a test question on CrowdFlower, but is causing me problems as many &#34;miss&#34; this question.</p>",Classification for Gun Violence Related Articles,"<p>For the gun violence archive URLs, should we keep the pages that have the gunviolencearchive domain? Most of these pages do have interesting facts about the incident, but I&#39;m assuming by articles you wanted the source material? (For example, foråÊ<a href=""http://www.gunviolencearchive.org/incident/500593,"">http://www.gunviolencearchive.org/incident/500593,</a>åÊwe&#39;d write the linkåÊ<a href=""http://abc7chicago.com/news/2-teens-found-fatally-shot-in-gary/1189625/"" target=""_new"">http://abc7chicago.com/news/2-teens-found-fatally-shot-in-gary/1189625/</a>åÊto the newly created textfile).</p>
<p></p>
<p>Or should we include the gunviolencearchive page as well?</p>",Gun Violence Archive URLs,1
940847336,4/26/2016 17:39:37,false,1969464628,,4/26/2016 17:38:56,false,neodev,0.8889,33568303,VEN,23,Cabimas,190.77.7.36,0,,"When I try to scrape the violence archive, I get: &#34;<p>Thanks for your interest in our statistics! Unfortunately, we must limit result sets for the general public. If you would like more information or the full result set, please send an email with your name, organization, project and needs toåÊ<a href=""https://www.piazza.com/cdn-cgi/l/email-protection#f89196898d918a81b89f8e99d68d8bd69b9795"" target=""_blank"">[emailåÊprotected]</a></p>",Scraping violence archive,"<p>Where exactly on the gun violence page are we supposed to crawl? I modified my code to look like:</p>
<p></p>
<pre>   for i in xrange(1, 11):<br />     crawl(&#39;http://gunviolencearchive.org/last-72-hours?page=%s&#39;%i, domain=DOMAIN)</pre>
<p>With a domain restriction of gunviolencearchive.org, but I&#39;m repeatedly running my crawler and it&#39;s not returning and websites (which seems to be accurate - there don&#39;t seem to be many hyperlinks). The same thing happens when I just tell the crawler to crawl the home page. Am I doing something wrong?</p>",Crawling Gun Violence Archive,"<p>should we just copy the output from scraping the gun violence archive and append it to the beginning of all the urls that we got from querying through the bing api? could you please clarify what you mean by:</p>
<p></p>
<p>&#34;These urls must include those that you collect from the Gun Violence Archive in Step 4, and the Bing API urls you collect using the three queries in Step 7. The remaining urls can come from whereever you want&#34;</p>
<p></p>
<p>This is all one big txt file?åÊ</p>",copying output from gun violence archive &amp; bing api ping results,"<p>I am creating my cloudflower task and want to give the most descriptive definition of gun violence as possible.</p>
<p></p>
<p>Are we only interested in articles that mention a specific act of gun violence? For example how should we classify articles that discuss gun violence statistics or gun crime as a whole.</p>
<p>If a gun was involved in the article but no one was injured, should this be considered gun violence?</p>",Definition of Gun Violence,"<p>Should articles related to gun violence, but not describing a particular incident of gun violence, be labelled as related or not?</p>
<p>For example</p>
<ul><li>Gun control legislation (<a href=""http://abcnews.go.com/ABC_Univision/Politics/chicagos-gun-violence-role-national-gun-debate/story?id=18365373"">http://abcnews.go.com/ABC_Univision/Politics/chicagos-gun-violence-role-national-gun-debate/story?id=18365373</a>)</li><li>Statistics on gun violence (<a href=""http://www.nbcnews.com/health/terrible-tally-500-children-dead-gunshots-every-year-7-500-8C11469222"">http://www.nbcnews.com/health/terrible-tally-500-children-dead-gunshots-every-year-7-500-8C11469222</a>)</li><li>Article that refers to another incident of gun violence (<a href=""http://www.kjct8.com/content/news/CIRT-team-called-in-to-investigate-the-officer-involved-shooting--368258921.html"">http://www.kjct8.com/content/news/CIRT-team-called-in-to-investigate-the-officer-involved-shooting--368258921.html</a>)</li></ul>
<p></p>
<p>I have classified these as not related, and am using it as a test question on CrowdFlower, but is causing me problems as many &#34;miss&#34; this question.</p>",Classification for Gun Violence Related Articles,"<p>For the gun violence archive URLs, should we keep the pages that have the gunviolencearchive domain? Most of these pages do have interesting facts about the incident, but I&#39;m assuming by articles you wanted the source material? (For example, foråÊ<a href=""http://www.gunviolencearchive.org/incident/500593,"">http://www.gunviolencearchive.org/incident/500593,</a>åÊwe&#39;d write the linkåÊ<a href=""http://abc7chicago.com/news/2-teens-found-fatally-shot-in-gary/1189625/"" target=""_new"">http://abc7chicago.com/news/2-teens-found-fatally-shot-in-gary/1189625/</a>åÊto the newly created textfile).</p>
<p></p>
<p>Or should we include the gunviolencearchive page as well?</p>",Gun Violence Archive URLs,1
940847336,4/26/2016 17:49:32,false,1969470403,,4/26/2016 17:45:16,false,clixsense,1.0,35444326,BRA,07,Brasília,177.15.130.106,0,,"When I try to scrape the violence archive, I get: &#34;<p>Thanks for your interest in our statistics! Unfortunately, we must limit result sets for the general public. If you would like more information or the full result set, please send an email with your name, organization, project and needs toåÊ<a href=""https://www.piazza.com/cdn-cgi/l/email-protection#f89196898d918a81b89f8e99d68d8bd69b9795"" target=""_blank"">[emailåÊprotected]</a></p>",Scraping violence archive,"<p>Where exactly on the gun violence page are we supposed to crawl? I modified my code to look like:</p>
<p></p>
<pre>   for i in xrange(1, 11):<br />     crawl(&#39;http://gunviolencearchive.org/last-72-hours?page=%s&#39;%i, domain=DOMAIN)</pre>
<p>With a domain restriction of gunviolencearchive.org, but I&#39;m repeatedly running my crawler and it&#39;s not returning and websites (which seems to be accurate - there don&#39;t seem to be many hyperlinks). The same thing happens when I just tell the crawler to crawl the home page. Am I doing something wrong?</p>",Crawling Gun Violence Archive,"<p>should we just copy the output from scraping the gun violence archive and append it to the beginning of all the urls that we got from querying through the bing api? could you please clarify what you mean by:</p>
<p></p>
<p>&#34;These urls must include those that you collect from the Gun Violence Archive in Step 4, and the Bing API urls you collect using the three queries in Step 7. The remaining urls can come from whereever you want&#34;</p>
<p></p>
<p>This is all one big txt file?åÊ</p>",copying output from gun violence archive &amp; bing api ping results,"<p>I am creating my cloudflower task and want to give the most descriptive definition of gun violence as possible.</p>
<p></p>
<p>Are we only interested in articles that mention a specific act of gun violence? For example how should we classify articles that discuss gun violence statistics or gun crime as a whole.</p>
<p>If a gun was involved in the article but no one was injured, should this be considered gun violence?</p>",Definition of Gun Violence,"<p>Should articles related to gun violence, but not describing a particular incident of gun violence, be labelled as related or not?</p>
<p>For example</p>
<ul><li>Gun control legislation (<a href=""http://abcnews.go.com/ABC_Univision/Politics/chicagos-gun-violence-role-national-gun-debate/story?id=18365373"">http://abcnews.go.com/ABC_Univision/Politics/chicagos-gun-violence-role-national-gun-debate/story?id=18365373</a>)</li><li>Statistics on gun violence (<a href=""http://www.nbcnews.com/health/terrible-tally-500-children-dead-gunshots-every-year-7-500-8C11469222"">http://www.nbcnews.com/health/terrible-tally-500-children-dead-gunshots-every-year-7-500-8C11469222</a>)</li><li>Article that refers to another incident of gun violence (<a href=""http://www.kjct8.com/content/news/CIRT-team-called-in-to-investigate-the-officer-involved-shooting--368258921.html"">http://www.kjct8.com/content/news/CIRT-team-called-in-to-investigate-the-officer-involved-shooting--368258921.html</a>)</li></ul>
<p></p>
<p>I have classified these as not related, and am using it as a test question on CrowdFlower, but is causing me problems as many &#34;miss&#34; this question.</p>",Classification for Gun Violence Related Articles,"<p>For the gun violence archive URLs, should we keep the pages that have the gunviolencearchive domain? Most of these pages do have interesting facts about the incident, but I&#39;m assuming by articles you wanted the source material? (For example, foråÊ<a href=""http://www.gunviolencearchive.org/incident/500593,"">http://www.gunviolencearchive.org/incident/500593,</a>åÊwe&#39;d write the linkåÊ<a href=""http://abc7chicago.com/news/2-teens-found-fatally-shot-in-gary/1189625/"" target=""_new"">http://abc7chicago.com/news/2-teens-found-fatally-shot-in-gary/1189625/</a>åÊto the newly created textfile).</p>
<p></p>
<p>Or should we include the gunviolencearchive page as well?</p>",Gun Violence Archive URLs,1
940847336,4/26/2016 18:04:05,false,1969477775,,4/26/2016 17:46:51,false,clixsense,0.8889,35338593,ITA,14,Cagliari,151.56.132.145,0,,"When I try to scrape the violence archive, I get: &#34;<p>Thanks for your interest in our statistics! Unfortunately, we must limit result sets for the general public. If you would like more information or the full result set, please send an email with your name, organization, project and needs toåÊ<a href=""https://www.piazza.com/cdn-cgi/l/email-protection#f89196898d918a81b89f8e99d68d8bd69b9795"" target=""_blank"">[emailåÊprotected]</a></p>",Scraping violence archive,"<p>Where exactly on the gun violence page are we supposed to crawl? I modified my code to look like:</p>
<p></p>
<pre>   for i in xrange(1, 11):<br />     crawl(&#39;http://gunviolencearchive.org/last-72-hours?page=%s&#39;%i, domain=DOMAIN)</pre>
<p>With a domain restriction of gunviolencearchive.org, but I&#39;m repeatedly running my crawler and it&#39;s not returning and websites (which seems to be accurate - there don&#39;t seem to be many hyperlinks). The same thing happens when I just tell the crawler to crawl the home page. Am I doing something wrong?</p>",Crawling Gun Violence Archive,"<p>should we just copy the output from scraping the gun violence archive and append it to the beginning of all the urls that we got from querying through the bing api? could you please clarify what you mean by:</p>
<p></p>
<p>&#34;These urls must include those that you collect from the Gun Violence Archive in Step 4, and the Bing API urls you collect using the three queries in Step 7. The remaining urls can come from whereever you want&#34;</p>
<p></p>
<p>This is all one big txt file?åÊ</p>",copying output from gun violence archive &amp; bing api ping results,"<p>I am creating my cloudflower task and want to give the most descriptive definition of gun violence as possible.</p>
<p></p>
<p>Are we only interested in articles that mention a specific act of gun violence? For example how should we classify articles that discuss gun violence statistics or gun crime as a whole.</p>
<p>If a gun was involved in the article but no one was injured, should this be considered gun violence?</p>",Definition of Gun Violence,"<p>Should articles related to gun violence, but not describing a particular incident of gun violence, be labelled as related or not?</p>
<p>For example</p>
<ul><li>Gun control legislation (<a href=""http://abcnews.go.com/ABC_Univision/Politics/chicagos-gun-violence-role-national-gun-debate/story?id=18365373"">http://abcnews.go.com/ABC_Univision/Politics/chicagos-gun-violence-role-national-gun-debate/story?id=18365373</a>)</li><li>Statistics on gun violence (<a href=""http://www.nbcnews.com/health/terrible-tally-500-children-dead-gunshots-every-year-7-500-8C11469222"">http://www.nbcnews.com/health/terrible-tally-500-children-dead-gunshots-every-year-7-500-8C11469222</a>)</li><li>Article that refers to another incident of gun violence (<a href=""http://www.kjct8.com/content/news/CIRT-team-called-in-to-investigate-the-officer-involved-shooting--368258921.html"">http://www.kjct8.com/content/news/CIRT-team-called-in-to-investigate-the-officer-involved-shooting--368258921.html</a>)</li></ul>
<p></p>
<p>I have classified these as not related, and am using it as a test question on CrowdFlower, but is causing me problems as many &#34;miss&#34; this question.</p>",Classification for Gun Violence Related Articles,"<p>For the gun violence archive URLs, should we keep the pages that have the gunviolencearchive domain? Most of these pages do have interesting facts about the incident, but I&#39;m assuming by articles you wanted the source material? (For example, foråÊ<a href=""http://www.gunviolencearchive.org/incident/500593,"">http://www.gunviolencearchive.org/incident/500593,</a>åÊwe&#39;d write the linkåÊ<a href=""http://abc7chicago.com/news/2-teens-found-fatally-shot-in-gary/1189625/"" target=""_new"">http://abc7chicago.com/news/2-teens-found-fatally-shot-in-gary/1189625/</a>åÊto the newly created textfile).</p>
<p></p>
<p>Or should we include the gunviolencearchive page as well?</p>",Gun Violence Archive URLs,1
940847337,4/26/2016 15:11:31,false,1969363818,,4/26/2016 15:10:32,false,tremorgames,1.0,32635967,LTU,60,Panevezys,78.63.38.165,0,,"<p>I&#39;ve already started my OH in the GRW bump space so feel free to come on by early if you&#39;d like. I&#39;ll be here till 5 unless it get&#39;s crowded in which case find me near the study space next to the elevators on the 5th floor of Levine.</p>
<p></p>",Starting OH early,"<p>Hey everyone,åÊI&#39;m moving my office hours tonight to Moore 100 instead of the bump space.</p>",OH moved to Moore,"<p>Hi all,</p>
<p></p>
<p>I have to cancel my office hours today from 12-2. If you needed help on somethingåÊandåÊwere planning to come, send me an email or a private message here and we can set up a time to talk either in person or over video chat today or this weekend.åÊ</p>
<p></p>
<p>Sorry for the inconvenience!</p>
<p>Ellie</p>",Canceling OH for today,Will there be any OH over the weekend since this assignment is due on Monday instead of the usual Friday?,OH over the weekend?,<p>Sorry everyone I&#39;m running a little late. Will start my OH at 3:15 and go till 5:15</p>,Pushing back OH by 15,"<p>Hi everyone,åÊ</p>
<p></p>
<p>I&#39;m not feeling well, so I&#39;m canceling the office hours I was planning to hold today from 4-6pm. However, Sierra will still be holding her office hours at the same time. I&#39;ll try to hold make-up OH soon.åÊ</p>
<p></p>
<p>Best,</p>
<p>Kate</p>",OH Today,1
940847337,4/26/2016 15:15:42,false,1969369975,,4/26/2016 15:14:06,false,clixsense,1.0,24287706,TWN,04,Keelung,61.231.195.173,0,,"<p>I&#39;ve already started my OH in the GRW bump space so feel free to come on by early if you&#39;d like. I&#39;ll be here till 5 unless it get&#39;s crowded in which case find me near the study space next to the elevators on the 5th floor of Levine.</p>
<p></p>",Starting OH early,"<p>Hey everyone,åÊI&#39;m moving my office hours tonight to Moore 100 instead of the bump space.</p>",OH moved to Moore,"<p>Hi all,</p>
<p></p>
<p>I have to cancel my office hours today from 12-2. If you needed help on somethingåÊandåÊwere planning to come, send me an email or a private message here and we can set up a time to talk either in person or over video chat today or this weekend.åÊ</p>
<p></p>
<p>Sorry for the inconvenience!</p>
<p>Ellie</p>",Canceling OH for today,Will there be any OH over the weekend since this assignment is due on Monday instead of the usual Friday?,OH over the weekend?,<p>Sorry everyone I&#39;m running a little late. Will start my OH at 3:15 and go till 5:15</p>,Pushing back OH by 15,"<p>Hi everyone,åÊ</p>
<p></p>
<p>I&#39;m not feeling well, so I&#39;m canceling the office hours I was planning to hold today from 4-6pm. However, Sierra will still be holding her office hours at the same time. I&#39;ll try to hold make-up OH soon.åÊ</p>
<p></p>
<p>Best,</p>
<p>Kate</p>",OH Today,1
940847337,4/26/2016 15:19:31,false,1969376280,,4/26/2016 15:17:21,false,clixsense,1.0,7837812,SRB,00,Belgrade,79.101.254.233,0,,"<p>I&#39;ve already started my OH in the GRW bump space so feel free to come on by early if you&#39;d like. I&#39;ll be here till 5 unless it get&#39;s crowded in which case find me near the study space next to the elevators on the 5th floor of Levine.</p>
<p></p>",Starting OH early,"<p>Hey everyone,åÊI&#39;m moving my office hours tonight to Moore 100 instead of the bump space.</p>",OH moved to Moore,"<p>Hi all,</p>
<p></p>
<p>I have to cancel my office hours today from 12-2. If you needed help on somethingåÊandåÊwere planning to come, send me an email or a private message here and we can set up a time to talk either in person or over video chat today or this weekend.åÊ</p>
<p></p>
<p>Sorry for the inconvenience!</p>
<p>Ellie</p>",Canceling OH for today,Will there be any OH over the weekend since this assignment is due on Monday instead of the usual Friday?,OH over the weekend?,<p>Sorry everyone I&#39;m running a little late. Will start my OH at 3:15 and go till 5:15</p>,Pushing back OH by 15,"<p>Hi everyone,åÊ</p>
<p></p>
<p>I&#39;m not feeling well, so I&#39;m canceling the office hours I was planning to hold today from 4-6pm. However, Sierra will still be holding her office hours at the same time. I&#39;ll try to hold make-up OH soon.åÊ</p>
<p></p>
<p>Best,</p>
<p>Kate</p>",OH Today,1
940847337,4/26/2016 15:19:53,false,1969376819,,4/26/2016 15:19:30,false,neodev,1.0,19132694,LKA,36,Colombo,123.231.124.170,0,,"<p>I&#39;ve already started my OH in the GRW bump space so feel free to come on by early if you&#39;d like. I&#39;ll be here till 5 unless it get&#39;s crowded in which case find me near the study space next to the elevators on the 5th floor of Levine.</p>
<p></p>",Starting OH early,"<p>Hey everyone,åÊI&#39;m moving my office hours tonight to Moore 100 instead of the bump space.</p>",OH moved to Moore,"<p>Hi all,</p>
<p></p>
<p>I have to cancel my office hours today from 12-2. If you needed help on somethingåÊandåÊwere planning to come, send me an email or a private message here and we can set up a time to talk either in person or over video chat today or this weekend.åÊ</p>
<p></p>
<p>Sorry for the inconvenience!</p>
<p>Ellie</p>",Canceling OH for today,Will there be any OH over the weekend since this assignment is due on Monday instead of the usual Friday?,OH over the weekend?,<p>Sorry everyone I&#39;m running a little late. Will start my OH at 3:15 and go till 5:15</p>,Pushing back OH by 15,"<p>Hi everyone,åÊ</p>
<p></p>
<p>I&#39;m not feeling well, so I&#39;m canceling the office hours I was planning to hold today from 4-6pm. However, Sierra will still be holding her office hours at the same time. I&#39;ll try to hold make-up OH soon.åÊ</p>
<p></p>
<p>Best,</p>
<p>Kate</p>",OH Today,1
940847337,4/26/2016 15:24:26,false,1969384552,,4/26/2016 15:21:07,false,elite,1.0,30280423,ITA,15,Siracusa,151.54.84.121,0,,"<p>I&#39;ve already started my OH in the GRW bump space so feel free to come on by early if you&#39;d like. I&#39;ll be here till 5 unless it get&#39;s crowded in which case find me near the study space next to the elevators on the 5th floor of Levine.</p>
<p></p>",Starting OH early,"<p>Hey everyone,åÊI&#39;m moving my office hours tonight to Moore 100 instead of the bump space.</p>",OH moved to Moore,"<p>Hi all,</p>
<p></p>
<p>I have to cancel my office hours today from 12-2. If you needed help on somethingåÊandåÊwere planning to come, send me an email or a private message here and we can set up a time to talk either in person or over video chat today or this weekend.åÊ</p>
<p></p>
<p>Sorry for the inconvenience!</p>
<p>Ellie</p>",Canceling OH for today,Will there be any OH over the weekend since this assignment is due on Monday instead of the usual Friday?,OH over the weekend?,<p>Sorry everyone I&#39;m running a little late. Will start my OH at 3:15 and go till 5:15</p>,Pushing back OH by 15,"<p>Hi everyone,åÊ</p>
<p></p>
<p>I&#39;m not feeling well, so I&#39;m canceling the office hours I was planning to hold today from 4-6pm. However, Sierra will still be holding her office hours at the same time. I&#39;ll try to hold make-up OH soon.åÊ</p>
<p></p>
<p>Best,</p>
<p>Kate</p>",OH Today,1
940847338,4/26/2016 15:27:29,false,1969389361,,4/26/2016 15:26:56,false,instagc,0.8889,13581319,USA,IL,Waltonville,208.70.36.12,0,,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>If we shared an account with someone, is it ok to just list our individual hourly rate based on which tasks we individually did and just indicate that in the survey?</p>",Highest Hourly Rate (if we shared an account with someone),4
940847338,4/26/2016 15:28:58,false,1969390423,,4/26/2016 15:27:17,false,elite,1.0,30280423,ITA,15,Siracusa,151.54.84.121,0,,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>If we shared an account with someone, is it ok to just list our individual hourly rate based on which tasks we individually did and just indicate that in the survey?</p>",Highest Hourly Rate (if we shared an account with someone),4
940847338,4/26/2016 15:30:05,false,1969390863,,4/26/2016 15:26:19,false,clixsense,0.8889,36052512,PHL,F2,Quezon City,49.149.150.150,0,,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>If we shared an account with someone, is it ok to just list our individual hourly rate based on which tasks we individually did and just indicate that in the survey?</p>",Highest Hourly Rate (if we shared an account with someone),4
940847338,4/26/2016 15:51:22,false,1969399814,,4/26/2016 15:36:03,false,neodev,1.0,13396426,VEN,15,Santa Teresa,190.38.163.149,0,,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>If we shared an account with someone, is it ok to just list our individual hourly rate based on which tasks we individually did and just indicate that in the survey?</p>",Highest Hourly Rate (if we shared an account with someone),4
940847338,4/26/2016 15:58:24,false,1969402714,,4/26/2016 15:56:48,false,elite,1.0,33243069,IND,10,Faridabad,116.203.79.150,0,,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>If we shared an account with someone, is it ok to just list our individual hourly rate based on which tasks we individually did and just indicate that in the survey?</p>",Highest Hourly Rate (if we shared an account with someone),4
940847339,4/26/2016 15:28:06,false,1969389962,,4/26/2016 15:27:30,false,instagc,0.8889,13581319,USA,IL,Waltonville,208.70.36.12,0,,"<p>The slides for today&#39;s lecture topics are now online:</p>
<p><a href=""http://crowdsourcing-class.org/slides/machine-learning-part-2.pdf"">Machine Learning - part 2</a></p>
<p><a href=""http://crowdsourcing-class.org/slides/amazon-mechanical-turk.pdf"">The Amazon Mechanical Turk crowdsourcing platform</a></p>
<p></p>
<p>Please post any questions that you have about either topic to this thread.åÊ</p>",Slides for today&#39;s lecture,"<p>On Fridays I&#39;m planning on doing more applied hands-on lectures where the TAs and I walk you through the homework assignments and help you get started. åÊHow did you like today&#39;s lecture?</p>
<p></p>
<p>Feel free to leave comments on things you would like to see in the Friday lectures, after you have voted.</p>
<br/> [o] Today&#39;s lecture was good, and helped me understand the assignment
[o] I didn&#39;t find it particularly useful, or I would have preferred a standard non-applied lecture",Poll: Friday hands-on lectures,"<p>Hi everyone,</p>
<p></p>
<p>Just a friendly reminder that you&#39;ll get credit towards your participation grade for attending today&#39;s NETS 213 lecture. åÊThe TAs will be passing out unique participation codes, one per student, as you leave the classroom. åÊBe sure to get one, and then enter it <a href=""https://docs.google.com/forms/d/14UZWosW5_W_-qDNI8KJ_zUGkiyTO9yuwv7yCkCvuZgQ/viewform"" target=""_blank"">here</a>åÊafter class.</p>
<p></p>
<p>--Chris</p>",Reminder: participation credit for attending today&#39;s NETS 213 lecture,<p>There will be a guest lecturer in NETS 213 today. åÊPlease attend the lecture. åÊWe mightåÊtake attendance.åÊ</p>,Guest lecture today,"<p>I&#39;m going to repeat the lecture that I gave in the last class since so few students made it to that one. åÊIf you attended the reputation system lecture on Friday March 4thåÊthenåÊyou don&#39;t need to come today.</p>
<p></p>
<p>The relevant piece of information that I&#39;ll announce in class today is that the <a href=""http://crowdsourcing-class.org/assignment7.html"" target=""_blank"">quality control homework has been released</a>. åÊThis assignment is due before class onåÊMonday, March 21, 2016.</p>
<p></p>
<p></p>",Repeating the last lecture,"<p>If you&#39;d like to discussåÊAl Filries&#39; lecture about MOOC&#39;s, let&#39;s use this discussion thread.</p>
<p></p>
<p>Here&#39;s a link toåÊhis <a href=""https://www.coursera.org/course/modernpoetry"" target=""_blank"">ModPo Coursera course</a>.</p>
<p></p>",Al Filries&#39; lecture,0
940847339,4/26/2016 15:30:54,false,1969391261,,4/26/2016 15:28:59,false,elite,1.0,30280423,ITA,15,Siracusa,151.54.84.121,0,,"<p>The slides for today&#39;s lecture topics are now online:</p>
<p><a href=""http://crowdsourcing-class.org/slides/machine-learning-part-2.pdf"">Machine Learning - part 2</a></p>
<p><a href=""http://crowdsourcing-class.org/slides/amazon-mechanical-turk.pdf"">The Amazon Mechanical Turk crowdsourcing platform</a></p>
<p></p>
<p>Please post any questions that you have about either topic to this thread.åÊ</p>",Slides for today&#39;s lecture,"<p>On Fridays I&#39;m planning on doing more applied hands-on lectures where the TAs and I walk you through the homework assignments and help you get started. åÊHow did you like today&#39;s lecture?</p>
<p></p>
<p>Feel free to leave comments on things you would like to see in the Friday lectures, after you have voted.</p>
<br/> [o] Today&#39;s lecture was good, and helped me understand the assignment
[o] I didn&#39;t find it particularly useful, or I would have preferred a standard non-applied lecture",Poll: Friday hands-on lectures,"<p>Hi everyone,</p>
<p></p>
<p>Just a friendly reminder that you&#39;ll get credit towards your participation grade for attending today&#39;s NETS 213 lecture. åÊThe TAs will be passing out unique participation codes, one per student, as you leave the classroom. åÊBe sure to get one, and then enter it <a href=""https://docs.google.com/forms/d/14UZWosW5_W_-qDNI8KJ_zUGkiyTO9yuwv7yCkCvuZgQ/viewform"" target=""_blank"">here</a>åÊafter class.</p>
<p></p>
<p>--Chris</p>",Reminder: participation credit for attending today&#39;s NETS 213 lecture,<p>There will be a guest lecturer in NETS 213 today. åÊPlease attend the lecture. åÊWe mightåÊtake attendance.åÊ</p>,Guest lecture today,"<p>I&#39;m going to repeat the lecture that I gave in the last class since so few students made it to that one. åÊIf you attended the reputation system lecture on Friday March 4thåÊthenåÊyou don&#39;t need to come today.</p>
<p></p>
<p>The relevant piece of information that I&#39;ll announce in class today is that the <a href=""http://crowdsourcing-class.org/assignment7.html"" target=""_blank"">quality control homework has been released</a>. åÊThis assignment is due before class onåÊMonday, March 21, 2016.</p>
<p></p>
<p></p>",Repeating the last lecture,"<p>If you&#39;d like to discussåÊAl Filries&#39; lecture about MOOC&#39;s, let&#39;s use this discussion thread.</p>
<p></p>
<p>Here&#39;s a link toåÊhis <a href=""https://www.coursera.org/course/modernpoetry"" target=""_blank"">ModPo Coursera course</a>.</p>
<p></p>",Al Filries&#39; lecture,0
940847339,4/26/2016 15:35:58,false,1969392979,,4/26/2016 15:30:06,false,clixsense,0.8889,36052512,PHL,F2,Quezon City,49.149.150.150,0,,"<p>The slides for today&#39;s lecture topics are now online:</p>
<p><a href=""http://crowdsourcing-class.org/slides/machine-learning-part-2.pdf"">Machine Learning - part 2</a></p>
<p><a href=""http://crowdsourcing-class.org/slides/amazon-mechanical-turk.pdf"">The Amazon Mechanical Turk crowdsourcing platform</a></p>
<p></p>
<p>Please post any questions that you have about either topic to this thread.åÊ</p>",Slides for today&#39;s lecture,"<p>On Fridays I&#39;m planning on doing more applied hands-on lectures where the TAs and I walk you through the homework assignments and help you get started. åÊHow did you like today&#39;s lecture?</p>
<p></p>
<p>Feel free to leave comments on things you would like to see in the Friday lectures, after you have voted.</p>
<br/> [o] Today&#39;s lecture was good, and helped me understand the assignment
[o] I didn&#39;t find it particularly useful, or I would have preferred a standard non-applied lecture",Poll: Friday hands-on lectures,"<p>Hi everyone,</p>
<p></p>
<p>Just a friendly reminder that you&#39;ll get credit towards your participation grade for attending today&#39;s NETS 213 lecture. åÊThe TAs will be passing out unique participation codes, one per student, as you leave the classroom. åÊBe sure to get one, and then enter it <a href=""https://docs.google.com/forms/d/14UZWosW5_W_-qDNI8KJ_zUGkiyTO9yuwv7yCkCvuZgQ/viewform"" target=""_blank"">here</a>åÊafter class.</p>
<p></p>
<p>--Chris</p>",Reminder: participation credit for attending today&#39;s NETS 213 lecture,<p>There will be a guest lecturer in NETS 213 today. åÊPlease attend the lecture. åÊWe mightåÊtake attendance.åÊ</p>,Guest lecture today,"<p>I&#39;m going to repeat the lecture that I gave in the last class since so few students made it to that one. åÊIf you attended the reputation system lecture on Friday March 4thåÊthenåÊyou don&#39;t need to come today.</p>
<p></p>
<p>The relevant piece of information that I&#39;ll announce in class today is that the <a href=""http://crowdsourcing-class.org/assignment7.html"" target=""_blank"">quality control homework has been released</a>. åÊThis assignment is due before class onåÊMonday, March 21, 2016.</p>
<p></p>
<p></p>",Repeating the last lecture,"<p>If you&#39;d like to discussåÊAl Filries&#39; lecture about MOOC&#39;s, let&#39;s use this discussion thread.</p>
<p></p>
<p>Here&#39;s a link toåÊhis <a href=""https://www.coursera.org/course/modernpoetry"" target=""_blank"">ModPo Coursera course</a>.</p>
<p></p>",Al Filries&#39; lecture,0
940847339,4/26/2016 15:45:41,false,1969397207,,4/26/2016 15:39:57,false,clixsense,1.0,21875134,GBR,H9,London,87.112.158.81,0,,"<p>The slides for today&#39;s lecture topics are now online:</p>
<p><a href=""http://crowdsourcing-class.org/slides/machine-learning-part-2.pdf"">Machine Learning - part 2</a></p>
<p><a href=""http://crowdsourcing-class.org/slides/amazon-mechanical-turk.pdf"">The Amazon Mechanical Turk crowdsourcing platform</a></p>
<p></p>
<p>Please post any questions that you have about either topic to this thread.åÊ</p>",Slides for today&#39;s lecture,"<p>On Fridays I&#39;m planning on doing more applied hands-on lectures where the TAs and I walk you through the homework assignments and help you get started. åÊHow did you like today&#39;s lecture?</p>
<p></p>
<p>Feel free to leave comments on things you would like to see in the Friday lectures, after you have voted.</p>
<br/> [o] Today&#39;s lecture was good, and helped me understand the assignment
[o] I didn&#39;t find it particularly useful, or I would have preferred a standard non-applied lecture",Poll: Friday hands-on lectures,"<p>Hi everyone,</p>
<p></p>
<p>Just a friendly reminder that you&#39;ll get credit towards your participation grade for attending today&#39;s NETS 213 lecture. åÊThe TAs will be passing out unique participation codes, one per student, as you leave the classroom. åÊBe sure to get one, and then enter it <a href=""https://docs.google.com/forms/d/14UZWosW5_W_-qDNI8KJ_zUGkiyTO9yuwv7yCkCvuZgQ/viewform"" target=""_blank"">here</a>åÊafter class.</p>
<p></p>
<p>--Chris</p>",Reminder: participation credit for attending today&#39;s NETS 213 lecture,<p>There will be a guest lecturer in NETS 213 today. åÊPlease attend the lecture. åÊWe mightåÊtake attendance.åÊ</p>,Guest lecture today,"<p>I&#39;m going to repeat the lecture that I gave in the last class since so few students made it to that one. åÊIf you attended the reputation system lecture on Friday March 4thåÊthenåÊyou don&#39;t need to come today.</p>
<p></p>
<p>The relevant piece of information that I&#39;ll announce in class today is that the <a href=""http://crowdsourcing-class.org/assignment7.html"" target=""_blank"">quality control homework has been released</a>. åÊThis assignment is due before class onåÊMonday, March 21, 2016.</p>
<p></p>
<p></p>",Repeating the last lecture,"<p>If you&#39;d like to discussåÊAl Filries&#39; lecture about MOOC&#39;s, let&#39;s use this discussion thread.</p>
<p></p>
<p>Here&#39;s a link toåÊhis <a href=""https://www.coursera.org/course/modernpoetry"" target=""_blank"">ModPo Coursera course</a>.</p>
<p></p>",Al Filries&#39; lecture,0
940847339,4/26/2016 15:47:45,false,1969398096,,4/26/2016 15:41:08,false,neodev,0.7778,32569659,USA,MN,Minneapolis,97.127.88.224,2,,"<p>The slides for today&#39;s lecture topics are now online:</p>
<p><a href=""http://crowdsourcing-class.org/slides/machine-learning-part-2.pdf"">Machine Learning - part 2</a></p>
<p><a href=""http://crowdsourcing-class.org/slides/amazon-mechanical-turk.pdf"">The Amazon Mechanical Turk crowdsourcing platform</a></p>
<p></p>
<p>Please post any questions that you have about either topic to this thread.åÊ</p>",Slides for today&#39;s lecture,"<p>On Fridays I&#39;m planning on doing more applied hands-on lectures where the TAs and I walk you through the homework assignments and help you get started. åÊHow did you like today&#39;s lecture?</p>
<p></p>
<p>Feel free to leave comments on things you would like to see in the Friday lectures, after you have voted.</p>
<br/> [o] Today&#39;s lecture was good, and helped me understand the assignment
[o] I didn&#39;t find it particularly useful, or I would have preferred a standard non-applied lecture",Poll: Friday hands-on lectures,"<p>Hi everyone,</p>
<p></p>
<p>Just a friendly reminder that you&#39;ll get credit towards your participation grade for attending today&#39;s NETS 213 lecture. åÊThe TAs will be passing out unique participation codes, one per student, as you leave the classroom. åÊBe sure to get one, and then enter it <a href=""https://docs.google.com/forms/d/14UZWosW5_W_-qDNI8KJ_zUGkiyTO9yuwv7yCkCvuZgQ/viewform"" target=""_blank"">here</a>åÊafter class.</p>
<p></p>
<p>--Chris</p>",Reminder: participation credit for attending today&#39;s NETS 213 lecture,<p>There will be a guest lecturer in NETS 213 today. åÊPlease attend the lecture. åÊWe mightåÊtake attendance.åÊ</p>,Guest lecture today,"<p>I&#39;m going to repeat the lecture that I gave in the last class since so few students made it to that one. åÊIf you attended the reputation system lecture on Friday March 4thåÊthenåÊyou don&#39;t need to come today.</p>
<p></p>
<p>The relevant piece of information that I&#39;ll announce in class today is that the <a href=""http://crowdsourcing-class.org/assignment7.html"" target=""_blank"">quality control homework has been released</a>. åÊThis assignment is due before class onåÊMonday, March 21, 2016.</p>
<p></p>
<p></p>",Repeating the last lecture,"<p>If you&#39;d like to discussåÊAl Filries&#39; lecture about MOOC&#39;s, let&#39;s use this discussion thread.</p>
<p></p>
<p>Here&#39;s a link toåÊhis <a href=""https://www.coursera.org/course/modernpoetry"" target=""_blank"">ModPo Coursera course</a>.</p>
<p></p>",Al Filries&#39; lecture,0
940847340,4/26/2016 15:11:45,false,1969364127,,4/26/2016 15:11:33,false,tremorgames,1.0,32635967,LTU,60,Panevezys,78.63.38.165,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,<p>One of the video links for the peer review assignment we are supposed to do is broken (iStockPhoto). What should I do?</p>,Video link broken,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,<p>We signed up for a meeting tomorrow (per the final project part 2 instructions) ÛÒ where will these meetings be held?</p>,Prof/TA meetings,4
940847340,4/26/2016 15:19:14,false,1969375925,,4/26/2016 15:15:44,false,clixsense,1.0,24287706,TWN,04,Keelung,61.231.195.173,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,<p>One of the video links for the peer review assignment we are supposed to do is broken (iStockPhoto). What should I do?</p>,Video link broken,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,<p>We signed up for a meeting tomorrow (per the final project part 2 instructions) ÛÒ where will these meetings be held?</p>,Prof/TA meetings,4
940847340,4/26/2016 15:20:09,false,1969377303,,4/26/2016 15:19:55,false,neodev,1.0,19132694,LKA,36,Colombo,123.231.124.170,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,<p>One of the video links for the peer review assignment we are supposed to do is broken (iStockPhoto). What should I do?</p>,Video link broken,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,<p>We signed up for a meeting tomorrow (per the final project part 2 instructions) ÛÒ where will these meetings be held?</p>,Prof/TA meetings,4
940847340,4/26/2016 15:21:37,false,1969379718,,4/26/2016 15:19:32,false,clixsense,1.0,7837812,SRB,00,Belgrade,79.101.254.233,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,<p>One of the video links for the peer review assignment we are supposed to do is broken (iStockPhoto). What should I do?</p>,Video link broken,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,<p>We signed up for a meeting tomorrow (per the final project part 2 instructions) ÛÒ where will these meetings be held?</p>,Prof/TA meetings,4
940847340,4/26/2016 15:26:18,false,1969387708,,4/26/2016 15:21:33,false,clixsense,0.8889,36052512,PHL,F2,Quezon City,49.149.150.150,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,<p>One of the video links for the peer review assignment we are supposed to do is broken (iStockPhoto). What should I do?</p>,Video link broken,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,<p>We signed up for a meeting tomorrow (per the final project part 2 instructions) ÛÒ where will these meetings be held?</p>,Prof/TA meetings,4
940847341,4/26/2016 15:28:37,false,1969390280,,4/26/2016 15:28:07,false,instagc,0.8889,13581319,USA,IL,Waltonville,208.70.36.12,0,,"<p>When I try to submit the homework, it does this marvelous thing where it tells me that net213 is an invalid config file.</p>
<p></p>
<p>What to do now? Somewhat concerned.</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hqh2r7yc6p926r/ikaaipxq9b3g/image.png"" /></p>",Invalid config file,"<p>InåÊthe bing_api.py, I am trying to print out the xml response that we receive.</p>
<p>Any ideas how to do this?</p>",Print out xml file,"<p>We keep getting the following error on Crowdflower when trying to upload the date for our first HIT:åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/h6snkankbb96wq/im6rcyi1fkbg/Screen_Shot_20160324_at_4.53.49_PM.png"" /></p>
<p>We are not sure what this means since we have only inserted one row into the excel sheet to add the headers.</p>
<p></p>
<p>On another note, some of the text separates into several columns instead of staying in one. Is this just accidental tab separations in the text that Excel recognizes as a new column?</p>",Upload CSV/Excel File Error,<p>I am very confused about what the 6 files we are expected to have after Part 1. Can someone help me out?</p>,6 Files,"<p>how would you be able to copy to biglab again? i went through this with TA in OH but I forgot.åÊ</p>
<p></p>
<p>It was along the lines of (code) (<a href=""mailto:username&#64;biglab.seas...&#64;"">username&#64;biglab.seas...&#64;</a>)(filename)</p>",Copying files to BigLab,<p>My laptop has crashed but the assignment was saved online. Is there anyway I can access it from another laptop?</p>,Laptop Crashing and file recovery,0
940847341,4/26/2016 15:39:31,false,1969394681,,4/26/2016 15:36:13,false,clixsense,0.8889,36052512,PHL,F2,Quezon City,49.149.150.150,0,,"<p>When I try to submit the homework, it does this marvelous thing where it tells me that net213 is an invalid config file.</p>
<p></p>
<p>What to do now? Somewhat concerned.</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hqh2r7yc6p926r/ikaaipxq9b3g/image.png"" /></p>",Invalid config file,"<p>InåÊthe bing_api.py, I am trying to print out the xml response that we receive.</p>
<p>Any ideas how to do this?</p>",Print out xml file,"<p>We keep getting the following error on Crowdflower when trying to upload the date for our first HIT:åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/h6snkankbb96wq/im6rcyi1fkbg/Screen_Shot_20160324_at_4.53.49_PM.png"" /></p>
<p>We are not sure what this means since we have only inserted one row into the excel sheet to add the headers.</p>
<p></p>
<p>On another note, some of the text separates into several columns instead of staying in one. Is this just accidental tab separations in the text that Excel recognizes as a new column?</p>",Upload CSV/Excel File Error,<p>I am very confused about what the 6 files we are expected to have after Part 1. Can someone help me out?</p>,6 Files,"<p>how would you be able to copy to biglab again? i went through this with TA in OH but I forgot.åÊ</p>
<p></p>
<p>It was along the lines of (code) (<a href=""mailto:username&#64;biglab.seas...&#64;"">username&#64;biglab.seas...&#64;</a>)(filename)</p>",Copying files to BigLab,<p>My laptop has crashed but the assignment was saved online. Is there anyway I can access it from another laptop?</p>,Laptop Crashing and file recovery,0
940847341,4/26/2016 15:47:54,false,1969398209,,4/26/2016 15:44:26,false,neodev,1.0,28875937,PAK,05,Karachi,182.180.125.133,0,,"<p>When I try to submit the homework, it does this marvelous thing where it tells me that net213 is an invalid config file.</p>
<p></p>
<p>What to do now? Somewhat concerned.</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hqh2r7yc6p926r/ikaaipxq9b3g/image.png"" /></p>",Invalid config file,"<p>InåÊthe bing_api.py, I am trying to print out the xml response that we receive.</p>
<p>Any ideas how to do this?</p>",Print out xml file,"<p>We keep getting the following error on Crowdflower when trying to upload the date for our first HIT:åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/h6snkankbb96wq/im6rcyi1fkbg/Screen_Shot_20160324_at_4.53.49_PM.png"" /></p>
<p>We are not sure what this means since we have only inserted one row into the excel sheet to add the headers.</p>
<p></p>
<p>On another note, some of the text separates into several columns instead of staying in one. Is this just accidental tab separations in the text that Excel recognizes as a new column?</p>",Upload CSV/Excel File Error,<p>I am very confused about what the 6 files we are expected to have after Part 1. Can someone help me out?</p>,6 Files,"<p>how would you be able to copy to biglab again? i went through this with TA in OH but I forgot.åÊ</p>
<p></p>
<p>It was along the lines of (code) (<a href=""mailto:username&#64;biglab.seas...&#64;"">username&#64;biglab.seas...&#64;</a>)(filename)</p>",Copying files to BigLab,<p>My laptop has crashed but the assignment was saved online. Is there anyway I can access it from another laptop?</p>,Laptop Crashing and file recovery,0
940847341,4/26/2016 15:49:11,false,1969398841,,4/26/2016 15:45:52,false,elite,0.8889,36575101,IND,07,New Delhi,112.196.144.2,0,,"<p>When I try to submit the homework, it does this marvelous thing where it tells me that net213 is an invalid config file.</p>
<p></p>
<p>What to do now? Somewhat concerned.</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hqh2r7yc6p926r/ikaaipxq9b3g/image.png"" /></p>",Invalid config file,"<p>InåÊthe bing_api.py, I am trying to print out the xml response that we receive.</p>
<p>Any ideas how to do this?</p>",Print out xml file,"<p>We keep getting the following error on Crowdflower when trying to upload the date for our first HIT:åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/h6snkankbb96wq/im6rcyi1fkbg/Screen_Shot_20160324_at_4.53.49_PM.png"" /></p>
<p>We are not sure what this means since we have only inserted one row into the excel sheet to add the headers.</p>
<p></p>
<p>On another note, some of the text separates into several columns instead of staying in one. Is this just accidental tab separations in the text that Excel recognizes as a new column?</p>",Upload CSV/Excel File Error,<p>I am very confused about what the 6 files we are expected to have after Part 1. Can someone help me out?</p>,6 Files,"<p>how would you be able to copy to biglab again? i went through this with TA in OH but I forgot.åÊ</p>
<p></p>
<p>It was along the lines of (code) (<a href=""mailto:username&#64;biglab.seas...&#64;"">username&#64;biglab.seas...&#64;</a>)(filename)</p>",Copying files to BigLab,<p>My laptop has crashed but the assignment was saved online. Is there anyway I can access it from another laptop?</p>,Laptop Crashing and file recovery,0
940847341,4/26/2016 15:56:09,false,1969401804,,4/26/2016 15:45:42,false,clixsense,1.0,21875134,GBR,H9,London,87.112.158.81,0,,"<p>When I try to submit the homework, it does this marvelous thing where it tells me that net213 is an invalid config file.</p>
<p></p>
<p>What to do now? Somewhat concerned.</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hqh2r7yc6p926r/ikaaipxq9b3g/image.png"" /></p>",Invalid config file,"<p>InåÊthe bing_api.py, I am trying to print out the xml response that we receive.</p>
<p>Any ideas how to do this?</p>",Print out xml file,"<p>We keep getting the following error on Crowdflower when trying to upload the date for our first HIT:åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/h6snkankbb96wq/im6rcyi1fkbg/Screen_Shot_20160324_at_4.53.49_PM.png"" /></p>
<p>We are not sure what this means since we have only inserted one row into the excel sheet to add the headers.</p>
<p></p>
<p>On another note, some of the text separates into several columns instead of staying in one. Is this just accidental tab separations in the text that Excel recognizes as a new column?</p>",Upload CSV/Excel File Error,<p>I am very confused about what the 6 files we are expected to have after Part 1. Can someone help me out?</p>,6 Files,"<p>how would you be able to copy to biglab again? i went through this with TA in OH but I forgot.åÊ</p>
<p></p>
<p>It was along the lines of (code) (<a href=""mailto:username&#64;biglab.seas...&#64;"">username&#64;biglab.seas...&#64;</a>)(filename)</p>",Copying files to BigLab,<p>My laptop has crashed but the assignment was saved online. Is there anyway I can access it from another laptop?</p>,Laptop Crashing and file recovery,0
940847342,4/26/2016 15:11:45,false,1969364125,,4/26/2016 15:11:33,false,tremorgames,1.0,32635967,LTU,60,Panevezys,78.63.38.165,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>In computing Kendall&#39;s tau function, are we taking in two lists of worker quality from two of the three data sets and compare? Does the worker quality in the two lists need to be sorted? In other words, if in data set 1, workeråÊ1&#39;s quality is 0.9 and worker 2&#39;s quality is 0.8. And in another data set, worker 1&#39;s quality is 0.95 and worker 2&#39;s quality is 0.85, do we have to compare [0.9, 0.8] with [0.95, 0.85] or can we compare [0.9, 0.8] with [0.85, 0.95]?</p>",KendallÛªs tau,4
940847342,4/26/2016 15:19:14,false,1969375923,,4/26/2016 15:15:44,false,clixsense,1.0,24287706,TWN,04,Keelung,61.231.195.173,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>In computing Kendall&#39;s tau function, are we taking in two lists of worker quality from two of the three data sets and compare? Does the worker quality in the two lists need to be sorted? In other words, if in data set 1, workeråÊ1&#39;s quality is 0.9 and worker 2&#39;s quality is 0.8. And in another data set, worker 1&#39;s quality is 0.95 and worker 2&#39;s quality is 0.85, do we have to compare [0.9, 0.8] with [0.95, 0.85] or can we compare [0.9, 0.8] with [0.85, 0.95]?</p>",KendallÛªs tau,4
940847342,4/26/2016 15:20:09,false,1969377307,,4/26/2016 15:19:55,false,neodev,1.0,19132694,LKA,36,Colombo,123.231.124.170,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>In computing Kendall&#39;s tau function, are we taking in two lists of worker quality from two of the three data sets and compare? Does the worker quality in the two lists need to be sorted? In other words, if in data set 1, workeråÊ1&#39;s quality is 0.9 and worker 2&#39;s quality is 0.8. And in another data set, worker 1&#39;s quality is 0.95 and worker 2&#39;s quality is 0.85, do we have to compare [0.9, 0.8] with [0.95, 0.85] or can we compare [0.9, 0.8] with [0.85, 0.95]?</p>",KendallÛªs tau,4
940847342,4/26/2016 15:21:37,false,1969379721,,4/26/2016 15:19:32,false,clixsense,1.0,7837812,SRB,00,Belgrade,79.101.254.233,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>In computing Kendall&#39;s tau function, are we taking in two lists of worker quality from two of the three data sets and compare? Does the worker quality in the two lists need to be sorted? In other words, if in data set 1, workeråÊ1&#39;s quality is 0.9 and worker 2&#39;s quality is 0.8. And in another data set, worker 1&#39;s quality is 0.95 and worker 2&#39;s quality is 0.85, do we have to compare [0.9, 0.8] with [0.95, 0.85] or can we compare [0.9, 0.8] with [0.85, 0.95]?</p>",KendallÛªs tau,4
940847342,4/26/2016 15:26:18,false,1969387711,,4/26/2016 15:21:33,false,clixsense,0.8889,36052512,PHL,F2,Quezon City,49.149.150.150,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>In computing Kendall&#39;s tau function, are we taking in two lists of worker quality from two of the three data sets and compare? Does the worker quality in the two lists need to be sorted? In other words, if in data set 1, workeråÊ1&#39;s quality is 0.9 and worker 2&#39;s quality is 0.8. And in another data set, worker 1&#39;s quality is 0.95 and worker 2&#39;s quality is 0.85, do we have to compare [0.9, 0.8] with [0.95, 0.85] or can we compare [0.9, 0.8] with [0.85, 0.95]?</p>",KendallÛªs tau,4
940847343,4/26/2016 15:28:37,false,1969390284,,4/26/2016 15:28:07,false,instagc,0.8889,13581319,USA,IL,Waltonville,208.70.36.12,0,,"<p>I got .5 points off &#34;for not using the crawler to collect URLs in addition to the Bing API&#34; -- my collected URLs included URLs from the Gun Violence Archive, and I collected the rest using the Bing API.åÊ</p>
<p></p>
<p>The instructions didn&#39;t specify that we necessarily needed to use other methods to get 2500 URLs. The homework says to &#34;turn in a list of at least 2,500 urls, including some crawled from the Gun Violence Archive and some obtained using the Bing API.&#34;åÊ</p>
<p></p>
<p>Am I understanding correctly why I got the points off? If so, I&#39;m confused why I got points off given the instructions. Thank you!åÊ</p>",Question about grade,<p>What exactly is meant by the words in the &#34;red&#34; reviews that DO NOT appear in the &#34;white&#34; reviews? Does this mean we should not count a word that appears in a red review and at least oneåÊwhite review? Or is this simply a check for reviews that have both &#34;red&#34; and &#34;white&#34; in them (and thus we can&#39;t classify the words as either red or white)?åÊ</p>,Questions 9 &amp; 10: Red and White reviews,"<p>I&#39;m having an issue in the last part of the assignment creating the questions that require information from the spreadsheet. For example, for the question &#34;In what city did the incident take place?&#34; I am trying to add {{city_1}} as an option in the question so that the information from the spreadsheet will populate that field, but I get this error: &#34;&#39;{{city_1}}&#39; will show up in your results as &#39;city_1&#39;, which already exists as a header in your uploaded data.&#34; How can I achieve what is supposed to happen, instead of getting this error?</p>
<p></p>
<p>Thanks!</p>",Crowdflower question creation,<p>I&#39;m working on one of the iterativeåÊimage annotation assignment...I&#39;m having trouble understanding how to implement the iterative component to designing a task. Are there any hints/advice on where to start?åÊ</p>,Iterative questions on CrowdFlower,<p>Crowdflower recommends 13 Test Questions. Is this number ok?</p>,How many test questions on Crowdflower?,"<p></p><div>In my score report for the market research questionnaire (homework 2), it says I receivedåÊ-1 for &#34;saying that Uber was a completely new business model. It updates the taxi business model.&#34; åÊ</div>
<div>åÊ</div>
<div>By new business model, I wasn&#39;t referring to the already established model of drivers picking up riders; rather, I meant that Uber is a new business model in the way that it connects drivers and riders through a mobile app, and then takes a percentage of the profit.åÊ Please let me know your thoughts on this.
<div>åÊ</div>
<div>Thanks,</div>
<div>Nick WeinåÊ
<div></div>
</div>
</div>",Homework 2 Report Question,1
940847343,4/26/2016 15:39:31,false,1969394678,,4/26/2016 15:36:13,false,clixsense,0.8889,36052512,PHL,F2,Quezon City,49.149.150.150,0,,"<p>I got .5 points off &#34;for not using the crawler to collect URLs in addition to the Bing API&#34; -- my collected URLs included URLs from the Gun Violence Archive, and I collected the rest using the Bing API.åÊ</p>
<p></p>
<p>The instructions didn&#39;t specify that we necessarily needed to use other methods to get 2500 URLs. The homework says to &#34;turn in a list of at least 2,500 urls, including some crawled from the Gun Violence Archive and some obtained using the Bing API.&#34;åÊ</p>
<p></p>
<p>Am I understanding correctly why I got the points off? If so, I&#39;m confused why I got points off given the instructions. Thank you!åÊ</p>",Question about grade,<p>What exactly is meant by the words in the &#34;red&#34; reviews that DO NOT appear in the &#34;white&#34; reviews? Does this mean we should not count a word that appears in a red review and at least oneåÊwhite review? Or is this simply a check for reviews that have both &#34;red&#34; and &#34;white&#34; in them (and thus we can&#39;t classify the words as either red or white)?åÊ</p>,Questions 9 &amp; 10: Red and White reviews,"<p>I&#39;m having an issue in the last part of the assignment creating the questions that require information from the spreadsheet. For example, for the question &#34;In what city did the incident take place?&#34; I am trying to add {{city_1}} as an option in the question so that the information from the spreadsheet will populate that field, but I get this error: &#34;&#39;{{city_1}}&#39; will show up in your results as &#39;city_1&#39;, which already exists as a header in your uploaded data.&#34; How can I achieve what is supposed to happen, instead of getting this error?</p>
<p></p>
<p>Thanks!</p>",Crowdflower question creation,<p>I&#39;m working on one of the iterativeåÊimage annotation assignment...I&#39;m having trouble understanding how to implement the iterative component to designing a task. Are there any hints/advice on where to start?åÊ</p>,Iterative questions on CrowdFlower,<p>Crowdflower recommends 13 Test Questions. Is this number ok?</p>,How many test questions on Crowdflower?,"<p></p><div>In my score report for the market research questionnaire (homework 2), it says I receivedåÊ-1 for &#34;saying that Uber was a completely new business model. It updates the taxi business model.&#34; åÊ</div>
<div>åÊ</div>
<div>By new business model, I wasn&#39;t referring to the already established model of drivers picking up riders; rather, I meant that Uber is a new business model in the way that it connects drivers and riders through a mobile app, and then takes a percentage of the profit.åÊ Please let me know your thoughts on this.
<div>åÊ</div>
<div>Thanks,</div>
<div>Nick WeinåÊ
<div></div>
</div>
</div>",Homework 2 Report Question,1
940847343,4/26/2016 15:47:54,false,1969398210,,4/26/2016 15:44:26,false,neodev,1.0,28875937,PAK,05,Karachi,182.180.125.133,0,,"<p>I got .5 points off &#34;for not using the crawler to collect URLs in addition to the Bing API&#34; -- my collected URLs included URLs from the Gun Violence Archive, and I collected the rest using the Bing API.åÊ</p>
<p></p>
<p>The instructions didn&#39;t specify that we necessarily needed to use other methods to get 2500 URLs. The homework says to &#34;turn in a list of at least 2,500 urls, including some crawled from the Gun Violence Archive and some obtained using the Bing API.&#34;åÊ</p>
<p></p>
<p>Am I understanding correctly why I got the points off? If so, I&#39;m confused why I got points off given the instructions. Thank you!åÊ</p>",Question about grade,<p>What exactly is meant by the words in the &#34;red&#34; reviews that DO NOT appear in the &#34;white&#34; reviews? Does this mean we should not count a word that appears in a red review and at least oneåÊwhite review? Or is this simply a check for reviews that have both &#34;red&#34; and &#34;white&#34; in them (and thus we can&#39;t classify the words as either red or white)?åÊ</p>,Questions 9 &amp; 10: Red and White reviews,"<p>I&#39;m having an issue in the last part of the assignment creating the questions that require information from the spreadsheet. For example, for the question &#34;In what city did the incident take place?&#34; I am trying to add {{city_1}} as an option in the question so that the information from the spreadsheet will populate that field, but I get this error: &#34;&#39;{{city_1}}&#39; will show up in your results as &#39;city_1&#39;, which already exists as a header in your uploaded data.&#34; How can I achieve what is supposed to happen, instead of getting this error?</p>
<p></p>
<p>Thanks!</p>",Crowdflower question creation,<p>I&#39;m working on one of the iterativeåÊimage annotation assignment...I&#39;m having trouble understanding how to implement the iterative component to designing a task. Are there any hints/advice on where to start?åÊ</p>,Iterative questions on CrowdFlower,<p>Crowdflower recommends 13 Test Questions. Is this number ok?</p>,How many test questions on Crowdflower?,"<p></p><div>In my score report for the market research questionnaire (homework 2), it says I receivedåÊ-1 for &#34;saying that Uber was a completely new business model. It updates the taxi business model.&#34; åÊ</div>
<div>åÊ</div>
<div>By new business model, I wasn&#39;t referring to the already established model of drivers picking up riders; rather, I meant that Uber is a new business model in the way that it connects drivers and riders through a mobile app, and then takes a percentage of the profit.åÊ Please let me know your thoughts on this.
<div>åÊ</div>
<div>Thanks,</div>
<div>Nick WeinåÊ
<div></div>
</div>
</div>",Homework 2 Report Question,1
940847343,4/26/2016 15:49:11,false,1969398836,,4/26/2016 15:45:52,false,elite,0.8889,36575101,IND,07,New Delhi,112.196.144.2,0,,"<p>I got .5 points off &#34;for not using the crawler to collect URLs in addition to the Bing API&#34; -- my collected URLs included URLs from the Gun Violence Archive, and I collected the rest using the Bing API.åÊ</p>
<p></p>
<p>The instructions didn&#39;t specify that we necessarily needed to use other methods to get 2500 URLs. The homework says to &#34;turn in a list of at least 2,500 urls, including some crawled from the Gun Violence Archive and some obtained using the Bing API.&#34;åÊ</p>
<p></p>
<p>Am I understanding correctly why I got the points off? If so, I&#39;m confused why I got points off given the instructions. Thank you!åÊ</p>",Question about grade,<p>What exactly is meant by the words in the &#34;red&#34; reviews that DO NOT appear in the &#34;white&#34; reviews? Does this mean we should not count a word that appears in a red review and at least oneåÊwhite review? Or is this simply a check for reviews that have both &#34;red&#34; and &#34;white&#34; in them (and thus we can&#39;t classify the words as either red or white)?åÊ</p>,Questions 9 &amp; 10: Red and White reviews,"<p>I&#39;m having an issue in the last part of the assignment creating the questions that require information from the spreadsheet. For example, for the question &#34;In what city did the incident take place?&#34; I am trying to add {{city_1}} as an option in the question so that the information from the spreadsheet will populate that field, but I get this error: &#34;&#39;{{city_1}}&#39; will show up in your results as &#39;city_1&#39;, which already exists as a header in your uploaded data.&#34; How can I achieve what is supposed to happen, instead of getting this error?</p>
<p></p>
<p>Thanks!</p>",Crowdflower question creation,<p>I&#39;m working on one of the iterativeåÊimage annotation assignment...I&#39;m having trouble understanding how to implement the iterative component to designing a task. Are there any hints/advice on where to start?åÊ</p>,Iterative questions on CrowdFlower,<p>Crowdflower recommends 13 Test Questions. Is this number ok?</p>,How many test questions on Crowdflower?,"<p></p><div>In my score report for the market research questionnaire (homework 2), it says I receivedåÊ-1 for &#34;saying that Uber was a completely new business model. It updates the taxi business model.&#34; åÊ</div>
<div>åÊ</div>
<div>By new business model, I wasn&#39;t referring to the already established model of drivers picking up riders; rather, I meant that Uber is a new business model in the way that it connects drivers and riders through a mobile app, and then takes a percentage of the profit.åÊ Please let me know your thoughts on this.
<div>åÊ</div>
<div>Thanks,</div>
<div>Nick WeinåÊ
<div></div>
</div>
</div>",Homework 2 Report Question,1
940847343,4/26/2016 15:56:09,false,1969401796,,4/26/2016 15:45:42,false,clixsense,1.0,21875134,GBR,H9,London,87.112.158.81,0,,"<p>I got .5 points off &#34;for not using the crawler to collect URLs in addition to the Bing API&#34; -- my collected URLs included URLs from the Gun Violence Archive, and I collected the rest using the Bing API.åÊ</p>
<p></p>
<p>The instructions didn&#39;t specify that we necessarily needed to use other methods to get 2500 URLs. The homework says to &#34;turn in a list of at least 2,500 urls, including some crawled from the Gun Violence Archive and some obtained using the Bing API.&#34;åÊ</p>
<p></p>
<p>Am I understanding correctly why I got the points off? If so, I&#39;m confused why I got points off given the instructions. Thank you!åÊ</p>",Question about grade,<p>What exactly is meant by the words in the &#34;red&#34; reviews that DO NOT appear in the &#34;white&#34; reviews? Does this mean we should not count a word that appears in a red review and at least oneåÊwhite review? Or is this simply a check for reviews that have both &#34;red&#34; and &#34;white&#34; in them (and thus we can&#39;t classify the words as either red or white)?åÊ</p>,Questions 9 &amp; 10: Red and White reviews,"<p>I&#39;m having an issue in the last part of the assignment creating the questions that require information from the spreadsheet. For example, for the question &#34;In what city did the incident take place?&#34; I am trying to add {{city_1}} as an option in the question so that the information from the spreadsheet will populate that field, but I get this error: &#34;&#39;{{city_1}}&#39; will show up in your results as &#39;city_1&#39;, which already exists as a header in your uploaded data.&#34; How can I achieve what is supposed to happen, instead of getting this error?</p>
<p></p>
<p>Thanks!</p>",Crowdflower question creation,<p>I&#39;m working on one of the iterativeåÊimage annotation assignment...I&#39;m having trouble understanding how to implement the iterative component to designing a task. Are there any hints/advice on where to start?åÊ</p>,Iterative questions on CrowdFlower,<p>Crowdflower recommends 13 Test Questions. Is this number ok?</p>,How many test questions on Crowdflower?,"<p></p><div>In my score report for the market research questionnaire (homework 2), it says I receivedåÊ-1 for &#34;saying that Uber was a completely new business model. It updates the taxi business model.&#34; åÊ</div>
<div>åÊ</div>
<div>By new business model, I wasn&#39;t referring to the already established model of drivers picking up riders; rather, I meant that Uber is a new business model in the way that it connects drivers and riders through a mobile app, and then takes a percentage of the profit.åÊ Please let me know your thoughts on this.
<div>åÊ</div>
<div>Thanks,</div>
<div>Nick WeinåÊ
<div></div>
</div>
</div>",Homework 2 Report Question,1
940847344,4/26/2016 17:40:29,false,1969465254,,4/26/2016 17:39:38,false,neodev,0.8889,33568303,VEN,23,Cabimas,190.77.7.36,0,,"<p>Hi,<br /><br />I didn&#39;t receive an email with my grade for the classifier homework. (The only grades email I have received was that of Homework 1)<br /><br />My pennkey is erdardet<br /><br />Thanks for your help,<br />Eric Dardet</p>",No Grade Email Received for Classifier Homework,"<p>Hi everyone,åÊ</p>
<p></p>
<p>We&#39;ve had a chance to look through submissions for HW3 and while it&#39;ll be a few days before grades are ready to be sent out, we wanted to offer some feedback and clarification. Most of you had really high accuracies and many of you noted that accuracy was &#34;higher&#34; when you eliminated cross-validation because of overfitting (in the same way scores are higher when the midterm matches the practice test very closely).åÊ</p>
<p></p>
<p>X represented the full set of articles. y represented the labels. There were hundreds of thousands of unigram features, because features corresponded to each word that appeared (repeats did not contribute to # of features -- see &#39;that guy shot that other guy&#39; for an example).åÊ</p>
<p></p>
<p>This week, you are re-training your classifier on the full set of articles that we gave you last week (&#39;articles&#39; is the name of the text file) and then output predictions on the new URLs you collected.åÊYou&#39;ll want to modify the code for your statistical classifier to generate the labels for crowdworkers to check - the file on the assignment page is a great start towards that.åÊ</p>
<p></p>
<p>If you have any questions, please come to office hours sooner rather than later and post to Piazza if your question hasn&#39;t already been asked. Some parts of this assignment just take time to finish -- crawling, waiting for crowd worker judgements, the questionnaire -- and it&#39;s better to leave yourself time to debug.åÊ</p>",Using your classifier in HW4: Becoming a Requester,"<p>Hi all!</p>
<p></p>
<p>We have finished grading your classifier assignments, and you should get your grades shortly if you have not already (look for an email from Kate!). You guys did very well overall. It was not an easy assignment and there wereåÊa lot of new concepts that you had to take in (and new packages you had to install) all at once, so very well done.åÊ</p>
<p></p>
<p>There were twoåÊcommon points of confusion. Please feel free to ask questions here or in OHs if you are having trouble with these concepts, since they are important if you intend to do more ML in the future.</p>
<p></p>
<p><strong>What exactly areåÊX and y? What are their dimensions?åÊ</strong></p>
<p>In short, y is a vector of labels (one label per article). X is a matrix of features (one row per article, one column per feature). (You mightåÊpreferåÊto think of X as aåÊlist of vectors of features).åÊA key point is thatåÊ<span style=""text-decoration:underline"">every article has a value for every feature</span>-- so every article (row) has the same number of features (columns) associated with it. What changes areåÊthe values of those features (e.g. 0 or 1 in our assignment). Since the &#34;features&#34; in our caseåÊwere words, <span style=""text-decoration:underline"">the number of features (number of columns of X) is equal to the number of words in our vocabulary</span> (this was probably ~300,000-500,000 in our dataset, depending on your preprocessing). If theåÊword appeared in the article, theåÊvalue for theåÊword is 1 for that article, and if it did not appear, theåÊvalue is 0.åÊ</p>
<p></p>
<p>You are not alone if you find this concept a bit abstract. The biggest hurdle to overcome when learning about ML is getting comfortable thinking about your data as a feature matrix. Once you have internalized this notion, the rest of ML is reasonably straightforward. Please, ask us questions!</p>
<p></p>
<p><strong>False positives and false negativesåÊ</strong></p>
<p>Many of you were confused by the concept of false positives and false negatives. <span style=""text-decoration:underline"">A *false positive* in our case is an article that should have been labeled &#34;not gun related&#34; (0) but was falsely labeled &#34;gun related&#34; (1).</span> These were likely articles with words like &#34;shoot&#34; but which referred to sports or movies instead of violent crimes. A *false negative* isåÊan article that should have been labeled &#34;gun related&#34; (1) but was falsely labeled &#34;not gun related&#34; (0). Its good to think about these different types of errors when you are working on prediction tasks-- often different types of errors have different costs, or one type of error is worse than another.åÊåÊ(E.g. in medicine, tests that produce many false negativesåÊare usually considered worse that ones that produce false positives. I&#39;m sure you can think of some reasons why?)</p>
<p>åÊ</p>
<p><strong>But what now? My life has been so empty since turning in that classifier assignment...åÊ</strong></p>
<p>Many of you have expressed interest in learning more about machine learning, and you should absolutely consider enrolling in <a href=""https://alliance.seas.upenn.edu/~cis520/wiki/"" target=""_blank"">penn&#39;s ML course</a>åÊor <a href=""http://www.seas.upenn.edu/~cis519/fall2015/"" target=""_blank"">penn&#39;s intro ML course</a>. Or check out the <a href=""https://www.coursera.org/learn/machine-learning"" target=""_blank"">Coursera course</a>åÊ(crowdsourcing woot woot!). If you can&#39;t wait that long,åÊyou have all the freedom in the world to build an ML component into your final project, and we are more than happy to help you do so!</p>
<p></p>
<p>Hope everyone hasåÊa relaxing, happy weekend!åÊ</p>
<p></p>
<p>#pin</p>",Take-aways from you classifier assignments,<p>ShouldåÊwe have a single if-and-or statement with all of the keywords we use? Or should we have a series of if statements when checking keywords?åÊ</p>,rule-based classifier,<p>Resolved!</p>,Error when running classifier template,"<p>When I run the classifier, I get the error:åÊ</p>
<p>Reading raw data</p>
<p>Loading training data</p>
<p>Traceback (most recent call last):</p>
<p>åÊ File &#34;predict_unlabelled.py&#34;, line 65, in &lt;module&gt;</p>
<p>åÊ åÊ y, X, texts, dv, le = get_matricies(training_data)</p>
<p>åÊ File &#34;predict_unlabelled.py&#34;, line 31, in get_matricies</p>
<p>åÊ åÊ texts = [d[1] for d in data]</p>
<p>IndexError: list index out of range</p>
<p></p>
<p>Which is an error indicated in part of the code we were given. Does anyone know why this is?åÊ</p>",Error when running classifier,1
940847344,4/26/2016 17:50:21,false,1969470894,,4/26/2016 17:49:33,false,clixsense,1.0,35444326,BRA,07,Brasília,177.15.130.106,0,,"<p>Hi,<br /><br />I didn&#39;t receive an email with my grade for the classifier homework. (The only grades email I have received was that of Homework 1)<br /><br />My pennkey is erdardet<br /><br />Thanks for your help,<br />Eric Dardet</p>",No Grade Email Received for Classifier Homework,"<p>Hi everyone,åÊ</p>
<p></p>
<p>We&#39;ve had a chance to look through submissions for HW3 and while it&#39;ll be a few days before grades are ready to be sent out, we wanted to offer some feedback and clarification. Most of you had really high accuracies and many of you noted that accuracy was &#34;higher&#34; when you eliminated cross-validation because of overfitting (in the same way scores are higher when the midterm matches the practice test very closely).åÊ</p>
<p></p>
<p>X represented the full set of articles. y represented the labels. There were hundreds of thousands of unigram features, because features corresponded to each word that appeared (repeats did not contribute to # of features -- see &#39;that guy shot that other guy&#39; for an example).åÊ</p>
<p></p>
<p>This week, you are re-training your classifier on the full set of articles that we gave you last week (&#39;articles&#39; is the name of the text file) and then output predictions on the new URLs you collected.åÊYou&#39;ll want to modify the code for your statistical classifier to generate the labels for crowdworkers to check - the file on the assignment page is a great start towards that.åÊ</p>
<p></p>
<p>If you have any questions, please come to office hours sooner rather than later and post to Piazza if your question hasn&#39;t already been asked. Some parts of this assignment just take time to finish -- crawling, waiting for crowd worker judgements, the questionnaire -- and it&#39;s better to leave yourself time to debug.åÊ</p>",Using your classifier in HW4: Becoming a Requester,"<p>Hi all!</p>
<p></p>
<p>We have finished grading your classifier assignments, and you should get your grades shortly if you have not already (look for an email from Kate!). You guys did very well overall. It was not an easy assignment and there wereåÊa lot of new concepts that you had to take in (and new packages you had to install) all at once, so very well done.åÊ</p>
<p></p>
<p>There were twoåÊcommon points of confusion. Please feel free to ask questions here or in OHs if you are having trouble with these concepts, since they are important if you intend to do more ML in the future.</p>
<p></p>
<p><strong>What exactly areåÊX and y? What are their dimensions?åÊ</strong></p>
<p>In short, y is a vector of labels (one label per article). X is a matrix of features (one row per article, one column per feature). (You mightåÊpreferåÊto think of X as aåÊlist of vectors of features).åÊA key point is thatåÊ<span style=""text-decoration:underline"">every article has a value for every feature</span>-- so every article (row) has the same number of features (columns) associated with it. What changes areåÊthe values of those features (e.g. 0 or 1 in our assignment). Since the &#34;features&#34; in our caseåÊwere words, <span style=""text-decoration:underline"">the number of features (number of columns of X) is equal to the number of words in our vocabulary</span> (this was probably ~300,000-500,000 in our dataset, depending on your preprocessing). If theåÊword appeared in the article, theåÊvalue for theåÊword is 1 for that article, and if it did not appear, theåÊvalue is 0.åÊ</p>
<p></p>
<p>You are not alone if you find this concept a bit abstract. The biggest hurdle to overcome when learning about ML is getting comfortable thinking about your data as a feature matrix. Once you have internalized this notion, the rest of ML is reasonably straightforward. Please, ask us questions!</p>
<p></p>
<p><strong>False positives and false negativesåÊ</strong></p>
<p>Many of you were confused by the concept of false positives and false negatives. <span style=""text-decoration:underline"">A *false positive* in our case is an article that should have been labeled &#34;not gun related&#34; (0) but was falsely labeled &#34;gun related&#34; (1).</span> These were likely articles with words like &#34;shoot&#34; but which referred to sports or movies instead of violent crimes. A *false negative* isåÊan article that should have been labeled &#34;gun related&#34; (1) but was falsely labeled &#34;not gun related&#34; (0). Its good to think about these different types of errors when you are working on prediction tasks-- often different types of errors have different costs, or one type of error is worse than another.åÊåÊ(E.g. in medicine, tests that produce many false negativesåÊare usually considered worse that ones that produce false positives. I&#39;m sure you can think of some reasons why?)</p>
<p>åÊ</p>
<p><strong>But what now? My life has been so empty since turning in that classifier assignment...åÊ</strong></p>
<p>Many of you have expressed interest in learning more about machine learning, and you should absolutely consider enrolling in <a href=""https://alliance.seas.upenn.edu/~cis520/wiki/"" target=""_blank"">penn&#39;s ML course</a>åÊor <a href=""http://www.seas.upenn.edu/~cis519/fall2015/"" target=""_blank"">penn&#39;s intro ML course</a>. Or check out the <a href=""https://www.coursera.org/learn/machine-learning"" target=""_blank"">Coursera course</a>åÊ(crowdsourcing woot woot!). If you can&#39;t wait that long,åÊyou have all the freedom in the world to build an ML component into your final project, and we are more than happy to help you do so!</p>
<p></p>
<p>Hope everyone hasåÊa relaxing, happy weekend!åÊ</p>
<p></p>
<p>#pin</p>",Take-aways from you classifier assignments,<p>ShouldåÊwe have a single if-and-or statement with all of the keywords we use? Or should we have a series of if statements when checking keywords?åÊ</p>,rule-based classifier,<p>Resolved!</p>,Error when running classifier template,"<p>When I run the classifier, I get the error:åÊ</p>
<p>Reading raw data</p>
<p>Loading training data</p>
<p>Traceback (most recent call last):</p>
<p>åÊ File &#34;predict_unlabelled.py&#34;, line 65, in &lt;module&gt;</p>
<p>åÊ åÊ y, X, texts, dv, le = get_matricies(training_data)</p>
<p>åÊ File &#34;predict_unlabelled.py&#34;, line 31, in get_matricies</p>
<p>åÊ åÊ texts = [d[1] for d in data]</p>
<p>IndexError: list index out of range</p>
<p></p>
<p>Which is an error indicated in part of the code we were given. Does anyone know why this is?åÊ</p>",Error when running classifier,1
940847344,4/26/2016 18:20:19,false,1969486833,,4/26/2016 18:19:10,false,neodev,0.8889,35550011,VEN,07,Valencia,190.204.238.112,0,,"<p>Hi,<br /><br />I didn&#39;t receive an email with my grade for the classifier homework. (The only grades email I have received was that of Homework 1)<br /><br />My pennkey is erdardet<br /><br />Thanks for your help,<br />Eric Dardet</p>",No Grade Email Received for Classifier Homework,"<p>Hi everyone,åÊ</p>
<p></p>
<p>We&#39;ve had a chance to look through submissions for HW3 and while it&#39;ll be a few days before grades are ready to be sent out, we wanted to offer some feedback and clarification. Most of you had really high accuracies and many of you noted that accuracy was &#34;higher&#34; when you eliminated cross-validation because of overfitting (in the same way scores are higher when the midterm matches the practice test very closely).åÊ</p>
<p></p>
<p>X represented the full set of articles. y represented the labels. There were hundreds of thousands of unigram features, because features corresponded to each word that appeared (repeats did not contribute to # of features -- see &#39;that guy shot that other guy&#39; for an example).åÊ</p>
<p></p>
<p>This week, you are re-training your classifier on the full set of articles that we gave you last week (&#39;articles&#39; is the name of the text file) and then output predictions on the new URLs you collected.åÊYou&#39;ll want to modify the code for your statistical classifier to generate the labels for crowdworkers to check - the file on the assignment page is a great start towards that.åÊ</p>
<p></p>
<p>If you have any questions, please come to office hours sooner rather than later and post to Piazza if your question hasn&#39;t already been asked. Some parts of this assignment just take time to finish -- crawling, waiting for crowd worker judgements, the questionnaire -- and it&#39;s better to leave yourself time to debug.åÊ</p>",Using your classifier in HW4: Becoming a Requester,"<p>Hi all!</p>
<p></p>
<p>We have finished grading your classifier assignments, and you should get your grades shortly if you have not already (look for an email from Kate!). You guys did very well overall. It was not an easy assignment and there wereåÊa lot of new concepts that you had to take in (and new packages you had to install) all at once, so very well done.åÊ</p>
<p></p>
<p>There were twoåÊcommon points of confusion. Please feel free to ask questions here or in OHs if you are having trouble with these concepts, since they are important if you intend to do more ML in the future.</p>
<p></p>
<p><strong>What exactly areåÊX and y? What are their dimensions?åÊ</strong></p>
<p>In short, y is a vector of labels (one label per article). X is a matrix of features (one row per article, one column per feature). (You mightåÊpreferåÊto think of X as aåÊlist of vectors of features).åÊA key point is thatåÊ<span style=""text-decoration:underline"">every article has a value for every feature</span>-- so every article (row) has the same number of features (columns) associated with it. What changes areåÊthe values of those features (e.g. 0 or 1 in our assignment). Since the &#34;features&#34; in our caseåÊwere words, <span style=""text-decoration:underline"">the number of features (number of columns of X) is equal to the number of words in our vocabulary</span> (this was probably ~300,000-500,000 in our dataset, depending on your preprocessing). If theåÊword appeared in the article, theåÊvalue for theåÊword is 1 for that article, and if it did not appear, theåÊvalue is 0.åÊ</p>
<p></p>
<p>You are not alone if you find this concept a bit abstract. The biggest hurdle to overcome when learning about ML is getting comfortable thinking about your data as a feature matrix. Once you have internalized this notion, the rest of ML is reasonably straightforward. Please, ask us questions!</p>
<p></p>
<p><strong>False positives and false negativesåÊ</strong></p>
<p>Many of you were confused by the concept of false positives and false negatives. <span style=""text-decoration:underline"">A *false positive* in our case is an article that should have been labeled &#34;not gun related&#34; (0) but was falsely labeled &#34;gun related&#34; (1).</span> These were likely articles with words like &#34;shoot&#34; but which referred to sports or movies instead of violent crimes. A *false negative* isåÊan article that should have been labeled &#34;gun related&#34; (1) but was falsely labeled &#34;not gun related&#34; (0). Its good to think about these different types of errors when you are working on prediction tasks-- often different types of errors have different costs, or one type of error is worse than another.åÊåÊ(E.g. in medicine, tests that produce many false negativesåÊare usually considered worse that ones that produce false positives. I&#39;m sure you can think of some reasons why?)</p>
<p>åÊ</p>
<p><strong>But what now? My life has been so empty since turning in that classifier assignment...åÊ</strong></p>
<p>Many of you have expressed interest in learning more about machine learning, and you should absolutely consider enrolling in <a href=""https://alliance.seas.upenn.edu/~cis520/wiki/"" target=""_blank"">penn&#39;s ML course</a>åÊor <a href=""http://www.seas.upenn.edu/~cis519/fall2015/"" target=""_blank"">penn&#39;s intro ML course</a>. Or check out the <a href=""https://www.coursera.org/learn/machine-learning"" target=""_blank"">Coursera course</a>åÊ(crowdsourcing woot woot!). If you can&#39;t wait that long,åÊyou have all the freedom in the world to build an ML component into your final project, and we are more than happy to help you do so!</p>
<p></p>
<p>Hope everyone hasåÊa relaxing, happy weekend!åÊ</p>
<p></p>
<p>#pin</p>",Take-aways from you classifier assignments,<p>ShouldåÊwe have a single if-and-or statement with all of the keywords we use? Or should we have a series of if statements when checking keywords?åÊ</p>,rule-based classifier,<p>Resolved!</p>,Error when running classifier template,"<p>When I run the classifier, I get the error:åÊ</p>
<p>Reading raw data</p>
<p>Loading training data</p>
<p>Traceback (most recent call last):</p>
<p>åÊ File &#34;predict_unlabelled.py&#34;, line 65, in &lt;module&gt;</p>
<p>åÊ åÊ y, X, texts, dv, le = get_matricies(training_data)</p>
<p>åÊ File &#34;predict_unlabelled.py&#34;, line 31, in get_matricies</p>
<p>åÊ åÊ texts = [d[1] for d in data]</p>
<p>IndexError: list index out of range</p>
<p></p>
<p>Which is an error indicated in part of the code we were given. Does anyone know why this is?åÊ</p>",Error when running classifier,1
940847344,4/26/2016 18:24:14,false,1969488812,,4/26/2016 18:22:47,false,elite,1.0,30128662,BGR,50,Pleven,212.233.177.195,0,,"<p>Hi,<br /><br />I didn&#39;t receive an email with my grade for the classifier homework. (The only grades email I have received was that of Homework 1)<br /><br />My pennkey is erdardet<br /><br />Thanks for your help,<br />Eric Dardet</p>",No Grade Email Received for Classifier Homework,"<p>Hi everyone,åÊ</p>
<p></p>
<p>We&#39;ve had a chance to look through submissions for HW3 and while it&#39;ll be a few days before grades are ready to be sent out, we wanted to offer some feedback and clarification. Most of you had really high accuracies and many of you noted that accuracy was &#34;higher&#34; when you eliminated cross-validation because of overfitting (in the same way scores are higher when the midterm matches the practice test very closely).åÊ</p>
<p></p>
<p>X represented the full set of articles. y represented the labels. There were hundreds of thousands of unigram features, because features corresponded to each word that appeared (repeats did not contribute to # of features -- see &#39;that guy shot that other guy&#39; for an example).åÊ</p>
<p></p>
<p>This week, you are re-training your classifier on the full set of articles that we gave you last week (&#39;articles&#39; is the name of the text file) and then output predictions on the new URLs you collected.åÊYou&#39;ll want to modify the code for your statistical classifier to generate the labels for crowdworkers to check - the file on the assignment page is a great start towards that.åÊ</p>
<p></p>
<p>If you have any questions, please come to office hours sooner rather than later and post to Piazza if your question hasn&#39;t already been asked. Some parts of this assignment just take time to finish -- crawling, waiting for crowd worker judgements, the questionnaire -- and it&#39;s better to leave yourself time to debug.åÊ</p>",Using your classifier in HW4: Becoming a Requester,"<p>Hi all!</p>
<p></p>
<p>We have finished grading your classifier assignments, and you should get your grades shortly if you have not already (look for an email from Kate!). You guys did very well overall. It was not an easy assignment and there wereåÊa lot of new concepts that you had to take in (and new packages you had to install) all at once, so very well done.åÊ</p>
<p></p>
<p>There were twoåÊcommon points of confusion. Please feel free to ask questions here or in OHs if you are having trouble with these concepts, since they are important if you intend to do more ML in the future.</p>
<p></p>
<p><strong>What exactly areåÊX and y? What are their dimensions?åÊ</strong></p>
<p>In short, y is a vector of labels (one label per article). X is a matrix of features (one row per article, one column per feature). (You mightåÊpreferåÊto think of X as aåÊlist of vectors of features).åÊA key point is thatåÊ<span style=""text-decoration:underline"">every article has a value for every feature</span>-- so every article (row) has the same number of features (columns) associated with it. What changes areåÊthe values of those features (e.g. 0 or 1 in our assignment). Since the &#34;features&#34; in our caseåÊwere words, <span style=""text-decoration:underline"">the number of features (number of columns of X) is equal to the number of words in our vocabulary</span> (this was probably ~300,000-500,000 in our dataset, depending on your preprocessing). If theåÊword appeared in the article, theåÊvalue for theåÊword is 1 for that article, and if it did not appear, theåÊvalue is 0.åÊ</p>
<p></p>
<p>You are not alone if you find this concept a bit abstract. The biggest hurdle to overcome when learning about ML is getting comfortable thinking about your data as a feature matrix. Once you have internalized this notion, the rest of ML is reasonably straightforward. Please, ask us questions!</p>
<p></p>
<p><strong>False positives and false negativesåÊ</strong></p>
<p>Many of you were confused by the concept of false positives and false negatives. <span style=""text-decoration:underline"">A *false positive* in our case is an article that should have been labeled &#34;not gun related&#34; (0) but was falsely labeled &#34;gun related&#34; (1).</span> These were likely articles with words like &#34;shoot&#34; but which referred to sports or movies instead of violent crimes. A *false negative* isåÊan article that should have been labeled &#34;gun related&#34; (1) but was falsely labeled &#34;not gun related&#34; (0). Its good to think about these different types of errors when you are working on prediction tasks-- often different types of errors have different costs, or one type of error is worse than another.åÊåÊ(E.g. in medicine, tests that produce many false negativesåÊare usually considered worse that ones that produce false positives. I&#39;m sure you can think of some reasons why?)</p>
<p>åÊ</p>
<p><strong>But what now? My life has been so empty since turning in that classifier assignment...åÊ</strong></p>
<p>Many of you have expressed interest in learning more about machine learning, and you should absolutely consider enrolling in <a href=""https://alliance.seas.upenn.edu/~cis520/wiki/"" target=""_blank"">penn&#39;s ML course</a>åÊor <a href=""http://www.seas.upenn.edu/~cis519/fall2015/"" target=""_blank"">penn&#39;s intro ML course</a>. Or check out the <a href=""https://www.coursera.org/learn/machine-learning"" target=""_blank"">Coursera course</a>åÊ(crowdsourcing woot woot!). If you can&#39;t wait that long,åÊyou have all the freedom in the world to build an ML component into your final project, and we are more than happy to help you do so!</p>
<p></p>
<p>Hope everyone hasåÊa relaxing, happy weekend!åÊ</p>
<p></p>
<p>#pin</p>",Take-aways from you classifier assignments,<p>ShouldåÊwe have a single if-and-or statement with all of the keywords we use? Or should we have a series of if statements when checking keywords?åÊ</p>,rule-based classifier,<p>Resolved!</p>,Error when running classifier template,"<p>When I run the classifier, I get the error:åÊ</p>
<p>Reading raw data</p>
<p>Loading training data</p>
<p>Traceback (most recent call last):</p>
<p>åÊ File &#34;predict_unlabelled.py&#34;, line 65, in &lt;module&gt;</p>
<p>åÊ åÊ y, X, texts, dv, le = get_matricies(training_data)</p>
<p>åÊ File &#34;predict_unlabelled.py&#34;, line 31, in get_matricies</p>
<p>åÊ åÊ texts = [d[1] for d in data]</p>
<p>IndexError: list index out of range</p>
<p></p>
<p>Which is an error indicated in part of the code we were given. Does anyone know why this is?åÊ</p>",Error when running classifier,1
940847344,4/26/2016 18:45:01,false,1969499309,,4/26/2016 18:41:05,false,neodev,1.0,35974955,VEN,17,Porlamar,190.198.232.239,0,,"<p>Hi,<br /><br />I didn&#39;t receive an email with my grade for the classifier homework. (The only grades email I have received was that of Homework 1)<br /><br />My pennkey is erdardet<br /><br />Thanks for your help,<br />Eric Dardet</p>",No Grade Email Received for Classifier Homework,"<p>Hi everyone,åÊ</p>
<p></p>
<p>We&#39;ve had a chance to look through submissions for HW3 and while it&#39;ll be a few days before grades are ready to be sent out, we wanted to offer some feedback and clarification. Most of you had really high accuracies and many of you noted that accuracy was &#34;higher&#34; when you eliminated cross-validation because of overfitting (in the same way scores are higher when the midterm matches the practice test very closely).åÊ</p>
<p></p>
<p>X represented the full set of articles. y represented the labels. There were hundreds of thousands of unigram features, because features corresponded to each word that appeared (repeats did not contribute to # of features -- see &#39;that guy shot that other guy&#39; for an example).åÊ</p>
<p></p>
<p>This week, you are re-training your classifier on the full set of articles that we gave you last week (&#39;articles&#39; is the name of the text file) and then output predictions on the new URLs you collected.åÊYou&#39;ll want to modify the code for your statistical classifier to generate the labels for crowdworkers to check - the file on the assignment page is a great start towards that.åÊ</p>
<p></p>
<p>If you have any questions, please come to office hours sooner rather than later and post to Piazza if your question hasn&#39;t already been asked. Some parts of this assignment just take time to finish -- crawling, waiting for crowd worker judgements, the questionnaire -- and it&#39;s better to leave yourself time to debug.åÊ</p>",Using your classifier in HW4: Becoming a Requester,"<p>Hi all!</p>
<p></p>
<p>We have finished grading your classifier assignments, and you should get your grades shortly if you have not already (look for an email from Kate!). You guys did very well overall. It was not an easy assignment and there wereåÊa lot of new concepts that you had to take in (and new packages you had to install) all at once, so very well done.åÊ</p>
<p></p>
<p>There were twoåÊcommon points of confusion. Please feel free to ask questions here or in OHs if you are having trouble with these concepts, since they are important if you intend to do more ML in the future.</p>
<p></p>
<p><strong>What exactly areåÊX and y? What are their dimensions?åÊ</strong></p>
<p>In short, y is a vector of labels (one label per article). X is a matrix of features (one row per article, one column per feature). (You mightåÊpreferåÊto think of X as aåÊlist of vectors of features).åÊA key point is thatåÊ<span style=""text-decoration:underline"">every article has a value for every feature</span>-- so every article (row) has the same number of features (columns) associated with it. What changes areåÊthe values of those features (e.g. 0 or 1 in our assignment). Since the &#34;features&#34; in our caseåÊwere words, <span style=""text-decoration:underline"">the number of features (number of columns of X) is equal to the number of words in our vocabulary</span> (this was probably ~300,000-500,000 in our dataset, depending on your preprocessing). If theåÊword appeared in the article, theåÊvalue for theåÊword is 1 for that article, and if it did not appear, theåÊvalue is 0.åÊ</p>
<p></p>
<p>You are not alone if you find this concept a bit abstract. The biggest hurdle to overcome when learning about ML is getting comfortable thinking about your data as a feature matrix. Once you have internalized this notion, the rest of ML is reasonably straightforward. Please, ask us questions!</p>
<p></p>
<p><strong>False positives and false negativesåÊ</strong></p>
<p>Many of you were confused by the concept of false positives and false negatives. <span style=""text-decoration:underline"">A *false positive* in our case is an article that should have been labeled &#34;not gun related&#34; (0) but was falsely labeled &#34;gun related&#34; (1).</span> These were likely articles with words like &#34;shoot&#34; but which referred to sports or movies instead of violent crimes. A *false negative* isåÊan article that should have been labeled &#34;gun related&#34; (1) but was falsely labeled &#34;not gun related&#34; (0). Its good to think about these different types of errors when you are working on prediction tasks-- often different types of errors have different costs, or one type of error is worse than another.åÊåÊ(E.g. in medicine, tests that produce many false negativesåÊare usually considered worse that ones that produce false positives. I&#39;m sure you can think of some reasons why?)</p>
<p>åÊ</p>
<p><strong>But what now? My life has been so empty since turning in that classifier assignment...åÊ</strong></p>
<p>Many of you have expressed interest in learning more about machine learning, and you should absolutely consider enrolling in <a href=""https://alliance.seas.upenn.edu/~cis520/wiki/"" target=""_blank"">penn&#39;s ML course</a>åÊor <a href=""http://www.seas.upenn.edu/~cis519/fall2015/"" target=""_blank"">penn&#39;s intro ML course</a>. Or check out the <a href=""https://www.coursera.org/learn/machine-learning"" target=""_blank"">Coursera course</a>åÊ(crowdsourcing woot woot!). If you can&#39;t wait that long,åÊyou have all the freedom in the world to build an ML component into your final project, and we are more than happy to help you do so!</p>
<p></p>
<p>Hope everyone hasåÊa relaxing, happy weekend!åÊ</p>
<p></p>
<p>#pin</p>",Take-aways from you classifier assignments,<p>ShouldåÊwe have a single if-and-or statement with all of the keywords we use? Or should we have a series of if statements when checking keywords?åÊ</p>,rule-based classifier,<p>Resolved!</p>,Error when running classifier template,"<p>When I run the classifier, I get the error:åÊ</p>
<p>Reading raw data</p>
<p>Loading training data</p>
<p>Traceback (most recent call last):</p>
<p>åÊ File &#34;predict_unlabelled.py&#34;, line 65, in &lt;module&gt;</p>
<p>åÊ åÊ y, X, texts, dv, le = get_matricies(training_data)</p>
<p>åÊ File &#34;predict_unlabelled.py&#34;, line 31, in get_matricies</p>
<p>åÊ åÊ texts = [d[1] for d in data]</p>
<p>IndexError: list index out of range</p>
<p></p>
<p>Which is an error indicated in part of the code we were given. Does anyone know why this is?åÊ</p>",Error when running classifier,1
940847345,4/26/2016 15:27:29,false,1969389352,,4/26/2016 15:26:56,false,instagc,0.8889,13581319,USA,IL,Waltonville,208.70.36.12,0,,"<p>Hi everyone,</p>
<p></p>
<p>I&#39;m looking for a partner toåÊcollaborate with on upcoming assignments and/or the final project. E-mail me at <a href=""mailto:hcutler&#64;seas.upenn.edu"">hcutler&#64;seas.upenn.edu</a>åÊif you&#39;re interested!</p>
<p></p>
<p>Have a great day.</p>
<p></p>
<p>Hannah Cutler</p>
<p>SEAS &#39;17</p>",Looking for a partner for homeworks and/or final project?,"<p>FYI, the second part of the final project was released last night:åÊ<a href=""http://crowdsourcing-class.org/final-project-part2.html"">http://crowdsourcing-class.org/final-project-part2.html</a></p>
<p></p>
<p>There are two deliverables. åÊThe first one is due on Saturday before 11:59pm.</p>",Final Project Part 2 has been released,<p>This is the link:åÊhttps://github.com/benpleitner/NETS213-Final-Project</p>,Group Project,"<p>For the NETS 213 final project, do we as students retain ownership of our projects? I know that some schools will hold the ownership rights, and juståÊwanted some clarification on this policy for the NETS 213 course.</p>",Student Ownership of Final Project?,"<p>Here is some updated details about the $10,000 prize for the best final project:</p>
<p><a href=""http://crowdsourcing-class.org/project.html"">http://crowdsourcing-class.org/project.html</a></p>
<p></p>
<p>Let me know what you think!</p>","$10,000 prize for the final project","<p>Are we allowed to modify one of the past final project ideas shown to us in class? We have an interesting ideaåÊsimilar to PictureThis and wanted to make sure it&#39;s OK that it&#39;s similar.</p>
<p></p>
<p>Thanks!</p>",Final project idea similar to PictureThis,4
940847345,4/26/2016 15:28:58,false,1969390417,,4/26/2016 15:27:17,false,elite,1.0,30280423,ITA,15,Siracusa,151.54.84.121,0,,"<p>Hi everyone,</p>
<p></p>
<p>I&#39;m looking for a partner toåÊcollaborate with on upcoming assignments and/or the final project. E-mail me at <a href=""mailto:hcutler&#64;seas.upenn.edu"">hcutler&#64;seas.upenn.edu</a>åÊif you&#39;re interested!</p>
<p></p>
<p>Have a great day.</p>
<p></p>
<p>Hannah Cutler</p>
<p>SEAS &#39;17</p>",Looking for a partner for homeworks and/or final project?,"<p>FYI, the second part of the final project was released last night:åÊ<a href=""http://crowdsourcing-class.org/final-project-part2.html"">http://crowdsourcing-class.org/final-project-part2.html</a></p>
<p></p>
<p>There are two deliverables. åÊThe first one is due on Saturday before 11:59pm.</p>",Final Project Part 2 has been released,<p>This is the link:åÊhttps://github.com/benpleitner/NETS213-Final-Project</p>,Group Project,"<p>For the NETS 213 final project, do we as students retain ownership of our projects? I know that some schools will hold the ownership rights, and juståÊwanted some clarification on this policy for the NETS 213 course.</p>",Student Ownership of Final Project?,"<p>Here is some updated details about the $10,000 prize for the best final project:</p>
<p><a href=""http://crowdsourcing-class.org/project.html"">http://crowdsourcing-class.org/project.html</a></p>
<p></p>
<p>Let me know what you think!</p>","$10,000 prize for the final project","<p>Are we allowed to modify one of the past final project ideas shown to us in class? We have an interesting ideaåÊsimilar to PictureThis and wanted to make sure it&#39;s OK that it&#39;s similar.</p>
<p></p>
<p>Thanks!</p>",Final project idea similar to PictureThis,4
940847345,4/26/2016 15:30:05,false,1969390867,,4/26/2016 15:26:19,false,clixsense,0.8889,36052512,PHL,F2,Quezon City,49.149.150.150,0,,"<p>Hi everyone,</p>
<p></p>
<p>I&#39;m looking for a partner toåÊcollaborate with on upcoming assignments and/or the final project. E-mail me at <a href=""mailto:hcutler&#64;seas.upenn.edu"">hcutler&#64;seas.upenn.edu</a>åÊif you&#39;re interested!</p>
<p></p>
<p>Have a great day.</p>
<p></p>
<p>Hannah Cutler</p>
<p>SEAS &#39;17</p>",Looking for a partner for homeworks and/or final project?,"<p>FYI, the second part of the final project was released last night:åÊ<a href=""http://crowdsourcing-class.org/final-project-part2.html"">http://crowdsourcing-class.org/final-project-part2.html</a></p>
<p></p>
<p>There are two deliverables. åÊThe first one is due on Saturday before 11:59pm.</p>",Final Project Part 2 has been released,<p>This is the link:åÊhttps://github.com/benpleitner/NETS213-Final-Project</p>,Group Project,"<p>For the NETS 213 final project, do we as students retain ownership of our projects? I know that some schools will hold the ownership rights, and juståÊwanted some clarification on this policy for the NETS 213 course.</p>",Student Ownership of Final Project?,"<p>Here is some updated details about the $10,000 prize for the best final project:</p>
<p><a href=""http://crowdsourcing-class.org/project.html"">http://crowdsourcing-class.org/project.html</a></p>
<p></p>
<p>Let me know what you think!</p>","$10,000 prize for the final project","<p>Are we allowed to modify one of the past final project ideas shown to us in class? We have an interesting ideaåÊsimilar to PictureThis and wanted to make sure it&#39;s OK that it&#39;s similar.</p>
<p></p>
<p>Thanks!</p>",Final project idea similar to PictureThis,4
940847345,4/26/2016 15:51:22,false,1969399817,,4/26/2016 15:36:03,false,neodev,1.0,13396426,VEN,15,Santa Teresa,190.38.163.149,0,,"<p>Hi everyone,</p>
<p></p>
<p>I&#39;m looking for a partner toåÊcollaborate with on upcoming assignments and/or the final project. E-mail me at <a href=""mailto:hcutler&#64;seas.upenn.edu"">hcutler&#64;seas.upenn.edu</a>åÊif you&#39;re interested!</p>
<p></p>
<p>Have a great day.</p>
<p></p>
<p>Hannah Cutler</p>
<p>SEAS &#39;17</p>",Looking for a partner for homeworks and/or final project?,"<p>FYI, the second part of the final project was released last night:åÊ<a href=""http://crowdsourcing-class.org/final-project-part2.html"">http://crowdsourcing-class.org/final-project-part2.html</a></p>
<p></p>
<p>There are two deliverables. åÊThe first one is due on Saturday before 11:59pm.</p>",Final Project Part 2 has been released,<p>This is the link:åÊhttps://github.com/benpleitner/NETS213-Final-Project</p>,Group Project,"<p>For the NETS 213 final project, do we as students retain ownership of our projects? I know that some schools will hold the ownership rights, and juståÊwanted some clarification on this policy for the NETS 213 course.</p>",Student Ownership of Final Project?,"<p>Here is some updated details about the $10,000 prize for the best final project:</p>
<p><a href=""http://crowdsourcing-class.org/project.html"">http://crowdsourcing-class.org/project.html</a></p>
<p></p>
<p>Let me know what you think!</p>","$10,000 prize for the final project","<p>Are we allowed to modify one of the past final project ideas shown to us in class? We have an interesting ideaåÊsimilar to PictureThis and wanted to make sure it&#39;s OK that it&#39;s similar.</p>
<p></p>
<p>Thanks!</p>",Final project idea similar to PictureThis,4
940847345,4/26/2016 15:58:24,false,1969402710,,4/26/2016 15:56:48,false,elite,1.0,33243069,IND,10,Faridabad,116.203.79.150,0,,"<p>Hi everyone,</p>
<p></p>
<p>I&#39;m looking for a partner toåÊcollaborate with on upcoming assignments and/or the final project. E-mail me at <a href=""mailto:hcutler&#64;seas.upenn.edu"">hcutler&#64;seas.upenn.edu</a>åÊif you&#39;re interested!</p>
<p></p>
<p>Have a great day.</p>
<p></p>
<p>Hannah Cutler</p>
<p>SEAS &#39;17</p>",Looking for a partner for homeworks and/or final project?,"<p>FYI, the second part of the final project was released last night:åÊ<a href=""http://crowdsourcing-class.org/final-project-part2.html"">http://crowdsourcing-class.org/final-project-part2.html</a></p>
<p></p>
<p>There are two deliverables. åÊThe first one is due on Saturday before 11:59pm.</p>",Final Project Part 2 has been released,<p>This is the link:åÊhttps://github.com/benpleitner/NETS213-Final-Project</p>,Group Project,"<p>For the NETS 213 final project, do we as students retain ownership of our projects? I know that some schools will hold the ownership rights, and juståÊwanted some clarification on this policy for the NETS 213 course.</p>",Student Ownership of Final Project?,"<p>Here is some updated details about the $10,000 prize for the best final project:</p>
<p><a href=""http://crowdsourcing-class.org/project.html"">http://crowdsourcing-class.org/project.html</a></p>
<p></p>
<p>Let me know what you think!</p>","$10,000 prize for the final project","<p>Are we allowed to modify one of the past final project ideas shown to us in class? We have an interesting ideaåÊsimilar to PictureThis and wanted to make sure it&#39;s OK that it&#39;s similar.</p>
<p></p>
<p>Thanks!</p>",Final project idea similar to PictureThis,4
940847346,4/26/2016 16:02:51,false,1969405077,,4/26/2016 16:02:22,false,personaly,1.0,33663352,ARG,01,Mar Del Plata,181.168.213.227,0,,"<p>Hi everyone,</p>
<p></p>
<p>I&#39;m looking for a partner toåÊcollaborate with on upcoming assignments and/or the final project. E-mail me at <a href=""mailto:hcutler&#64;seas.upenn.edu"">hcutler&#64;seas.upenn.edu</a>åÊif you&#39;re interested!</p>
<p></p>
<p>Have a great day.</p>
<p></p>
<p>Hannah Cutler</p>
<p>SEAS &#39;17</p>",Looking for a partner for homeworks and/or final project?,"<p>Are we allowed to modify one of the past final project ideas shown to us in class? We have an interesting ideaåÊsimilar to PictureThis and wanted to make sure it&#39;s OK that it&#39;s similar.</p>
<p></p>
<p>Thanks!</p>",Final project idea similar to PictureThis,"<p>FYI, the second part of the final project was released last night:åÊ<a href=""http://crowdsourcing-class.org/final-project-part2.html"">http://crowdsourcing-class.org/final-project-part2.html</a></p>
<p></p>
<p>There are two deliverables. åÊThe first one is due on Saturday before 11:59pm.</p>",Final Project Part 2 has been released,<p>This is the link:åÊhttps://github.com/benpleitner/NETS213-Final-Project</p>,Group Project,"<p>For the NETS 213 final project, do we as students retain ownership of our projects? I know that some schools will hold the ownership rights, and juståÊwanted some clarification on this policy for the NETS 213 course.</p>",Student Ownership of Final Project?,When should we expect feedback for our project ideas?<div>It would really help picking an idea and begin implementing it soon.åÊ</div>,Final Project Part 1 feedback,4
940847346,4/26/2016 16:18:19,false,1969413579,,4/26/2016 16:15:37,false,elite,1.0,30128662,BGR,50,Pleven,212.233.177.195,0,,"<p>Hi everyone,</p>
<p></p>
<p>I&#39;m looking for a partner toåÊcollaborate with on upcoming assignments and/or the final project. E-mail me at <a href=""mailto:hcutler&#64;seas.upenn.edu"">hcutler&#64;seas.upenn.edu</a>åÊif you&#39;re interested!</p>
<p></p>
<p>Have a great day.</p>
<p></p>
<p>Hannah Cutler</p>
<p>SEAS &#39;17</p>",Looking for a partner for homeworks and/or final project?,"<p>Are we allowed to modify one of the past final project ideas shown to us in class? We have an interesting ideaåÊsimilar to PictureThis and wanted to make sure it&#39;s OK that it&#39;s similar.</p>
<p></p>
<p>Thanks!</p>",Final project idea similar to PictureThis,"<p>FYI, the second part of the final project was released last night:åÊ<a href=""http://crowdsourcing-class.org/final-project-part2.html"">http://crowdsourcing-class.org/final-project-part2.html</a></p>
<p></p>
<p>There are two deliverables. åÊThe first one is due on Saturday before 11:59pm.</p>",Final Project Part 2 has been released,<p>This is the link:åÊhttps://github.com/benpleitner/NETS213-Final-Project</p>,Group Project,"<p>For the NETS 213 final project, do we as students retain ownership of our projects? I know that some schools will hold the ownership rights, and juståÊwanted some clarification on this policy for the NETS 213 course.</p>",Student Ownership of Final Project?,When should we expect feedback for our project ideas?<div>It would really help picking an idea and begin implementing it soon.åÊ</div>,Final Project Part 1 feedback,4
940847346,4/26/2016 16:23:45,false,1969418306,,4/26/2016 16:22:19,false,neodev,1.0,29175140,VEN,25,Caracas,190.72.125.134,0,,"<p>Hi everyone,</p>
<p></p>
<p>I&#39;m looking for a partner toåÊcollaborate with on upcoming assignments and/or the final project. E-mail me at <a href=""mailto:hcutler&#64;seas.upenn.edu"">hcutler&#64;seas.upenn.edu</a>åÊif you&#39;re interested!</p>
<p></p>
<p>Have a great day.</p>
<p></p>
<p>Hannah Cutler</p>
<p>SEAS &#39;17</p>",Looking for a partner for homeworks and/or final project?,"<p>Are we allowed to modify one of the past final project ideas shown to us in class? We have an interesting ideaåÊsimilar to PictureThis and wanted to make sure it&#39;s OK that it&#39;s similar.</p>
<p></p>
<p>Thanks!</p>",Final project idea similar to PictureThis,"<p>FYI, the second part of the final project was released last night:åÊ<a href=""http://crowdsourcing-class.org/final-project-part2.html"">http://crowdsourcing-class.org/final-project-part2.html</a></p>
<p></p>
<p>There are two deliverables. åÊThe first one is due on Saturday before 11:59pm.</p>",Final Project Part 2 has been released,<p>This is the link:åÊhttps://github.com/benpleitner/NETS213-Final-Project</p>,Group Project,"<p>For the NETS 213 final project, do we as students retain ownership of our projects? I know that some schools will hold the ownership rights, and juståÊwanted some clarification on this policy for the NETS 213 course.</p>",Student Ownership of Final Project?,When should we expect feedback for our project ideas?<div>It would really help picking an idea and begin implementing it soon.åÊ</div>,Final Project Part 1 feedback,4
940847346,4/26/2016 16:31:08,false,1969424241,,4/26/2016 16:19:53,false,neodev,0.7778,32569659,USA,MN,Minneapolis,97.127.88.224,0,,"<p>Hi everyone,</p>
<p></p>
<p>I&#39;m looking for a partner toåÊcollaborate with on upcoming assignments and/or the final project. E-mail me at <a href=""mailto:hcutler&#64;seas.upenn.edu"">hcutler&#64;seas.upenn.edu</a>åÊif you&#39;re interested!</p>
<p></p>
<p>Have a great day.</p>
<p></p>
<p>Hannah Cutler</p>
<p>SEAS &#39;17</p>",Looking for a partner for homeworks and/or final project?,"<p>Are we allowed to modify one of the past final project ideas shown to us in class? We have an interesting ideaåÊsimilar to PictureThis and wanted to make sure it&#39;s OK that it&#39;s similar.</p>
<p></p>
<p>Thanks!</p>",Final project idea similar to PictureThis,"<p>FYI, the second part of the final project was released last night:åÊ<a href=""http://crowdsourcing-class.org/final-project-part2.html"">http://crowdsourcing-class.org/final-project-part2.html</a></p>
<p></p>
<p>There are two deliverables. åÊThe first one is due on Saturday before 11:59pm.</p>",Final Project Part 2 has been released,<p>This is the link:åÊhttps://github.com/benpleitner/NETS213-Final-Project</p>,Group Project,"<p>For the NETS 213 final project, do we as students retain ownership of our projects? I know that some schools will hold the ownership rights, and juståÊwanted some clarification on this policy for the NETS 213 course.</p>",Student Ownership of Final Project?,When should we expect feedback for our project ideas?<div>It would really help picking an idea and begin implementing it soon.åÊ</div>,Final Project Part 1 feedback,4
940847346,4/26/2016 16:32:36,false,1969425036,,4/26/2016 16:13:00,false,clixsense,0.8889,8057247,PRT,17,Póvoa De Varzim,144.64.25.68,0,,"<p>Hi everyone,</p>
<p></p>
<p>I&#39;m looking for a partner toåÊcollaborate with on upcoming assignments and/or the final project. E-mail me at <a href=""mailto:hcutler&#64;seas.upenn.edu"">hcutler&#64;seas.upenn.edu</a>åÊif you&#39;re interested!</p>
<p></p>
<p>Have a great day.</p>
<p></p>
<p>Hannah Cutler</p>
<p>SEAS &#39;17</p>",Looking for a partner for homeworks and/or final project?,"<p>Are we allowed to modify one of the past final project ideas shown to us in class? We have an interesting ideaåÊsimilar to PictureThis and wanted to make sure it&#39;s OK that it&#39;s similar.</p>
<p></p>
<p>Thanks!</p>",Final project idea similar to PictureThis,"<p>FYI, the second part of the final project was released last night:åÊ<a href=""http://crowdsourcing-class.org/final-project-part2.html"">http://crowdsourcing-class.org/final-project-part2.html</a></p>
<p></p>
<p>There are two deliverables. åÊThe first one is due on Saturday before 11:59pm.</p>",Final Project Part 2 has been released,<p>This is the link:åÊhttps://github.com/benpleitner/NETS213-Final-Project</p>,Group Project,"<p>For the NETS 213 final project, do we as students retain ownership of our projects? I know that some schools will hold the ownership rights, and juståÊwanted some clarification on this policy for the NETS 213 course.</p>",Student Ownership of Final Project?,When should we expect feedback for our project ideas?<div>It would really help picking an idea and begin implementing it soon.åÊ</div>,Final Project Part 1 feedback,4
940847347,4/26/2016 15:54:11,false,1969401059,,4/26/2016 15:52:34,false,elite,0.8889,36575101,IND,07,New Delhi,112.196.144.2,0,,<p></p>,Confirming: separate late day policy for the final group project?,"<p>Hello, I&#39;m a little confused as to how late days work for groups. Is it that if the group wants to use a late day, all x members of the group must use their late day or is it that only one person of the group uses one late day. This question is only geared for regular homework assignments. I understand that there is a separate late day policy for the final group project. Thanks so much for the help!</p>",Late days for groups,"<p>This is a friendly reminder that <a href=""http://crowdsourcing-class.org/assignment1.html"" target=""_blank"">homework 1</a> is due tomorrow before class. åÊ</p>
<p></p>
<p>I would also like to draw your attention to the course&#39;s late day policy (given on the <a href=""http://crowdsourcing-class.org"" target=""_blank"">course web page</a>).</p>
<blockquote>
<p dir=""ltr"">Everyone can have 5 free late days without penalty. After you have used your free late days, you will lose 20% per day (or fraction thereof) that your assignment is submitted late. The final project will have its own late day policy.</p>
</blockquote>
<p>You do not need to ask permission to use a late day. åÊNote that I have a strict policy of not granting additional late days foråÊanyåÊreason, because I do not want to adjudicate what constitutes a fair excuse. åÊSo if you exhaust your 5 free late days early, please do not ask for more.</p>",Reminder: HW1 is due tomorrow by 2pm (and late day policy),<p>I just submitted my assignment for HW2. Does this count as two or three late days?</p>,Late days,"<p>If we submit something late, do we have to specify we are using a late day? Or is it automatically detected? Also, if we submit something after the class but the same day that it is due, is it still a late day?</p>",Late Day,<p>What if we need to use a late day for Friday?</p>,How do late days work for the final project?,4
940847347,4/26/2016 15:56:34,false,1969401962,,4/26/2016 15:53:47,false,neodev,1.0,28875937,PAK,08,Islamabad,119.153.105.50,0,,<p></p>,Confirming: separate late day policy for the final group project?,"<p>Hello, I&#39;m a little confused as to how late days work for groups. Is it that if the group wants to use a late day, all x members of the group must use their late day or is it that only one person of the group uses one late day. This question is only geared for regular homework assignments. I understand that there is a separate late day policy for the final group project. Thanks so much for the help!</p>",Late days for groups,"<p>This is a friendly reminder that <a href=""http://crowdsourcing-class.org/assignment1.html"" target=""_blank"">homework 1</a> is due tomorrow before class. åÊ</p>
<p></p>
<p>I would also like to draw your attention to the course&#39;s late day policy (given on the <a href=""http://crowdsourcing-class.org"" target=""_blank"">course web page</a>).</p>
<blockquote>
<p dir=""ltr"">Everyone can have 5 free late days without penalty. After you have used your free late days, you will lose 20% per day (or fraction thereof) that your assignment is submitted late. The final project will have its own late day policy.</p>
</blockquote>
<p>You do not need to ask permission to use a late day. åÊNote that I have a strict policy of not granting additional late days foråÊanyåÊreason, because I do not want to adjudicate what constitutes a fair excuse. åÊSo if you exhaust your 5 free late days early, please do not ask for more.</p>",Reminder: HW1 is due tomorrow by 2pm (and late day policy),<p>I just submitted my assignment for HW2. Does this count as two or three late days?</p>,Late days,"<p>If we submit something late, do we have to specify we are using a late day? Or is it automatically detected? Also, if we submit something after the class but the same day that it is due, is it still a late day?</p>",Late Day,<p>What if we need to use a late day for Friday?</p>,How do late days work for the final project?,4
940847347,4/26/2016 15:58:11,false,1969402625,,4/26/2016 15:56:24,false,neodev,1.0,13396426,VEN,15,Santa Teresa,190.38.163.149,5,,<p></p>,Confirming: separate late day policy for the final group project?,"<p>Hello, I&#39;m a little confused as to how late days work for groups. Is it that if the group wants to use a late day, all x members of the group must use their late day or is it that only one person of the group uses one late day. This question is only geared for regular homework assignments. I understand that there is a separate late day policy for the final group project. Thanks so much for the help!</p>",Late days for groups,"<p>This is a friendly reminder that <a href=""http://crowdsourcing-class.org/assignment1.html"" target=""_blank"">homework 1</a> is due tomorrow before class. åÊ</p>
<p></p>
<p>I would also like to draw your attention to the course&#39;s late day policy (given on the <a href=""http://crowdsourcing-class.org"" target=""_blank"">course web page</a>).</p>
<blockquote>
<p dir=""ltr"">Everyone can have 5 free late days without penalty. After you have used your free late days, you will lose 20% per day (or fraction thereof) that your assignment is submitted late. The final project will have its own late day policy.</p>
</blockquote>
<p>You do not need to ask permission to use a late day. åÊNote that I have a strict policy of not granting additional late days foråÊanyåÊreason, because I do not want to adjudicate what constitutes a fair excuse. åÊSo if you exhaust your 5 free late days early, please do not ask for more.</p>",Reminder: HW1 is due tomorrow by 2pm (and late day policy),<p>I just submitted my assignment for HW2. Does this count as two or three late days?</p>,Late days,"<p>If we submit something late, do we have to specify we are using a late day? Or is it automatically detected? Also, if we submit something after the class but the same day that it is due, is it still a late day?</p>",Late Day,<p>What if we need to use a late day for Friday?</p>,How do late days work for the final project?,4
940847347,4/26/2016 16:04:22,false,1969405944,,4/26/2016 15:56:16,false,neodev,0.8889,21971187,TTO,08,Valsayn,190.213.132.190,0,,<p></p>,Confirming: separate late day policy for the final group project?,"<p>Hello, I&#39;m a little confused as to how late days work for groups. Is it that if the group wants to use a late day, all x members of the group must use their late day or is it that only one person of the group uses one late day. This question is only geared for regular homework assignments. I understand that there is a separate late day policy for the final group project. Thanks so much for the help!</p>",Late days for groups,"<p>This is a friendly reminder that <a href=""http://crowdsourcing-class.org/assignment1.html"" target=""_blank"">homework 1</a> is due tomorrow before class. åÊ</p>
<p></p>
<p>I would also like to draw your attention to the course&#39;s late day policy (given on the <a href=""http://crowdsourcing-class.org"" target=""_blank"">course web page</a>).</p>
<blockquote>
<p dir=""ltr"">Everyone can have 5 free late days without penalty. After you have used your free late days, you will lose 20% per day (or fraction thereof) that your assignment is submitted late. The final project will have its own late day policy.</p>
</blockquote>
<p>You do not need to ask permission to use a late day. åÊNote that I have a strict policy of not granting additional late days foråÊanyåÊreason, because I do not want to adjudicate what constitutes a fair excuse. åÊSo if you exhaust your 5 free late days early, please do not ask for more.</p>",Reminder: HW1 is due tomorrow by 2pm (and late day policy),<p>I just submitted my assignment for HW2. Does this count as two or three late days?</p>,Late days,"<p>If we submit something late, do we have to specify we are using a late day? Or is it automatically detected? Also, if we submit something after the class but the same day that it is due, is it still a late day?</p>",Late Day,<p>What if we need to use a late day for Friday?</p>,How do late days work for the final project?,4
940847347,4/26/2016 16:19:52,false,1969415118,,4/26/2016 15:56:52,false,neodev,0.7778,32569659,USA,MN,Minneapolis,97.127.88.224,"2
5",,<p></p>,Confirming: separate late day policy for the final group project?,"<p>Hello, I&#39;m a little confused as to how late days work for groups. Is it that if the group wants to use a late day, all x members of the group must use their late day or is it that only one person of the group uses one late day. This question is only geared for regular homework assignments. I understand that there is a separate late day policy for the final group project. Thanks so much for the help!</p>",Late days for groups,"<p>This is a friendly reminder that <a href=""http://crowdsourcing-class.org/assignment1.html"" target=""_blank"">homework 1</a> is due tomorrow before class. åÊ</p>
<p></p>
<p>I would also like to draw your attention to the course&#39;s late day policy (given on the <a href=""http://crowdsourcing-class.org"" target=""_blank"">course web page</a>).</p>
<blockquote>
<p dir=""ltr"">Everyone can have 5 free late days without penalty. After you have used your free late days, you will lose 20% per day (or fraction thereof) that your assignment is submitted late. The final project will have its own late day policy.</p>
</blockquote>
<p>You do not need to ask permission to use a late day. åÊNote that I have a strict policy of not granting additional late days foråÊanyåÊreason, because I do not want to adjudicate what constitutes a fair excuse. åÊSo if you exhaust your 5 free late days early, please do not ask for more.</p>",Reminder: HW1 is due tomorrow by 2pm (and late day policy),<p>I just submitted my assignment for HW2. Does this count as two or three late days?</p>,Late days,"<p>If we submit something late, do we have to specify we are using a late day? Or is it automatically detected? Also, if we submit something after the class but the same day that it is due, is it still a late day?</p>",Late Day,<p>What if we need to use a late day for Friday?</p>,How do late days work for the final project?,4
940847348,4/26/2016 15:52:33,false,1969400262,,4/26/2016 15:50:49,false,elite,0.8889,36575101,IND,07,New Delhi,112.196.144.2,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>I got .5 points off &#34;for not using the crawler to collect URLs in addition to the Bing API&#34; -- my collected URLs included URLs from the Gun Violence Archive, and I collected the rest using the Bing API.åÊ</p>
<p></p>
<p>The instructions didn&#39;t specify that we necessarily needed to use other methods to get 2500 URLs. The homework says to &#34;turn in a list of at least 2,500 urls, including some crawled from the Gun Violence Archive and some obtained using the Bing API.&#34;åÊ</p>
<p></p>
<p>Am I understanding correctly why I got the points off? If so, I&#39;m confused why I got points off given the instructions. Thank you!åÊ</p>",Question about grade,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The xmlåÊgiven by bing api is filled with title tags containing NewsResult</p>
<p>Do these titles have to be printed as well?</p>
<p>If not how would recommend removing it?</p>",Title tag,1
940847348,4/26/2016 15:53:45,false,1969400809,,4/26/2016 15:51:13,false,neodev,1.0,28875937,PAK,08,Islamabad,119.153.105.50,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>I got .5 points off &#34;for not using the crawler to collect URLs in addition to the Bing API&#34; -- my collected URLs included URLs from the Gun Violence Archive, and I collected the rest using the Bing API.åÊ</p>
<p></p>
<p>The instructions didn&#39;t specify that we necessarily needed to use other methods to get 2500 URLs. The homework says to &#34;turn in a list of at least 2,500 urls, including some crawled from the Gun Violence Archive and some obtained using the Bing API.&#34;åÊ</p>
<p></p>
<p>Am I understanding correctly why I got the points off? If so, I&#39;m confused why I got points off given the instructions. Thank you!åÊ</p>",Question about grade,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The xmlåÊgiven by bing api is filled with title tags containing NewsResult</p>
<p>Do these titles have to be printed as well?</p>
<p>If not how would recommend removing it?</p>",Title tag,1
940847348,4/26/2016 15:56:15,false,1969401847,,4/26/2016 15:48:57,false,neodev,0.8889,21971187,TTO,08,Valsayn,190.213.132.190,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>I got .5 points off &#34;for not using the crawler to collect URLs in addition to the Bing API&#34; -- my collected URLs included URLs from the Gun Violence Archive, and I collected the rest using the Bing API.åÊ</p>
<p></p>
<p>The instructions didn&#39;t specify that we necessarily needed to use other methods to get 2500 URLs. The homework says to &#34;turn in a list of at least 2,500 urls, including some crawled from the Gun Violence Archive and some obtained using the Bing API.&#34;åÊ</p>
<p></p>
<p>Am I understanding correctly why I got the points off? If so, I&#39;m confused why I got points off given the instructions. Thank you!åÊ</p>",Question about grade,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The xmlåÊgiven by bing api is filled with title tags containing NewsResult</p>
<p>Do these titles have to be printed as well?</p>
<p>If not how would recommend removing it?</p>",Title tag,1
940847348,4/26/2016 15:56:23,false,1969401923,,4/26/2016 15:53:18,false,neodev,1.0,13396426,VEN,15,Santa Teresa,190.38.163.149,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>I got .5 points off &#34;for not using the crawler to collect URLs in addition to the Bing API&#34; -- my collected URLs included URLs from the Gun Violence Archive, and I collected the rest using the Bing API.åÊ</p>
<p></p>
<p>The instructions didn&#39;t specify that we necessarily needed to use other methods to get 2500 URLs. The homework says to &#34;turn in a list of at least 2,500 urls, including some crawled from the Gun Violence Archive and some obtained using the Bing API.&#34;åÊ</p>
<p></p>
<p>Am I understanding correctly why I got the points off? If so, I&#39;m confused why I got points off given the instructions. Thank you!åÊ</p>",Question about grade,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The xmlåÊgiven by bing api is filled with title tags containing NewsResult</p>
<p>Do these titles have to be printed as well?</p>
<p>If not how would recommend removing it?</p>",Title tag,1
940847348,4/26/2016 16:00:16,false,1969403672,,4/26/2016 15:56:11,false,clixsense,1.0,21875134,GBR,H9,London,87.112.158.81,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>I got .5 points off &#34;for not using the crawler to collect URLs in addition to the Bing API&#34; -- my collected URLs included URLs from the Gun Violence Archive, and I collected the rest using the Bing API.åÊ</p>
<p></p>
<p>The instructions didn&#39;t specify that we necessarily needed to use other methods to get 2500 URLs. The homework says to &#34;turn in a list of at least 2,500 urls, including some crawled from the Gun Violence Archive and some obtained using the Bing API.&#34;åÊ</p>
<p></p>
<p>Am I understanding correctly why I got the points off? If so, I&#39;m confused why I got points off given the instructions. Thank you!åÊ</p>",Question about grade,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The xmlåÊgiven by bing api is filled with title tags containing NewsResult</p>
<p>Do these titles have to be printed as well?</p>
<p>If not how would recommend removing it?</p>",Title tag,1
940847349,4/26/2016 15:48:55,false,1969398735,,4/26/2016 15:46:06,false,neodev,0.8889,21971187,TTO,08,Valsayn,190.213.132.190,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>Hey everyone!</p>
<p></p>
<p>Penn Play 2016 is this weekend! Penn Play is a hackathon for developers, designers, artists, and gaming fans from all fields to come together and build games.</p>
<p></p>
<p>Sign-in starts Saturday at 11 AM in Raisler Lounge, and final game demos will be Sunday starting at noon. No game dev or programming experience is neededÛÓthereÛªll be mentors, hacking space, food, prizes, and a lot of snacks! Our sponsors include Unity and EA Games, and students, game devs, and professorsåÊwill be holding workshops throughout the weekend.</p>
<p></p>
<p>It&#39;ll be all the funs and a great learning experience. If you&#39;re interested, registration is at <a href=""http://pennplay.org"" target=""_blank"">pennplay.org</a>! (Or you could click the link <a href=""https://docs.google.com/forms/d/18aRWbJwgwx2yq-wt9n2c7wThYXEVMIgtxwgoeKCpRww/viewform"" target=""_blank"">here</a>). Feel free to reach out to me if you have any questions. You can also follow us on <a href=""https://www.facebook.com/events/1718116175076546/"" target=""_blank"">Facebook</a> for updates.</p>
<p></p>
<p>Come out and play!åÊ</p>
<p></p>
<p></p>
<p></p>",Penn Play 16 is this weekend!,4
940847349,4/26/2016 15:50:47,false,1969399565,,4/26/2016 15:49:12,false,elite,0.8889,36575101,IND,07,New Delhi,112.196.144.2,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>Hey everyone!</p>
<p></p>
<p>Penn Play 2016 is this weekend! Penn Play is a hackathon for developers, designers, artists, and gaming fans from all fields to come together and build games.</p>
<p></p>
<p>Sign-in starts Saturday at 11 AM in Raisler Lounge, and final game demos will be Sunday starting at noon. No game dev or programming experience is neededÛÓthereÛªll be mentors, hacking space, food, prizes, and a lot of snacks! Our sponsors include Unity and EA Games, and students, game devs, and professorsåÊwill be holding workshops throughout the weekend.</p>
<p></p>
<p>It&#39;ll be all the funs and a great learning experience. If you&#39;re interested, registration is at <a href=""http://pennplay.org"" target=""_blank"">pennplay.org</a>! (Or you could click the link <a href=""https://docs.google.com/forms/d/18aRWbJwgwx2yq-wt9n2c7wThYXEVMIgtxwgoeKCpRww/viewform"" target=""_blank"">here</a>). Feel free to reach out to me if you have any questions. You can also follow us on <a href=""https://www.facebook.com/events/1718116175076546/"" target=""_blank"">Facebook</a> for updates.</p>
<p></p>
<p>Come out and play!åÊ</p>
<p></p>
<p></p>
<p></p>",Penn Play 16 is this weekend!,4
940847349,4/26/2016 15:51:11,false,1969399724,,4/26/2016 15:47:56,false,neodev,1.0,28875937,PAK,08,Islamabad,119.153.105.50,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>Hey everyone!</p>
<p></p>
<p>Penn Play 2016 is this weekend! Penn Play is a hackathon for developers, designers, artists, and gaming fans from all fields to come together and build games.</p>
<p></p>
<p>Sign-in starts Saturday at 11 AM in Raisler Lounge, and final game demos will be Sunday starting at noon. No game dev or programming experience is neededÛÓthereÛªll be mentors, hacking space, food, prizes, and a lot of snacks! Our sponsors include Unity and EA Games, and students, game devs, and professorsåÊwill be holding workshops throughout the weekend.</p>
<p></p>
<p>It&#39;ll be all the funs and a great learning experience. If you&#39;re interested, registration is at <a href=""http://pennplay.org"" target=""_blank"">pennplay.org</a>! (Or you could click the link <a href=""https://docs.google.com/forms/d/18aRWbJwgwx2yq-wt9n2c7wThYXEVMIgtxwgoeKCpRww/viewform"" target=""_blank"">here</a>). Feel free to reach out to me if you have any questions. You can also follow us on <a href=""https://www.facebook.com/events/1718116175076546/"" target=""_blank"">Facebook</a> for updates.</p>
<p></p>
<p>Come out and play!åÊ</p>
<p></p>
<p></p>
<p></p>",Penn Play 16 is this weekend!,4
940847349,4/26/2016 15:53:17,false,1969400601,,4/26/2016 15:51:23,false,neodev,1.0,13396426,VEN,15,Santa Teresa,190.38.163.149,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>Hey everyone!</p>
<p></p>
<p>Penn Play 2016 is this weekend! Penn Play is a hackathon for developers, designers, artists, and gaming fans from all fields to come together and build games.</p>
<p></p>
<p>Sign-in starts Saturday at 11 AM in Raisler Lounge, and final game demos will be Sunday starting at noon. No game dev or programming experience is neededÛÓthereÛªll be mentors, hacking space, food, prizes, and a lot of snacks! Our sponsors include Unity and EA Games, and students, game devs, and professorsåÊwill be holding workshops throughout the weekend.</p>
<p></p>
<p>It&#39;ll be all the funs and a great learning experience. If you&#39;re interested, registration is at <a href=""http://pennplay.org"" target=""_blank"">pennplay.org</a>! (Or you could click the link <a href=""https://docs.google.com/forms/d/18aRWbJwgwx2yq-wt9n2c7wThYXEVMIgtxwgoeKCpRww/viewform"" target=""_blank"">here</a>). Feel free to reach out to me if you have any questions. You can also follow us on <a href=""https://www.facebook.com/events/1718116175076546/"" target=""_blank"">Facebook</a> for updates.</p>
<p></p>
<p>Come out and play!åÊ</p>
<p></p>
<p></p>
<p></p>",Penn Play 16 is this weekend!,4
940847349,4/26/2016 15:56:46,false,1969402050,,4/26/2016 15:55:02,false,elite,1.0,33243069,IND,10,Faridabad,116.203.79.150,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>Hey everyone!</p>
<p></p>
<p>Penn Play 2016 is this weekend! Penn Play is a hackathon for developers, designers, artists, and gaming fans from all fields to come together and build games.</p>
<p></p>
<p>Sign-in starts Saturday at 11 AM in Raisler Lounge, and final game demos will be Sunday starting at noon. No game dev or programming experience is neededÛÓthereÛªll be mentors, hacking space, food, prizes, and a lot of snacks! Our sponsors include Unity and EA Games, and students, game devs, and professorsåÊwill be holding workshops throughout the weekend.</p>
<p></p>
<p>It&#39;ll be all the funs and a great learning experience. If you&#39;re interested, registration is at <a href=""http://pennplay.org"" target=""_blank"">pennplay.org</a>! (Or you could click the link <a href=""https://docs.google.com/forms/d/18aRWbJwgwx2yq-wt9n2c7wThYXEVMIgtxwgoeKCpRww/viewform"" target=""_blank"">here</a>). Feel free to reach out to me if you have any questions. You can also follow us on <a href=""https://www.facebook.com/events/1718116175076546/"" target=""_blank"">Facebook</a> for updates.</p>
<p></p>
<p>Come out and play!åÊ</p>
<p></p>
<p></p>
<p></p>",Penn Play 16 is this weekend!,4
940847350,4/26/2016 16:03:21,false,1969405341,,4/26/2016 16:02:52,false,personaly,1.0,33663352,ARG,01,Mar Del Plata,181.168.213.227,0,,"<p>Hi Everyone!</p>
<p></p>
<p>Sorry class got so rushed-- I was hoping to walk through more of the bootcamp. Hopefully you were able to get a good start. We have back to back office hours now and office hoursåÊfrom noon until 8 tomorrow, so come get help early!åÊ</p>
<p></p>
<p>Attached is the iPython notebook from class today, which takes care of question 1 for you. The relevant code is also below. You should be able to recycle it in some shape or form for most of the other questions in the assignment.åÊ</p>
<p></p>
<pre>#This line reads in full text of the file, the splits it up using the newline character (&#39;\n&#39;)
#The return value (stored in wine) is a list of strings, each one corresponding to a line in the file
#Fun fact: This is the same as calling open(&#39;data/wine.txt&#39;).read().split(&#39;\n&#39;)
wine = open(&#39;data/wine.txt&#39;).readlines()

ratings = {} #dictionary keep track of how many times each star value is seen
for line in wine:
    review_and_rating = line.strip().split(&#39;\t&#39;) #split each line into two strings, using the tab character
    stars = review_and_rating[1]
    if stars not in ratings: #check if the key is in the dictionary. If not, add it and give it a count of 0
        print(&#34;adding&#34;, stars, &#34;to ratings&#34;)
        ratings[stars] = 0
    ratings[stars] &#43;= 1 #increment the count by 1
    
for key in ratings:
    print(key, &#34;appeared&#34;, ratings[key], &#34;times&#34;)</pre>
<p>Please ask questions!</p>
<p></p>
<p><a href=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/h6sr8zhfyuw71v/ijxaiooi4tz/IPythonBootcamp.ipynb"" target=""_blank"">IPythonBootcamp.ipynb</a></p>",Python Bootcamp code from class today,"<p>My partner and I are having trouble finding the title for our articles. We managed to get the url (given), and the date (changed the xpath to //entry//url), but we cannot access the title. When we try to look at subchildren, it&#39;s empty. We&#39;ve also tried looking at stackoverflow links provided in the previous piazza post, but were unable to get the title. Is there a good way to go about getting the title of these articles?</p>",XML calls in python,"<p>Hi, I&#39;m not sure if I submitted HW3 correctly. Is there any way I can get a confirmation that my file was received?åÊ</p>",Check Python Bootcamp Submission,"<p>Hi Pr. Callison-Burch,</p>
<p></p>
<p>In lecture today you mentioned that our attendance would be taken in Python bootcamp, and that we&#39;d receive a grade based on it.</p>
<p></p>
<p>I already have significant experience with python; specifically, I&#39;ve been writing Python for over six years, and have already taken several courses that use Python for the assignments (CIS 391, and CIS 419, among others). Is there any chance I can avoid getting penalized without attending the bootcamp?åÊFrom what I understand, this is to ensure that we know enough Python to be able to competently do the homework assignments and the final project; I think that I already have sufficient experience to be able to do so.</p>
<p></p>
<p>Thanks for taking the time to read this!</p>
<p>Sam</p>",Can I get a waiver for attending Python bootcamp?,"<p>Will our output be compared character by character against the sample? For example, does our spacing and line breaks have to be the exact same? Does the order of the output have to be the exact same?</p>",Grading python bootcamp,<p>Would it be possible for the instructors to post the solution code they have written up to solve the 10 questions in the python bootcamp (hw3)? I&#39;m not sure if I did all åÊthe tasks in the easiest way possible.</p>,HW3 python bootcamp code,5
940847350,4/26/2016 16:24:34,false,1969419054,,4/26/2016 16:23:47,false,neodev,1.0,29175140,VEN,25,Caracas,190.72.125.134,0,,"<p>Hi Everyone!</p>
<p></p>
<p>Sorry class got so rushed-- I was hoping to walk through more of the bootcamp. Hopefully you were able to get a good start. We have back to back office hours now and office hoursåÊfrom noon until 8 tomorrow, so come get help early!åÊ</p>
<p></p>
<p>Attached is the iPython notebook from class today, which takes care of question 1 for you. The relevant code is also below. You should be able to recycle it in some shape or form for most of the other questions in the assignment.åÊ</p>
<p></p>
<pre>#This line reads in full text of the file, the splits it up using the newline character (&#39;\n&#39;)
#The return value (stored in wine) is a list of strings, each one corresponding to a line in the file
#Fun fact: This is the same as calling open(&#39;data/wine.txt&#39;).read().split(&#39;\n&#39;)
wine = open(&#39;data/wine.txt&#39;).readlines()

ratings = {} #dictionary keep track of how many times each star value is seen
for line in wine:
    review_and_rating = line.strip().split(&#39;\t&#39;) #split each line into two strings, using the tab character
    stars = review_and_rating[1]
    if stars not in ratings: #check if the key is in the dictionary. If not, add it and give it a count of 0
        print(&#34;adding&#34;, stars, &#34;to ratings&#34;)
        ratings[stars] = 0
    ratings[stars] &#43;= 1 #increment the count by 1
    
for key in ratings:
    print(key, &#34;appeared&#34;, ratings[key], &#34;times&#34;)</pre>
<p>Please ask questions!</p>
<p></p>
<p><a href=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/h6sr8zhfyuw71v/ijxaiooi4tz/IPythonBootcamp.ipynb"" target=""_blank"">IPythonBootcamp.ipynb</a></p>",Python Bootcamp code from class today,"<p>My partner and I are having trouble finding the title for our articles. We managed to get the url (given), and the date (changed the xpath to //entry//url), but we cannot access the title. When we try to look at subchildren, it&#39;s empty. We&#39;ve also tried looking at stackoverflow links provided in the previous piazza post, but were unable to get the title. Is there a good way to go about getting the title of these articles?</p>",XML calls in python,"<p>Hi, I&#39;m not sure if I submitted HW3 correctly. Is there any way I can get a confirmation that my file was received?åÊ</p>",Check Python Bootcamp Submission,"<p>Hi Pr. Callison-Burch,</p>
<p></p>
<p>In lecture today you mentioned that our attendance would be taken in Python bootcamp, and that we&#39;d receive a grade based on it.</p>
<p></p>
<p>I already have significant experience with python; specifically, I&#39;ve been writing Python for over six years, and have already taken several courses that use Python for the assignments (CIS 391, and CIS 419, among others). Is there any chance I can avoid getting penalized without attending the bootcamp?åÊFrom what I understand, this is to ensure that we know enough Python to be able to competently do the homework assignments and the final project; I think that I already have sufficient experience to be able to do so.</p>
<p></p>
<p>Thanks for taking the time to read this!</p>
<p>Sam</p>",Can I get a waiver for attending Python bootcamp?,"<p>Will our output be compared character by character against the sample? For example, does our spacing and line breaks have to be the exact same? Does the order of the output have to be the exact same?</p>",Grading python bootcamp,<p>Would it be possible for the instructors to post the solution code they have written up to solve the 10 questions in the python bootcamp (hw3)? I&#39;m not sure if I did all åÊthe tasks in the easiest way possible.</p>,HW3 python bootcamp code,5
940847350,4/26/2016 16:34:40,false,1969426094,,4/26/2016 16:32:37,false,clixsense,0.8889,8057247,PRT,17,Póvoa De Varzim,144.64.25.68,0,,"<p>Hi Everyone!</p>
<p></p>
<p>Sorry class got so rushed-- I was hoping to walk through more of the bootcamp. Hopefully you were able to get a good start. We have back to back office hours now and office hoursåÊfrom noon until 8 tomorrow, so come get help early!åÊ</p>
<p></p>
<p>Attached is the iPython notebook from class today, which takes care of question 1 for you. The relevant code is also below. You should be able to recycle it in some shape or form for most of the other questions in the assignment.åÊ</p>
<p></p>
<pre>#This line reads in full text of the file, the splits it up using the newline character (&#39;\n&#39;)
#The return value (stored in wine) is a list of strings, each one corresponding to a line in the file
#Fun fact: This is the same as calling open(&#39;data/wine.txt&#39;).read().split(&#39;\n&#39;)
wine = open(&#39;data/wine.txt&#39;).readlines()

ratings = {} #dictionary keep track of how many times each star value is seen
for line in wine:
    review_and_rating = line.strip().split(&#39;\t&#39;) #split each line into two strings, using the tab character
    stars = review_and_rating[1]
    if stars not in ratings: #check if the key is in the dictionary. If not, add it and give it a count of 0
        print(&#34;adding&#34;, stars, &#34;to ratings&#34;)
        ratings[stars] = 0
    ratings[stars] &#43;= 1 #increment the count by 1
    
for key in ratings:
    print(key, &#34;appeared&#34;, ratings[key], &#34;times&#34;)</pre>
<p>Please ask questions!</p>
<p></p>
<p><a href=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/h6sr8zhfyuw71v/ijxaiooi4tz/IPythonBootcamp.ipynb"" target=""_blank"">IPythonBootcamp.ipynb</a></p>",Python Bootcamp code from class today,"<p>My partner and I are having trouble finding the title for our articles. We managed to get the url (given), and the date (changed the xpath to //entry//url), but we cannot access the title. When we try to look at subchildren, it&#39;s empty. We&#39;ve also tried looking at stackoverflow links provided in the previous piazza post, but were unable to get the title. Is there a good way to go about getting the title of these articles?</p>",XML calls in python,"<p>Hi, I&#39;m not sure if I submitted HW3 correctly. Is there any way I can get a confirmation that my file was received?åÊ</p>",Check Python Bootcamp Submission,"<p>Hi Pr. Callison-Burch,</p>
<p></p>
<p>In lecture today you mentioned that our attendance would be taken in Python bootcamp, and that we&#39;d receive a grade based on it.</p>
<p></p>
<p>I already have significant experience with python; specifically, I&#39;ve been writing Python for over six years, and have already taken several courses that use Python for the assignments (CIS 391, and CIS 419, among others). Is there any chance I can avoid getting penalized without attending the bootcamp?åÊFrom what I understand, this is to ensure that we know enough Python to be able to competently do the homework assignments and the final project; I think that I already have sufficient experience to be able to do so.</p>
<p></p>
<p>Thanks for taking the time to read this!</p>
<p>Sam</p>",Can I get a waiver for attending Python bootcamp?,"<p>Will our output be compared character by character against the sample? For example, does our spacing and line breaks have to be the exact same? Does the order of the output have to be the exact same?</p>",Grading python bootcamp,<p>Would it be possible for the instructors to post the solution code they have written up to solve the 10 questions in the python bootcamp (hw3)? I&#39;m not sure if I did all åÊthe tasks in the easiest way possible.</p>,HW3 python bootcamp code,5
940847350,4/26/2016 16:41:30,false,1969430405,,4/26/2016 16:40:27,false,clixsense,1.0,6329782,IDN,07,Bekonang,202.67.40.31,0,,"<p>Hi Everyone!</p>
<p></p>
<p>Sorry class got so rushed-- I was hoping to walk through more of the bootcamp. Hopefully you were able to get a good start. We have back to back office hours now and office hoursåÊfrom noon until 8 tomorrow, so come get help early!åÊ</p>
<p></p>
<p>Attached is the iPython notebook from class today, which takes care of question 1 for you. The relevant code is also below. You should be able to recycle it in some shape or form for most of the other questions in the assignment.åÊ</p>
<p></p>
<pre>#This line reads in full text of the file, the splits it up using the newline character (&#39;\n&#39;)
#The return value (stored in wine) is a list of strings, each one corresponding to a line in the file
#Fun fact: This is the same as calling open(&#39;data/wine.txt&#39;).read().split(&#39;\n&#39;)
wine = open(&#39;data/wine.txt&#39;).readlines()

ratings = {} #dictionary keep track of how many times each star value is seen
for line in wine:
    review_and_rating = line.strip().split(&#39;\t&#39;) #split each line into two strings, using the tab character
    stars = review_and_rating[1]
    if stars not in ratings: #check if the key is in the dictionary. If not, add it and give it a count of 0
        print(&#34;adding&#34;, stars, &#34;to ratings&#34;)
        ratings[stars] = 0
    ratings[stars] &#43;= 1 #increment the count by 1
    
for key in ratings:
    print(key, &#34;appeared&#34;, ratings[key], &#34;times&#34;)</pre>
<p>Please ask questions!</p>
<p></p>
<p><a href=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/h6sr8zhfyuw71v/ijxaiooi4tz/IPythonBootcamp.ipynb"" target=""_blank"">IPythonBootcamp.ipynb</a></p>",Python Bootcamp code from class today,"<p>My partner and I are having trouble finding the title for our articles. We managed to get the url (given), and the date (changed the xpath to //entry//url), but we cannot access the title. When we try to look at subchildren, it&#39;s empty. We&#39;ve also tried looking at stackoverflow links provided in the previous piazza post, but were unable to get the title. Is there a good way to go about getting the title of these articles?</p>",XML calls in python,"<p>Hi, I&#39;m not sure if I submitted HW3 correctly. Is there any way I can get a confirmation that my file was received?åÊ</p>",Check Python Bootcamp Submission,"<p>Hi Pr. Callison-Burch,</p>
<p></p>
<p>In lecture today you mentioned that our attendance would be taken in Python bootcamp, and that we&#39;d receive a grade based on it.</p>
<p></p>
<p>I already have significant experience with python; specifically, I&#39;ve been writing Python for over six years, and have already taken several courses that use Python for the assignments (CIS 391, and CIS 419, among others). Is there any chance I can avoid getting penalized without attending the bootcamp?åÊFrom what I understand, this is to ensure that we know enough Python to be able to competently do the homework assignments and the final project; I think that I already have sufficient experience to be able to do so.</p>
<p></p>
<p>Thanks for taking the time to read this!</p>
<p>Sam</p>",Can I get a waiver for attending Python bootcamp?,"<p>Will our output be compared character by character against the sample? For example, does our spacing and line breaks have to be the exact same? Does the order of the output have to be the exact same?</p>",Grading python bootcamp,<p>Would it be possible for the instructors to post the solution code they have written up to solve the 10 questions in the python bootcamp (hw3)? I&#39;m not sure if I did all åÊthe tasks in the easiest way possible.</p>,HW3 python bootcamp code,5
940847350,4/26/2016 16:58:38,false,1969440697,,4/26/2016 16:56:43,false,clixsense,1.0,21408115,IDN,07,Semarang,36.79.23.180,0,,"<p>Hi Everyone!</p>
<p></p>
<p>Sorry class got so rushed-- I was hoping to walk through more of the bootcamp. Hopefully you were able to get a good start. We have back to back office hours now and office hoursåÊfrom noon until 8 tomorrow, so come get help early!åÊ</p>
<p></p>
<p>Attached is the iPython notebook from class today, which takes care of question 1 for you. The relevant code is also below. You should be able to recycle it in some shape or form for most of the other questions in the assignment.åÊ</p>
<p></p>
<pre>#This line reads in full text of the file, the splits it up using the newline character (&#39;\n&#39;)
#The return value (stored in wine) is a list of strings, each one corresponding to a line in the file
#Fun fact: This is the same as calling open(&#39;data/wine.txt&#39;).read().split(&#39;\n&#39;)
wine = open(&#39;data/wine.txt&#39;).readlines()

ratings = {} #dictionary keep track of how many times each star value is seen
for line in wine:
    review_and_rating = line.strip().split(&#39;\t&#39;) #split each line into two strings, using the tab character
    stars = review_and_rating[1]
    if stars not in ratings: #check if the key is in the dictionary. If not, add it and give it a count of 0
        print(&#34;adding&#34;, stars, &#34;to ratings&#34;)
        ratings[stars] = 0
    ratings[stars] &#43;= 1 #increment the count by 1
    
for key in ratings:
    print(key, &#34;appeared&#34;, ratings[key], &#34;times&#34;)</pre>
<p>Please ask questions!</p>
<p></p>
<p><a href=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/h6sr8zhfyuw71v/ijxaiooi4tz/IPythonBootcamp.ipynb"" target=""_blank"">IPythonBootcamp.ipynb</a></p>",Python Bootcamp code from class today,"<p>My partner and I are having trouble finding the title for our articles. We managed to get the url (given), and the date (changed the xpath to //entry//url), but we cannot access the title. When we try to look at subchildren, it&#39;s empty. We&#39;ve also tried looking at stackoverflow links provided in the previous piazza post, but were unable to get the title. Is there a good way to go about getting the title of these articles?</p>",XML calls in python,"<p>Hi, I&#39;m not sure if I submitted HW3 correctly. Is there any way I can get a confirmation that my file was received?åÊ</p>",Check Python Bootcamp Submission,"<p>Hi Pr. Callison-Burch,</p>
<p></p>
<p>In lecture today you mentioned that our attendance would be taken in Python bootcamp, and that we&#39;d receive a grade based on it.</p>
<p></p>
<p>I already have significant experience with python; specifically, I&#39;ve been writing Python for over six years, and have already taken several courses that use Python for the assignments (CIS 391, and CIS 419, among others). Is there any chance I can avoid getting penalized without attending the bootcamp?åÊFrom what I understand, this is to ensure that we know enough Python to be able to competently do the homework assignments and the final project; I think that I already have sufficient experience to be able to do so.</p>
<p></p>
<p>Thanks for taking the time to read this!</p>
<p>Sam</p>",Can I get a waiver for attending Python bootcamp?,"<p>Will our output be compared character by character against the sample? For example, does our spacing and line breaks have to be the exact same? Does the order of the output have to be the exact same?</p>",Grading python bootcamp,<p>Would it be possible for the instructors to post the solution code they have written up to solve the 10 questions in the python bootcamp (hw3)? I&#39;m not sure if I did all åÊthe tasks in the easiest way possible.</p>,HW3 python bootcamp code,5
940847351,4/26/2016 17:02:29,false,1969443192,,4/26/2016 17:01:24,false,clixsense,1.0,21408115,IDN,07,Semarang,36.79.23.180,3,,"<p>-3 A blank text file was submitted in lieu of code or diagrams. -0.5 features and dimensions of X not listed or incorrect. Most of questionnaire is blank. -1</p>
<p></p>
<p>I understand the last two deductions, but I am having trouble understanding where the -3 points came from.</p>
<p></p>
<ul><li>1 points - Create a rule based classifier</li><li>1 points - Create a decision tree and a decision tree diagram</li><li>1 points - Implement a statistical unigram model</li></ul>
<p></p>
<p>The rule based classifier is in the code under than method.</p>
<p>The decision tree and diagram is submitted as decision-tree.png and rule-based-tree.png</p>
<p>The unigram is implemented in get_features()</p>",hw3 grade,"<p>Hi Everyone!</p>
<p></p>
<p>We just sent everyone emails about your peer grading assignment. You are required to watch 3 videos and fill out a questionnaire for each video. You&#39;ll also be asked to rank the three videos in a separate questionnaire.</p>
<p>All the information you need is in that email. The email was sent from &#34;nets213&#64;seas.upenn.edu&#34;. Please check your spam in case you don&#39;t find the email in your inbox.</p>
<p></p>",Peer Grading Assignment,"<p>Hi everyone,</p>
<p></p>
<p>Grades for the first assignment have been emailed to your Penn email addresses. If you did not receive an email with your grade (and have checked your spam folder), please reach out to me immediately so we can resolve it. Overall, we were very happy with your participation and takeaways from your experience as crowdworkers. You can expect your grades for the second assignment very soon as well, and we encourage you to incorporate any feedback on your company profile into the video (Ellie&#39;s post on takeaways for the entire course will also be helpful for this).åÊ</p>
<p></p>
<p>If you have questions about your grade, please don&#39;t respond directly to the score email. You&#39;ll get a much better response time if you post privately to Piazza or email the entire teaching staff at <a href=""mailto:nets213&#64;googlegroups.com"">nets213-staff&#64;googlegroups.com</a>.åÊ</p>
<p></p>
<p>We&#39;ll be getting the next few assignments out to you as soon as possible so that you can assess how the class is going.åÊOne upcoming date to be aware of is the drop deadline on February 19th. If you are concerned about how you are doing at any point, please talk to a member of the teaching staff.åÊ</p>",Grades Released for Homework 1: Becoming a Crowdworker,"<p>Hi everyone,åÊ</p>
<p></p>
<p>Grades have been sent out for &#34;Becoming a Requester.&#34; If you haven&#39;t received the email, and it&#39;s not in your spam folder, you can complete the form <a href=""http://goo.gl/forms/dPv5TI7zuB"" target=""_blank"">here</a> and I will try to get your grades back to you by midnight (which is also the drop deadline).åÊ</p>
<p></p>
<p>Overall, you did very well with it and are now masters of crawling the internet. You gained some new tools that you should keep in mind for the final project (and the first part has been released, yay!).åÊ</p>
<p></p>
<p>We did see a few common misconceptions that we&#39;d like to clear up.åÊ</p>
<p></p>
<ul><li>The <strong>hourly wage</strong> referred to what the workers completing your job earned (you can view this value on Crowdflower&#39;s analytics tab). Without simply paying more per task, the other variable we could affect was the amount of time it takes to complete. Making the links clickable, scraping the text of the article and uploading that to Crowdflower instead of URLs, and providing clear, unambiguous directions to make deciding the label easier were all great ways, but there&#39;s a lot that we can do on this side.åÊ</li><li><strong>Precision</strong> is the ratio of true positives over all positives, or the probability that once your classifier has labeled an article as gun violence, it&#39;s actually about an instance of gun violence. This differed from <strong>accuracy</strong> because accuracy is the ratio of correct labels over all labels (true positives &#43; true negatives)/(all positives &#43; all negatives). Some of you submitted definitions from other fields, and we would encourage you to go back through and review the slides.åÊ</li><li><strong>Recall</strong> is the ratio of (true positives)/(true positives &#43; false negatives). For our data set, this was the probability that an article about gun violence would be given a positive label by our classifier. We couldn&#39;t calculate that because we only gave the Crowdflower workers positively-labeled data, and therefore can&#39;t draw any conclusions about the number of false negatives in our labels.åÊ</li><li>Within the <strong>Bing API</strong> piece, many of the titles submitted were &#34;News Result.&#34; This is a field in the Bing API, but doesn&#39;t correspond to the actual title of the article (or it would be very difficult to decide which articles to read).åÊ</li><li>There are a lot of sources of <strong>bias</strong> in the way that we collected data. One of our goals is to minimize this. It&#39;s easy to see that minor injuries are unlikely to be reported, that areas with less media coverage are less likely to be scraped (some of you started with the New York Times and the LA Times, but nobody checked any newspapers from the middle of nowhere), that reporting around gun violence can be sensationalized and since we aren&#39;t deduplicating results yet, there are likely repeat incidents that were reported on at multiple URLs you submitted. There are other sources too, and thinking about this is an important part of building a comprehensive system to measure anything (not just gun violence).åÊ</li><li>Some of you got really original with what you named the files that you submitted. While we appreciate your creative genius, it makes it a lot easier to grade when the names of files approximately correspond to what&#39;s in them. Please keep this in mind for future assignments.åÊ</li></ul>
<p></p>
<p>If you have a regrade request or specific questions about your grades, you shouldåÊcome to office hours to discuss it.åÊ</p>
<p></p>
<p>#pin</p>",Takeaways &amp; Grades from &#34;Becoming a Requester&#34;,"<p>My code for get_misclassified_examples was:</p>
<p></p>
<pre>def get_misclassified_examples(y, X, texts) :
	x_train, x_test, y_train, y_test, train_texts, test_texts = train_test_split(X, y, texts)
	clf = train_classifier(x_train, y_train)

	for i, x in enumerate(x_test):
		expected_label = y_test[i]
		test_acc = test_classifier(clf, x, [expected_label])
		if expected_label == 0 and test_acc == 0.0:
			print test_texts[i]</pre>
<p>Where I tried to identify the false positive rows, by checking if the true value was 0, but the resulting prediction was 1 and hence the test_acc was 0.</p>
<p>I was deducted marks for this. Please let me know if I made a mistake here. Thank you!</p>",Clarification of grading for get_misclassified_examples,<p>Will we ever know about our homework grades?</p>,HW Grades,5
940847351,4/26/2016 17:07:43,false,1969446213,,4/26/2016 17:05:48,false,neodev,1.0,36167043,GBR,G6,Hull,77.86.101.69,0,,"<p>-3 A blank text file was submitted in lieu of code or diagrams. -0.5 features and dimensions of X not listed or incorrect. Most of questionnaire is blank. -1</p>
<p></p>
<p>I understand the last two deductions, but I am having trouble understanding where the -3 points came from.</p>
<p></p>
<ul><li>1 points - Create a rule based classifier</li><li>1 points - Create a decision tree and a decision tree diagram</li><li>1 points - Implement a statistical unigram model</li></ul>
<p></p>
<p>The rule based classifier is in the code under than method.</p>
<p>The decision tree and diagram is submitted as decision-tree.png and rule-based-tree.png</p>
<p>The unigram is implemented in get_features()</p>",hw3 grade,"<p>Hi Everyone!</p>
<p></p>
<p>We just sent everyone emails about your peer grading assignment. You are required to watch 3 videos and fill out a questionnaire for each video. You&#39;ll also be asked to rank the three videos in a separate questionnaire.</p>
<p>All the information you need is in that email. The email was sent from &#34;nets213&#64;seas.upenn.edu&#34;. Please check your spam in case you don&#39;t find the email in your inbox.</p>
<p></p>",Peer Grading Assignment,"<p>Hi everyone,</p>
<p></p>
<p>Grades for the first assignment have been emailed to your Penn email addresses. If you did not receive an email with your grade (and have checked your spam folder), please reach out to me immediately so we can resolve it. Overall, we were very happy with your participation and takeaways from your experience as crowdworkers. You can expect your grades for the second assignment very soon as well, and we encourage you to incorporate any feedback on your company profile into the video (Ellie&#39;s post on takeaways for the entire course will also be helpful for this).åÊ</p>
<p></p>
<p>If you have questions about your grade, please don&#39;t respond directly to the score email. You&#39;ll get a much better response time if you post privately to Piazza or email the entire teaching staff at <a href=""mailto:nets213&#64;googlegroups.com"">nets213-staff&#64;googlegroups.com</a>.åÊ</p>
<p></p>
<p>We&#39;ll be getting the next few assignments out to you as soon as possible so that you can assess how the class is going.åÊOne upcoming date to be aware of is the drop deadline on February 19th. If you are concerned about how you are doing at any point, please talk to a member of the teaching staff.åÊ</p>",Grades Released for Homework 1: Becoming a Crowdworker,"<p>Hi everyone,åÊ</p>
<p></p>
<p>Grades have been sent out for &#34;Becoming a Requester.&#34; If you haven&#39;t received the email, and it&#39;s not in your spam folder, you can complete the form <a href=""http://goo.gl/forms/dPv5TI7zuB"" target=""_blank"">here</a> and I will try to get your grades back to you by midnight (which is also the drop deadline).åÊ</p>
<p></p>
<p>Overall, you did very well with it and are now masters of crawling the internet. You gained some new tools that you should keep in mind for the final project (and the first part has been released, yay!).åÊ</p>
<p></p>
<p>We did see a few common misconceptions that we&#39;d like to clear up.åÊ</p>
<p></p>
<ul><li>The <strong>hourly wage</strong> referred to what the workers completing your job earned (you can view this value on Crowdflower&#39;s analytics tab). Without simply paying more per task, the other variable we could affect was the amount of time it takes to complete. Making the links clickable, scraping the text of the article and uploading that to Crowdflower instead of URLs, and providing clear, unambiguous directions to make deciding the label easier were all great ways, but there&#39;s a lot that we can do on this side.åÊ</li><li><strong>Precision</strong> is the ratio of true positives over all positives, or the probability that once your classifier has labeled an article as gun violence, it&#39;s actually about an instance of gun violence. This differed from <strong>accuracy</strong> because accuracy is the ratio of correct labels over all labels (true positives &#43; true negatives)/(all positives &#43; all negatives). Some of you submitted definitions from other fields, and we would encourage you to go back through and review the slides.åÊ</li><li><strong>Recall</strong> is the ratio of (true positives)/(true positives &#43; false negatives). For our data set, this was the probability that an article about gun violence would be given a positive label by our classifier. We couldn&#39;t calculate that because we only gave the Crowdflower workers positively-labeled data, and therefore can&#39;t draw any conclusions about the number of false negatives in our labels.åÊ</li><li>Within the <strong>Bing API</strong> piece, many of the titles submitted were &#34;News Result.&#34; This is a field in the Bing API, but doesn&#39;t correspond to the actual title of the article (or it would be very difficult to decide which articles to read).åÊ</li><li>There are a lot of sources of <strong>bias</strong> in the way that we collected data. One of our goals is to minimize this. It&#39;s easy to see that minor injuries are unlikely to be reported, that areas with less media coverage are less likely to be scraped (some of you started with the New York Times and the LA Times, but nobody checked any newspapers from the middle of nowhere), that reporting around gun violence can be sensationalized and since we aren&#39;t deduplicating results yet, there are likely repeat incidents that were reported on at multiple URLs you submitted. There are other sources too, and thinking about this is an important part of building a comprehensive system to measure anything (not just gun violence).åÊ</li><li>Some of you got really original with what you named the files that you submitted. While we appreciate your creative genius, it makes it a lot easier to grade when the names of files approximately correspond to what&#39;s in them. Please keep this in mind for future assignments.åÊ</li></ul>
<p></p>
<p>If you have a regrade request or specific questions about your grades, you shouldåÊcome to office hours to discuss it.åÊ</p>
<p></p>
<p>#pin</p>",Takeaways &amp; Grades from &#34;Becoming a Requester&#34;,"<p>My code for get_misclassified_examples was:</p>
<p></p>
<pre>def get_misclassified_examples(y, X, texts) :
	x_train, x_test, y_train, y_test, train_texts, test_texts = train_test_split(X, y, texts)
	clf = train_classifier(x_train, y_train)

	for i, x in enumerate(x_test):
		expected_label = y_test[i]
		test_acc = test_classifier(clf, x, [expected_label])
		if expected_label == 0 and test_acc == 0.0:
			print test_texts[i]</pre>
<p>Where I tried to identify the false positive rows, by checking if the true value was 0, but the resulting prediction was 1 and hence the test_acc was 0.</p>
<p>I was deducted marks for this. Please let me know if I made a mistake here. Thank you!</p>",Clarification of grading for get_misclassified_examples,<p>Will we ever know about our homework grades?</p>,HW Grades,5
940847351,4/26/2016 17:08:43,false,1969446807,,4/26/2016 17:06:43,false,neodev,1.0,33973110,VEN,23,Maracaibo,186.94.238.104,0,,"<p>-3 A blank text file was submitted in lieu of code or diagrams. -0.5 features and dimensions of X not listed or incorrect. Most of questionnaire is blank. -1</p>
<p></p>
<p>I understand the last two deductions, but I am having trouble understanding where the -3 points came from.</p>
<p></p>
<ul><li>1 points - Create a rule based classifier</li><li>1 points - Create a decision tree and a decision tree diagram</li><li>1 points - Implement a statistical unigram model</li></ul>
<p></p>
<p>The rule based classifier is in the code under than method.</p>
<p>The decision tree and diagram is submitted as decision-tree.png and rule-based-tree.png</p>
<p>The unigram is implemented in get_features()</p>",hw3 grade,"<p>Hi Everyone!</p>
<p></p>
<p>We just sent everyone emails about your peer grading assignment. You are required to watch 3 videos and fill out a questionnaire for each video. You&#39;ll also be asked to rank the three videos in a separate questionnaire.</p>
<p>All the information you need is in that email. The email was sent from &#34;nets213&#64;seas.upenn.edu&#34;. Please check your spam in case you don&#39;t find the email in your inbox.</p>
<p></p>",Peer Grading Assignment,"<p>Hi everyone,</p>
<p></p>
<p>Grades for the first assignment have been emailed to your Penn email addresses. If you did not receive an email with your grade (and have checked your spam folder), please reach out to me immediately so we can resolve it. Overall, we were very happy with your participation and takeaways from your experience as crowdworkers. You can expect your grades for the second assignment very soon as well, and we encourage you to incorporate any feedback on your company profile into the video (Ellie&#39;s post on takeaways for the entire course will also be helpful for this).åÊ</p>
<p></p>
<p>If you have questions about your grade, please don&#39;t respond directly to the score email. You&#39;ll get a much better response time if you post privately to Piazza or email the entire teaching staff at <a href=""mailto:nets213&#64;googlegroups.com"">nets213-staff&#64;googlegroups.com</a>.åÊ</p>
<p></p>
<p>We&#39;ll be getting the next few assignments out to you as soon as possible so that you can assess how the class is going.åÊOne upcoming date to be aware of is the drop deadline on February 19th. If you are concerned about how you are doing at any point, please talk to a member of the teaching staff.åÊ</p>",Grades Released for Homework 1: Becoming a Crowdworker,"<p>Hi everyone,åÊ</p>
<p></p>
<p>Grades have been sent out for &#34;Becoming a Requester.&#34; If you haven&#39;t received the email, and it&#39;s not in your spam folder, you can complete the form <a href=""http://goo.gl/forms/dPv5TI7zuB"" target=""_blank"">here</a> and I will try to get your grades back to you by midnight (which is also the drop deadline).åÊ</p>
<p></p>
<p>Overall, you did very well with it and are now masters of crawling the internet. You gained some new tools that you should keep in mind for the final project (and the first part has been released, yay!).åÊ</p>
<p></p>
<p>We did see a few common misconceptions that we&#39;d like to clear up.åÊ</p>
<p></p>
<ul><li>The <strong>hourly wage</strong> referred to what the workers completing your job earned (you can view this value on Crowdflower&#39;s analytics tab). Without simply paying more per task, the other variable we could affect was the amount of time it takes to complete. Making the links clickable, scraping the text of the article and uploading that to Crowdflower instead of URLs, and providing clear, unambiguous directions to make deciding the label easier were all great ways, but there&#39;s a lot that we can do on this side.åÊ</li><li><strong>Precision</strong> is the ratio of true positives over all positives, or the probability that once your classifier has labeled an article as gun violence, it&#39;s actually about an instance of gun violence. This differed from <strong>accuracy</strong> because accuracy is the ratio of correct labels over all labels (true positives &#43; true negatives)/(all positives &#43; all negatives). Some of you submitted definitions from other fields, and we would encourage you to go back through and review the slides.åÊ</li><li><strong>Recall</strong> is the ratio of (true positives)/(true positives &#43; false negatives). For our data set, this was the probability that an article about gun violence would be given a positive label by our classifier. We couldn&#39;t calculate that because we only gave the Crowdflower workers positively-labeled data, and therefore can&#39;t draw any conclusions about the number of false negatives in our labels.åÊ</li><li>Within the <strong>Bing API</strong> piece, many of the titles submitted were &#34;News Result.&#34; This is a field in the Bing API, but doesn&#39;t correspond to the actual title of the article (or it would be very difficult to decide which articles to read).åÊ</li><li>There are a lot of sources of <strong>bias</strong> in the way that we collected data. One of our goals is to minimize this. It&#39;s easy to see that minor injuries are unlikely to be reported, that areas with less media coverage are less likely to be scraped (some of you started with the New York Times and the LA Times, but nobody checked any newspapers from the middle of nowhere), that reporting around gun violence can be sensationalized and since we aren&#39;t deduplicating results yet, there are likely repeat incidents that were reported on at multiple URLs you submitted. There are other sources too, and thinking about this is an important part of building a comprehensive system to measure anything (not just gun violence).åÊ</li><li>Some of you got really original with what you named the files that you submitted. While we appreciate your creative genius, it makes it a lot easier to grade when the names of files approximately correspond to what&#39;s in them. Please keep this in mind for future assignments.åÊ</li></ul>
<p></p>
<p>If you have a regrade request or specific questions about your grades, you shouldåÊcome to office hours to discuss it.åÊ</p>
<p></p>
<p>#pin</p>",Takeaways &amp; Grades from &#34;Becoming a Requester&#34;,"<p>My code for get_misclassified_examples was:</p>
<p></p>
<pre>def get_misclassified_examples(y, X, texts) :
	x_train, x_test, y_train, y_test, train_texts, test_texts = train_test_split(X, y, texts)
	clf = train_classifier(x_train, y_train)

	for i, x in enumerate(x_test):
		expected_label = y_test[i]
		test_acc = test_classifier(clf, x, [expected_label])
		if expected_label == 0 and test_acc == 0.0:
			print test_texts[i]</pre>
<p>Where I tried to identify the false positive rows, by checking if the true value was 0, but the resulting prediction was 1 and hence the test_acc was 0.</p>
<p>I was deducted marks for this. Please let me know if I made a mistake here. Thank you!</p>",Clarification of grading for get_misclassified_examples,<p>Will we ever know about our homework grades?</p>,HW Grades,5
940847351,4/26/2016 17:16:41,false,1969451755,,4/26/2016 17:14:02,false,elite,1.0,25411289,HRV,"","",31.147.119.175,"2
3
4",,"<p>-3 A blank text file was submitted in lieu of code or diagrams. -0.5 features and dimensions of X not listed or incorrect. Most of questionnaire is blank. -1</p>
<p></p>
<p>I understand the last two deductions, but I am having trouble understanding where the -3 points came from.</p>
<p></p>
<ul><li>1 points - Create a rule based classifier</li><li>1 points - Create a decision tree and a decision tree diagram</li><li>1 points - Implement a statistical unigram model</li></ul>
<p></p>
<p>The rule based classifier is in the code under than method.</p>
<p>The decision tree and diagram is submitted as decision-tree.png and rule-based-tree.png</p>
<p>The unigram is implemented in get_features()</p>",hw3 grade,"<p>Hi Everyone!</p>
<p></p>
<p>We just sent everyone emails about your peer grading assignment. You are required to watch 3 videos and fill out a questionnaire for each video. You&#39;ll also be asked to rank the three videos in a separate questionnaire.</p>
<p>All the information you need is in that email. The email was sent from &#34;nets213&#64;seas.upenn.edu&#34;. Please check your spam in case you don&#39;t find the email in your inbox.</p>
<p></p>",Peer Grading Assignment,"<p>Hi everyone,</p>
<p></p>
<p>Grades for the first assignment have been emailed to your Penn email addresses. If you did not receive an email with your grade (and have checked your spam folder), please reach out to me immediately so we can resolve it. Overall, we were very happy with your participation and takeaways from your experience as crowdworkers. You can expect your grades for the second assignment very soon as well, and we encourage you to incorporate any feedback on your company profile into the video (Ellie&#39;s post on takeaways for the entire course will also be helpful for this).åÊ</p>
<p></p>
<p>If you have questions about your grade, please don&#39;t respond directly to the score email. You&#39;ll get a much better response time if you post privately to Piazza or email the entire teaching staff at <a href=""mailto:nets213&#64;googlegroups.com"">nets213-staff&#64;googlegroups.com</a>.åÊ</p>
<p></p>
<p>We&#39;ll be getting the next few assignments out to you as soon as possible so that you can assess how the class is going.åÊOne upcoming date to be aware of is the drop deadline on February 19th. If you are concerned about how you are doing at any point, please talk to a member of the teaching staff.åÊ</p>",Grades Released for Homework 1: Becoming a Crowdworker,"<p>Hi everyone,åÊ</p>
<p></p>
<p>Grades have been sent out for &#34;Becoming a Requester.&#34; If you haven&#39;t received the email, and it&#39;s not in your spam folder, you can complete the form <a href=""http://goo.gl/forms/dPv5TI7zuB"" target=""_blank"">here</a> and I will try to get your grades back to you by midnight (which is also the drop deadline).åÊ</p>
<p></p>
<p>Overall, you did very well with it and are now masters of crawling the internet. You gained some new tools that you should keep in mind for the final project (and the first part has been released, yay!).åÊ</p>
<p></p>
<p>We did see a few common misconceptions that we&#39;d like to clear up.åÊ</p>
<p></p>
<ul><li>The <strong>hourly wage</strong> referred to what the workers completing your job earned (you can view this value on Crowdflower&#39;s analytics tab). Without simply paying more per task, the other variable we could affect was the amount of time it takes to complete. Making the links clickable, scraping the text of the article and uploading that to Crowdflower instead of URLs, and providing clear, unambiguous directions to make deciding the label easier were all great ways, but there&#39;s a lot that we can do on this side.åÊ</li><li><strong>Precision</strong> is the ratio of true positives over all positives, or the probability that once your classifier has labeled an article as gun violence, it&#39;s actually about an instance of gun violence. This differed from <strong>accuracy</strong> because accuracy is the ratio of correct labels over all labels (true positives &#43; true negatives)/(all positives &#43; all negatives). Some of you submitted definitions from other fields, and we would encourage you to go back through and review the slides.åÊ</li><li><strong>Recall</strong> is the ratio of (true positives)/(true positives &#43; false negatives). For our data set, this was the probability that an article about gun violence would be given a positive label by our classifier. We couldn&#39;t calculate that because we only gave the Crowdflower workers positively-labeled data, and therefore can&#39;t draw any conclusions about the number of false negatives in our labels.åÊ</li><li>Within the <strong>Bing API</strong> piece, many of the titles submitted were &#34;News Result.&#34; This is a field in the Bing API, but doesn&#39;t correspond to the actual title of the article (or it would be very difficult to decide which articles to read).åÊ</li><li>There are a lot of sources of <strong>bias</strong> in the way that we collected data. One of our goals is to minimize this. It&#39;s easy to see that minor injuries are unlikely to be reported, that areas with less media coverage are less likely to be scraped (some of you started with the New York Times and the LA Times, but nobody checked any newspapers from the middle of nowhere), that reporting around gun violence can be sensationalized and since we aren&#39;t deduplicating results yet, there are likely repeat incidents that were reported on at multiple URLs you submitted. There are other sources too, and thinking about this is an important part of building a comprehensive system to measure anything (not just gun violence).åÊ</li><li>Some of you got really original with what you named the files that you submitted. While we appreciate your creative genius, it makes it a lot easier to grade when the names of files approximately correspond to what&#39;s in them. Please keep this in mind for future assignments.åÊ</li></ul>
<p></p>
<p>If you have a regrade request or specific questions about your grades, you shouldåÊcome to office hours to discuss it.åÊ</p>
<p></p>
<p>#pin</p>",Takeaways &amp; Grades from &#34;Becoming a Requester&#34;,"<p>My code for get_misclassified_examples was:</p>
<p></p>
<pre>def get_misclassified_examples(y, X, texts) :
	x_train, x_test, y_train, y_test, train_texts, test_texts = train_test_split(X, y, texts)
	clf = train_classifier(x_train, y_train)

	for i, x in enumerate(x_test):
		expected_label = y_test[i]
		test_acc = test_classifier(clf, x, [expected_label])
		if expected_label == 0 and test_acc == 0.0:
			print test_texts[i]</pre>
<p>Where I tried to identify the false positive rows, by checking if the true value was 0, but the resulting prediction was 1 and hence the test_acc was 0.</p>
<p>I was deducted marks for this. Please let me know if I made a mistake here. Thank you!</p>",Clarification of grading for get_misclassified_examples,<p>Will we ever know about our homework grades?</p>,HW Grades,5
940847351,4/26/2016 17:30:30,false,1969459777,,4/26/2016 17:11:11,false,neodev,0.8889,19625264,DZA,41,Chlef,41.102.7.217,0,,"<p>-3 A blank text file was submitted in lieu of code or diagrams. -0.5 features and dimensions of X not listed or incorrect. Most of questionnaire is blank. -1</p>
<p></p>
<p>I understand the last two deductions, but I am having trouble understanding where the -3 points came from.</p>
<p></p>
<ul><li>1 points - Create a rule based classifier</li><li>1 points - Create a decision tree and a decision tree diagram</li><li>1 points - Implement a statistical unigram model</li></ul>
<p></p>
<p>The rule based classifier is in the code under than method.</p>
<p>The decision tree and diagram is submitted as decision-tree.png and rule-based-tree.png</p>
<p>The unigram is implemented in get_features()</p>",hw3 grade,"<p>Hi Everyone!</p>
<p></p>
<p>We just sent everyone emails about your peer grading assignment. You are required to watch 3 videos and fill out a questionnaire for each video. You&#39;ll also be asked to rank the three videos in a separate questionnaire.</p>
<p>All the information you need is in that email. The email was sent from &#34;nets213&#64;seas.upenn.edu&#34;. Please check your spam in case you don&#39;t find the email in your inbox.</p>
<p></p>",Peer Grading Assignment,"<p>Hi everyone,</p>
<p></p>
<p>Grades for the first assignment have been emailed to your Penn email addresses. If you did not receive an email with your grade (and have checked your spam folder), please reach out to me immediately so we can resolve it. Overall, we were very happy with your participation and takeaways from your experience as crowdworkers. You can expect your grades for the second assignment very soon as well, and we encourage you to incorporate any feedback on your company profile into the video (Ellie&#39;s post on takeaways for the entire course will also be helpful for this).åÊ</p>
<p></p>
<p>If you have questions about your grade, please don&#39;t respond directly to the score email. You&#39;ll get a much better response time if you post privately to Piazza or email the entire teaching staff at <a href=""mailto:nets213&#64;googlegroups.com"">nets213-staff&#64;googlegroups.com</a>.åÊ</p>
<p></p>
<p>We&#39;ll be getting the next few assignments out to you as soon as possible so that you can assess how the class is going.åÊOne upcoming date to be aware of is the drop deadline on February 19th. If you are concerned about how you are doing at any point, please talk to a member of the teaching staff.åÊ</p>",Grades Released for Homework 1: Becoming a Crowdworker,"<p>Hi everyone,åÊ</p>
<p></p>
<p>Grades have been sent out for &#34;Becoming a Requester.&#34; If you haven&#39;t received the email, and it&#39;s not in your spam folder, you can complete the form <a href=""http://goo.gl/forms/dPv5TI7zuB"" target=""_blank"">here</a> and I will try to get your grades back to you by midnight (which is also the drop deadline).åÊ</p>
<p></p>
<p>Overall, you did very well with it and are now masters of crawling the internet. You gained some new tools that you should keep in mind for the final project (and the first part has been released, yay!).åÊ</p>
<p></p>
<p>We did see a few common misconceptions that we&#39;d like to clear up.åÊ</p>
<p></p>
<ul><li>The <strong>hourly wage</strong> referred to what the workers completing your job earned (you can view this value on Crowdflower&#39;s analytics tab). Without simply paying more per task, the other variable we could affect was the amount of time it takes to complete. Making the links clickable, scraping the text of the article and uploading that to Crowdflower instead of URLs, and providing clear, unambiguous directions to make deciding the label easier were all great ways, but there&#39;s a lot that we can do on this side.åÊ</li><li><strong>Precision</strong> is the ratio of true positives over all positives, or the probability that once your classifier has labeled an article as gun violence, it&#39;s actually about an instance of gun violence. This differed from <strong>accuracy</strong> because accuracy is the ratio of correct labels over all labels (true positives &#43; true negatives)/(all positives &#43; all negatives). Some of you submitted definitions from other fields, and we would encourage you to go back through and review the slides.åÊ</li><li><strong>Recall</strong> is the ratio of (true positives)/(true positives &#43; false negatives). For our data set, this was the probability that an article about gun violence would be given a positive label by our classifier. We couldn&#39;t calculate that because we only gave the Crowdflower workers positively-labeled data, and therefore can&#39;t draw any conclusions about the number of false negatives in our labels.åÊ</li><li>Within the <strong>Bing API</strong> piece, many of the titles submitted were &#34;News Result.&#34; This is a field in the Bing API, but doesn&#39;t correspond to the actual title of the article (or it would be very difficult to decide which articles to read).åÊ</li><li>There are a lot of sources of <strong>bias</strong> in the way that we collected data. One of our goals is to minimize this. It&#39;s easy to see that minor injuries are unlikely to be reported, that areas with less media coverage are less likely to be scraped (some of you started with the New York Times and the LA Times, but nobody checked any newspapers from the middle of nowhere), that reporting around gun violence can be sensationalized and since we aren&#39;t deduplicating results yet, there are likely repeat incidents that were reported on at multiple URLs you submitted. There are other sources too, and thinking about this is an important part of building a comprehensive system to measure anything (not just gun violence).åÊ</li><li>Some of you got really original with what you named the files that you submitted. While we appreciate your creative genius, it makes it a lot easier to grade when the names of files approximately correspond to what&#39;s in them. Please keep this in mind for future assignments.åÊ</li></ul>
<p></p>
<p>If you have a regrade request or specific questions about your grades, you shouldåÊcome to office hours to discuss it.åÊ</p>
<p></p>
<p>#pin</p>",Takeaways &amp; Grades from &#34;Becoming a Requester&#34;,"<p>My code for get_misclassified_examples was:</p>
<p></p>
<pre>def get_misclassified_examples(y, X, texts) :
	x_train, x_test, y_train, y_test, train_texts, test_texts = train_test_split(X, y, texts)
	clf = train_classifier(x_train, y_train)

	for i, x in enumerate(x_test):
		expected_label = y_test[i]
		test_acc = test_classifier(clf, x, [expected_label])
		if expected_label == 0 and test_acc == 0.0:
			print test_texts[i]</pre>
<p>Where I tried to identify the false positive rows, by checking if the true value was 0, but the resulting prediction was 1 and hence the test_acc was 0.</p>
<p>I was deducted marks for this. Please let me know if I made a mistake here. Thank you!</p>",Clarification of grading for get_misclassified_examples,<p>Will we ever know about our homework grades?</p>,HW Grades,5
940847352,4/26/2016 16:01:37,false,1969404539,,4/26/2016 16:00:00,false,elite,1.0,33243069,IND,10,Faridabad,116.203.79.150,0,,<p>How can we findåÊhourly wage through CrowdFlower? Do we need to estimate it or there is a section that gives a precise number?</p>,Hourly Wage on CrowdFlower,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,"<p>Based on the instructions, the sample.txt file that we upload to CloudFlower should contain 500 positively labelled articles. In the screenshot at step 4 however the urls have a mix of both 0 and 1 labels. Which one is correct? And more generally, why are we having only the positive articles being labelled by crowdworkers?</p>",CrowdFlower &#34;sample.txt&#34; Clarification,<p>My crowdflower account doesn&#39;t have any funds yet ÛÓ is it possible to share accounts just for this assignment?</p>,sharing crowdflower,No one has completed my request in the last 5 hours and it is still only 20% completed. Is there any possible way to update my task such that it becomes more popular to contributors?,No More Contributors in CrowdFlower,should we submit the csvs as well?,crowdflower csvs,0
940847352,4/26/2016 16:02:20,false,1969404865,,4/26/2016 16:01:47,false,personaly,1.0,33663352,ARG,01,Mar Del Plata,181.168.213.227,0,,<p>How can we findåÊhourly wage through CrowdFlower? Do we need to estimate it or there is a section that gives a precise number?</p>,Hourly Wage on CrowdFlower,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,"<p>Based on the instructions, the sample.txt file that we upload to CloudFlower should contain 500 positively labelled articles. In the screenshot at step 4 however the urls have a mix of both 0 and 1 labels. Which one is correct? And more generally, why are we having only the positive articles being labelled by crowdworkers?</p>",CrowdFlower &#34;sample.txt&#34; Clarification,<p>My crowdflower account doesn&#39;t have any funds yet ÛÓ is it possible to share accounts just for this assignment?</p>,sharing crowdflower,No one has completed my request in the last 5 hours and it is still only 20% completed. Is there any possible way to update my task such that it becomes more popular to contributors?,No More Contributors in CrowdFlower,should we submit the csvs as well?,crowdflower csvs,0
940847352,4/26/2016 16:12:58,false,1969410166,,4/26/2016 16:10:21,false,clixsense,0.8889,8057247,PRT,17,Póvoa De Varzim,144.64.25.68,0,,<p>How can we findåÊhourly wage through CrowdFlower? Do we need to estimate it or there is a section that gives a precise number?</p>,Hourly Wage on CrowdFlower,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,"<p>Based on the instructions, the sample.txt file that we upload to CloudFlower should contain 500 positively labelled articles. In the screenshot at step 4 however the urls have a mix of both 0 and 1 labels. Which one is correct? And more generally, why are we having only the positive articles being labelled by crowdworkers?</p>",CrowdFlower &#34;sample.txt&#34; Clarification,<p>My crowdflower account doesn&#39;t have any funds yet ÛÓ is it possible to share accounts just for this assignment?</p>,sharing crowdflower,No one has completed my request in the last 5 hours and it is still only 20% completed. Is there any possible way to update my task such that it becomes more popular to contributors?,No More Contributors in CrowdFlower,should we submit the csvs as well?,crowdflower csvs,0
940847352,4/26/2016 16:40:25,false,1969429701,,4/26/2016 16:39:20,false,clixsense,1.0,6329782,IDN,07,Bekonang,202.67.40.31,0,,<p>How can we findåÊhourly wage through CrowdFlower? Do we need to estimate it or there is a section that gives a precise number?</p>,Hourly Wage on CrowdFlower,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,"<p>Based on the instructions, the sample.txt file that we upload to CloudFlower should contain 500 positively labelled articles. In the screenshot at step 4 however the urls have a mix of both 0 and 1 labels. Which one is correct? And more generally, why are we having only the positive articles being labelled by crowdworkers?</p>",CrowdFlower &#34;sample.txt&#34; Clarification,<p>My crowdflower account doesn&#39;t have any funds yet ÛÓ is it possible to share accounts just for this assignment?</p>,sharing crowdflower,No one has completed my request in the last 5 hours and it is still only 20% completed. Is there any possible way to update my task such that it becomes more popular to contributors?,No More Contributors in CrowdFlower,should we submit the csvs as well?,crowdflower csvs,0
940847352,4/26/2016 17:22:13,false,1969454942,,4/26/2016 17:19:06,false,elite,1.0,25411289,HRV,"","",31.147.119.175,0,,<p>How can we findåÊhourly wage through CrowdFlower? Do we need to estimate it or there is a section that gives a precise number?</p>,Hourly Wage on CrowdFlower,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,"<p>Based on the instructions, the sample.txt file that we upload to CloudFlower should contain 500 positively labelled articles. In the screenshot at step 4 however the urls have a mix of both 0 and 1 labels. Which one is correct? And more generally, why are we having only the positive articles being labelled by crowdworkers?</p>",CrowdFlower &#34;sample.txt&#34; Clarification,<p>My crowdflower account doesn&#39;t have any funds yet ÛÓ is it possible to share accounts just for this assignment?</p>,sharing crowdflower,No one has completed my request in the last 5 hours and it is still only 20% completed. Is there any possible way to update my task such that it becomes more popular to contributors?,No More Contributors in CrowdFlower,should we submit the csvs as well?,crowdflower csvs,0
940847353,4/26/2016 16:01:37,false,1969404536,,4/26/2016 16:00:00,false,elite,1.0,33243069,IND,10,Faridabad,116.203.79.150,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>In computing Kendall&#39;s tau function, are we taking in two lists of worker quality from two of the three data sets and compare? Does the worker quality in the two lists need to be sorted? In other words, if in data set 1, workeråÊ1&#39;s quality is 0.9 and worker 2&#39;s quality is 0.8. And in another data set, worker 1&#39;s quality is 0.95 and worker 2&#39;s quality is 0.85, do we have to compare [0.9, 0.8] with [0.95, 0.85] or can we compare [0.9, 0.8] with [0.85, 0.95]?</p>",KendallÛªs tau,2
940847353,4/26/2016 16:02:20,false,1969404862,,4/26/2016 16:01:47,false,personaly,1.0,33663352,ARG,01,Mar Del Plata,181.168.213.227,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>In computing Kendall&#39;s tau function, are we taking in two lists of worker quality from two of the three data sets and compare? Does the worker quality in the two lists need to be sorted? In other words, if in data set 1, workeråÊ1&#39;s quality is 0.9 and worker 2&#39;s quality is 0.8. And in another data set, worker 1&#39;s quality is 0.95 and worker 2&#39;s quality is 0.85, do we have to compare [0.9, 0.8] with [0.95, 0.85] or can we compare [0.9, 0.8] with [0.85, 0.95]?</p>",KendallÛªs tau,2
940847353,4/26/2016 16:12:58,false,1969410160,,4/26/2016 16:10:21,false,clixsense,0.8889,8057247,PRT,17,Póvoa De Varzim,144.64.25.68,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>In computing Kendall&#39;s tau function, are we taking in two lists of worker quality from two of the three data sets and compare? Does the worker quality in the two lists need to be sorted? In other words, if in data set 1, workeråÊ1&#39;s quality is 0.9 and worker 2&#39;s quality is 0.8. And in another data set, worker 1&#39;s quality is 0.95 and worker 2&#39;s quality is 0.85, do we have to compare [0.9, 0.8] with [0.95, 0.85] or can we compare [0.9, 0.8] with [0.85, 0.95]?</p>",KendallÛªs tau,2
940847353,4/26/2016 16:40:25,false,1969429697,,4/26/2016 16:39:20,false,clixsense,1.0,6329782,IDN,07,Bekonang,202.67.40.31,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>In computing Kendall&#39;s tau function, are we taking in two lists of worker quality from two of the three data sets and compare? Does the worker quality in the two lists need to be sorted? In other words, if in data set 1, workeråÊ1&#39;s quality is 0.9 and worker 2&#39;s quality is 0.8. And in another data set, worker 1&#39;s quality is 0.95 and worker 2&#39;s quality is 0.85, do we have to compare [0.9, 0.8] with [0.95, 0.85] or can we compare [0.9, 0.8] with [0.85, 0.95]?</p>",KendallÛªs tau,2
940847353,4/26/2016 17:22:13,false,1969454941,,4/26/2016 17:19:06,false,elite,1.0,25411289,HRV,"","",31.147.119.175,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>In computing Kendall&#39;s tau function, are we taking in two lists of worker quality from two of the three data sets and compare? Does the worker quality in the two lists need to be sorted? In other words, if in data set 1, workeråÊ1&#39;s quality is 0.9 and worker 2&#39;s quality is 0.8. And in another data set, worker 1&#39;s quality is 0.95 and worker 2&#39;s quality is 0.85, do we have to compare [0.9, 0.8] with [0.95, 0.85] or can we compare [0.9, 0.8] with [0.85, 0.95]?</p>",KendallÛªs tau,2
940847354,4/26/2016 17:27:57,false,1969458129,,4/26/2016 17:23:11,false,neodev,1.0,33973110,VEN,23,Maracaibo,186.94.238.104,0,,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"I had about 3200 urls when I ran the get_clean_text.py on the biglab machines, but when I counted the number of lines of the file obtained after running BeautifulSoup, I only have 1379 lines (i.e articles and urls). Is this normal, or should I scrape the urls again? I believe a lot of urls&#39; articles didn&#39;t get scraped because of exception handling in get_clean_text.py",Number of articles and urls is significantly lesser than urls,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,5
940847354,4/26/2016 17:32:16,false,1969460787,,4/26/2016 17:31:58,false,neodev,0.8889,33131546,IDN,04,Jakarta,139.194.89.60,0,,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"I had about 3200 urls when I ran the get_clean_text.py on the biglab machines, but when I counted the number of lines of the file obtained after running BeautifulSoup, I only have 1379 lines (i.e articles and urls). Is this normal, or should I scrape the urls again? I believe a lot of urls&#39; articles didn&#39;t get scraped because of exception handling in get_clean_text.py",Number of articles and urls is significantly lesser than urls,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,5
940847354,4/26/2016 17:35:19,false,1969462338,,4/26/2016 17:34:26,false,clixsense,1.0,30712378,ROU,21,Deva,79.119.241.200,0,,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"I had about 3200 urls when I ran the get_clean_text.py on the biglab machines, but when I counted the number of lines of the file obtained after running BeautifulSoup, I only have 1379 lines (i.e articles and urls). Is this normal, or should I scrape the urls again? I believe a lot of urls&#39; articles didn&#39;t get scraped because of exception handling in get_clean_text.py",Number of articles and urls is significantly lesser than urls,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,5
940847354,4/26/2016 18:27:49,false,1969490436,,4/26/2016 18:04:06,false,clixsense,0.8889,35338593,ITA,14,Cagliari,151.56.132.145,0,,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"I had about 3200 urls when I ran the get_clean_text.py on the biglab machines, but when I counted the number of lines of the file obtained after running BeautifulSoup, I only have 1379 lines (i.e articles and urls). Is this normal, or should I scrape the urls again? I believe a lot of urls&#39; articles didn&#39;t get scraped because of exception handling in get_clean_text.py",Number of articles and urls is significantly lesser than urls,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,5
940847354,4/26/2016 18:41:46,false,1969497906,,4/26/2016 18:39:26,false,neodev,1.0,29879245,RUS,69,Smolensk,37.144.124.118,0,,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"I had about 3200 urls when I ran the get_clean_text.py on the biglab machines, but when I counted the number of lines of the file obtained after running BeautifulSoup, I only have 1379 lines (i.e articles and urls). Is this normal, or should I scrape the urls again? I believe a lot of urls&#39; articles didn&#39;t get scraped because of exception handling in get_clean_text.py",Number of articles and urls is significantly lesser than urls,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,5
940847355,4/26/2016 15:12:15,false,1969364811,,4/26/2016 15:12:00,false,tremorgames,1.0,32635967,LTU,60,Panevezys,78.63.38.165,0,,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p></p><pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p>When I run the code, I keep getting this error. I have tried looking at my API Key and seeing if I have an extra space in the API key field, but it looks perfectly fine to me.åÊ</p>",Error on running Bing API.py,"<p>I wanted to resubmit my project to include the .md file. But whenever I try to submit I get this error:</p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8n7u3ifw4mc/il5eikvnqd55/Screen_Shot_20160227_at_12.27.14_PM.png"" /></p>
<p></p>
<p>Is it ok if I email my directory to a TA? I don&#39;t really know how to fix this issue.</p>
<p></p>
<p>Best,</p>
<p>Ben</p>
<p></p>",Error on resubmission,3
940847355,4/26/2016 15:20:55,false,1969378543,,4/26/2016 15:20:31,false,neodev,1.0,19132694,LKA,36,Colombo,123.231.124.170,0,,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p></p><pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p>When I run the code, I keep getting this error. I have tried looking at my API Key and seeing if I have an extra space in the API key field, but it looks perfectly fine to me.åÊ</p>",Error on running Bing API.py,"<p>I wanted to resubmit my project to include the .md file. But whenever I try to submit I get this error:</p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8n7u3ifw4mc/il5eikvnqd55/Screen_Shot_20160227_at_12.27.14_PM.png"" /></p>
<p></p>
<p>Is it ok if I email my directory to a TA? I don&#39;t really know how to fix this issue.</p>
<p></p>
<p>Best,</p>
<p>Ben</p>
<p></p>",Error on resubmission,3
940847355,4/26/2016 15:23:37,false,1969383211,,4/26/2016 15:20:30,false,clixsense,1.0,24287706,TWN,04,Keelung,61.231.195.173,0,,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p></p><pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p>When I run the code, I keep getting this error. I have tried looking at my API Key and seeing if I have an extra space in the API key field, but it looks perfectly fine to me.åÊ</p>",Error on running Bing API.py,"<p>I wanted to resubmit my project to include the .md file. But whenever I try to submit I get this error:</p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8n7u3ifw4mc/il5eikvnqd55/Screen_Shot_20160227_at_12.27.14_PM.png"" /></p>
<p></p>
<p>Is it ok if I email my directory to a TA? I don&#39;t really know how to fix this issue.</p>
<p></p>
<p>Best,</p>
<p>Ben</p>
<p></p>",Error on resubmission,3
940847355,4/26/2016 15:26:30,false,1969388025,,4/26/2016 15:24:24,false,clixsense,1.0,7837812,SRB,00,Belgrade,79.101.254.233,0,,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p></p><pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p>When I run the code, I keep getting this error. I have tried looking at my API Key and seeing if I have an extra space in the API key field, but it looks perfectly fine to me.åÊ</p>",Error on running Bing API.py,"<p>I wanted to resubmit my project to include the .md file. But whenever I try to submit I get this error:</p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8n7u3ifw4mc/il5eikvnqd55/Screen_Shot_20160227_at_12.27.14_PM.png"" /></p>
<p></p>
<p>Is it ok if I email my directory to a TA? I don&#39;t really know how to fix this issue.</p>
<p></p>
<p>Best,</p>
<p>Ben</p>
<p></p>",Error on resubmission,3
940847355,4/26/2016 15:26:56,false,1969388627,,4/26/2016 15:26:14,false,instagc,0.8889,13581319,USA,IL,Waltonville,208.70.36.12,0,,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p></p><pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p>When I run the code, I keep getting this error. I have tried looking at my API Key and seeing if I have an extra space in the API key field, but it looks perfectly fine to me.åÊ</p>",Error on running Bing API.py,"<p>I wanted to resubmit my project to include the .md file. But whenever I try to submit I get this error:</p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8n7u3ifw4mc/il5eikvnqd55/Screen_Shot_20160227_at_12.27.14_PM.png"" /></p>
<p></p>
<p>Is it ok if I email my directory to a TA? I don&#39;t really know how to fix this issue.</p>
<p></p>
<p>Best,</p>
<p>Ben</p>
<p></p>",Error on resubmission,3
940847356,4/26/2016 16:03:21,false,1969405342,,4/26/2016 16:02:52,false,personaly,1.0,33663352,ARG,01,Mar Del Plata,181.168.213.227,0,,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"I had about 3200 urls when I ran the get_clean_text.py on the biglab machines, but when I counted the number of lines of the file obtained after running BeautifulSoup, I only have 1379 lines (i.e articles and urls). Is this normal, or should I scrape the urls again? I believe a lot of urls&#39; articles didn&#39;t get scraped because of exception handling in get_clean_text.py",Number of articles and urls is significantly lesser than urls,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,1
940847356,4/26/2016 16:24:34,false,1969419055,,4/26/2016 16:23:47,false,neodev,1.0,29175140,VEN,25,Caracas,190.72.125.134,0,,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"I had about 3200 urls when I ran the get_clean_text.py on the biglab machines, but when I counted the number of lines of the file obtained after running BeautifulSoup, I only have 1379 lines (i.e articles and urls). Is this normal, or should I scrape the urls again? I believe a lot of urls&#39; articles didn&#39;t get scraped because of exception handling in get_clean_text.py",Number of articles and urls is significantly lesser than urls,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,1
940847356,4/26/2016 16:34:40,false,1969426095,,4/26/2016 16:32:37,false,clixsense,0.8889,8057247,PRT,17,Póvoa De Varzim,144.64.25.68,0,,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"I had about 3200 urls when I ran the get_clean_text.py on the biglab machines, but when I counted the number of lines of the file obtained after running BeautifulSoup, I only have 1379 lines (i.e articles and urls). Is this normal, or should I scrape the urls again? I believe a lot of urls&#39; articles didn&#39;t get scraped because of exception handling in get_clean_text.py",Number of articles and urls is significantly lesser than urls,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,1
940847356,4/26/2016 16:41:30,false,1969430410,,4/26/2016 16:40:27,false,clixsense,1.0,6329782,IDN,07,Bekonang,202.67.40.31,0,,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"I had about 3200 urls when I ran the get_clean_text.py on the biglab machines, but when I counted the number of lines of the file obtained after running BeautifulSoup, I only have 1379 lines (i.e articles and urls). Is this normal, or should I scrape the urls again? I believe a lot of urls&#39; articles didn&#39;t get scraped because of exception handling in get_clean_text.py",Number of articles and urls is significantly lesser than urls,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,1
940847356,4/26/2016 16:58:38,false,1969440702,,4/26/2016 16:56:43,false,clixsense,1.0,21408115,IDN,07,Semarang,36.79.23.180,0,,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"I had about 3200 urls when I ran the get_clean_text.py on the biglab machines, but when I counted the number of lines of the file obtained after running BeautifulSoup, I only have 1379 lines (i.e articles and urls). Is this normal, or should I scrape the urls again? I believe a lot of urls&#39; articles didn&#39;t get scraped because of exception handling in get_clean_text.py",Number of articles and urls is significantly lesser than urls,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,1
940847357,4/26/2016 15:11:59,false,1969364454,,4/26/2016 15:11:46,false,tremorgames,1.0,32635967,LTU,60,Panevezys,78.63.38.165,0,,<p>How can we findåÊhourly wage through CrowdFlower? Do we need to estimate it or there is a section that gives a precise number?</p>,Hourly Wage on CrowdFlower,<p>My crowdflower account doesn&#39;t have any funds yet ÛÓ is it possible to share accounts just for this assignment?</p>,sharing crowdflower,No one has completed my request in the last 5 hours and it is still only 20% completed. Is there any possible way to update my task such that it becomes more popular to contributors?,No More Contributors in CrowdFlower,"<p>For homework 6, me and my partner are replicating one of the tasks described in theåÊ<a href=""http://crowdsourcing-class.org/readings/downloads/programming/iterative-and-parallel-processing-in-hcomp.pdf"">Exploring Iterative and Parallel Human Computation Processes</a>åÊarticle. After running the iterative and parallel processes, one of the last things to do is to create rating tasks in order to make further conclusions. However, this will cost us around $50. Do you think it would be okay if we just make our own analysis and conclusions from the data we currently have?åÊ</p>",Homework 6: Crowdflower rating task,"<p>I launched a task on Crowdflower around 3:30 and got ~200 responses within the first hour. However, it&#39;s been stalled at 222 judgements for the past hour or so. Any ideas why this might have happened?</p>",Crowdflower task stalled,"<p>Based on the instructions, the sample.txt file that we upload to CloudFlower should contain 500 positively labelled articles. In the screenshot at step 4 however the urls have a mix of both 0 and 1 labels. Which one is correct? And more generally, why are we having only the positive articles being labelled by crowdworkers?</p>",CrowdFlower &#34;sample.txt&#34; Clarification,5
940847357,4/26/2016 15:20:28,false,1969377831,,4/26/2016 15:19:15,false,clixsense,1.0,24287706,TWN,04,Keelung,61.231.195.173,0,,<p>How can we findåÊhourly wage through CrowdFlower? Do we need to estimate it or there is a section that gives a precise number?</p>,Hourly Wage on CrowdFlower,<p>My crowdflower account doesn&#39;t have any funds yet ÛÓ is it possible to share accounts just for this assignment?</p>,sharing crowdflower,No one has completed my request in the last 5 hours and it is still only 20% completed. Is there any possible way to update my task such that it becomes more popular to contributors?,No More Contributors in CrowdFlower,"<p>For homework 6, me and my partner are replicating one of the tasks described in theåÊ<a href=""http://crowdsourcing-class.org/readings/downloads/programming/iterative-and-parallel-processing-in-hcomp.pdf"">Exploring Iterative and Parallel Human Computation Processes</a>åÊarticle. After running the iterative and parallel processes, one of the last things to do is to create rating tasks in order to make further conclusions. However, this will cost us around $50. Do you think it would be okay if we just make our own analysis and conclusions from the data we currently have?åÊ</p>",Homework 6: Crowdflower rating task,"<p>I launched a task on Crowdflower around 3:30 and got ~200 responses within the first hour. However, it&#39;s been stalled at 222 judgements for the past hour or so. Any ideas why this might have happened?</p>",Crowdflower task stalled,"<p>Based on the instructions, the sample.txt file that we upload to CloudFlower should contain 500 positively labelled articles. In the screenshot at step 4 however the urls have a mix of both 0 and 1 labels. Which one is correct? And more generally, why are we having only the positive articles being labelled by crowdworkers?</p>",CrowdFlower &#34;sample.txt&#34; Clarification,5
940847357,4/26/2016 15:20:29,false,1969377851,,4/26/2016 15:20:10,false,neodev,1.0,19132694,LKA,36,Colombo,123.231.124.170,0,,<p>How can we findåÊhourly wage through CrowdFlower? Do we need to estimate it or there is a section that gives a precise number?</p>,Hourly Wage on CrowdFlower,<p>My crowdflower account doesn&#39;t have any funds yet ÛÓ is it possible to share accounts just for this assignment?</p>,sharing crowdflower,No one has completed my request in the last 5 hours and it is still only 20% completed. Is there any possible way to update my task such that it becomes more popular to contributors?,No More Contributors in CrowdFlower,"<p>For homework 6, me and my partner are replicating one of the tasks described in theåÊ<a href=""http://crowdsourcing-class.org/readings/downloads/programming/iterative-and-parallel-processing-in-hcomp.pdf"">Exploring Iterative and Parallel Human Computation Processes</a>åÊarticle. After running the iterative and parallel processes, one of the last things to do is to create rating tasks in order to make further conclusions. However, this will cost us around $50. Do you think it would be okay if we just make our own analysis and conclusions from the data we currently have?åÊ</p>",Homework 6: Crowdflower rating task,"<p>I launched a task on Crowdflower around 3:30 and got ~200 responses within the first hour. However, it&#39;s been stalled at 222 judgements for the past hour or so. Any ideas why this might have happened?</p>",Crowdflower task stalled,"<p>Based on the instructions, the sample.txt file that we upload to CloudFlower should contain 500 positively labelled articles. In the screenshot at step 4 however the urls have a mix of both 0 and 1 labels. Which one is correct? And more generally, why are we having only the positive articles being labelled by crowdworkers?</p>",CrowdFlower &#34;sample.txt&#34; Clarification,5
940847357,4/26/2016 15:24:23,false,1969384419,,4/26/2016 15:21:38,false,clixsense,1.0,7837812,SRB,00,Belgrade,79.101.254.233,0,,<p>How can we findåÊhourly wage through CrowdFlower? Do we need to estimate it or there is a section that gives a precise number?</p>,Hourly Wage on CrowdFlower,<p>My crowdflower account doesn&#39;t have any funds yet ÛÓ is it possible to share accounts just for this assignment?</p>,sharing crowdflower,No one has completed my request in the last 5 hours and it is still only 20% completed. Is there any possible way to update my task such that it becomes more popular to contributors?,No More Contributors in CrowdFlower,"<p>For homework 6, me and my partner are replicating one of the tasks described in theåÊ<a href=""http://crowdsourcing-class.org/readings/downloads/programming/iterative-and-parallel-processing-in-hcomp.pdf"">Exploring Iterative and Parallel Human Computation Processes</a>åÊarticle. After running the iterative and parallel processes, one of the last things to do is to create rating tasks in order to make further conclusions. However, this will cost us around $50. Do you think it would be okay if we just make our own analysis and conclusions from the data we currently have?åÊ</p>",Homework 6: Crowdflower rating task,"<p>I launched a task on Crowdflower around 3:30 and got ~200 responses within the first hour. However, it&#39;s been stalled at 222 judgements for the past hour or so. Any ideas why this might have happened?</p>",Crowdflower task stalled,"<p>Based on the instructions, the sample.txt file that we upload to CloudFlower should contain 500 positively labelled articles. In the screenshot at step 4 however the urls have a mix of both 0 and 1 labels. Which one is correct? And more generally, why are we having only the positive articles being labelled by crowdworkers?</p>",CrowdFlower &#34;sample.txt&#34; Clarification,5
940847357,4/26/2016 15:27:16,false,1969389144,,4/26/2016 15:24:27,false,elite,1.0,30280423,ITA,15,Siracusa,151.54.84.121,0,,<p>How can we findåÊhourly wage through CrowdFlower? Do we need to estimate it or there is a section that gives a precise number?</p>,Hourly Wage on CrowdFlower,<p>My crowdflower account doesn&#39;t have any funds yet ÛÓ is it possible to share accounts just for this assignment?</p>,sharing crowdflower,No one has completed my request in the last 5 hours and it is still only 20% completed. Is there any possible way to update my task such that it becomes more popular to contributors?,No More Contributors in CrowdFlower,"<p>For homework 6, me and my partner are replicating one of the tasks described in theåÊ<a href=""http://crowdsourcing-class.org/readings/downloads/programming/iterative-and-parallel-processing-in-hcomp.pdf"">Exploring Iterative and Parallel Human Computation Processes</a>åÊarticle. After running the iterative and parallel processes, one of the last things to do is to create rating tasks in order to make further conclusions. However, this will cost us around $50. Do you think it would be okay if we just make our own analysis and conclusions from the data we currently have?åÊ</p>",Homework 6: Crowdflower rating task,"<p>I launched a task on Crowdflower around 3:30 and got ~200 responses within the first hour. However, it&#39;s been stalled at 222 judgements for the past hour or so. Any ideas why this might have happened?</p>",Crowdflower task stalled,"<p>Based on the instructions, the sample.txt file that we upload to CloudFlower should contain 500 positively labelled articles. In the screenshot at step 4 however the urls have a mix of both 0 and 1 labels. Which one is correct? And more generally, why are we having only the positive articles being labelled by crowdworkers?</p>",CrowdFlower &#34;sample.txt&#34; Clarification,5
940847358,4/26/2016 17:40:29,false,1969465255,,4/26/2016 17:39:38,false,neodev,0.8889,33568303,VEN,23,Cabimas,190.77.7.36,0,,"<p>The slides for today&#39;s lecture topics are now online:</p>
<p><a href=""http://crowdsourcing-class.org/slides/machine-learning-part-2.pdf"">Machine Learning - part 2</a></p>
<p><a href=""http://crowdsourcing-class.org/slides/amazon-mechanical-turk.pdf"">The Amazon Mechanical Turk crowdsourcing platform</a></p>
<p></p>
<p>Please post any questions that you have about either topic to this thread.åÊ</p>",Slides for today&#39;s lecture,"<p>If you&#39;d like to discussåÊAl Filries&#39; lecture about MOOC&#39;s, let&#39;s use this discussion thread.</p>
<p></p>
<p>Here&#39;s a link toåÊhis <a href=""https://www.coursera.org/course/modernpoetry"" target=""_blank"">ModPo Coursera course</a>.</p>
<p></p>",Al Filries&#39; lecture,"<p>On Fridays I&#39;m planning on doing more applied hands-on lectures where the TAs and I walk you through the homework assignments and help you get started. åÊHow did you like today&#39;s lecture?</p>
<p></p>
<p>Feel free to leave comments on things you would like to see in the Friday lectures, after you have voted.</p>
<br/> [o] Today&#39;s lecture was good, and helped me understand the assignment
[o] I didn&#39;t find it particularly useful, or I would have preferred a standard non-applied lecture",Poll: Friday hands-on lectures,"<p>Hi everyone,</p>
<p></p>
<p>Just a friendly reminder that you&#39;ll get credit towards your participation grade for attending today&#39;s NETS 213 lecture. åÊThe TAs will be passing out unique participation codes, one per student, as you leave the classroom. åÊBe sure to get one, and then enter it <a href=""https://docs.google.com/forms/d/14UZWosW5_W_-qDNI8KJ_zUGkiyTO9yuwv7yCkCvuZgQ/viewform"" target=""_blank"">here</a>åÊafter class.</p>
<p></p>
<p>--Chris</p>",Reminder: participation credit for attending today&#39;s NETS 213 lecture,<p>There will be a guest lecturer in NETS 213 today. åÊPlease attend the lecture. åÊWe mightåÊtake attendance.åÊ</p>,Guest lecture today,"For tomorrow&#39;s lecture, I have invited Professor Doug Weibe of the School of Medicine to give a talk about studying gun violence from a public health perspective. The goal is to contextualize why we are focused on gun violence in our homework assignments. åÊDoug is an epidemiologist whose research will hopefully be facilitated by the data that we are collecting via crowdsourcing.åÊ<div><br /></div><div>Here is more about Doug:<div>http://www.upennprc.org/people/douglas-wiebe/</div></div><div><br /></div><div>Please attend tomorrow&#39;s lecture. It will count towards your participation grade.åÊ</div>",guest lecture in NETS 213 tomorrow,3
940847358,4/26/2016 17:50:21,false,1969470891,,4/26/2016 17:49:33,false,clixsense,1.0,35444326,BRA,07,Brasília,177.15.130.106,0,,"<p>The slides for today&#39;s lecture topics are now online:</p>
<p><a href=""http://crowdsourcing-class.org/slides/machine-learning-part-2.pdf"">Machine Learning - part 2</a></p>
<p><a href=""http://crowdsourcing-class.org/slides/amazon-mechanical-turk.pdf"">The Amazon Mechanical Turk crowdsourcing platform</a></p>
<p></p>
<p>Please post any questions that you have about either topic to this thread.åÊ</p>",Slides for today&#39;s lecture,"<p>If you&#39;d like to discussåÊAl Filries&#39; lecture about MOOC&#39;s, let&#39;s use this discussion thread.</p>
<p></p>
<p>Here&#39;s a link toåÊhis <a href=""https://www.coursera.org/course/modernpoetry"" target=""_blank"">ModPo Coursera course</a>.</p>
<p></p>",Al Filries&#39; lecture,"<p>On Fridays I&#39;m planning on doing more applied hands-on lectures where the TAs and I walk you through the homework assignments and help you get started. åÊHow did you like today&#39;s lecture?</p>
<p></p>
<p>Feel free to leave comments on things you would like to see in the Friday lectures, after you have voted.</p>
<br/> [o] Today&#39;s lecture was good, and helped me understand the assignment
[o] I didn&#39;t find it particularly useful, or I would have preferred a standard non-applied lecture",Poll: Friday hands-on lectures,"<p>Hi everyone,</p>
<p></p>
<p>Just a friendly reminder that you&#39;ll get credit towards your participation grade for attending today&#39;s NETS 213 lecture. åÊThe TAs will be passing out unique participation codes, one per student, as you leave the classroom. åÊBe sure to get one, and then enter it <a href=""https://docs.google.com/forms/d/14UZWosW5_W_-qDNI8KJ_zUGkiyTO9yuwv7yCkCvuZgQ/viewform"" target=""_blank"">here</a>åÊafter class.</p>
<p></p>
<p>--Chris</p>",Reminder: participation credit for attending today&#39;s NETS 213 lecture,<p>There will be a guest lecturer in NETS 213 today. åÊPlease attend the lecture. åÊWe mightåÊtake attendance.åÊ</p>,Guest lecture today,"For tomorrow&#39;s lecture, I have invited Professor Doug Weibe of the School of Medicine to give a talk about studying gun violence from a public health perspective. The goal is to contextualize why we are focused on gun violence in our homework assignments. åÊDoug is an epidemiologist whose research will hopefully be facilitated by the data that we are collecting via crowdsourcing.åÊ<div><br /></div><div>Here is more about Doug:<div>http://www.upennprc.org/people/douglas-wiebe/</div></div><div><br /></div><div>Please attend tomorrow&#39;s lecture. It will count towards your participation grade.åÊ</div>",guest lecture in NETS 213 tomorrow,3
940847358,4/26/2016 18:20:19,false,1969486837,,4/26/2016 18:19:10,false,neodev,0.8889,35550011,VEN,07,Valencia,190.204.238.112,0,,"<p>The slides for today&#39;s lecture topics are now online:</p>
<p><a href=""http://crowdsourcing-class.org/slides/machine-learning-part-2.pdf"">Machine Learning - part 2</a></p>
<p><a href=""http://crowdsourcing-class.org/slides/amazon-mechanical-turk.pdf"">The Amazon Mechanical Turk crowdsourcing platform</a></p>
<p></p>
<p>Please post any questions that you have about either topic to this thread.åÊ</p>",Slides for today&#39;s lecture,"<p>If you&#39;d like to discussåÊAl Filries&#39; lecture about MOOC&#39;s, let&#39;s use this discussion thread.</p>
<p></p>
<p>Here&#39;s a link toåÊhis <a href=""https://www.coursera.org/course/modernpoetry"" target=""_blank"">ModPo Coursera course</a>.</p>
<p></p>",Al Filries&#39; lecture,"<p>On Fridays I&#39;m planning on doing more applied hands-on lectures where the TAs and I walk you through the homework assignments and help you get started. åÊHow did you like today&#39;s lecture?</p>
<p></p>
<p>Feel free to leave comments on things you would like to see in the Friday lectures, after you have voted.</p>
<br/> [o] Today&#39;s lecture was good, and helped me understand the assignment
[o] I didn&#39;t find it particularly useful, or I would have preferred a standard non-applied lecture",Poll: Friday hands-on lectures,"<p>Hi everyone,</p>
<p></p>
<p>Just a friendly reminder that you&#39;ll get credit towards your participation grade for attending today&#39;s NETS 213 lecture. åÊThe TAs will be passing out unique participation codes, one per student, as you leave the classroom. åÊBe sure to get one, and then enter it <a href=""https://docs.google.com/forms/d/14UZWosW5_W_-qDNI8KJ_zUGkiyTO9yuwv7yCkCvuZgQ/viewform"" target=""_blank"">here</a>åÊafter class.</p>
<p></p>
<p>--Chris</p>",Reminder: participation credit for attending today&#39;s NETS 213 lecture,<p>There will be a guest lecturer in NETS 213 today. åÊPlease attend the lecture. åÊWe mightåÊtake attendance.åÊ</p>,Guest lecture today,"For tomorrow&#39;s lecture, I have invited Professor Doug Weibe of the School of Medicine to give a talk about studying gun violence from a public health perspective. The goal is to contextualize why we are focused on gun violence in our homework assignments. åÊDoug is an epidemiologist whose research will hopefully be facilitated by the data that we are collecting via crowdsourcing.åÊ<div><br /></div><div>Here is more about Doug:<div>http://www.upennprc.org/people/douglas-wiebe/</div></div><div><br /></div><div>Please attend tomorrow&#39;s lecture. It will count towards your participation grade.åÊ</div>",guest lecture in NETS 213 tomorrow,3
940847358,4/26/2016 18:24:14,false,1969488802,,4/26/2016 18:22:47,false,elite,1.0,30128662,BGR,50,Pleven,212.233.177.195,0,,"<p>The slides for today&#39;s lecture topics are now online:</p>
<p><a href=""http://crowdsourcing-class.org/slides/machine-learning-part-2.pdf"">Machine Learning - part 2</a></p>
<p><a href=""http://crowdsourcing-class.org/slides/amazon-mechanical-turk.pdf"">The Amazon Mechanical Turk crowdsourcing platform</a></p>
<p></p>
<p>Please post any questions that you have about either topic to this thread.åÊ</p>",Slides for today&#39;s lecture,"<p>If you&#39;d like to discussåÊAl Filries&#39; lecture about MOOC&#39;s, let&#39;s use this discussion thread.</p>
<p></p>
<p>Here&#39;s a link toåÊhis <a href=""https://www.coursera.org/course/modernpoetry"" target=""_blank"">ModPo Coursera course</a>.</p>
<p></p>",Al Filries&#39; lecture,"<p>On Fridays I&#39;m planning on doing more applied hands-on lectures where the TAs and I walk you through the homework assignments and help you get started. åÊHow did you like today&#39;s lecture?</p>
<p></p>
<p>Feel free to leave comments on things you would like to see in the Friday lectures, after you have voted.</p>
<br/> [o] Today&#39;s lecture was good, and helped me understand the assignment
[o] I didn&#39;t find it particularly useful, or I would have preferred a standard non-applied lecture",Poll: Friday hands-on lectures,"<p>Hi everyone,</p>
<p></p>
<p>Just a friendly reminder that you&#39;ll get credit towards your participation grade for attending today&#39;s NETS 213 lecture. åÊThe TAs will be passing out unique participation codes, one per student, as you leave the classroom. åÊBe sure to get one, and then enter it <a href=""https://docs.google.com/forms/d/14UZWosW5_W_-qDNI8KJ_zUGkiyTO9yuwv7yCkCvuZgQ/viewform"" target=""_blank"">here</a>åÊafter class.</p>
<p></p>
<p>--Chris</p>",Reminder: participation credit for attending today&#39;s NETS 213 lecture,<p>There will be a guest lecturer in NETS 213 today. åÊPlease attend the lecture. åÊWe mightåÊtake attendance.åÊ</p>,Guest lecture today,"For tomorrow&#39;s lecture, I have invited Professor Doug Weibe of the School of Medicine to give a talk about studying gun violence from a public health perspective. The goal is to contextualize why we are focused on gun violence in our homework assignments. åÊDoug is an epidemiologist whose research will hopefully be facilitated by the data that we are collecting via crowdsourcing.åÊ<div><br /></div><div>Here is more about Doug:<div>http://www.upennprc.org/people/douglas-wiebe/</div></div><div><br /></div><div>Please attend tomorrow&#39;s lecture. It will count towards your participation grade.åÊ</div>",guest lecture in NETS 213 tomorrow,3
940847358,4/26/2016 18:45:01,false,1969499307,,4/26/2016 18:41:05,false,neodev,1.0,35974955,VEN,17,Porlamar,190.198.232.239,0,,"<p>The slides for today&#39;s lecture topics are now online:</p>
<p><a href=""http://crowdsourcing-class.org/slides/machine-learning-part-2.pdf"">Machine Learning - part 2</a></p>
<p><a href=""http://crowdsourcing-class.org/slides/amazon-mechanical-turk.pdf"">The Amazon Mechanical Turk crowdsourcing platform</a></p>
<p></p>
<p>Please post any questions that you have about either topic to this thread.åÊ</p>",Slides for today&#39;s lecture,"<p>If you&#39;d like to discussåÊAl Filries&#39; lecture about MOOC&#39;s, let&#39;s use this discussion thread.</p>
<p></p>
<p>Here&#39;s a link toåÊhis <a href=""https://www.coursera.org/course/modernpoetry"" target=""_blank"">ModPo Coursera course</a>.</p>
<p></p>",Al Filries&#39; lecture,"<p>On Fridays I&#39;m planning on doing more applied hands-on lectures where the TAs and I walk you through the homework assignments and help you get started. åÊHow did you like today&#39;s lecture?</p>
<p></p>
<p>Feel free to leave comments on things you would like to see in the Friday lectures, after you have voted.</p>
<br/> [o] Today&#39;s lecture was good, and helped me understand the assignment
[o] I didn&#39;t find it particularly useful, or I would have preferred a standard non-applied lecture",Poll: Friday hands-on lectures,"<p>Hi everyone,</p>
<p></p>
<p>Just a friendly reminder that you&#39;ll get credit towards your participation grade for attending today&#39;s NETS 213 lecture. åÊThe TAs will be passing out unique participation codes, one per student, as you leave the classroom. åÊBe sure to get one, and then enter it <a href=""https://docs.google.com/forms/d/14UZWosW5_W_-qDNI8KJ_zUGkiyTO9yuwv7yCkCvuZgQ/viewform"" target=""_blank"">here</a>åÊafter class.</p>
<p></p>
<p>--Chris</p>",Reminder: participation credit for attending today&#39;s NETS 213 lecture,<p>There will be a guest lecturer in NETS 213 today. åÊPlease attend the lecture. åÊWe mightåÊtake attendance.åÊ</p>,Guest lecture today,"For tomorrow&#39;s lecture, I have invited Professor Doug Weibe of the School of Medicine to give a talk about studying gun violence from a public health perspective. The goal is to contextualize why we are focused on gun violence in our homework assignments. åÊDoug is an epidemiologist whose research will hopefully be facilitated by the data that we are collecting via crowdsourcing.åÊ<div><br /></div><div>Here is more about Doug:<div>http://www.upennprc.org/people/douglas-wiebe/</div></div><div><br /></div><div>Please attend tomorrow&#39;s lecture. It will count towards your participation grade.åÊ</div>",guest lecture in NETS 213 tomorrow,3
940847359,4/26/2016 17:27:57,false,1969458135,,4/26/2016 17:23:11,false,neodev,1.0,33973110,VEN,23,Maracaibo,186.94.238.104,0,,"<p></p><div>Please register to be a worker on Amazon Mechanical Turk TODAY atåÊ<a href=""https://www.mturk.com/mturk/welcome"">https://www.mturk.com/mturk/welcome</a>åÊ</div>
<div>You will need an account to for the <a href=""http://crowdsourcing-class.org/assignment1.html"" target=""_blank"">first homework assignment</a>. åÊWe will be doing a walk through on Friday, and it would be great if you have your account activated by then. åÊTypically it takes Amazon several days to activate workers accounts, so please create yours ASAP.</div>
<div></div>
<div>Note that all foreign students, and some domestic students had problems creating Mechanical Turk accounts last year. åÊIf you have a problem, please try to sign up to be a Crowd Flower worker (also called a &#34;contributor&#34;). You can sign up here: <a href=""https://elite.crowdflower.com/?view=register"">https://elite.crowdflower.com/?view=register</a><a href=""https://elite.crowdflower.com/?view=register"" target=""_blank""></a></div>
<div></div>
<div>--CCB</div>",TODO: Register as a Mechanical Turk worker,"<p>I just realized that, for some of my test questions, a lot of workers complained that the answers I gave were wrong or lacked concrete instructions. Some of them asked me to &#34;make corrections&#34; to the test questions and &#34;fix their accuracy&#34;. How would I go about doing this?åÊ</p>
<p></p>
<p>Looking back, maybe my answers to the test questions were incorrect.</p>",Fixing Worker&#39;s accuracy,<p>One of the workers in our csv seems to have answered no test questions and doesn&#39;t appear on the weighted majority worker quality set. Should this worker just be given a weight of 0? åÊ</p>,Weighted majority worker weight,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hljibjyn5s85mp/ijmy2ryhm0ve/Screenshot_20160120_09.45.32.png"" />FYI, I had my developers add a &#34;My hourly rate&#34; feature to the Track section of the Analytics page on <a href=""http://crowd-workers.com/analytics"" target=""_blank"">crowd-workers.com</a>. åÊYou can view it by first reloading your web page, then clicking on the Display button, and selecting theåÊ&#34;MyåÊhourlyåÊrate&#34; option. åÊ Screenshot is attached.</p>
<p></p>
<p></p>",&#34;My hourly rate&#34; now available on Crowd Workers,"<p>To clarify this calculation, I understand that you shouldåÊmultiply the following: (accuracy when labelåÊis porn = P,P)(% of URLs which are porn) &#43; (accuracy when label is not porn = NP, NP)(% of URLs which are not porn). Just to clarify this further,åÊif we are calculating worker quality for iteration 2, do we take the % of URLs that are porn and not porn from the true labels we calculated in iteration 2 or the ones we calculated iteration 1. I guess I am confused because we calculated the URL true labels andåÊmajority vote of iteration 2 based on the worker qualities of iteration 2. Correct me if I am wrong. Thanks!</p>
<p></p>
<p>Also other people are stating that we can add the top left corner (P,P) &#43; bottom right corner values (NP,NP) and then divide by 2. However, I don&#39;t believe those values are the same.</p>
<p>åÊ</p>",Reducing Worker Quality to One Value,"<p>It turns out that in creating the open-ended hit, we set it up such that workers worked on 5 articles per page, and got paid $0.10 for the page ($0.02 per article). It was an honest mistake and we pulled our results from this file. What should we do for the assignment? Should we go back and redo it? Or can we use our responses for the questionnaire?</p>",Accidentally paid workers too little?,1
940847359,4/26/2016 17:32:16,false,1969460793,,4/26/2016 17:31:58,false,neodev,0.8889,33131546,IDN,04,Jakarta,139.194.89.60,0,,"<p></p><div>Please register to be a worker on Amazon Mechanical Turk TODAY atåÊ<a href=""https://www.mturk.com/mturk/welcome"">https://www.mturk.com/mturk/welcome</a>åÊ</div>
<div>You will need an account to for the <a href=""http://crowdsourcing-class.org/assignment1.html"" target=""_blank"">first homework assignment</a>. åÊWe will be doing a walk through on Friday, and it would be great if you have your account activated by then. åÊTypically it takes Amazon several days to activate workers accounts, so please create yours ASAP.</div>
<div></div>
<div>Note that all foreign students, and some domestic students had problems creating Mechanical Turk accounts last year. åÊIf you have a problem, please try to sign up to be a Crowd Flower worker (also called a &#34;contributor&#34;). You can sign up here: <a href=""https://elite.crowdflower.com/?view=register"">https://elite.crowdflower.com/?view=register</a><a href=""https://elite.crowdflower.com/?view=register"" target=""_blank""></a></div>
<div></div>
<div>--CCB</div>",TODO: Register as a Mechanical Turk worker,"<p>I just realized that, for some of my test questions, a lot of workers complained that the answers I gave were wrong or lacked concrete instructions. Some of them asked me to &#34;make corrections&#34; to the test questions and &#34;fix their accuracy&#34;. How would I go about doing this?åÊ</p>
<p></p>
<p>Looking back, maybe my answers to the test questions were incorrect.</p>",Fixing Worker&#39;s accuracy,<p>One of the workers in our csv seems to have answered no test questions and doesn&#39;t appear on the weighted majority worker quality set. Should this worker just be given a weight of 0? åÊ</p>,Weighted majority worker weight,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hljibjyn5s85mp/ijmy2ryhm0ve/Screenshot_20160120_09.45.32.png"" />FYI, I had my developers add a &#34;My hourly rate&#34; feature to the Track section of the Analytics page on <a href=""http://crowd-workers.com/analytics"" target=""_blank"">crowd-workers.com</a>. åÊYou can view it by first reloading your web page, then clicking on the Display button, and selecting theåÊ&#34;MyåÊhourlyåÊrate&#34; option. åÊ Screenshot is attached.</p>
<p></p>
<p></p>",&#34;My hourly rate&#34; now available on Crowd Workers,"<p>To clarify this calculation, I understand that you shouldåÊmultiply the following: (accuracy when labelåÊis porn = P,P)(% of URLs which are porn) &#43; (accuracy when label is not porn = NP, NP)(% of URLs which are not porn). Just to clarify this further,åÊif we are calculating worker quality for iteration 2, do we take the % of URLs that are porn and not porn from the true labels we calculated in iteration 2 or the ones we calculated iteration 1. I guess I am confused because we calculated the URL true labels andåÊmajority vote of iteration 2 based on the worker qualities of iteration 2. Correct me if I am wrong. Thanks!</p>
<p></p>
<p>Also other people are stating that we can add the top left corner (P,P) &#43; bottom right corner values (NP,NP) and then divide by 2. However, I don&#39;t believe those values are the same.</p>
<p>åÊ</p>",Reducing Worker Quality to One Value,"<p>It turns out that in creating the open-ended hit, we set it up such that workers worked on 5 articles per page, and got paid $0.10 for the page ($0.02 per article). It was an honest mistake and we pulled our results from this file. What should we do for the assignment? Should we go back and redo it? Or can we use our responses for the questionnaire?</p>",Accidentally paid workers too little?,1
940847359,4/26/2016 17:35:19,false,1969462335,,4/26/2016 17:34:26,false,clixsense,1.0,30712378,ROU,21,Deva,79.119.241.200,0,,"<p></p><div>Please register to be a worker on Amazon Mechanical Turk TODAY atåÊ<a href=""https://www.mturk.com/mturk/welcome"">https://www.mturk.com/mturk/welcome</a>åÊ</div>
<div>You will need an account to for the <a href=""http://crowdsourcing-class.org/assignment1.html"" target=""_blank"">first homework assignment</a>. åÊWe will be doing a walk through on Friday, and it would be great if you have your account activated by then. åÊTypically it takes Amazon several days to activate workers accounts, so please create yours ASAP.</div>
<div></div>
<div>Note that all foreign students, and some domestic students had problems creating Mechanical Turk accounts last year. åÊIf you have a problem, please try to sign up to be a Crowd Flower worker (also called a &#34;contributor&#34;). You can sign up here: <a href=""https://elite.crowdflower.com/?view=register"">https://elite.crowdflower.com/?view=register</a><a href=""https://elite.crowdflower.com/?view=register"" target=""_blank""></a></div>
<div></div>
<div>--CCB</div>",TODO: Register as a Mechanical Turk worker,"<p>I just realized that, for some of my test questions, a lot of workers complained that the answers I gave were wrong or lacked concrete instructions. Some of them asked me to &#34;make corrections&#34; to the test questions and &#34;fix their accuracy&#34;. How would I go about doing this?åÊ</p>
<p></p>
<p>Looking back, maybe my answers to the test questions were incorrect.</p>",Fixing Worker&#39;s accuracy,<p>One of the workers in our csv seems to have answered no test questions and doesn&#39;t appear on the weighted majority worker quality set. Should this worker just be given a weight of 0? åÊ</p>,Weighted majority worker weight,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hljibjyn5s85mp/ijmy2ryhm0ve/Screenshot_20160120_09.45.32.png"" />FYI, I had my developers add a &#34;My hourly rate&#34; feature to the Track section of the Analytics page on <a href=""http://crowd-workers.com/analytics"" target=""_blank"">crowd-workers.com</a>. åÊYou can view it by first reloading your web page, then clicking on the Display button, and selecting theåÊ&#34;MyåÊhourlyåÊrate&#34; option. åÊ Screenshot is attached.</p>
<p></p>
<p></p>",&#34;My hourly rate&#34; now available on Crowd Workers,"<p>To clarify this calculation, I understand that you shouldåÊmultiply the following: (accuracy when labelåÊis porn = P,P)(% of URLs which are porn) &#43; (accuracy when label is not porn = NP, NP)(% of URLs which are not porn). Just to clarify this further,åÊif we are calculating worker quality for iteration 2, do we take the % of URLs that are porn and not porn from the true labels we calculated in iteration 2 or the ones we calculated iteration 1. I guess I am confused because we calculated the URL true labels andåÊmajority vote of iteration 2 based on the worker qualities of iteration 2. Correct me if I am wrong. Thanks!</p>
<p></p>
<p>Also other people are stating that we can add the top left corner (P,P) &#43; bottom right corner values (NP,NP) and then divide by 2. However, I don&#39;t believe those values are the same.</p>
<p>åÊ</p>",Reducing Worker Quality to One Value,"<p>It turns out that in creating the open-ended hit, we set it up such that workers worked on 5 articles per page, and got paid $0.10 for the page ($0.02 per article). It was an honest mistake and we pulled our results from this file. What should we do for the assignment? Should we go back and redo it? Or can we use our responses for the questionnaire?</p>",Accidentally paid workers too little?,1
940847359,4/26/2016 18:27:49,false,1969490439,,4/26/2016 18:04:06,false,clixsense,0.8889,35338593,ITA,14,Cagliari,151.56.132.145,0,,"<p></p><div>Please register to be a worker on Amazon Mechanical Turk TODAY atåÊ<a href=""https://www.mturk.com/mturk/welcome"">https://www.mturk.com/mturk/welcome</a>åÊ</div>
<div>You will need an account to for the <a href=""http://crowdsourcing-class.org/assignment1.html"" target=""_blank"">first homework assignment</a>. åÊWe will be doing a walk through on Friday, and it would be great if you have your account activated by then. åÊTypically it takes Amazon several days to activate workers accounts, so please create yours ASAP.</div>
<div></div>
<div>Note that all foreign students, and some domestic students had problems creating Mechanical Turk accounts last year. åÊIf you have a problem, please try to sign up to be a Crowd Flower worker (also called a &#34;contributor&#34;). You can sign up here: <a href=""https://elite.crowdflower.com/?view=register"">https://elite.crowdflower.com/?view=register</a><a href=""https://elite.crowdflower.com/?view=register"" target=""_blank""></a></div>
<div></div>
<div>--CCB</div>",TODO: Register as a Mechanical Turk worker,"<p>I just realized that, for some of my test questions, a lot of workers complained that the answers I gave were wrong or lacked concrete instructions. Some of them asked me to &#34;make corrections&#34; to the test questions and &#34;fix their accuracy&#34;. How would I go about doing this?åÊ</p>
<p></p>
<p>Looking back, maybe my answers to the test questions were incorrect.</p>",Fixing Worker&#39;s accuracy,<p>One of the workers in our csv seems to have answered no test questions and doesn&#39;t appear on the weighted majority worker quality set. Should this worker just be given a weight of 0? åÊ</p>,Weighted majority worker weight,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hljibjyn5s85mp/ijmy2ryhm0ve/Screenshot_20160120_09.45.32.png"" />FYI, I had my developers add a &#34;My hourly rate&#34; feature to the Track section of the Analytics page on <a href=""http://crowd-workers.com/analytics"" target=""_blank"">crowd-workers.com</a>. åÊYou can view it by first reloading your web page, then clicking on the Display button, and selecting theåÊ&#34;MyåÊhourlyåÊrate&#34; option. åÊ Screenshot is attached.</p>
<p></p>
<p></p>",&#34;My hourly rate&#34; now available on Crowd Workers,"<p>To clarify this calculation, I understand that you shouldåÊmultiply the following: (accuracy when labelåÊis porn = P,P)(% of URLs which are porn) &#43; (accuracy when label is not porn = NP, NP)(% of URLs which are not porn). Just to clarify this further,åÊif we are calculating worker quality for iteration 2, do we take the % of URLs that are porn and not porn from the true labels we calculated in iteration 2 or the ones we calculated iteration 1. I guess I am confused because we calculated the URL true labels andåÊmajority vote of iteration 2 based on the worker qualities of iteration 2. Correct me if I am wrong. Thanks!</p>
<p></p>
<p>Also other people are stating that we can add the top left corner (P,P) &#43; bottom right corner values (NP,NP) and then divide by 2. However, I don&#39;t believe those values are the same.</p>
<p>åÊ</p>",Reducing Worker Quality to One Value,"<p>It turns out that in creating the open-ended hit, we set it up such that workers worked on 5 articles per page, and got paid $0.10 for the page ($0.02 per article). It was an honest mistake and we pulled our results from this file. What should we do for the assignment? Should we go back and redo it? Or can we use our responses for the questionnaire?</p>",Accidentally paid workers too little?,1
940847359,4/26/2016 18:41:46,false,1969497907,,4/26/2016 18:39:26,false,neodev,1.0,29879245,RUS,69,Smolensk,37.144.124.118,0,,"<p></p><div>Please register to be a worker on Amazon Mechanical Turk TODAY atåÊ<a href=""https://www.mturk.com/mturk/welcome"">https://www.mturk.com/mturk/welcome</a>åÊ</div>
<div>You will need an account to for the <a href=""http://crowdsourcing-class.org/assignment1.html"" target=""_blank"">first homework assignment</a>. åÊWe will be doing a walk through on Friday, and it would be great if you have your account activated by then. åÊTypically it takes Amazon several days to activate workers accounts, so please create yours ASAP.</div>
<div></div>
<div>Note that all foreign students, and some domestic students had problems creating Mechanical Turk accounts last year. åÊIf you have a problem, please try to sign up to be a Crowd Flower worker (also called a &#34;contributor&#34;). You can sign up here: <a href=""https://elite.crowdflower.com/?view=register"">https://elite.crowdflower.com/?view=register</a><a href=""https://elite.crowdflower.com/?view=register"" target=""_blank""></a></div>
<div></div>
<div>--CCB</div>",TODO: Register as a Mechanical Turk worker,"<p>I just realized that, for some of my test questions, a lot of workers complained that the answers I gave were wrong or lacked concrete instructions. Some of them asked me to &#34;make corrections&#34; to the test questions and &#34;fix their accuracy&#34;. How would I go about doing this?åÊ</p>
<p></p>
<p>Looking back, maybe my answers to the test questions were incorrect.</p>",Fixing Worker&#39;s accuracy,<p>One of the workers in our csv seems to have answered no test questions and doesn&#39;t appear on the weighted majority worker quality set. Should this worker just be given a weight of 0? åÊ</p>,Weighted majority worker weight,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hljibjyn5s85mp/ijmy2ryhm0ve/Screenshot_20160120_09.45.32.png"" />FYI, I had my developers add a &#34;My hourly rate&#34; feature to the Track section of the Analytics page on <a href=""http://crowd-workers.com/analytics"" target=""_blank"">crowd-workers.com</a>. åÊYou can view it by first reloading your web page, then clicking on the Display button, and selecting theåÊ&#34;MyåÊhourlyåÊrate&#34; option. åÊ Screenshot is attached.</p>
<p></p>
<p></p>",&#34;My hourly rate&#34; now available on Crowd Workers,"<p>To clarify this calculation, I understand that you shouldåÊmultiply the following: (accuracy when labelåÊis porn = P,P)(% of URLs which are porn) &#43; (accuracy when label is not porn = NP, NP)(% of URLs which are not porn). Just to clarify this further,åÊif we are calculating worker quality for iteration 2, do we take the % of URLs that are porn and not porn from the true labels we calculated in iteration 2 or the ones we calculated iteration 1. I guess I am confused because we calculated the URL true labels andåÊmajority vote of iteration 2 based on the worker qualities of iteration 2. Correct me if I am wrong. Thanks!</p>
<p></p>
<p>Also other people are stating that we can add the top left corner (P,P) &#43; bottom right corner values (NP,NP) and then divide by 2. However, I don&#39;t believe those values are the same.</p>
<p>åÊ</p>",Reducing Worker Quality to One Value,"<p>It turns out that in creating the open-ended hit, we set it up such that workers worked on 5 articles per page, and got paid $0.10 for the page ($0.02 per article). It was an honest mistake and we pulled our results from this file. What should we do for the assignment? Should we go back and redo it? Or can we use our responses for the questionnaire?</p>",Accidentally paid workers too little?,1
940847360,4/26/2016 15:28:37,false,1969390287,,4/26/2016 15:28:07,false,instagc,0.8889,13581319,USA,IL,Waltonville,208.70.36.12,0,,"<p>I got .5 points off &#34;for not using the crawler to collect URLs in addition to the Bing API&#34; -- my collected URLs included URLs from the Gun Violence Archive, and I collected the rest using the Bing API.åÊ</p>
<p></p>
<p>The instructions didn&#39;t specify that we necessarily needed to use other methods to get 2500 URLs. The homework says to &#34;turn in a list of at least 2,500 urls, including some crawled from the Gun Violence Archive and some obtained using the Bing API.&#34;åÊ</p>
<p></p>
<p>Am I understanding correctly why I got the points off? If so, I&#39;m confused why I got points off given the instructions. Thank you!åÊ</p>",Question about grade,<p>What exactly is meant by the words in the &#34;red&#34; reviews that DO NOT appear in the &#34;white&#34; reviews? Does this mean we should not count a word that appears in a red review and at least oneåÊwhite review? Or is this simply a check for reviews that have both &#34;red&#34; and &#34;white&#34; in them (and thus we can&#39;t classify the words as either red or white)?åÊ</p>,Questions 9 &amp; 10: Red and White reviews,"<p>I&#39;m having an issue in the last part of the assignment creating the questions that require information from the spreadsheet. For example, for the question &#34;In what city did the incident take place?&#34; I am trying to add {{city_1}} as an option in the question so that the information from the spreadsheet will populate that field, but I get this error: &#34;&#39;{{city_1}}&#39; will show up in your results as &#39;city_1&#39;, which already exists as a header in your uploaded data.&#34; How can I achieve what is supposed to happen, instead of getting this error?</p>
<p></p>
<p>Thanks!</p>",Crowdflower question creation,<p>I&#39;m working on one of the iterativeåÊimage annotation assignment...I&#39;m having trouble understanding how to implement the iterative component to designing a task. Are there any hints/advice on where to start?åÊ</p>,Iterative questions on CrowdFlower,<p>Crowdflower recommends 13 Test Questions. Is this number ok?</p>,How many test questions on Crowdflower?,"<p>Hello! I&#39;m a little confused with the following instruction:</p>
<p>You can work in pairs on this assignment. You must declare the fact that you are working together when you turn in the questionnaire. If you are working with a partner, only one of you needs to turn in the code, but you must specify who will be turning it in on the questionnaire.&#34;</p>
<p></p>
<p>My partner submitted the code/questionnaire. Do I also need to submit aåÊquestionnaire? Or am I ok so long as he specified my name in his submittal?</p>
<p></p>
<p>Thanks!</p>
<p></p>
<p>-Alex</p>",Questionnare question:,0
940847360,4/26/2016 15:39:31,false,1969394677,,4/26/2016 15:36:13,false,clixsense,0.8889,36052512,PHL,F2,Quezon City,49.149.150.150,0,,"<p>I got .5 points off &#34;for not using the crawler to collect URLs in addition to the Bing API&#34; -- my collected URLs included URLs from the Gun Violence Archive, and I collected the rest using the Bing API.åÊ</p>
<p></p>
<p>The instructions didn&#39;t specify that we necessarily needed to use other methods to get 2500 URLs. The homework says to &#34;turn in a list of at least 2,500 urls, including some crawled from the Gun Violence Archive and some obtained using the Bing API.&#34;åÊ</p>
<p></p>
<p>Am I understanding correctly why I got the points off? If so, I&#39;m confused why I got points off given the instructions. Thank you!åÊ</p>",Question about grade,<p>What exactly is meant by the words in the &#34;red&#34; reviews that DO NOT appear in the &#34;white&#34; reviews? Does this mean we should not count a word that appears in a red review and at least oneåÊwhite review? Or is this simply a check for reviews that have both &#34;red&#34; and &#34;white&#34; in them (and thus we can&#39;t classify the words as either red or white)?åÊ</p>,Questions 9 &amp; 10: Red and White reviews,"<p>I&#39;m having an issue in the last part of the assignment creating the questions that require information from the spreadsheet. For example, for the question &#34;In what city did the incident take place?&#34; I am trying to add {{city_1}} as an option in the question so that the information from the spreadsheet will populate that field, but I get this error: &#34;&#39;{{city_1}}&#39; will show up in your results as &#39;city_1&#39;, which already exists as a header in your uploaded data.&#34; How can I achieve what is supposed to happen, instead of getting this error?</p>
<p></p>
<p>Thanks!</p>",Crowdflower question creation,<p>I&#39;m working on one of the iterativeåÊimage annotation assignment...I&#39;m having trouble understanding how to implement the iterative component to designing a task. Are there any hints/advice on where to start?åÊ</p>,Iterative questions on CrowdFlower,<p>Crowdflower recommends 13 Test Questions. Is this number ok?</p>,How many test questions on Crowdflower?,"<p>Hello! I&#39;m a little confused with the following instruction:</p>
<p>You can work in pairs on this assignment. You must declare the fact that you are working together when you turn in the questionnaire. If you are working with a partner, only one of you needs to turn in the code, but you must specify who will be turning it in on the questionnaire.&#34;</p>
<p></p>
<p>My partner submitted the code/questionnaire. Do I also need to submit aåÊquestionnaire? Or am I ok so long as he specified my name in his submittal?</p>
<p></p>
<p>Thanks!</p>
<p></p>
<p>-Alex</p>",Questionnare question:,0
940847360,4/26/2016 15:47:54,false,1969398208,,4/26/2016 15:44:26,false,neodev,1.0,28875937,PAK,05,Karachi,182.180.125.133,0,,"<p>I got .5 points off &#34;for not using the crawler to collect URLs in addition to the Bing API&#34; -- my collected URLs included URLs from the Gun Violence Archive, and I collected the rest using the Bing API.åÊ</p>
<p></p>
<p>The instructions didn&#39;t specify that we necessarily needed to use other methods to get 2500 URLs. The homework says to &#34;turn in a list of at least 2,500 urls, including some crawled from the Gun Violence Archive and some obtained using the Bing API.&#34;åÊ</p>
<p></p>
<p>Am I understanding correctly why I got the points off? If so, I&#39;m confused why I got points off given the instructions. Thank you!åÊ</p>",Question about grade,<p>What exactly is meant by the words in the &#34;red&#34; reviews that DO NOT appear in the &#34;white&#34; reviews? Does this mean we should not count a word that appears in a red review and at least oneåÊwhite review? Or is this simply a check for reviews that have both &#34;red&#34; and &#34;white&#34; in them (and thus we can&#39;t classify the words as either red or white)?åÊ</p>,Questions 9 &amp; 10: Red and White reviews,"<p>I&#39;m having an issue in the last part of the assignment creating the questions that require information from the spreadsheet. For example, for the question &#34;In what city did the incident take place?&#34; I am trying to add {{city_1}} as an option in the question so that the information from the spreadsheet will populate that field, but I get this error: &#34;&#39;{{city_1}}&#39; will show up in your results as &#39;city_1&#39;, which already exists as a header in your uploaded data.&#34; How can I achieve what is supposed to happen, instead of getting this error?</p>
<p></p>
<p>Thanks!</p>",Crowdflower question creation,<p>I&#39;m working on one of the iterativeåÊimage annotation assignment...I&#39;m having trouble understanding how to implement the iterative component to designing a task. Are there any hints/advice on where to start?åÊ</p>,Iterative questions on CrowdFlower,<p>Crowdflower recommends 13 Test Questions. Is this number ok?</p>,How many test questions on Crowdflower?,"<p>Hello! I&#39;m a little confused with the following instruction:</p>
<p>You can work in pairs on this assignment. You must declare the fact that you are working together when you turn in the questionnaire. If you are working with a partner, only one of you needs to turn in the code, but you must specify who will be turning it in on the questionnaire.&#34;</p>
<p></p>
<p>My partner submitted the code/questionnaire. Do I also need to submit aåÊquestionnaire? Or am I ok so long as he specified my name in his submittal?</p>
<p></p>
<p>Thanks!</p>
<p></p>
<p>-Alex</p>",Questionnare question:,0
940847360,4/26/2016 15:49:11,false,1969398840,,4/26/2016 15:45:52,false,elite,0.8889,36575101,IND,07,New Delhi,112.196.144.2,0,,"<p>I got .5 points off &#34;for not using the crawler to collect URLs in addition to the Bing API&#34; -- my collected URLs included URLs from the Gun Violence Archive, and I collected the rest using the Bing API.åÊ</p>
<p></p>
<p>The instructions didn&#39;t specify that we necessarily needed to use other methods to get 2500 URLs. The homework says to &#34;turn in a list of at least 2,500 urls, including some crawled from the Gun Violence Archive and some obtained using the Bing API.&#34;åÊ</p>
<p></p>
<p>Am I understanding correctly why I got the points off? If so, I&#39;m confused why I got points off given the instructions. Thank you!åÊ</p>",Question about grade,<p>What exactly is meant by the words in the &#34;red&#34; reviews that DO NOT appear in the &#34;white&#34; reviews? Does this mean we should not count a word that appears in a red review and at least oneåÊwhite review? Or is this simply a check for reviews that have both &#34;red&#34; and &#34;white&#34; in them (and thus we can&#39;t classify the words as either red or white)?åÊ</p>,Questions 9 &amp; 10: Red and White reviews,"<p>I&#39;m having an issue in the last part of the assignment creating the questions that require information from the spreadsheet. For example, for the question &#34;In what city did the incident take place?&#34; I am trying to add {{city_1}} as an option in the question so that the information from the spreadsheet will populate that field, but I get this error: &#34;&#39;{{city_1}}&#39; will show up in your results as &#39;city_1&#39;, which already exists as a header in your uploaded data.&#34; How can I achieve what is supposed to happen, instead of getting this error?</p>
<p></p>
<p>Thanks!</p>",Crowdflower question creation,<p>I&#39;m working on one of the iterativeåÊimage annotation assignment...I&#39;m having trouble understanding how to implement the iterative component to designing a task. Are there any hints/advice on where to start?åÊ</p>,Iterative questions on CrowdFlower,<p>Crowdflower recommends 13 Test Questions. Is this number ok?</p>,How many test questions on Crowdflower?,"<p>Hello! I&#39;m a little confused with the following instruction:</p>
<p>You can work in pairs on this assignment. You must declare the fact that you are working together when you turn in the questionnaire. If you are working with a partner, only one of you needs to turn in the code, but you must specify who will be turning it in on the questionnaire.&#34;</p>
<p></p>
<p>My partner submitted the code/questionnaire. Do I also need to submit aåÊquestionnaire? Or am I ok so long as he specified my name in his submittal?</p>
<p></p>
<p>Thanks!</p>
<p></p>
<p>-Alex</p>",Questionnare question:,0
940847360,4/26/2016 15:56:09,false,1969401806,,4/26/2016 15:45:42,false,clixsense,1.0,21875134,GBR,H9,London,87.112.158.81,0,,"<p>I got .5 points off &#34;for not using the crawler to collect URLs in addition to the Bing API&#34; -- my collected URLs included URLs from the Gun Violence Archive, and I collected the rest using the Bing API.åÊ</p>
<p></p>
<p>The instructions didn&#39;t specify that we necessarily needed to use other methods to get 2500 URLs. The homework says to &#34;turn in a list of at least 2,500 urls, including some crawled from the Gun Violence Archive and some obtained using the Bing API.&#34;åÊ</p>
<p></p>
<p>Am I understanding correctly why I got the points off? If so, I&#39;m confused why I got points off given the instructions. Thank you!åÊ</p>",Question about grade,<p>What exactly is meant by the words in the &#34;red&#34; reviews that DO NOT appear in the &#34;white&#34; reviews? Does this mean we should not count a word that appears in a red review and at least oneåÊwhite review? Or is this simply a check for reviews that have both &#34;red&#34; and &#34;white&#34; in them (and thus we can&#39;t classify the words as either red or white)?åÊ</p>,Questions 9 &amp; 10: Red and White reviews,"<p>I&#39;m having an issue in the last part of the assignment creating the questions that require information from the spreadsheet. For example, for the question &#34;In what city did the incident take place?&#34; I am trying to add {{city_1}} as an option in the question so that the information from the spreadsheet will populate that field, but I get this error: &#34;&#39;{{city_1}}&#39; will show up in your results as &#39;city_1&#39;, which already exists as a header in your uploaded data.&#34; How can I achieve what is supposed to happen, instead of getting this error?</p>
<p></p>
<p>Thanks!</p>",Crowdflower question creation,<p>I&#39;m working on one of the iterativeåÊimage annotation assignment...I&#39;m having trouble understanding how to implement the iterative component to designing a task. Are there any hints/advice on where to start?åÊ</p>,Iterative questions on CrowdFlower,<p>Crowdflower recommends 13 Test Questions. Is this number ok?</p>,How many test questions on Crowdflower?,"<p>Hello! I&#39;m a little confused with the following instruction:</p>
<p>You can work in pairs on this assignment. You must declare the fact that you are working together when you turn in the questionnaire. If you are working with a partner, only one of you needs to turn in the code, but you must specify who will be turning it in on the questionnaire.&#34;</p>
<p></p>
<p>My partner submitted the code/questionnaire. Do I also need to submit aåÊquestionnaire? Or am I ok so long as he specified my name in his submittal?</p>
<p></p>
<p>Thanks!</p>
<p></p>
<p>-Alex</p>",Questionnare question:,0
940847361,4/26/2016 18:22:36,false,1969487896,,4/26/2016 18:21:16,false,neodev,0.8889,35550011,VEN,07,Valencia,190.204.238.112,0,,"<p>Hi, so I read though the other piazza post about how the statistical analysis takes a long time, but I am still having some trouble with my code.</p>
<p></p>
<p>I have already tried waiting 10-15 minutes and removing all stopwords, but for some reason the statistical analysis refuses to complete in a reasonable amount of time</p>
<p></p>
<p>Below is my code:</p>
<pre>def isStopWord(word):
	with open(&#39;stopwords.txt&#39;, &#39;r&#39;) as stopwords:
		for line in stopwords:
			if word in line:
				return True
		return False

def get_features(X) : 
	features = []
	for x in X : 
		f = {}
		#TODO replace this dummy feature function with a unigram model, like we did in class
		for word in x.split():
			if isStopWord(word):
				continue
			if word in f:
				f[word] = f[word] &#43; 1.0
			else:
				f[word] =  1.0
		features.append(f)
	return features</pre>
<p>Not really sure what to do at this point because of this. Also might this be related to using the VM?</p>",Statistical Analysis Taking A Very Long Time,It takes a really long time. Is that expected?,statistical classification runtime,"<p>My script runs forever on my virtual machine, so I&#39;m running it off of biglab. I got the statistical analysis to work once (took about 2 minutes), but after un-commenting get_top_features and get_misclassified, my script gets killed before finishing the initial statistical analysis. I&#39;ve tried several times and I&#39;m unsure what the issue is; I&#39;ve copied my code below.</p>
<p></p>
<pre>#!/bin/python<br /><br />import os<br />import sys<br />import string<br />import random<br />import operator<br />from sklearn.tree import export_graphviz<br />from sklearn.tree import DecisionTreeClassifier<br />from sklearn.naive_bayes import MultinomialNB<br />from sklearn.linear_model import LogisticRegression<br />from sklearn.preprocessing import LabelEncoder<br />from sklearn.feature_extraction import DictVectorizer<br />from sklearn.cross_validation import train_test_split<br />from sklearn.externals.six import StringIO  <br /><br />#read in raw data from file and return a list of (label, article) tuples<br />def get_data(filename): <br />	data = [line.strip().split(&#39;\t&#39;) for line in open(filename).readlines()]<br />	random.shuffle(data)<br />	return data<br /><br />#this function builds the feature matrix for the Decision Tree.<br />def get_dtree_features(X) :<br />	features = []<br />	#TODO : Add the features you would like to use to train the Decision Tree here.<br />	feature_list = [&#39;gun&#39;, &#39;shooter&#39;, &#39;shot&#39;, &#39;shooting&#39;, &#39;player&#39;, &#39;coach&#39;, &#39;game&#39;, &#39;movie&#39;, &#39;actor&#39;, &#39;film&#39;, &#39;bullet&#39;, &#39;photo&#39;, &#39;model&#39;, &#39;point&#39;, &#39;suspect&#39;, &#39;victim&#39;, &#39;police&#39;, &#39;fired&#39;]<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w in feature_list : <br />				f[w] = 1.0<br />		features.append(f)<br />	return features<br /><br /><br />#this is the main function you care about; pack all the cleverest features you can think of into here.<br />def get_features(X) :<br />	stopwords = open(&#39;stopwords.txt&#39;).read()<br />	features = []<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w not in stopwords:            <br />				if w not in f : <br />					f[w] = 0<br />				f[w] &#43;= 1                <br />		features.append(f)<br />	return features<br /><br />#vectorize feature dictionaries and return feature and label matricies<br />def get_matricies(data, typ=&#34;unigram&#34;) : <br />	dv = DictVectorizer(sparse=True) <br />	le = LabelEncoder()<br />	y = [d[0] for d in data]<br />	texts = [d[1] for d in data]<br />	if typ == &#34;tree&#34;:<br />		X = get_dtree_features(texts)<br />	else :<br />		X = get_features(texts)<br />	#Here we are returning 5 things, the label vector y and feature matrix X, but also the texts from which the features were extracted and the <br />	#objects that were used to encode them. These will come in handy for your analysis, but you can ignore them for the initial parts of the assignment<br />	return le.fit_transform(y), dv.fit_transform(X), texts, dv, le<br /><br />#train and multinomial naive bayes classifier<br />def train_classifier(X, y):<br />	clf = LogisticRegression()<br />	clf.fit(X,y)<br />	return clf <br /><br />#train a Decision Tree classifier<br />def train_dtree_classifier(X, y):<br />	clf = DecisionTreeClassifier(max_depth=10)<br />	clf.fit(X,y)<br />	return clf<br /><br />#test the classifier<br />def test_classifier(clf, X, y):<br />	return clf.score(X,y)<br /><br />#cross validation	<br />def cross_validate(X, y, dv=None, typ=&#34;unigram&#34;, numfolds=5,):<br />	test_accs = []<br />	split = 1.0 / numfolds<br />	for i in range(numfolds):<br />		x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=i)<br />		if typ == &#34;tree&#34; :<br />			clf = train_dtree_classifier(x_train, y_train)<br />		else :<br />			clf = train_classifier(x_train, y_train)<br />		test_acc = test_classifier(clf, x_test, y_test)<br />		test_accs.append(test_acc)<br />		print &#39;Fold %d : %.05f&#39;%(i,test_acc)<br />	test_average = float(sum(test_accs))/ numfolds<br />	if typ == &#34;tree&#34; :<br />		with open(&#34;output.dot&#34;, &#39;w&#39;) as f:<br />			f = export_graphviz(clf, out_file=f, feature_names=dv.get_feature_names(), class_names=[&#39;Non Gun Related&#39;,&#39;Gun Related&#39;])<br />		create_graph(&#34;decision-tree.png&#34;)<br />	print &#39;Test Average : %.05f&#39;%(test_average)<br />	print<br />	return test_average<br /><br />#run a rule based classifier and calculate the accuracy<br />def rule_based_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO add more keywords, see how well they do alone and in combination<br />		if &#34;shooter&#34; in text or &#34;shot&#34; in text or &#34;shooting&#34; in text :<br />			if &#34;player&#34; in text or &#34;coach&#34; in text or &#34;game&#34; in text: <br />				prediction = &#39;0&#39;<br />			elif &#34;movie&#34; in text or &#34;film&#34; in text or &#34;actor&#34; in text:<br />				prediction = &#39;0&#39;<br />			else : prediction = &#39;1&#39;<br />		if &#34;gun&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Rule-based classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#Extra Credit Rule Based Classifier<br />def extra_credit_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO develop your conditional statements here<br />		if &#34;shooting&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Reverse Engineered classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#train and multinomial naive bayes classifier<br />def get_top_features(X, y, dv):<br />	clf = train_classifier(X, y)<br />	#the DictVectorizer object remembers which column number corresponds to which feature, and return the feature names in the correct order<br />	feature_names = dv.get_feature_names() <br /><br />	#The below code will get the weights from the classifier, and print out the weights of the features you are interested in<br />	features = [] #this will be a list of (feature_idx, weight) tuples<br />	for i,w in enumerate(clf.coef_[0]): <br />		features.append((i,w))<br />	#Sort the list by values, with the largest ones first<br />	features = sorted(features, key=lambda e: e[1], reverse=True)<br /><br />        #Print out the feature names and thier weights<br />	for i,w in features:<br />	  print &#39;%s\t%s&#39;%(feature_names[i], w)<br /><br />def get_misclassified_examples(y, X, texts) :<br />	x_train, x_test, y_train, y_test, train_texts, test_texts = train_test_split(X, y, texts)<br />	clf = train_classifier(x_train, y_train)<br /><br />	#TODO: You will have to write some code to call your classifier on each of the test examples, and check whether its prediction was right or wrong<br /><br />def create_graph(file_name) :<br />	os.system(&#34;dot -Tpng output.dot -o &#34; &#43; file_name)<br />	os.unlink(&#34;output.dot&#34;)<br /><br />if __name__ == &#39;__main__&#39; : <br /><br />	raw_data = get_data(sys.argv[1])<br />	<br />	print &#39;\nRule-based classification&#39;<br />	rule_based_classifier(raw_data)<br /><br />################ Decision Tree ################<br /><br /># print &#39;\nDecision Tree classification&#39;<br /># y, X, texts, dv, le = get_matricies(raw_data, &#34;tree&#34;)<br /># cross_validate(X,y,dv,&#34;tree&#34;)<br /><br />################ Statistical Classification ################<br />print &#39;\nStatistical classification&#39;<br />y, X, texts, dv, le = get_matricies(raw_data)<br />cross_validate(X,y)<br /><br />get_top_features(X, y, dv)<br />get_misclassified_examples(y, X, texts)<br />
</pre>
<p>Thanks in advance for any help you can give.</p>",Statistical Analysis &#34;Killed&#34; - even on biglab,"<p>When my program gets to the Statistical Analysis bit, it makes it through all the words and creates wordcount dictionaries that are appended to the list of features. Then it hangs for a bit until the terminal just prints &#34;Killed&#34; and program ends. Any idea what might be causing this? Not enough memory?</p>",Statistical Analysis &#34;Killed&#34;,,,"<p>I&#39;m confused on what exactly we&#39;re supposed to do for part 6. Are the y dimensions the same as the number of articles? Also, how is logistic regression supposed to be used here?</p>",Statistical Analysis Part 6,5
940847361,4/26/2016 18:27:26,false,1969490292,,4/26/2016 18:25:35,false,elite,1.0,30128662,BGR,50,Pleven,212.233.177.195,0,,"<p>Hi, so I read though the other piazza post about how the statistical analysis takes a long time, but I am still having some trouble with my code.</p>
<p></p>
<p>I have already tried waiting 10-15 minutes and removing all stopwords, but for some reason the statistical analysis refuses to complete in a reasonable amount of time</p>
<p></p>
<p>Below is my code:</p>
<pre>def isStopWord(word):
	with open(&#39;stopwords.txt&#39;, &#39;r&#39;) as stopwords:
		for line in stopwords:
			if word in line:
				return True
		return False

def get_features(X) : 
	features = []
	for x in X : 
		f = {}
		#TODO replace this dummy feature function with a unigram model, like we did in class
		for word in x.split():
			if isStopWord(word):
				continue
			if word in f:
				f[word] = f[word] &#43; 1.0
			else:
				f[word] =  1.0
		features.append(f)
	return features</pre>
<p>Not really sure what to do at this point because of this. Also might this be related to using the VM?</p>",Statistical Analysis Taking A Very Long Time,It takes a really long time. Is that expected?,statistical classification runtime,"<p>My script runs forever on my virtual machine, so I&#39;m running it off of biglab. I got the statistical analysis to work once (took about 2 minutes), but after un-commenting get_top_features and get_misclassified, my script gets killed before finishing the initial statistical analysis. I&#39;ve tried several times and I&#39;m unsure what the issue is; I&#39;ve copied my code below.</p>
<p></p>
<pre>#!/bin/python<br /><br />import os<br />import sys<br />import string<br />import random<br />import operator<br />from sklearn.tree import export_graphviz<br />from sklearn.tree import DecisionTreeClassifier<br />from sklearn.naive_bayes import MultinomialNB<br />from sklearn.linear_model import LogisticRegression<br />from sklearn.preprocessing import LabelEncoder<br />from sklearn.feature_extraction import DictVectorizer<br />from sklearn.cross_validation import train_test_split<br />from sklearn.externals.six import StringIO  <br /><br />#read in raw data from file and return a list of (label, article) tuples<br />def get_data(filename): <br />	data = [line.strip().split(&#39;\t&#39;) for line in open(filename).readlines()]<br />	random.shuffle(data)<br />	return data<br /><br />#this function builds the feature matrix for the Decision Tree.<br />def get_dtree_features(X) :<br />	features = []<br />	#TODO : Add the features you would like to use to train the Decision Tree here.<br />	feature_list = [&#39;gun&#39;, &#39;shooter&#39;, &#39;shot&#39;, &#39;shooting&#39;, &#39;player&#39;, &#39;coach&#39;, &#39;game&#39;, &#39;movie&#39;, &#39;actor&#39;, &#39;film&#39;, &#39;bullet&#39;, &#39;photo&#39;, &#39;model&#39;, &#39;point&#39;, &#39;suspect&#39;, &#39;victim&#39;, &#39;police&#39;, &#39;fired&#39;]<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w in feature_list : <br />				f[w] = 1.0<br />		features.append(f)<br />	return features<br /><br /><br />#this is the main function you care about; pack all the cleverest features you can think of into here.<br />def get_features(X) :<br />	stopwords = open(&#39;stopwords.txt&#39;).read()<br />	features = []<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w not in stopwords:            <br />				if w not in f : <br />					f[w] = 0<br />				f[w] &#43;= 1                <br />		features.append(f)<br />	return features<br /><br />#vectorize feature dictionaries and return feature and label matricies<br />def get_matricies(data, typ=&#34;unigram&#34;) : <br />	dv = DictVectorizer(sparse=True) <br />	le = LabelEncoder()<br />	y = [d[0] for d in data]<br />	texts = [d[1] for d in data]<br />	if typ == &#34;tree&#34;:<br />		X = get_dtree_features(texts)<br />	else :<br />		X = get_features(texts)<br />	#Here we are returning 5 things, the label vector y and feature matrix X, but also the texts from which the features were extracted and the <br />	#objects that were used to encode them. These will come in handy for your analysis, but you can ignore them for the initial parts of the assignment<br />	return le.fit_transform(y), dv.fit_transform(X), texts, dv, le<br /><br />#train and multinomial naive bayes classifier<br />def train_classifier(X, y):<br />	clf = LogisticRegression()<br />	clf.fit(X,y)<br />	return clf <br /><br />#train a Decision Tree classifier<br />def train_dtree_classifier(X, y):<br />	clf = DecisionTreeClassifier(max_depth=10)<br />	clf.fit(X,y)<br />	return clf<br /><br />#test the classifier<br />def test_classifier(clf, X, y):<br />	return clf.score(X,y)<br /><br />#cross validation	<br />def cross_validate(X, y, dv=None, typ=&#34;unigram&#34;, numfolds=5,):<br />	test_accs = []<br />	split = 1.0 / numfolds<br />	for i in range(numfolds):<br />		x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=i)<br />		if typ == &#34;tree&#34; :<br />			clf = train_dtree_classifier(x_train, y_train)<br />		else :<br />			clf = train_classifier(x_train, y_train)<br />		test_acc = test_classifier(clf, x_test, y_test)<br />		test_accs.append(test_acc)<br />		print &#39;Fold %d : %.05f&#39;%(i,test_acc)<br />	test_average = float(sum(test_accs))/ numfolds<br />	if typ == &#34;tree&#34; :<br />		with open(&#34;output.dot&#34;, &#39;w&#39;) as f:<br />			f = export_graphviz(clf, out_file=f, feature_names=dv.get_feature_names(), class_names=[&#39;Non Gun Related&#39;,&#39;Gun Related&#39;])<br />		create_graph(&#34;decision-tree.png&#34;)<br />	print &#39;Test Average : %.05f&#39;%(test_average)<br />	print<br />	return test_average<br /><br />#run a rule based classifier and calculate the accuracy<br />def rule_based_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO add more keywords, see how well they do alone and in combination<br />		if &#34;shooter&#34; in text or &#34;shot&#34; in text or &#34;shooting&#34; in text :<br />			if &#34;player&#34; in text or &#34;coach&#34; in text or &#34;game&#34; in text: <br />				prediction = &#39;0&#39;<br />			elif &#34;movie&#34; in text or &#34;film&#34; in text or &#34;actor&#34; in text:<br />				prediction = &#39;0&#39;<br />			else : prediction = &#39;1&#39;<br />		if &#34;gun&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Rule-based classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#Extra Credit Rule Based Classifier<br />def extra_credit_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO develop your conditional statements here<br />		if &#34;shooting&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Reverse Engineered classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#train and multinomial naive bayes classifier<br />def get_top_features(X, y, dv):<br />	clf = train_classifier(X, y)<br />	#the DictVectorizer object remembers which column number corresponds to which feature, and return the feature names in the correct order<br />	feature_names = dv.get_feature_names() <br /><br />	#The below code will get the weights from the classifier, and print out the weights of the features you are interested in<br />	features = [] #this will be a list of (feature_idx, weight) tuples<br />	for i,w in enumerate(clf.coef_[0]): <br />		features.append((i,w))<br />	#Sort the list by values, with the largest ones first<br />	features = sorted(features, key=lambda e: e[1], reverse=True)<br /><br />        #Print out the feature names and thier weights<br />	for i,w in features:<br />	  print &#39;%s\t%s&#39;%(feature_names[i], w)<br /><br />def get_misclassified_examples(y, X, texts) :<br />	x_train, x_test, y_train, y_test, train_texts, test_texts = train_test_split(X, y, texts)<br />	clf = train_classifier(x_train, y_train)<br /><br />	#TODO: You will have to write some code to call your classifier on each of the test examples, and check whether its prediction was right or wrong<br /><br />def create_graph(file_name) :<br />	os.system(&#34;dot -Tpng output.dot -o &#34; &#43; file_name)<br />	os.unlink(&#34;output.dot&#34;)<br /><br />if __name__ == &#39;__main__&#39; : <br /><br />	raw_data = get_data(sys.argv[1])<br />	<br />	print &#39;\nRule-based classification&#39;<br />	rule_based_classifier(raw_data)<br /><br />################ Decision Tree ################<br /><br /># print &#39;\nDecision Tree classification&#39;<br /># y, X, texts, dv, le = get_matricies(raw_data, &#34;tree&#34;)<br /># cross_validate(X,y,dv,&#34;tree&#34;)<br /><br />################ Statistical Classification ################<br />print &#39;\nStatistical classification&#39;<br />y, X, texts, dv, le = get_matricies(raw_data)<br />cross_validate(X,y)<br /><br />get_top_features(X, y, dv)<br />get_misclassified_examples(y, X, texts)<br />
</pre>
<p>Thanks in advance for any help you can give.</p>",Statistical Analysis &#34;Killed&#34; - even on biglab,"<p>When my program gets to the Statistical Analysis bit, it makes it through all the words and creates wordcount dictionaries that are appended to the list of features. Then it hangs for a bit until the terminal just prints &#34;Killed&#34; and program ends. Any idea what might be causing this? Not enough memory?</p>",Statistical Analysis &#34;Killed&#34;,,,"<p>I&#39;m confused on what exactly we&#39;re supposed to do for part 6. Are the y dimensions the same as the number of articles? Also, how is logistic regression supposed to be used here?</p>",Statistical Analysis Part 6,5
940847361,4/26/2016 18:37:13,false,1969495297,,4/26/2016 18:33:39,false,neodev,1.0,29879245,RUS,69,Smolensk,37.144.124.118,0,,"<p>Hi, so I read though the other piazza post about how the statistical analysis takes a long time, but I am still having some trouble with my code.</p>
<p></p>
<p>I have already tried waiting 10-15 minutes and removing all stopwords, but for some reason the statistical analysis refuses to complete in a reasonable amount of time</p>
<p></p>
<p>Below is my code:</p>
<pre>def isStopWord(word):
	with open(&#39;stopwords.txt&#39;, &#39;r&#39;) as stopwords:
		for line in stopwords:
			if word in line:
				return True
		return False

def get_features(X) : 
	features = []
	for x in X : 
		f = {}
		#TODO replace this dummy feature function with a unigram model, like we did in class
		for word in x.split():
			if isStopWord(word):
				continue
			if word in f:
				f[word] = f[word] &#43; 1.0
			else:
				f[word] =  1.0
		features.append(f)
	return features</pre>
<p>Not really sure what to do at this point because of this. Also might this be related to using the VM?</p>",Statistical Analysis Taking A Very Long Time,It takes a really long time. Is that expected?,statistical classification runtime,"<p>My script runs forever on my virtual machine, so I&#39;m running it off of biglab. I got the statistical analysis to work once (took about 2 minutes), but after un-commenting get_top_features and get_misclassified, my script gets killed before finishing the initial statistical analysis. I&#39;ve tried several times and I&#39;m unsure what the issue is; I&#39;ve copied my code below.</p>
<p></p>
<pre>#!/bin/python<br /><br />import os<br />import sys<br />import string<br />import random<br />import operator<br />from sklearn.tree import export_graphviz<br />from sklearn.tree import DecisionTreeClassifier<br />from sklearn.naive_bayes import MultinomialNB<br />from sklearn.linear_model import LogisticRegression<br />from sklearn.preprocessing import LabelEncoder<br />from sklearn.feature_extraction import DictVectorizer<br />from sklearn.cross_validation import train_test_split<br />from sklearn.externals.six import StringIO  <br /><br />#read in raw data from file and return a list of (label, article) tuples<br />def get_data(filename): <br />	data = [line.strip().split(&#39;\t&#39;) for line in open(filename).readlines()]<br />	random.shuffle(data)<br />	return data<br /><br />#this function builds the feature matrix for the Decision Tree.<br />def get_dtree_features(X) :<br />	features = []<br />	#TODO : Add the features you would like to use to train the Decision Tree here.<br />	feature_list = [&#39;gun&#39;, &#39;shooter&#39;, &#39;shot&#39;, &#39;shooting&#39;, &#39;player&#39;, &#39;coach&#39;, &#39;game&#39;, &#39;movie&#39;, &#39;actor&#39;, &#39;film&#39;, &#39;bullet&#39;, &#39;photo&#39;, &#39;model&#39;, &#39;point&#39;, &#39;suspect&#39;, &#39;victim&#39;, &#39;police&#39;, &#39;fired&#39;]<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w in feature_list : <br />				f[w] = 1.0<br />		features.append(f)<br />	return features<br /><br /><br />#this is the main function you care about; pack all the cleverest features you can think of into here.<br />def get_features(X) :<br />	stopwords = open(&#39;stopwords.txt&#39;).read()<br />	features = []<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w not in stopwords:            <br />				if w not in f : <br />					f[w] = 0<br />				f[w] &#43;= 1                <br />		features.append(f)<br />	return features<br /><br />#vectorize feature dictionaries and return feature and label matricies<br />def get_matricies(data, typ=&#34;unigram&#34;) : <br />	dv = DictVectorizer(sparse=True) <br />	le = LabelEncoder()<br />	y = [d[0] for d in data]<br />	texts = [d[1] for d in data]<br />	if typ == &#34;tree&#34;:<br />		X = get_dtree_features(texts)<br />	else :<br />		X = get_features(texts)<br />	#Here we are returning 5 things, the label vector y and feature matrix X, but also the texts from which the features were extracted and the <br />	#objects that were used to encode them. These will come in handy for your analysis, but you can ignore them for the initial parts of the assignment<br />	return le.fit_transform(y), dv.fit_transform(X), texts, dv, le<br /><br />#train and multinomial naive bayes classifier<br />def train_classifier(X, y):<br />	clf = LogisticRegression()<br />	clf.fit(X,y)<br />	return clf <br /><br />#train a Decision Tree classifier<br />def train_dtree_classifier(X, y):<br />	clf = DecisionTreeClassifier(max_depth=10)<br />	clf.fit(X,y)<br />	return clf<br /><br />#test the classifier<br />def test_classifier(clf, X, y):<br />	return clf.score(X,y)<br /><br />#cross validation	<br />def cross_validate(X, y, dv=None, typ=&#34;unigram&#34;, numfolds=5,):<br />	test_accs = []<br />	split = 1.0 / numfolds<br />	for i in range(numfolds):<br />		x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=i)<br />		if typ == &#34;tree&#34; :<br />			clf = train_dtree_classifier(x_train, y_train)<br />		else :<br />			clf = train_classifier(x_train, y_train)<br />		test_acc = test_classifier(clf, x_test, y_test)<br />		test_accs.append(test_acc)<br />		print &#39;Fold %d : %.05f&#39;%(i,test_acc)<br />	test_average = float(sum(test_accs))/ numfolds<br />	if typ == &#34;tree&#34; :<br />		with open(&#34;output.dot&#34;, &#39;w&#39;) as f:<br />			f = export_graphviz(clf, out_file=f, feature_names=dv.get_feature_names(), class_names=[&#39;Non Gun Related&#39;,&#39;Gun Related&#39;])<br />		create_graph(&#34;decision-tree.png&#34;)<br />	print &#39;Test Average : %.05f&#39;%(test_average)<br />	print<br />	return test_average<br /><br />#run a rule based classifier and calculate the accuracy<br />def rule_based_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO add more keywords, see how well they do alone and in combination<br />		if &#34;shooter&#34; in text or &#34;shot&#34; in text or &#34;shooting&#34; in text :<br />			if &#34;player&#34; in text or &#34;coach&#34; in text or &#34;game&#34; in text: <br />				prediction = &#39;0&#39;<br />			elif &#34;movie&#34; in text or &#34;film&#34; in text or &#34;actor&#34; in text:<br />				prediction = &#39;0&#39;<br />			else : prediction = &#39;1&#39;<br />		if &#34;gun&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Rule-based classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#Extra Credit Rule Based Classifier<br />def extra_credit_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO develop your conditional statements here<br />		if &#34;shooting&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Reverse Engineered classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#train and multinomial naive bayes classifier<br />def get_top_features(X, y, dv):<br />	clf = train_classifier(X, y)<br />	#the DictVectorizer object remembers which column number corresponds to which feature, and return the feature names in the correct order<br />	feature_names = dv.get_feature_names() <br /><br />	#The below code will get the weights from the classifier, and print out the weights of the features you are interested in<br />	features = [] #this will be a list of (feature_idx, weight) tuples<br />	for i,w in enumerate(clf.coef_[0]): <br />		features.append((i,w))<br />	#Sort the list by values, with the largest ones first<br />	features = sorted(features, key=lambda e: e[1], reverse=True)<br /><br />        #Print out the feature names and thier weights<br />	for i,w in features:<br />	  print &#39;%s\t%s&#39;%(feature_names[i], w)<br /><br />def get_misclassified_examples(y, X, texts) :<br />	x_train, x_test, y_train, y_test, train_texts, test_texts = train_test_split(X, y, texts)<br />	clf = train_classifier(x_train, y_train)<br /><br />	#TODO: You will have to write some code to call your classifier on each of the test examples, and check whether its prediction was right or wrong<br /><br />def create_graph(file_name) :<br />	os.system(&#34;dot -Tpng output.dot -o &#34; &#43; file_name)<br />	os.unlink(&#34;output.dot&#34;)<br /><br />if __name__ == &#39;__main__&#39; : <br /><br />	raw_data = get_data(sys.argv[1])<br />	<br />	print &#39;\nRule-based classification&#39;<br />	rule_based_classifier(raw_data)<br /><br />################ Decision Tree ################<br /><br /># print &#39;\nDecision Tree classification&#39;<br /># y, X, texts, dv, le = get_matricies(raw_data, &#34;tree&#34;)<br /># cross_validate(X,y,dv,&#34;tree&#34;)<br /><br />################ Statistical Classification ################<br />print &#39;\nStatistical classification&#39;<br />y, X, texts, dv, le = get_matricies(raw_data)<br />cross_validate(X,y)<br /><br />get_top_features(X, y, dv)<br />get_misclassified_examples(y, X, texts)<br />
</pre>
<p>Thanks in advance for any help you can give.</p>",Statistical Analysis &#34;Killed&#34; - even on biglab,"<p>When my program gets to the Statistical Analysis bit, it makes it through all the words and creates wordcount dictionaries that are appended to the list of features. Then it hangs for a bit until the terminal just prints &#34;Killed&#34; and program ends. Any idea what might be causing this? Not enough memory?</p>",Statistical Analysis &#34;Killed&#34;,,,"<p>I&#39;m confused on what exactly we&#39;re supposed to do for part 6. Are the y dimensions the same as the number of articles? Also, how is logistic regression supposed to be used here?</p>",Statistical Analysis Part 6,5
940847361,4/26/2016 19:02:49,false,1969507175,,4/26/2016 18:44:58,false,neodev,1.0,11172894,IND,28,Champdani,117.194.5.117,0,,"<p>Hi, so I read though the other piazza post about how the statistical analysis takes a long time, but I am still having some trouble with my code.</p>
<p></p>
<p>I have already tried waiting 10-15 minutes and removing all stopwords, but for some reason the statistical analysis refuses to complete in a reasonable amount of time</p>
<p></p>
<p>Below is my code:</p>
<pre>def isStopWord(word):
	with open(&#39;stopwords.txt&#39;, &#39;r&#39;) as stopwords:
		for line in stopwords:
			if word in line:
				return True
		return False

def get_features(X) : 
	features = []
	for x in X : 
		f = {}
		#TODO replace this dummy feature function with a unigram model, like we did in class
		for word in x.split():
			if isStopWord(word):
				continue
			if word in f:
				f[word] = f[word] &#43; 1.0
			else:
				f[word] =  1.0
		features.append(f)
	return features</pre>
<p>Not really sure what to do at this point because of this. Also might this be related to using the VM?</p>",Statistical Analysis Taking A Very Long Time,It takes a really long time. Is that expected?,statistical classification runtime,"<p>My script runs forever on my virtual machine, so I&#39;m running it off of biglab. I got the statistical analysis to work once (took about 2 minutes), but after un-commenting get_top_features and get_misclassified, my script gets killed before finishing the initial statistical analysis. I&#39;ve tried several times and I&#39;m unsure what the issue is; I&#39;ve copied my code below.</p>
<p></p>
<pre>#!/bin/python<br /><br />import os<br />import sys<br />import string<br />import random<br />import operator<br />from sklearn.tree import export_graphviz<br />from sklearn.tree import DecisionTreeClassifier<br />from sklearn.naive_bayes import MultinomialNB<br />from sklearn.linear_model import LogisticRegression<br />from sklearn.preprocessing import LabelEncoder<br />from sklearn.feature_extraction import DictVectorizer<br />from sklearn.cross_validation import train_test_split<br />from sklearn.externals.six import StringIO  <br /><br />#read in raw data from file and return a list of (label, article) tuples<br />def get_data(filename): <br />	data = [line.strip().split(&#39;\t&#39;) for line in open(filename).readlines()]<br />	random.shuffle(data)<br />	return data<br /><br />#this function builds the feature matrix for the Decision Tree.<br />def get_dtree_features(X) :<br />	features = []<br />	#TODO : Add the features you would like to use to train the Decision Tree here.<br />	feature_list = [&#39;gun&#39;, &#39;shooter&#39;, &#39;shot&#39;, &#39;shooting&#39;, &#39;player&#39;, &#39;coach&#39;, &#39;game&#39;, &#39;movie&#39;, &#39;actor&#39;, &#39;film&#39;, &#39;bullet&#39;, &#39;photo&#39;, &#39;model&#39;, &#39;point&#39;, &#39;suspect&#39;, &#39;victim&#39;, &#39;police&#39;, &#39;fired&#39;]<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w in feature_list : <br />				f[w] = 1.0<br />		features.append(f)<br />	return features<br /><br /><br />#this is the main function you care about; pack all the cleverest features you can think of into here.<br />def get_features(X) :<br />	stopwords = open(&#39;stopwords.txt&#39;).read()<br />	features = []<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w not in stopwords:            <br />				if w not in f : <br />					f[w] = 0<br />				f[w] &#43;= 1                <br />		features.append(f)<br />	return features<br /><br />#vectorize feature dictionaries and return feature and label matricies<br />def get_matricies(data, typ=&#34;unigram&#34;) : <br />	dv = DictVectorizer(sparse=True) <br />	le = LabelEncoder()<br />	y = [d[0] for d in data]<br />	texts = [d[1] for d in data]<br />	if typ == &#34;tree&#34;:<br />		X = get_dtree_features(texts)<br />	else :<br />		X = get_features(texts)<br />	#Here we are returning 5 things, the label vector y and feature matrix X, but also the texts from which the features were extracted and the <br />	#objects that were used to encode them. These will come in handy for your analysis, but you can ignore them for the initial parts of the assignment<br />	return le.fit_transform(y), dv.fit_transform(X), texts, dv, le<br /><br />#train and multinomial naive bayes classifier<br />def train_classifier(X, y):<br />	clf = LogisticRegression()<br />	clf.fit(X,y)<br />	return clf <br /><br />#train a Decision Tree classifier<br />def train_dtree_classifier(X, y):<br />	clf = DecisionTreeClassifier(max_depth=10)<br />	clf.fit(X,y)<br />	return clf<br /><br />#test the classifier<br />def test_classifier(clf, X, y):<br />	return clf.score(X,y)<br /><br />#cross validation	<br />def cross_validate(X, y, dv=None, typ=&#34;unigram&#34;, numfolds=5,):<br />	test_accs = []<br />	split = 1.0 / numfolds<br />	for i in range(numfolds):<br />		x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=i)<br />		if typ == &#34;tree&#34; :<br />			clf = train_dtree_classifier(x_train, y_train)<br />		else :<br />			clf = train_classifier(x_train, y_train)<br />		test_acc = test_classifier(clf, x_test, y_test)<br />		test_accs.append(test_acc)<br />		print &#39;Fold %d : %.05f&#39;%(i,test_acc)<br />	test_average = float(sum(test_accs))/ numfolds<br />	if typ == &#34;tree&#34; :<br />		with open(&#34;output.dot&#34;, &#39;w&#39;) as f:<br />			f = export_graphviz(clf, out_file=f, feature_names=dv.get_feature_names(), class_names=[&#39;Non Gun Related&#39;,&#39;Gun Related&#39;])<br />		create_graph(&#34;decision-tree.png&#34;)<br />	print &#39;Test Average : %.05f&#39;%(test_average)<br />	print<br />	return test_average<br /><br />#run a rule based classifier and calculate the accuracy<br />def rule_based_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO add more keywords, see how well they do alone and in combination<br />		if &#34;shooter&#34; in text or &#34;shot&#34; in text or &#34;shooting&#34; in text :<br />			if &#34;player&#34; in text or &#34;coach&#34; in text or &#34;game&#34; in text: <br />				prediction = &#39;0&#39;<br />			elif &#34;movie&#34; in text or &#34;film&#34; in text or &#34;actor&#34; in text:<br />				prediction = &#39;0&#39;<br />			else : prediction = &#39;1&#39;<br />		if &#34;gun&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Rule-based classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#Extra Credit Rule Based Classifier<br />def extra_credit_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO develop your conditional statements here<br />		if &#34;shooting&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Reverse Engineered classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#train and multinomial naive bayes classifier<br />def get_top_features(X, y, dv):<br />	clf = train_classifier(X, y)<br />	#the DictVectorizer object remembers which column number corresponds to which feature, and return the feature names in the correct order<br />	feature_names = dv.get_feature_names() <br /><br />	#The below code will get the weights from the classifier, and print out the weights of the features you are interested in<br />	features = [] #this will be a list of (feature_idx, weight) tuples<br />	for i,w in enumerate(clf.coef_[0]): <br />		features.append((i,w))<br />	#Sort the list by values, with the largest ones first<br />	features = sorted(features, key=lambda e: e[1], reverse=True)<br /><br />        #Print out the feature names and thier weights<br />	for i,w in features:<br />	  print &#39;%s\t%s&#39;%(feature_names[i], w)<br /><br />def get_misclassified_examples(y, X, texts) :<br />	x_train, x_test, y_train, y_test, train_texts, test_texts = train_test_split(X, y, texts)<br />	clf = train_classifier(x_train, y_train)<br /><br />	#TODO: You will have to write some code to call your classifier on each of the test examples, and check whether its prediction was right or wrong<br /><br />def create_graph(file_name) :<br />	os.system(&#34;dot -Tpng output.dot -o &#34; &#43; file_name)<br />	os.unlink(&#34;output.dot&#34;)<br /><br />if __name__ == &#39;__main__&#39; : <br /><br />	raw_data = get_data(sys.argv[1])<br />	<br />	print &#39;\nRule-based classification&#39;<br />	rule_based_classifier(raw_data)<br /><br />################ Decision Tree ################<br /><br /># print &#39;\nDecision Tree classification&#39;<br /># y, X, texts, dv, le = get_matricies(raw_data, &#34;tree&#34;)<br /># cross_validate(X,y,dv,&#34;tree&#34;)<br /><br />################ Statistical Classification ################<br />print &#39;\nStatistical classification&#39;<br />y, X, texts, dv, le = get_matricies(raw_data)<br />cross_validate(X,y)<br /><br />get_top_features(X, y, dv)<br />get_misclassified_examples(y, X, texts)<br />
</pre>
<p>Thanks in advance for any help you can give.</p>",Statistical Analysis &#34;Killed&#34; - even on biglab,"<p>When my program gets to the Statistical Analysis bit, it makes it through all the words and creates wordcount dictionaries that are appended to the list of features. Then it hangs for a bit until the terminal just prints &#34;Killed&#34; and program ends. Any idea what might be causing this? Not enough memory?</p>",Statistical Analysis &#34;Killed&#34;,,,"<p>I&#39;m confused on what exactly we&#39;re supposed to do for part 6. Are the y dimensions the same as the number of articles? Also, how is logistic regression supposed to be used here?</p>",Statistical Analysis Part 6,5
940847361,4/26/2016 19:25:38,false,1969519499,,4/26/2016 19:25:12,false,tremorgames,1.0,25197223,HRV,15,Split,94.253.234.240,0,,"<p>Hi, so I read though the other piazza post about how the statistical analysis takes a long time, but I am still having some trouble with my code.</p>
<p></p>
<p>I have already tried waiting 10-15 minutes and removing all stopwords, but for some reason the statistical analysis refuses to complete in a reasonable amount of time</p>
<p></p>
<p>Below is my code:</p>
<pre>def isStopWord(word):
	with open(&#39;stopwords.txt&#39;, &#39;r&#39;) as stopwords:
		for line in stopwords:
			if word in line:
				return True
		return False

def get_features(X) : 
	features = []
	for x in X : 
		f = {}
		#TODO replace this dummy feature function with a unigram model, like we did in class
		for word in x.split():
			if isStopWord(word):
				continue
			if word in f:
				f[word] = f[word] &#43; 1.0
			else:
				f[word] =  1.0
		features.append(f)
	return features</pre>
<p>Not really sure what to do at this point because of this. Also might this be related to using the VM?</p>",Statistical Analysis Taking A Very Long Time,It takes a really long time. Is that expected?,statistical classification runtime,"<p>My script runs forever on my virtual machine, so I&#39;m running it off of biglab. I got the statistical analysis to work once (took about 2 minutes), but after un-commenting get_top_features and get_misclassified, my script gets killed before finishing the initial statistical analysis. I&#39;ve tried several times and I&#39;m unsure what the issue is; I&#39;ve copied my code below.</p>
<p></p>
<pre>#!/bin/python<br /><br />import os<br />import sys<br />import string<br />import random<br />import operator<br />from sklearn.tree import export_graphviz<br />from sklearn.tree import DecisionTreeClassifier<br />from sklearn.naive_bayes import MultinomialNB<br />from sklearn.linear_model import LogisticRegression<br />from sklearn.preprocessing import LabelEncoder<br />from sklearn.feature_extraction import DictVectorizer<br />from sklearn.cross_validation import train_test_split<br />from sklearn.externals.six import StringIO  <br /><br />#read in raw data from file and return a list of (label, article) tuples<br />def get_data(filename): <br />	data = [line.strip().split(&#39;\t&#39;) for line in open(filename).readlines()]<br />	random.shuffle(data)<br />	return data<br /><br />#this function builds the feature matrix for the Decision Tree.<br />def get_dtree_features(X) :<br />	features = []<br />	#TODO : Add the features you would like to use to train the Decision Tree here.<br />	feature_list = [&#39;gun&#39;, &#39;shooter&#39;, &#39;shot&#39;, &#39;shooting&#39;, &#39;player&#39;, &#39;coach&#39;, &#39;game&#39;, &#39;movie&#39;, &#39;actor&#39;, &#39;film&#39;, &#39;bullet&#39;, &#39;photo&#39;, &#39;model&#39;, &#39;point&#39;, &#39;suspect&#39;, &#39;victim&#39;, &#39;police&#39;, &#39;fired&#39;]<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w in feature_list : <br />				f[w] = 1.0<br />		features.append(f)<br />	return features<br /><br /><br />#this is the main function you care about; pack all the cleverest features you can think of into here.<br />def get_features(X) :<br />	stopwords = open(&#39;stopwords.txt&#39;).read()<br />	features = []<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w not in stopwords:            <br />				if w not in f : <br />					f[w] = 0<br />				f[w] &#43;= 1                <br />		features.append(f)<br />	return features<br /><br />#vectorize feature dictionaries and return feature and label matricies<br />def get_matricies(data, typ=&#34;unigram&#34;) : <br />	dv = DictVectorizer(sparse=True) <br />	le = LabelEncoder()<br />	y = [d[0] for d in data]<br />	texts = [d[1] for d in data]<br />	if typ == &#34;tree&#34;:<br />		X = get_dtree_features(texts)<br />	else :<br />		X = get_features(texts)<br />	#Here we are returning 5 things, the label vector y and feature matrix X, but also the texts from which the features were extracted and the <br />	#objects that were used to encode them. These will come in handy for your analysis, but you can ignore them for the initial parts of the assignment<br />	return le.fit_transform(y), dv.fit_transform(X), texts, dv, le<br /><br />#train and multinomial naive bayes classifier<br />def train_classifier(X, y):<br />	clf = LogisticRegression()<br />	clf.fit(X,y)<br />	return clf <br /><br />#train a Decision Tree classifier<br />def train_dtree_classifier(X, y):<br />	clf = DecisionTreeClassifier(max_depth=10)<br />	clf.fit(X,y)<br />	return clf<br /><br />#test the classifier<br />def test_classifier(clf, X, y):<br />	return clf.score(X,y)<br /><br />#cross validation	<br />def cross_validate(X, y, dv=None, typ=&#34;unigram&#34;, numfolds=5,):<br />	test_accs = []<br />	split = 1.0 / numfolds<br />	for i in range(numfolds):<br />		x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=i)<br />		if typ == &#34;tree&#34; :<br />			clf = train_dtree_classifier(x_train, y_train)<br />		else :<br />			clf = train_classifier(x_train, y_train)<br />		test_acc = test_classifier(clf, x_test, y_test)<br />		test_accs.append(test_acc)<br />		print &#39;Fold %d : %.05f&#39;%(i,test_acc)<br />	test_average = float(sum(test_accs))/ numfolds<br />	if typ == &#34;tree&#34; :<br />		with open(&#34;output.dot&#34;, &#39;w&#39;) as f:<br />			f = export_graphviz(clf, out_file=f, feature_names=dv.get_feature_names(), class_names=[&#39;Non Gun Related&#39;,&#39;Gun Related&#39;])<br />		create_graph(&#34;decision-tree.png&#34;)<br />	print &#39;Test Average : %.05f&#39;%(test_average)<br />	print<br />	return test_average<br /><br />#run a rule based classifier and calculate the accuracy<br />def rule_based_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO add more keywords, see how well they do alone and in combination<br />		if &#34;shooter&#34; in text or &#34;shot&#34; in text or &#34;shooting&#34; in text :<br />			if &#34;player&#34; in text or &#34;coach&#34; in text or &#34;game&#34; in text: <br />				prediction = &#39;0&#39;<br />			elif &#34;movie&#34; in text or &#34;film&#34; in text or &#34;actor&#34; in text:<br />				prediction = &#39;0&#39;<br />			else : prediction = &#39;1&#39;<br />		if &#34;gun&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Rule-based classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#Extra Credit Rule Based Classifier<br />def extra_credit_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO develop your conditional statements here<br />		if &#34;shooting&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Reverse Engineered classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#train and multinomial naive bayes classifier<br />def get_top_features(X, y, dv):<br />	clf = train_classifier(X, y)<br />	#the DictVectorizer object remembers which column number corresponds to which feature, and return the feature names in the correct order<br />	feature_names = dv.get_feature_names() <br /><br />	#The below code will get the weights from the classifier, and print out the weights of the features you are interested in<br />	features = [] #this will be a list of (feature_idx, weight) tuples<br />	for i,w in enumerate(clf.coef_[0]): <br />		features.append((i,w))<br />	#Sort the list by values, with the largest ones first<br />	features = sorted(features, key=lambda e: e[1], reverse=True)<br /><br />        #Print out the feature names and thier weights<br />	for i,w in features:<br />	  print &#39;%s\t%s&#39;%(feature_names[i], w)<br /><br />def get_misclassified_examples(y, X, texts) :<br />	x_train, x_test, y_train, y_test, train_texts, test_texts = train_test_split(X, y, texts)<br />	clf = train_classifier(x_train, y_train)<br /><br />	#TODO: You will have to write some code to call your classifier on each of the test examples, and check whether its prediction was right or wrong<br /><br />def create_graph(file_name) :<br />	os.system(&#34;dot -Tpng output.dot -o &#34; &#43; file_name)<br />	os.unlink(&#34;output.dot&#34;)<br /><br />if __name__ == &#39;__main__&#39; : <br /><br />	raw_data = get_data(sys.argv[1])<br />	<br />	print &#39;\nRule-based classification&#39;<br />	rule_based_classifier(raw_data)<br /><br />################ Decision Tree ################<br /><br /># print &#39;\nDecision Tree classification&#39;<br /># y, X, texts, dv, le = get_matricies(raw_data, &#34;tree&#34;)<br /># cross_validate(X,y,dv,&#34;tree&#34;)<br /><br />################ Statistical Classification ################<br />print &#39;\nStatistical classification&#39;<br />y, X, texts, dv, le = get_matricies(raw_data)<br />cross_validate(X,y)<br /><br />get_top_features(X, y, dv)<br />get_misclassified_examples(y, X, texts)<br />
</pre>
<p>Thanks in advance for any help you can give.</p>",Statistical Analysis &#34;Killed&#34; - even on biglab,"<p>When my program gets to the Statistical Analysis bit, it makes it through all the words and creates wordcount dictionaries that are appended to the list of features. Then it hangs for a bit until the terminal just prints &#34;Killed&#34; and program ends. Any idea what might be causing this? Not enough memory?</p>",Statistical Analysis &#34;Killed&#34;,,,"<p>I&#39;m confused on what exactly we&#39;re supposed to do for part 6. Are the y dimensions the same as the number of articles? Also, how is logistic regression supposed to be used here?</p>",Statistical Analysis Part 6,5
940847362,4/26/2016 17:32:34,false,1969460927,,4/26/2016 17:32:18,false,neodev,0.8889,33131546,IDN,04,Jakarta,139.194.89.60,0,,"<p>When I try to submit the homework, it does this marvelous thing where it tells me that net213 is an invalid config file.</p>
<p></p>
<p>What to do now? Somewhat concerned.</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hqh2r7yc6p926r/ikaaipxq9b3g/image.png"" /></p>",Invalid config file,"<p>InåÊthe bing_api.py, I am trying to print out the xml response that we receive.</p>
<p>Any ideas how to do this?</p>",Print out xml file,<p>I am very confused about what the 6 files we are expected to have after Part 1. Can someone help me out?</p>,6 Files,"<p>how would you be able to copy to biglab again? i went through this with TA in OH but I forgot.åÊ</p>
<p></p>
<p>It was along the lines of (code) (<a href=""mailto:username&#64;biglab.seas...&#64;"">username&#64;biglab.seas...&#64;</a>)(filename)</p>",Copying files to BigLab,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8na1baym4qp/ijwydxbuj4vu/Screen_Shot_20160127_at_9.44.27_AM.png"" /></p>
<p>I keep getting this error on the last step for installing iPython Notebook. Followed all of the instructions earlier and they seem to have all worked.åÊ</p>",[iPython Notebook Error] Bad Interpreter: No such file or directory,"<p>We keep getting the following error on Crowdflower when trying to upload the date for our first HIT:åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/h6snkankbb96wq/im6rcyi1fkbg/Screen_Shot_20160324_at_4.53.49_PM.png"" /></p>
<p>We are not sure what this means since we have only inserted one row into the excel sheet to add the headers.</p>
<p></p>
<p>On another note, some of the text separates into several columns instead of staying in one. Is this just accidental tab separations in the text that Excel recognizes as a new column?</p>",Upload CSV/Excel File Error,1
940847362,4/26/2016 17:36:01,false,1969462738,,4/26/2016 17:35:20,false,clixsense,1.0,30712378,ROU,21,Deva,79.119.241.200,0,,"<p>When I try to submit the homework, it does this marvelous thing where it tells me that net213 is an invalid config file.</p>
<p></p>
<p>What to do now? Somewhat concerned.</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hqh2r7yc6p926r/ikaaipxq9b3g/image.png"" /></p>",Invalid config file,"<p>InåÊthe bing_api.py, I am trying to print out the xml response that we receive.</p>
<p>Any ideas how to do this?</p>",Print out xml file,<p>I am very confused about what the 6 files we are expected to have after Part 1. Can someone help me out?</p>,6 Files,"<p>how would you be able to copy to biglab again? i went through this with TA in OH but I forgot.åÊ</p>
<p></p>
<p>It was along the lines of (code) (<a href=""mailto:username&#64;biglab.seas...&#64;"">username&#64;biglab.seas...&#64;</a>)(filename)</p>",Copying files to BigLab,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8na1baym4qp/ijwydxbuj4vu/Screen_Shot_20160127_at_9.44.27_AM.png"" /></p>
<p>I keep getting this error on the last step for installing iPython Notebook. Followed all of the instructions earlier and they seem to have all worked.åÊ</p>",[iPython Notebook Error] Bad Interpreter: No such file or directory,"<p>We keep getting the following error on Crowdflower when trying to upload the date for our first HIT:åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/h6snkankbb96wq/im6rcyi1fkbg/Screen_Shot_20160324_at_4.53.49_PM.png"" /></p>
<p>We are not sure what this means since we have only inserted one row into the excel sheet to add the headers.</p>
<p></p>
<p>On another note, some of the text separates into several columns instead of staying in one. Is this just accidental tab separations in the text that Excel recognizes as a new column?</p>",Upload CSV/Excel File Error,1
940847362,4/26/2016 17:38:55,false,1969464255,,4/26/2016 17:36:08,false,neodev,0.8889,33568303,VEN,23,Cabimas,190.77.7.36,0,,"<p>When I try to submit the homework, it does this marvelous thing where it tells me that net213 is an invalid config file.</p>
<p></p>
<p>What to do now? Somewhat concerned.</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hqh2r7yc6p926r/ikaaipxq9b3g/image.png"" /></p>",Invalid config file,"<p>InåÊthe bing_api.py, I am trying to print out the xml response that we receive.</p>
<p>Any ideas how to do this?</p>",Print out xml file,<p>I am very confused about what the 6 files we are expected to have after Part 1. Can someone help me out?</p>,6 Files,"<p>how would you be able to copy to biglab again? i went through this with TA in OH but I forgot.åÊ</p>
<p></p>
<p>It was along the lines of (code) (<a href=""mailto:username&#64;biglab.seas...&#64;"">username&#64;biglab.seas...&#64;</a>)(filename)</p>",Copying files to BigLab,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8na1baym4qp/ijwydxbuj4vu/Screen_Shot_20160127_at_9.44.27_AM.png"" /></p>
<p>I keep getting this error on the last step for installing iPython Notebook. Followed all of the instructions earlier and they seem to have all worked.åÊ</p>",[iPython Notebook Error] Bad Interpreter: No such file or directory,"<p>We keep getting the following error on Crowdflower when trying to upload the date for our first HIT:åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/h6snkankbb96wq/im6rcyi1fkbg/Screen_Shot_20160324_at_4.53.49_PM.png"" /></p>
<p>We are not sure what this means since we have only inserted one row into the excel sheet to add the headers.</p>
<p></p>
<p>On another note, some of the text separates into several columns instead of staying in one. Is this just accidental tab separations in the text that Excel recognizes as a new column?</p>",Upload CSV/Excel File Error,1
940847362,4/26/2016 17:45:14,false,1969468160,,4/26/2016 17:39:40,false,clixsense,1.0,35444326,BRA,07,Brasília,177.15.130.106,0,,"<p>When I try to submit the homework, it does this marvelous thing where it tells me that net213 is an invalid config file.</p>
<p></p>
<p>What to do now? Somewhat concerned.</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hqh2r7yc6p926r/ikaaipxq9b3g/image.png"" /></p>",Invalid config file,"<p>InåÊthe bing_api.py, I am trying to print out the xml response that we receive.</p>
<p>Any ideas how to do this?</p>",Print out xml file,<p>I am very confused about what the 6 files we are expected to have after Part 1. Can someone help me out?</p>,6 Files,"<p>how would you be able to copy to biglab again? i went through this with TA in OH but I forgot.åÊ</p>
<p></p>
<p>It was along the lines of (code) (<a href=""mailto:username&#64;biglab.seas...&#64;"">username&#64;biglab.seas...&#64;</a>)(filename)</p>",Copying files to BigLab,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8na1baym4qp/ijwydxbuj4vu/Screen_Shot_20160127_at_9.44.27_AM.png"" /></p>
<p>I keep getting this error on the last step for installing iPython Notebook. Followed all of the instructions earlier and they seem to have all worked.åÊ</p>",[iPython Notebook Error] Bad Interpreter: No such file or directory,"<p>We keep getting the following error on Crowdflower when trying to upload the date for our first HIT:åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/h6snkankbb96wq/im6rcyi1fkbg/Screen_Shot_20160324_at_4.53.49_PM.png"" /></p>
<p>We are not sure what this means since we have only inserted one row into the excel sheet to add the headers.</p>
<p></p>
<p>On another note, some of the text separates into several columns instead of staying in one. Is this just accidental tab separations in the text that Excel recognizes as a new column?</p>",Upload CSV/Excel File Error,1
940847362,4/26/2016 18:19:09,false,1969486220,,4/26/2016 18:16:24,false,neodev,0.8889,35550011,VEN,07,Valencia,190.204.238.112,0,,"<p>When I try to submit the homework, it does this marvelous thing where it tells me that net213 is an invalid config file.</p>
<p></p>
<p>What to do now? Somewhat concerned.</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hqh2r7yc6p926r/ikaaipxq9b3g/image.png"" /></p>",Invalid config file,"<p>InåÊthe bing_api.py, I am trying to print out the xml response that we receive.</p>
<p>Any ideas how to do this?</p>",Print out xml file,<p>I am very confused about what the 6 files we are expected to have after Part 1. Can someone help me out?</p>,6 Files,"<p>how would you be able to copy to biglab again? i went through this with TA in OH but I forgot.åÊ</p>
<p></p>
<p>It was along the lines of (code) (<a href=""mailto:username&#64;biglab.seas...&#64;"">username&#64;biglab.seas...&#64;</a>)(filename)</p>",Copying files to BigLab,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8na1baym4qp/ijwydxbuj4vu/Screen_Shot_20160127_at_9.44.27_AM.png"" /></p>
<p>I keep getting this error on the last step for installing iPython Notebook. Followed all of the instructions earlier and they seem to have all worked.åÊ</p>",[iPython Notebook Error] Bad Interpreter: No such file or directory,"<p>We keep getting the following error on Crowdflower when trying to upload the date for our first HIT:åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/h6snkankbb96wq/im6rcyi1fkbg/Screen_Shot_20160324_at_4.53.49_PM.png"" /></p>
<p>We are not sure what this means since we have only inserted one row into the excel sheet to add the headers.</p>
<p></p>
<p>On another note, some of the text separates into several columns instead of staying in one. Is this just accidental tab separations in the text that Excel recognizes as a new column?</p>",Upload CSV/Excel File Error,1
940847363,4/26/2016 17:23:10,false,1969455465,,4/26/2016 17:16:53,false,neodev,1.0,33973110,VEN,23,Maracaibo,186.94.238.104,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,<p>How can I access the correlation coefficient (weight) for each of the terms in the CLF linear regression object? I believe we need this for &#34;get top features.&#34;</p>,CLF Correlation Coefficient,0
940847363,4/26/2016 17:24:11,false,1969455975,,4/26/2016 17:22:14,false,elite,1.0,25411289,HRV,"","",31.147.119.175,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,<p>How can I access the correlation coefficient (weight) for each of the terms in the CLF linear regression object? I believe we need this for &#34;get top features.&#34;</p>,CLF Correlation Coefficient,0
940847363,4/26/2016 17:31:56,false,1969460625,,4/26/2016 17:31:26,false,neodev,0.8889,33131546,IDN,04,Jakarta,139.194.89.60,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,<p>How can I access the correlation coefficient (weight) for each of the terms in the CLF linear regression object? I believe we need this for &#34;get top features.&#34;</p>,CLF Correlation Coefficient,0
940847363,4/26/2016 17:41:50,false,1969466054,,4/26/2016 17:32:10,false,neodev,0.8889,19625264,DZA,41,Chlef,41.102.7.217,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,<p>How can I access the correlation coefficient (weight) for each of the terms in the CLF linear regression object? I believe we need this for &#34;get top features.&#34;</p>,CLF Correlation Coefficient,0
940847363,4/26/2016 17:46:50,false,1969469059,,4/26/2016 17:32:15,false,clixsense,0.8889,35338593,ITA,14,Cagliari,151.56.132.145,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,<p>How can I access the correlation coefficient (weight) for each of the terms in the CLF linear regression object? I believe we need this for &#34;get top features.&#34;</p>,CLF Correlation Coefficient,0
940847364,4/26/2016 15:28:06,false,1969389965,,4/26/2016 15:27:30,false,instagc,0.8889,13581319,USA,IL,Waltonville,208.70.36.12,0,,"<p>Hi everyone,</p>
<p></p>
<p>I&#39;m looking for a partner toåÊcollaborate with on upcoming assignments and/or the final project. E-mail me at <a href=""mailto:hcutler&#64;seas.upenn.edu"">hcutler&#64;seas.upenn.edu</a>åÊif you&#39;re interested!</p>
<p></p>
<p>Have a great day.</p>
<p></p>
<p>Hannah Cutler</p>
<p>SEAS &#39;17</p>",Looking for a partner for homeworks and/or final project?,"<p>Are we allowed to modify one of the past final project ideas shown to us in class? We have an interesting ideaåÊsimilar to PictureThis and wanted to make sure it&#39;s OK that it&#39;s similar.</p>
<p></p>
<p>Thanks!</p>",Final project idea similar to PictureThis,"<p>FYI, the second part of the final project was released last night:åÊ<a href=""http://crowdsourcing-class.org/final-project-part2.html"">http://crowdsourcing-class.org/final-project-part2.html</a></p>
<p></p>
<p>There are two deliverables. åÊThe first one is due on Saturday before 11:59pm.</p>",Final Project Part 2 has been released,<p>This is the link:åÊhttps://github.com/benpleitner/NETS213-Final-Project</p>,Group Project,"<p>For the NETS 213 final project, do we as students retain ownership of our projects? I know that some schools will hold the ownership rights, and juståÊwanted some clarification on this policy for the NETS 213 course.</p>",Student Ownership of Final Project?,"<p>Just a quick question, in my email I have 6 pitches, do I submit two questionnaires? I only see space for 3 projects on the questionnaire.</p>",Final Project Pitches,5
940847364,4/26/2016 15:30:54,false,1969391258,,4/26/2016 15:28:59,false,elite,1.0,30280423,ITA,15,Siracusa,151.54.84.121,0,,"<p>Hi everyone,</p>
<p></p>
<p>I&#39;m looking for a partner toåÊcollaborate with on upcoming assignments and/or the final project. E-mail me at <a href=""mailto:hcutler&#64;seas.upenn.edu"">hcutler&#64;seas.upenn.edu</a>åÊif you&#39;re interested!</p>
<p></p>
<p>Have a great day.</p>
<p></p>
<p>Hannah Cutler</p>
<p>SEAS &#39;17</p>",Looking for a partner for homeworks and/or final project?,"<p>Are we allowed to modify one of the past final project ideas shown to us in class? We have an interesting ideaåÊsimilar to PictureThis and wanted to make sure it&#39;s OK that it&#39;s similar.</p>
<p></p>
<p>Thanks!</p>",Final project idea similar to PictureThis,"<p>FYI, the second part of the final project was released last night:åÊ<a href=""http://crowdsourcing-class.org/final-project-part2.html"">http://crowdsourcing-class.org/final-project-part2.html</a></p>
<p></p>
<p>There are two deliverables. åÊThe first one is due on Saturday before 11:59pm.</p>",Final Project Part 2 has been released,<p>This is the link:åÊhttps://github.com/benpleitner/NETS213-Final-Project</p>,Group Project,"<p>For the NETS 213 final project, do we as students retain ownership of our projects? I know that some schools will hold the ownership rights, and juståÊwanted some clarification on this policy for the NETS 213 course.</p>",Student Ownership of Final Project?,"<p>Just a quick question, in my email I have 6 pitches, do I submit two questionnaires? I only see space for 3 projects on the questionnaire.</p>",Final Project Pitches,5
940847364,4/26/2016 15:35:58,false,1969392976,,4/26/2016 15:30:06,false,clixsense,0.8889,36052512,PHL,F2,Quezon City,49.149.150.150,0,,"<p>Hi everyone,</p>
<p></p>
<p>I&#39;m looking for a partner toåÊcollaborate with on upcoming assignments and/or the final project. E-mail me at <a href=""mailto:hcutler&#64;seas.upenn.edu"">hcutler&#64;seas.upenn.edu</a>åÊif you&#39;re interested!</p>
<p></p>
<p>Have a great day.</p>
<p></p>
<p>Hannah Cutler</p>
<p>SEAS &#39;17</p>",Looking for a partner for homeworks and/or final project?,"<p>Are we allowed to modify one of the past final project ideas shown to us in class? We have an interesting ideaåÊsimilar to PictureThis and wanted to make sure it&#39;s OK that it&#39;s similar.</p>
<p></p>
<p>Thanks!</p>",Final project idea similar to PictureThis,"<p>FYI, the second part of the final project was released last night:åÊ<a href=""http://crowdsourcing-class.org/final-project-part2.html"">http://crowdsourcing-class.org/final-project-part2.html</a></p>
<p></p>
<p>There are two deliverables. åÊThe first one is due on Saturday before 11:59pm.</p>",Final Project Part 2 has been released,<p>This is the link:åÊhttps://github.com/benpleitner/NETS213-Final-Project</p>,Group Project,"<p>For the NETS 213 final project, do we as students retain ownership of our projects? I know that some schools will hold the ownership rights, and juståÊwanted some clarification on this policy for the NETS 213 course.</p>",Student Ownership of Final Project?,"<p>Just a quick question, in my email I have 6 pitches, do I submit two questionnaires? I only see space for 3 projects on the questionnaire.</p>",Final Project Pitches,5
940847364,4/26/2016 15:45:41,false,1969397215,,4/26/2016 15:39:57,false,clixsense,1.0,21875134,GBR,H9,London,87.112.158.81,0,,"<p>Hi everyone,</p>
<p></p>
<p>I&#39;m looking for a partner toåÊcollaborate with on upcoming assignments and/or the final project. E-mail me at <a href=""mailto:hcutler&#64;seas.upenn.edu"">hcutler&#64;seas.upenn.edu</a>åÊif you&#39;re interested!</p>
<p></p>
<p>Have a great day.</p>
<p></p>
<p>Hannah Cutler</p>
<p>SEAS &#39;17</p>",Looking for a partner for homeworks and/or final project?,"<p>Are we allowed to modify one of the past final project ideas shown to us in class? We have an interesting ideaåÊsimilar to PictureThis and wanted to make sure it&#39;s OK that it&#39;s similar.</p>
<p></p>
<p>Thanks!</p>",Final project idea similar to PictureThis,"<p>FYI, the second part of the final project was released last night:åÊ<a href=""http://crowdsourcing-class.org/final-project-part2.html"">http://crowdsourcing-class.org/final-project-part2.html</a></p>
<p></p>
<p>There are two deliverables. åÊThe first one is due on Saturday before 11:59pm.</p>",Final Project Part 2 has been released,<p>This is the link:åÊhttps://github.com/benpleitner/NETS213-Final-Project</p>,Group Project,"<p>For the NETS 213 final project, do we as students retain ownership of our projects? I know that some schools will hold the ownership rights, and juståÊwanted some clarification on this policy for the NETS 213 course.</p>",Student Ownership of Final Project?,"<p>Just a quick question, in my email I have 6 pitches, do I submit two questionnaires? I only see space for 3 projects on the questionnaire.</p>",Final Project Pitches,5
940847364,4/26/2016 15:47:45,false,1969398093,,4/26/2016 15:41:08,false,neodev,0.7778,32569659,USA,MN,Minneapolis,97.127.88.224,0,,"<p>Hi everyone,</p>
<p></p>
<p>I&#39;m looking for a partner toåÊcollaborate with on upcoming assignments and/or the final project. E-mail me at <a href=""mailto:hcutler&#64;seas.upenn.edu"">hcutler&#64;seas.upenn.edu</a>åÊif you&#39;re interested!</p>
<p></p>
<p>Have a great day.</p>
<p></p>
<p>Hannah Cutler</p>
<p>SEAS &#39;17</p>",Looking for a partner for homeworks and/or final project?,"<p>Are we allowed to modify one of the past final project ideas shown to us in class? We have an interesting ideaåÊsimilar to PictureThis and wanted to make sure it&#39;s OK that it&#39;s similar.</p>
<p></p>
<p>Thanks!</p>",Final project idea similar to PictureThis,"<p>FYI, the second part of the final project was released last night:åÊ<a href=""http://crowdsourcing-class.org/final-project-part2.html"">http://crowdsourcing-class.org/final-project-part2.html</a></p>
<p></p>
<p>There are two deliverables. åÊThe first one is due on Saturday before 11:59pm.</p>",Final Project Part 2 has been released,<p>This is the link:åÊhttps://github.com/benpleitner/NETS213-Final-Project</p>,Group Project,"<p>For the NETS 213 final project, do we as students retain ownership of our projects? I know that some schools will hold the ownership rights, and juståÊwanted some clarification on this policy for the NETS 213 course.</p>",Student Ownership of Final Project?,"<p>Just a quick question, in my email I have 6 pitches, do I submit two questionnaires? I only see space for 3 projects on the questionnaire.</p>",Final Project Pitches,5
940847365,4/26/2016 16:02:51,false,1969405070,,4/26/2016 16:02:22,false,personaly,1.0,33663352,ARG,01,Mar Del Plata,181.168.213.227,0,,"<p></p><p>I tried the stuff listed in <a href=""https://www.piazza.com/class/ijblb017ius5zp?cid=185"">&#64;185</a> but it&#39;s still not working. It doesn&#39;t recognize pip as a command in biglab unless I use sudo before it. What is the root password for biglab?<br /></p>",Installing Beautiful Soup on Biglab,"<p>So if you spend a lot of yesterday&#39;s class obsessing over package incompatibilities or the fact that Windows was causing you undue misery, read on!</p>
<p></p>
<p>To ensure people don&#39;t have to worry about platform issues, we&#39;ve created a Linux (Ubuntu specifically) Virtual Machine that everyone can run and use for the course. The necessary packages such as iPython, virtualenv etc have already been pre-installed.</p>
<p></p>
<p>Instructions to download and run the VM can be found atåÊ<a href=""http://crowdsourcing-class.org/vm-instructions.html"">http://crowdsourcing-class.org/vm-instructions.html</a>. The link has also been provided under the Resources page on the course website.åÊ</p>
<p></p>
<p>This is the first time we&#39;re doing this so please use Piazza / Office Hours liberally if you run into any issues. Try and run Assignment 3 (Python Bootcamp) on the VM to make sure everything is working.åÊ</p>
<p></p>
<p></p>",Installing and Running a Linux Virtual Machine,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/h6qnjzmr2yn4r3/ik5vr8manng6/12648004_10208733185314151_1959554261_n.jpg"" /></p>
<p></p>
<p>If any of you are getting the following screen when you download and run the VM, chances are you are trying to run a 64-Bit Ubuntu Disk Image with a 32-Bit Ubuntu configuration on VirtualBox. Make sure you select the 64-bit Ubuntu version. If this option isn&#39;t showing up for you, you need to go and change a setting in your BIOS.</p>
<p></p>
<p>1. Reboot your computer</p>
<p>2. Keep F2 pressedåÊ</p>
<p>3. Enter BIOS mode</p>
<p>4. In BIOS mode, navigate to either the Security / Configuration tab</p>
<p>5. You need to find the &#34;Intel Virtualization&#34; option and Enable it.</p>
<p>6. Restart your computer. You should have the 64-bit option enabled now.</p>
<p></p>
<p>If you&#39;re uncomfortable doing this by yourself, swing by office hours.</p>
<p></p>
<p>These links may be useful</p>
<p><a href=""http://www.thewindowsplanet.com/3192/how-to-enable-virtualization-on-a-lenovo-computer-on-windows.htm"">http://www.thewindowsplanet.com/3192/how-to-enable-virtualization-on-a-lenovo-computer-on-windows.htm</a></p>
<p><a href=""http://www.fixedbyvonnie.com/2014/11/virtualbox-showing-32-bit-guest-versions-64-bit-host-os/#.VrEYOhgrI1I"">http://www.fixedbyvonnie.com/2014/11/virtualbox-showing-32-bit-guest-versions-64-bit-host-os/#.VrEYOhgrI1I</a></p>
<p></p>",Installing the VM with Windows,"<p>Due to a high number of people running into problems with uncompressing the VM image - I&#39;ve made availableåÊthe whole uncompressed .vdi file instead. The instructions atåÊ<a href=""http://crowdsourcing-class.org/vm-instructions.html"">http://crowdsourcing-class.org/vm-instructions.html</a>åÊhave been updated as well. If you had trouble uncompressing the disk image, please redownload the new file and try installing the VM again.</p>",VM Installation Update,,,"<p>How do we install Beautiful Soup on Biglab? I am asked for &#34;root&#39;s password&#34; when I use sudo pip install beautifulsoup4. I am currently running everything on my laptop, but it is very slow.åÊ</p>
<p></p>
<p></p>",Beautiful Soup installation,2
940847365,4/26/2016 16:18:19,false,1969413580,,4/26/2016 16:15:37,false,elite,1.0,30128662,BGR,50,Pleven,212.233.177.195,0,,"<p></p><p>I tried the stuff listed in <a href=""https://www.piazza.com/class/ijblb017ius5zp?cid=185"">&#64;185</a> but it&#39;s still not working. It doesn&#39;t recognize pip as a command in biglab unless I use sudo before it. What is the root password for biglab?<br /></p>",Installing Beautiful Soup on Biglab,"<p>So if you spend a lot of yesterday&#39;s class obsessing over package incompatibilities or the fact that Windows was causing you undue misery, read on!</p>
<p></p>
<p>To ensure people don&#39;t have to worry about platform issues, we&#39;ve created a Linux (Ubuntu specifically) Virtual Machine that everyone can run and use for the course. The necessary packages such as iPython, virtualenv etc have already been pre-installed.</p>
<p></p>
<p>Instructions to download and run the VM can be found atåÊ<a href=""http://crowdsourcing-class.org/vm-instructions.html"">http://crowdsourcing-class.org/vm-instructions.html</a>. The link has also been provided under the Resources page on the course website.åÊ</p>
<p></p>
<p>This is the first time we&#39;re doing this so please use Piazza / Office Hours liberally if you run into any issues. Try and run Assignment 3 (Python Bootcamp) on the VM to make sure everything is working.åÊ</p>
<p></p>
<p></p>",Installing and Running a Linux Virtual Machine,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/h6qnjzmr2yn4r3/ik5vr8manng6/12648004_10208733185314151_1959554261_n.jpg"" /></p>
<p></p>
<p>If any of you are getting the following screen when you download and run the VM, chances are you are trying to run a 64-Bit Ubuntu Disk Image with a 32-Bit Ubuntu configuration on VirtualBox. Make sure you select the 64-bit Ubuntu version. If this option isn&#39;t showing up for you, you need to go and change a setting in your BIOS.</p>
<p></p>
<p>1. Reboot your computer</p>
<p>2. Keep F2 pressedåÊ</p>
<p>3. Enter BIOS mode</p>
<p>4. In BIOS mode, navigate to either the Security / Configuration tab</p>
<p>5. You need to find the &#34;Intel Virtualization&#34; option and Enable it.</p>
<p>6. Restart your computer. You should have the 64-bit option enabled now.</p>
<p></p>
<p>If you&#39;re uncomfortable doing this by yourself, swing by office hours.</p>
<p></p>
<p>These links may be useful</p>
<p><a href=""http://www.thewindowsplanet.com/3192/how-to-enable-virtualization-on-a-lenovo-computer-on-windows.htm"">http://www.thewindowsplanet.com/3192/how-to-enable-virtualization-on-a-lenovo-computer-on-windows.htm</a></p>
<p><a href=""http://www.fixedbyvonnie.com/2014/11/virtualbox-showing-32-bit-guest-versions-64-bit-host-os/#.VrEYOhgrI1I"">http://www.fixedbyvonnie.com/2014/11/virtualbox-showing-32-bit-guest-versions-64-bit-host-os/#.VrEYOhgrI1I</a></p>
<p></p>",Installing the VM with Windows,"<p>Due to a high number of people running into problems with uncompressing the VM image - I&#39;ve made availableåÊthe whole uncompressed .vdi file instead. The instructions atåÊ<a href=""http://crowdsourcing-class.org/vm-instructions.html"">http://crowdsourcing-class.org/vm-instructions.html</a>åÊhave been updated as well. If you had trouble uncompressing the disk image, please redownload the new file and try installing the VM again.</p>",VM Installation Update,,,"<p>How do we install Beautiful Soup on Biglab? I am asked for &#34;root&#39;s password&#34; when I use sudo pip install beautifulsoup4. I am currently running everything on my laptop, but it is very slow.åÊ</p>
<p></p>
<p></p>",Beautiful Soup installation,2
940847365,4/26/2016 16:23:45,false,1969418307,,4/26/2016 16:22:19,false,neodev,1.0,29175140,VEN,25,Caracas,190.72.125.134,0,,"<p></p><p>I tried the stuff listed in <a href=""https://www.piazza.com/class/ijblb017ius5zp?cid=185"">&#64;185</a> but it&#39;s still not working. It doesn&#39;t recognize pip as a command in biglab unless I use sudo before it. What is the root password for biglab?<br /></p>",Installing Beautiful Soup on Biglab,"<p>So if you spend a lot of yesterday&#39;s class obsessing over package incompatibilities or the fact that Windows was causing you undue misery, read on!</p>
<p></p>
<p>To ensure people don&#39;t have to worry about platform issues, we&#39;ve created a Linux (Ubuntu specifically) Virtual Machine that everyone can run and use for the course. The necessary packages such as iPython, virtualenv etc have already been pre-installed.</p>
<p></p>
<p>Instructions to download and run the VM can be found atåÊ<a href=""http://crowdsourcing-class.org/vm-instructions.html"">http://crowdsourcing-class.org/vm-instructions.html</a>. The link has also been provided under the Resources page on the course website.åÊ</p>
<p></p>
<p>This is the first time we&#39;re doing this so please use Piazza / Office Hours liberally if you run into any issues. Try and run Assignment 3 (Python Bootcamp) on the VM to make sure everything is working.åÊ</p>
<p></p>
<p></p>",Installing and Running a Linux Virtual Machine,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/h6qnjzmr2yn4r3/ik5vr8manng6/12648004_10208733185314151_1959554261_n.jpg"" /></p>
<p></p>
<p>If any of you are getting the following screen when you download and run the VM, chances are you are trying to run a 64-Bit Ubuntu Disk Image with a 32-Bit Ubuntu configuration on VirtualBox. Make sure you select the 64-bit Ubuntu version. If this option isn&#39;t showing up for you, you need to go and change a setting in your BIOS.</p>
<p></p>
<p>1. Reboot your computer</p>
<p>2. Keep F2 pressedåÊ</p>
<p>3. Enter BIOS mode</p>
<p>4. In BIOS mode, navigate to either the Security / Configuration tab</p>
<p>5. You need to find the &#34;Intel Virtualization&#34; option and Enable it.</p>
<p>6. Restart your computer. You should have the 64-bit option enabled now.</p>
<p></p>
<p>If you&#39;re uncomfortable doing this by yourself, swing by office hours.</p>
<p></p>
<p>These links may be useful</p>
<p><a href=""http://www.thewindowsplanet.com/3192/how-to-enable-virtualization-on-a-lenovo-computer-on-windows.htm"">http://www.thewindowsplanet.com/3192/how-to-enable-virtualization-on-a-lenovo-computer-on-windows.htm</a></p>
<p><a href=""http://www.fixedbyvonnie.com/2014/11/virtualbox-showing-32-bit-guest-versions-64-bit-host-os/#.VrEYOhgrI1I"">http://www.fixedbyvonnie.com/2014/11/virtualbox-showing-32-bit-guest-versions-64-bit-host-os/#.VrEYOhgrI1I</a></p>
<p></p>",Installing the VM with Windows,"<p>Due to a high number of people running into problems with uncompressing the VM image - I&#39;ve made availableåÊthe whole uncompressed .vdi file instead. The instructions atåÊ<a href=""http://crowdsourcing-class.org/vm-instructions.html"">http://crowdsourcing-class.org/vm-instructions.html</a>åÊhave been updated as well. If you had trouble uncompressing the disk image, please redownload the new file and try installing the VM again.</p>",VM Installation Update,,,"<p>How do we install Beautiful Soup on Biglab? I am asked for &#34;root&#39;s password&#34; when I use sudo pip install beautifulsoup4. I am currently running everything on my laptop, but it is very slow.åÊ</p>
<p></p>
<p></p>",Beautiful Soup installation,2
940847365,4/26/2016 16:31:08,false,1969424239,,4/26/2016 16:19:53,false,neodev,0.7778,32569659,USA,MN,Minneapolis,97.127.88.224,0,,"<p></p><p>I tried the stuff listed in <a href=""https://www.piazza.com/class/ijblb017ius5zp?cid=185"">&#64;185</a> but it&#39;s still not working. It doesn&#39;t recognize pip as a command in biglab unless I use sudo before it. What is the root password for biglab?<br /></p>",Installing Beautiful Soup on Biglab,"<p>So if you spend a lot of yesterday&#39;s class obsessing over package incompatibilities or the fact that Windows was causing you undue misery, read on!</p>
<p></p>
<p>To ensure people don&#39;t have to worry about platform issues, we&#39;ve created a Linux (Ubuntu specifically) Virtual Machine that everyone can run and use for the course. The necessary packages such as iPython, virtualenv etc have already been pre-installed.</p>
<p></p>
<p>Instructions to download and run the VM can be found atåÊ<a href=""http://crowdsourcing-class.org/vm-instructions.html"">http://crowdsourcing-class.org/vm-instructions.html</a>. The link has also been provided under the Resources page on the course website.åÊ</p>
<p></p>
<p>This is the first time we&#39;re doing this so please use Piazza / Office Hours liberally if you run into any issues. Try and run Assignment 3 (Python Bootcamp) on the VM to make sure everything is working.åÊ</p>
<p></p>
<p></p>",Installing and Running a Linux Virtual Machine,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/h6qnjzmr2yn4r3/ik5vr8manng6/12648004_10208733185314151_1959554261_n.jpg"" /></p>
<p></p>
<p>If any of you are getting the following screen when you download and run the VM, chances are you are trying to run a 64-Bit Ubuntu Disk Image with a 32-Bit Ubuntu configuration on VirtualBox. Make sure you select the 64-bit Ubuntu version. If this option isn&#39;t showing up for you, you need to go and change a setting in your BIOS.</p>
<p></p>
<p>1. Reboot your computer</p>
<p>2. Keep F2 pressedåÊ</p>
<p>3. Enter BIOS mode</p>
<p>4. In BIOS mode, navigate to either the Security / Configuration tab</p>
<p>5. You need to find the &#34;Intel Virtualization&#34; option and Enable it.</p>
<p>6. Restart your computer. You should have the 64-bit option enabled now.</p>
<p></p>
<p>If you&#39;re uncomfortable doing this by yourself, swing by office hours.</p>
<p></p>
<p>These links may be useful</p>
<p><a href=""http://www.thewindowsplanet.com/3192/how-to-enable-virtualization-on-a-lenovo-computer-on-windows.htm"">http://www.thewindowsplanet.com/3192/how-to-enable-virtualization-on-a-lenovo-computer-on-windows.htm</a></p>
<p><a href=""http://www.fixedbyvonnie.com/2014/11/virtualbox-showing-32-bit-guest-versions-64-bit-host-os/#.VrEYOhgrI1I"">http://www.fixedbyvonnie.com/2014/11/virtualbox-showing-32-bit-guest-versions-64-bit-host-os/#.VrEYOhgrI1I</a></p>
<p></p>",Installing the VM with Windows,"<p>Due to a high number of people running into problems with uncompressing the VM image - I&#39;ve made availableåÊthe whole uncompressed .vdi file instead. The instructions atåÊ<a href=""http://crowdsourcing-class.org/vm-instructions.html"">http://crowdsourcing-class.org/vm-instructions.html</a>åÊhave been updated as well. If you had trouble uncompressing the disk image, please redownload the new file and try installing the VM again.</p>",VM Installation Update,,,"<p>How do we install Beautiful Soup on Biglab? I am asked for &#34;root&#39;s password&#34; when I use sudo pip install beautifulsoup4. I am currently running everything on my laptop, but it is very slow.åÊ</p>
<p></p>
<p></p>",Beautiful Soup installation,2
940847365,4/26/2016 16:32:36,false,1969425038,,4/26/2016 16:13:00,false,clixsense,0.8889,8057247,PRT,17,Póvoa De Varzim,144.64.25.68,0,,"<p></p><p>I tried the stuff listed in <a href=""https://www.piazza.com/class/ijblb017ius5zp?cid=185"">&#64;185</a> but it&#39;s still not working. It doesn&#39;t recognize pip as a command in biglab unless I use sudo before it. What is the root password for biglab?<br /></p>",Installing Beautiful Soup on Biglab,"<p>So if you spend a lot of yesterday&#39;s class obsessing over package incompatibilities or the fact that Windows was causing you undue misery, read on!</p>
<p></p>
<p>To ensure people don&#39;t have to worry about platform issues, we&#39;ve created a Linux (Ubuntu specifically) Virtual Machine that everyone can run and use for the course. The necessary packages such as iPython, virtualenv etc have already been pre-installed.</p>
<p></p>
<p>Instructions to download and run the VM can be found atåÊ<a href=""http://crowdsourcing-class.org/vm-instructions.html"">http://crowdsourcing-class.org/vm-instructions.html</a>. The link has also been provided under the Resources page on the course website.åÊ</p>
<p></p>
<p>This is the first time we&#39;re doing this so please use Piazza / Office Hours liberally if you run into any issues. Try and run Assignment 3 (Python Bootcamp) on the VM to make sure everything is working.åÊ</p>
<p></p>
<p></p>",Installing and Running a Linux Virtual Machine,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/h6qnjzmr2yn4r3/ik5vr8manng6/12648004_10208733185314151_1959554261_n.jpg"" /></p>
<p></p>
<p>If any of you are getting the following screen when you download and run the VM, chances are you are trying to run a 64-Bit Ubuntu Disk Image with a 32-Bit Ubuntu configuration on VirtualBox. Make sure you select the 64-bit Ubuntu version. If this option isn&#39;t showing up for you, you need to go and change a setting in your BIOS.</p>
<p></p>
<p>1. Reboot your computer</p>
<p>2. Keep F2 pressedåÊ</p>
<p>3. Enter BIOS mode</p>
<p>4. In BIOS mode, navigate to either the Security / Configuration tab</p>
<p>5. You need to find the &#34;Intel Virtualization&#34; option and Enable it.</p>
<p>6. Restart your computer. You should have the 64-bit option enabled now.</p>
<p></p>
<p>If you&#39;re uncomfortable doing this by yourself, swing by office hours.</p>
<p></p>
<p>These links may be useful</p>
<p><a href=""http://www.thewindowsplanet.com/3192/how-to-enable-virtualization-on-a-lenovo-computer-on-windows.htm"">http://www.thewindowsplanet.com/3192/how-to-enable-virtualization-on-a-lenovo-computer-on-windows.htm</a></p>
<p><a href=""http://www.fixedbyvonnie.com/2014/11/virtualbox-showing-32-bit-guest-versions-64-bit-host-os/#.VrEYOhgrI1I"">http://www.fixedbyvonnie.com/2014/11/virtualbox-showing-32-bit-guest-versions-64-bit-host-os/#.VrEYOhgrI1I</a></p>
<p></p>",Installing the VM with Windows,"<p>Due to a high number of people running into problems with uncompressing the VM image - I&#39;ve made availableåÊthe whole uncompressed .vdi file instead. The instructions atåÊ<a href=""http://crowdsourcing-class.org/vm-instructions.html"">http://crowdsourcing-class.org/vm-instructions.html</a>åÊhave been updated as well. If you had trouble uncompressing the disk image, please redownload the new file and try installing the VM again.</p>",VM Installation Update,,,"<p>How do we install Beautiful Soup on Biglab? I am asked for &#34;root&#39;s password&#34; when I use sudo pip install beautifulsoup4. I am currently running everything on my laptop, but it is very slow.åÊ</p>
<p></p>
<p></p>",Beautiful Soup installation,2
940847366,4/26/2016 15:12:15,false,1969364814,,4/26/2016 15:12:00,false,tremorgames,1.0,32635967,LTU,60,Panevezys,78.63.38.165,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I recorded my video with QuickTime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant.åÊHas anyone else had this problem?</p>,quicktime,5
940847366,4/26/2016 15:20:55,false,1969378547,,4/26/2016 15:20:31,false,neodev,1.0,19132694,LKA,36,Colombo,123.231.124.170,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I recorded my video with QuickTime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant.åÊHas anyone else had this problem?</p>,quicktime,5
940847366,4/26/2016 15:23:37,false,1969383214,,4/26/2016 15:20:30,false,clixsense,1.0,24287706,TWN,04,Keelung,61.231.195.173,2,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I recorded my video with QuickTime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant.åÊHas anyone else had this problem?</p>,quicktime,5
940847366,4/26/2016 15:26:30,false,1969388024,,4/26/2016 15:24:24,false,clixsense,1.0,7837812,SRB,00,Belgrade,79.101.254.233,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I recorded my video with QuickTime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant.åÊHas anyone else had this problem?</p>,quicktime,5
940847366,4/26/2016 15:26:56,false,1969388624,,4/26/2016 15:26:14,false,instagc,0.8889,13581319,USA,IL,Waltonville,208.70.36.12,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I recorded my video with QuickTime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant.åÊHas anyone else had this problem?</p>,quicktime,5
940847367,4/26/2016 16:25:31,false,1969419720,,4/26/2016 16:24:35,false,neodev,1.0,29175140,VEN,25,Caracas,190.72.125.134,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>Getting the ValueError: too many values to process when running my extract_entries.py. I know this error has been brought up in piazza before, but the solutions given haven&#39;t worked for me. I&#39;ve removed the articles without text to process and all, but it won&#39;t run, and I&#39;m afraid to keep running so that I don&#39;t hit the limit on the API. Can someone tell me what might be wrong with my code?åÊ</p>
<p></p>
<p>Note: It will give about 33 lines of output before running into an error, but removing the next article didn&#39;t change anything.åÊ</p>
<p></p>
<pre>import sys
import urllib2
import json
import csv

# To run, type  the following into the terminal:
# cat gun-violence-urls-and-text.txt | python extract_entities.py &gt; gun-violence-urls-and-text-annotated.txt

#  My Alchemy API key
api_key = &#39;439d12cb72c484ecc556ba1a4595279d5ae94132&#39;

def construct_api_call(url):
  return &#39;http://access.alchemyapi.com/calls/url/URLGetCombinedData?apikey=&#39; &#43; api_key &#43; &#39;&amp;url=&#39; &#43; url &#43; &#39;&amp;extract=title,pub-date,entity&amp;outputMode=json&#39;

def get_fields(url) :
  try:
    data = urllib2.urlopen(construct_api_call(url))
  except urllib2.HTTPError:
    sys.stderr.write(&#39;BAD REQUEST\n&#39;)
    return None
  try:
    response = json.loads(data.read())
  except ValueError:
    sys.stderr.write(&#39;JSON ERROR\n&#39;)
    return None
  if response[&#39;status&#39;] == &#39;OK&#39;:
    title = response[&#39;title&#39;].encode(&#39;ascii&#39;, &#39;ignore&#39;)
    date = response[&#39;publicationDate&#39;][&#39;date&#39;].encode(&#39;ascii&#39;, &#39;ignore&#39;)
    entities = json.dumps(response[&#39;entities&#39;])
    return title, date, entities
  return None

for url in sys.stdin :
  url, txt = url.strip().split(&#39;\t&#39;)
  fields = get_fields(url)
  if fields is not None:
      title, date, entities = fields
      # format and output the url and text
      line = url &#43; &#39;\t&#39; &#43; title &#43; &#39;\t&#39; &#43; date &#43; &#39;\t&#39; &#43; txt &#43; &#39;\t&#39; &#43; entities &#43; &#39;\n&#39;
      sys.stdout.write(line)</pre>
<p></p>",extract_entities.py,0
940847367,4/26/2016 16:44:00,false,1969431833,,4/26/2016 16:41:32,false,clixsense,1.0,6329782,IDN,10,Sleman,202.67.40.222,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>Getting the ValueError: too many values to process when running my extract_entries.py. I know this error has been brought up in piazza before, but the solutions given haven&#39;t worked for me. I&#39;ve removed the articles without text to process and all, but it won&#39;t run, and I&#39;m afraid to keep running so that I don&#39;t hit the limit on the API. Can someone tell me what might be wrong with my code?åÊ</p>
<p></p>
<p>Note: It will give about 33 lines of output before running into an error, but removing the next article didn&#39;t change anything.åÊ</p>
<p></p>
<pre>import sys
import urllib2
import json
import csv

# To run, type  the following into the terminal:
# cat gun-violence-urls-and-text.txt | python extract_entities.py &gt; gun-violence-urls-and-text-annotated.txt

#  My Alchemy API key
api_key = &#39;439d12cb72c484ecc556ba1a4595279d5ae94132&#39;

def construct_api_call(url):
  return &#39;http://access.alchemyapi.com/calls/url/URLGetCombinedData?apikey=&#39; &#43; api_key &#43; &#39;&amp;url=&#39; &#43; url &#43; &#39;&amp;extract=title,pub-date,entity&amp;outputMode=json&#39;

def get_fields(url) :
  try:
    data = urllib2.urlopen(construct_api_call(url))
  except urllib2.HTTPError:
    sys.stderr.write(&#39;BAD REQUEST\n&#39;)
    return None
  try:
    response = json.loads(data.read())
  except ValueError:
    sys.stderr.write(&#39;JSON ERROR\n&#39;)
    return None
  if response[&#39;status&#39;] == &#39;OK&#39;:
    title = response[&#39;title&#39;].encode(&#39;ascii&#39;, &#39;ignore&#39;)
    date = response[&#39;publicationDate&#39;][&#39;date&#39;].encode(&#39;ascii&#39;, &#39;ignore&#39;)
    entities = json.dumps(response[&#39;entities&#39;])
    return title, date, entities
  return None

for url in sys.stdin :
  url, txt = url.strip().split(&#39;\t&#39;)
  fields = get_fields(url)
  if fields is not None:
      title, date, entities = fields
      # format and output the url and text
      line = url &#43; &#39;\t&#39; &#43; title &#43; &#39;\t&#39; &#43; date &#43; &#39;\t&#39; &#43; txt &#43; &#39;\t&#39; &#43; entities &#43; &#39;\n&#39;
      sys.stdout.write(line)</pre>
<p></p>",extract_entities.py,0
940847367,4/26/2016 17:00:02,false,1969441673,,4/26/2016 16:58:39,false,clixsense,1.0,21408115,IDN,07,Semarang,36.79.23.180,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>Getting the ValueError: too many values to process when running my extract_entries.py. I know this error has been brought up in piazza before, but the solutions given haven&#39;t worked for me. I&#39;ve removed the articles without text to process and all, but it won&#39;t run, and I&#39;m afraid to keep running so that I don&#39;t hit the limit on the API. Can someone tell me what might be wrong with my code?åÊ</p>
<p></p>
<p>Note: It will give about 33 lines of output before running into an error, but removing the next article didn&#39;t change anything.åÊ</p>
<p></p>
<pre>import sys
import urllib2
import json
import csv

# To run, type  the following into the terminal:
# cat gun-violence-urls-and-text.txt | python extract_entities.py &gt; gun-violence-urls-and-text-annotated.txt

#  My Alchemy API key
api_key = &#39;439d12cb72c484ecc556ba1a4595279d5ae94132&#39;

def construct_api_call(url):
  return &#39;http://access.alchemyapi.com/calls/url/URLGetCombinedData?apikey=&#39; &#43; api_key &#43; &#39;&amp;url=&#39; &#43; url &#43; &#39;&amp;extract=title,pub-date,entity&amp;outputMode=json&#39;

def get_fields(url) :
  try:
    data = urllib2.urlopen(construct_api_call(url))
  except urllib2.HTTPError:
    sys.stderr.write(&#39;BAD REQUEST\n&#39;)
    return None
  try:
    response = json.loads(data.read())
  except ValueError:
    sys.stderr.write(&#39;JSON ERROR\n&#39;)
    return None
  if response[&#39;status&#39;] == &#39;OK&#39;:
    title = response[&#39;title&#39;].encode(&#39;ascii&#39;, &#39;ignore&#39;)
    date = response[&#39;publicationDate&#39;][&#39;date&#39;].encode(&#39;ascii&#39;, &#39;ignore&#39;)
    entities = json.dumps(response[&#39;entities&#39;])
    return title, date, entities
  return None

for url in sys.stdin :
  url, txt = url.strip().split(&#39;\t&#39;)
  fields = get_fields(url)
  if fields is not None:
      title, date, entities = fields
      # format and output the url and text
      line = url &#43; &#39;\t&#39; &#43; title &#43; &#39;\t&#39; &#43; date &#43; &#39;\t&#39; &#43; txt &#43; &#39;\t&#39; &#43; entities &#43; &#39;\n&#39;
      sys.stdout.write(line)</pre>
<p></p>",extract_entities.py,0
940847367,4/26/2016 17:04:17,false,1969444196,,4/26/2016 17:02:14,false,neodev,1.0,36167043,GBR,G6,Hull,77.86.101.69,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>Getting the ValueError: too many values to process when running my extract_entries.py. I know this error has been brought up in piazza before, but the solutions given haven&#39;t worked for me. I&#39;ve removed the articles without text to process and all, but it won&#39;t run, and I&#39;m afraid to keep running so that I don&#39;t hit the limit on the API. Can someone tell me what might be wrong with my code?åÊ</p>
<p></p>
<p>Note: It will give about 33 lines of output before running into an error, but removing the next article didn&#39;t change anything.åÊ</p>
<p></p>
<pre>import sys
import urllib2
import json
import csv

# To run, type  the following into the terminal:
# cat gun-violence-urls-and-text.txt | python extract_entities.py &gt; gun-violence-urls-and-text-annotated.txt

#  My Alchemy API key
api_key = &#39;439d12cb72c484ecc556ba1a4595279d5ae94132&#39;

def construct_api_call(url):
  return &#39;http://access.alchemyapi.com/calls/url/URLGetCombinedData?apikey=&#39; &#43; api_key &#43; &#39;&amp;url=&#39; &#43; url &#43; &#39;&amp;extract=title,pub-date,entity&amp;outputMode=json&#39;

def get_fields(url) :
  try:
    data = urllib2.urlopen(construct_api_call(url))
  except urllib2.HTTPError:
    sys.stderr.write(&#39;BAD REQUEST\n&#39;)
    return None
  try:
    response = json.loads(data.read())
  except ValueError:
    sys.stderr.write(&#39;JSON ERROR\n&#39;)
    return None
  if response[&#39;status&#39;] == &#39;OK&#39;:
    title = response[&#39;title&#39;].encode(&#39;ascii&#39;, &#39;ignore&#39;)
    date = response[&#39;publicationDate&#39;][&#39;date&#39;].encode(&#39;ascii&#39;, &#39;ignore&#39;)
    entities = json.dumps(response[&#39;entities&#39;])
    return title, date, entities
  return None

for url in sys.stdin :
  url, txt = url.strip().split(&#39;\t&#39;)
  fields = get_fields(url)
  if fields is not None:
      title, date, entities = fields
      # format and output the url and text
      line = url &#43; &#39;\t&#39; &#43; title &#43; &#39;\t&#39; &#43; date &#43; &#39;\t&#39; &#43; txt &#43; &#39;\t&#39; &#43; entities &#43; &#39;\n&#39;
      sys.stdout.write(line)</pre>
<p></p>",extract_entities.py,0
940847367,4/26/2016 17:47:57,false,1969469603,,4/26/2016 17:41:51,false,neodev,0.8889,19625264,DZA,41,Chlef,41.102.7.217,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>Getting the ValueError: too many values to process when running my extract_entries.py. I know this error has been brought up in piazza before, but the solutions given haven&#39;t worked for me. I&#39;ve removed the articles without text to process and all, but it won&#39;t run, and I&#39;m afraid to keep running so that I don&#39;t hit the limit on the API. Can someone tell me what might be wrong with my code?åÊ</p>
<p></p>
<p>Note: It will give about 33 lines of output before running into an error, but removing the next article didn&#39;t change anything.åÊ</p>
<p></p>
<pre>import sys
import urllib2
import json
import csv

# To run, type  the following into the terminal:
# cat gun-violence-urls-and-text.txt | python extract_entities.py &gt; gun-violence-urls-and-text-annotated.txt

#  My Alchemy API key
api_key = &#39;439d12cb72c484ecc556ba1a4595279d5ae94132&#39;

def construct_api_call(url):
  return &#39;http://access.alchemyapi.com/calls/url/URLGetCombinedData?apikey=&#39; &#43; api_key &#43; &#39;&amp;url=&#39; &#43; url &#43; &#39;&amp;extract=title,pub-date,entity&amp;outputMode=json&#39;

def get_fields(url) :
  try:
    data = urllib2.urlopen(construct_api_call(url))
  except urllib2.HTTPError:
    sys.stderr.write(&#39;BAD REQUEST\n&#39;)
    return None
  try:
    response = json.loads(data.read())
  except ValueError:
    sys.stderr.write(&#39;JSON ERROR\n&#39;)
    return None
  if response[&#39;status&#39;] == &#39;OK&#39;:
    title = response[&#39;title&#39;].encode(&#39;ascii&#39;, &#39;ignore&#39;)
    date = response[&#39;publicationDate&#39;][&#39;date&#39;].encode(&#39;ascii&#39;, &#39;ignore&#39;)
    entities = json.dumps(response[&#39;entities&#39;])
    return title, date, entities
  return None

for url in sys.stdin :
  url, txt = url.strip().split(&#39;\t&#39;)
  fields = get_fields(url)
  if fields is not None:
      title, date, entities = fields
      # format and output the url and text
      line = url &#43; &#39;\t&#39; &#43; title &#43; &#39;\t&#39; &#43; date &#43; &#39;\t&#39; &#43; txt &#43; &#39;\t&#39; &#43; entities &#43; &#39;\n&#39;
      sys.stdout.write(line)</pre>
<p></p>",extract_entities.py,0
940847368,4/26/2016 17:02:29,false,1969443196,,4/26/2016 17:01:24,false,clixsense,1.0,21408115,IDN,07,Semarang,36.79.23.180,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>In the manual EM iterations part of the hw, doåÊwe count the initialization phase as an iteration?åÊ</p>
<p></p>",EM iterations,4
940847368,4/26/2016 17:07:43,false,1969446210,,4/26/2016 17:05:48,false,neodev,1.0,36167043,GBR,G6,Hull,77.86.101.69,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>In the manual EM iterations part of the hw, doåÊwe count the initialization phase as an iteration?åÊ</p>
<p></p>",EM iterations,4
940847368,4/26/2016 17:08:43,false,1969446806,,4/26/2016 17:06:43,false,neodev,1.0,33973110,VEN,23,Maracaibo,186.94.238.104,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>In the manual EM iterations part of the hw, doåÊwe count the initialization phase as an iteration?åÊ</p>
<p></p>",EM iterations,4
940847368,4/26/2016 17:16:41,false,1969451753,,4/26/2016 17:14:02,false,elite,1.0,25411289,HRV,"","",31.147.119.175,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>In the manual EM iterations part of the hw, doåÊwe count the initialization phase as an iteration?åÊ</p>
<p></p>",EM iterations,4
940847368,4/26/2016 17:30:30,false,1969459771,,4/26/2016 17:11:11,false,neodev,0.8889,19625264,DZA,41,Chlef,41.102.7.217,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>In the manual EM iterations part of the hw, doåÊwe count the initialization phase as an iteration?åÊ</p>
<p></p>",EM iterations,4
940847369,4/26/2016 16:01:37,false,1969404543,,4/26/2016 16:00:00,false,elite,1.0,33243069,IND,10,Faridabad,116.203.79.150,0,,"After adding my fratures to the features list, and running the program, i still get the same decision tree as we saw initially with just gun. Not sure whats wrong",decision tree with add features,<p>How do I print the tree like we did in class Friday?</p>,Printing Decision Tree,"<p>I&#39;ve installed the graphivz onto my computer, but my program doesn&#39;t generate a diagram.</p>",Unable to generate Decision Tree Diagram,"<p>In our rule-based classifier, if we have a rule with the format &#34;If A in text and B not in text,&#34; should we put &#34;A and not B&#34; in the same box of the decision tree, or should we have separate boxes for A and B?åÊ</p>",Decision Tree,"<p>Hi, I looked over the chapter on decision trees and I am still a tad confused about how they work.</p>
<p></p>
<p>For my rule-based classifier, I used the following format:</p>
<p>if &#34;word1&#34; in text : decision = 1</p>
<p>if &#34;word2&#34; in text : decision = 1</p>
<p>... (rest of decision = 1 words)</p>
<p>if &#34;word10&#34; in text : decision = 0</p>
<p>if &#34;word11&#34; in text : decision = 0</p>
<p></p>
<p>I am still a bit confused about how a decision tree would work for this when the conditions are not interrelated</p>
<p></p>
<p>Would the tree look like this?</p>
<p></p>
<p>åÊåÊåÊ w1åÊ -------| åÊåÊåÊåÊåÊ åÊ åÊåÊ (and so on for w2, w3, etc)</p>
<p>åÊåÊ /åÊåÊåÊ \åÊåÊåÊåÊåÊåÊåÊåÊåÊåÊ | åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊåÊ</p>
<p>w10åÊåÊ w11åÊåÊåÊåÊ |åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ</p>
<p>|åÊåÊåÊåÊåÊåÊåÊåÊåÊåÊ |åÊåÊåÊåÊåÊåÊ yes</p>
<p>noåÊåÊåÊåÊåÊåÊåÊ no</p>",Decision Tree Where Conditions Are Not Dependent On One Another,<p>I keep getting an error message when I save my draw.io file as a .png on Linux VM and try to open the file with any sort of image viewer. Any idea what might be going on?</p>,Could not load decision tree image from draw.io,0
940847369,4/26/2016 16:02:20,false,1969404861,,4/26/2016 16:01:47,false,personaly,1.0,33663352,ARG,01,Mar Del Plata,181.168.213.227,0,,"After adding my fratures to the features list, and running the program, i still get the same decision tree as we saw initially with just gun. Not sure whats wrong",decision tree with add features,<p>How do I print the tree like we did in class Friday?</p>,Printing Decision Tree,"<p>I&#39;ve installed the graphivz onto my computer, but my program doesn&#39;t generate a diagram.</p>",Unable to generate Decision Tree Diagram,"<p>In our rule-based classifier, if we have a rule with the format &#34;If A in text and B not in text,&#34; should we put &#34;A and not B&#34; in the same box of the decision tree, or should we have separate boxes for A and B?åÊ</p>",Decision Tree,"<p>Hi, I looked over the chapter on decision trees and I am still a tad confused about how they work.</p>
<p></p>
<p>For my rule-based classifier, I used the following format:</p>
<p>if &#34;word1&#34; in text : decision = 1</p>
<p>if &#34;word2&#34; in text : decision = 1</p>
<p>... (rest of decision = 1 words)</p>
<p>if &#34;word10&#34; in text : decision = 0</p>
<p>if &#34;word11&#34; in text : decision = 0</p>
<p></p>
<p>I am still a bit confused about how a decision tree would work for this when the conditions are not interrelated</p>
<p></p>
<p>Would the tree look like this?</p>
<p></p>
<p>åÊåÊåÊ w1åÊ -------| åÊåÊåÊåÊåÊ åÊ åÊåÊ (and so on for w2, w3, etc)</p>
<p>åÊåÊ /åÊåÊåÊ \åÊåÊåÊåÊåÊåÊåÊåÊåÊåÊ | åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊåÊ</p>
<p>w10åÊåÊ w11åÊåÊåÊåÊ |åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ</p>
<p>|åÊåÊåÊåÊåÊåÊåÊåÊåÊåÊ |åÊåÊåÊåÊåÊåÊ yes</p>
<p>noåÊåÊåÊåÊåÊåÊåÊ no</p>",Decision Tree Where Conditions Are Not Dependent On One Another,<p>I keep getting an error message when I save my draw.io file as a .png on Linux VM and try to open the file with any sort of image viewer. Any idea what might be going on?</p>,Could not load decision tree image from draw.io,0
940847369,4/26/2016 16:12:58,false,1969410159,,4/26/2016 16:10:21,false,clixsense,0.8889,8057247,PRT,17,Póvoa De Varzim,144.64.25.68,0,,"After adding my fratures to the features list, and running the program, i still get the same decision tree as we saw initially with just gun. Not sure whats wrong",decision tree with add features,<p>How do I print the tree like we did in class Friday?</p>,Printing Decision Tree,"<p>I&#39;ve installed the graphivz onto my computer, but my program doesn&#39;t generate a diagram.</p>",Unable to generate Decision Tree Diagram,"<p>In our rule-based classifier, if we have a rule with the format &#34;If A in text and B not in text,&#34; should we put &#34;A and not B&#34; in the same box of the decision tree, or should we have separate boxes for A and B?åÊ</p>",Decision Tree,"<p>Hi, I looked over the chapter on decision trees and I am still a tad confused about how they work.</p>
<p></p>
<p>For my rule-based classifier, I used the following format:</p>
<p>if &#34;word1&#34; in text : decision = 1</p>
<p>if &#34;word2&#34; in text : decision = 1</p>
<p>... (rest of decision = 1 words)</p>
<p>if &#34;word10&#34; in text : decision = 0</p>
<p>if &#34;word11&#34; in text : decision = 0</p>
<p></p>
<p>I am still a bit confused about how a decision tree would work for this when the conditions are not interrelated</p>
<p></p>
<p>Would the tree look like this?</p>
<p></p>
<p>åÊåÊåÊ w1åÊ -------| åÊåÊåÊåÊåÊ åÊ åÊåÊ (and so on for w2, w3, etc)</p>
<p>åÊåÊ /åÊåÊåÊ \åÊåÊåÊåÊåÊåÊåÊåÊåÊåÊ | åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊåÊ</p>
<p>w10åÊåÊ w11åÊåÊåÊåÊ |åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ</p>
<p>|åÊåÊåÊåÊåÊåÊåÊåÊåÊåÊ |åÊåÊåÊåÊåÊåÊ yes</p>
<p>noåÊåÊåÊåÊåÊåÊåÊ no</p>",Decision Tree Where Conditions Are Not Dependent On One Another,<p>I keep getting an error message when I save my draw.io file as a .png on Linux VM and try to open the file with any sort of image viewer. Any idea what might be going on?</p>,Could not load decision tree image from draw.io,0
940847369,4/26/2016 16:40:25,false,1969429703,,4/26/2016 16:39:20,false,clixsense,1.0,6329782,IDN,07,Bekonang,202.67.40.31,0,,"After adding my fratures to the features list, and running the program, i still get the same decision tree as we saw initially with just gun. Not sure whats wrong",decision tree with add features,<p>How do I print the tree like we did in class Friday?</p>,Printing Decision Tree,"<p>I&#39;ve installed the graphivz onto my computer, but my program doesn&#39;t generate a diagram.</p>",Unable to generate Decision Tree Diagram,"<p>In our rule-based classifier, if we have a rule with the format &#34;If A in text and B not in text,&#34; should we put &#34;A and not B&#34; in the same box of the decision tree, or should we have separate boxes for A and B?åÊ</p>",Decision Tree,"<p>Hi, I looked over the chapter on decision trees and I am still a tad confused about how they work.</p>
<p></p>
<p>For my rule-based classifier, I used the following format:</p>
<p>if &#34;word1&#34; in text : decision = 1</p>
<p>if &#34;word2&#34; in text : decision = 1</p>
<p>... (rest of decision = 1 words)</p>
<p>if &#34;word10&#34; in text : decision = 0</p>
<p>if &#34;word11&#34; in text : decision = 0</p>
<p></p>
<p>I am still a bit confused about how a decision tree would work for this when the conditions are not interrelated</p>
<p></p>
<p>Would the tree look like this?</p>
<p></p>
<p>åÊåÊåÊ w1åÊ -------| åÊåÊåÊåÊåÊ åÊ åÊåÊ (and so on for w2, w3, etc)</p>
<p>åÊåÊ /åÊåÊåÊ \åÊåÊåÊåÊåÊåÊåÊåÊåÊåÊ | åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊåÊ</p>
<p>w10åÊåÊ w11åÊåÊåÊåÊ |åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ</p>
<p>|åÊåÊåÊåÊåÊåÊåÊåÊåÊåÊ |åÊåÊåÊåÊåÊåÊ yes</p>
<p>noåÊåÊåÊåÊåÊåÊåÊ no</p>",Decision Tree Where Conditions Are Not Dependent On One Another,<p>I keep getting an error message when I save my draw.io file as a .png on Linux VM and try to open the file with any sort of image viewer. Any idea what might be going on?</p>,Could not load decision tree image from draw.io,0
940847369,4/26/2016 17:22:13,false,1969454948,,4/26/2016 17:19:06,false,elite,1.0,25411289,HRV,"","",31.147.119.175,0,,"After adding my fratures to the features list, and running the program, i still get the same decision tree as we saw initially with just gun. Not sure whats wrong",decision tree with add features,<p>How do I print the tree like we did in class Friday?</p>,Printing Decision Tree,"<p>I&#39;ve installed the graphivz onto my computer, but my program doesn&#39;t generate a diagram.</p>",Unable to generate Decision Tree Diagram,"<p>In our rule-based classifier, if we have a rule with the format &#34;If A in text and B not in text,&#34; should we put &#34;A and not B&#34; in the same box of the decision tree, or should we have separate boxes for A and B?åÊ</p>",Decision Tree,"<p>Hi, I looked over the chapter on decision trees and I am still a tad confused about how they work.</p>
<p></p>
<p>For my rule-based classifier, I used the following format:</p>
<p>if &#34;word1&#34; in text : decision = 1</p>
<p>if &#34;word2&#34; in text : decision = 1</p>
<p>... (rest of decision = 1 words)</p>
<p>if &#34;word10&#34; in text : decision = 0</p>
<p>if &#34;word11&#34; in text : decision = 0</p>
<p></p>
<p>I am still a bit confused about how a decision tree would work for this when the conditions are not interrelated</p>
<p></p>
<p>Would the tree look like this?</p>
<p></p>
<p>åÊåÊåÊ w1åÊ -------| åÊåÊåÊåÊåÊ åÊ åÊåÊ (and so on for w2, w3, etc)</p>
<p>åÊåÊ /åÊåÊåÊ \åÊåÊåÊåÊåÊåÊåÊåÊåÊåÊ | åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊåÊ</p>
<p>w10åÊåÊ w11åÊåÊåÊåÊ |åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ</p>
<p>|åÊåÊåÊåÊåÊåÊåÊåÊåÊåÊ |åÊåÊåÊåÊåÊåÊ yes</p>
<p>noåÊåÊåÊåÊåÊåÊåÊ no</p>",Decision Tree Where Conditions Are Not Dependent On One Another,<p>I keep getting an error message when I save my draw.io file as a .png on Linux VM and try to open the file with any sort of image viewer. Any idea what might be going on?</p>,Could not load decision tree image from draw.io,0
940847370,4/26/2016 17:32:48,false,1969461087,,4/26/2016 17:32:36,false,neodev,0.8889,33131546,IDN,04,Jakarta,139.194.89.60,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>When I try to run the commandåÊ</p>
<pre>python classifier_template.py articles </pre>
<p>I get a message sayingåÊ</p>
<pre>Traceback (most recent call last):
  File &#34;classifier_template.py&#34;, line 8, in &lt;module&gt;
    from sklearn.tree import export_graphviz<br />ImportError: No module named sklearn.tree</pre>
<p>How do I fix this? Do the files need to be in a specific directory?</p>",No Module named sklearn.tree,1
940847370,4/26/2016 17:36:20,false,1969462856,,4/26/2016 17:36:03,false,clixsense,1.0,30712378,ROU,21,Deva,79.119.241.200,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>When I try to run the commandåÊ</p>
<pre>python classifier_template.py articles </pre>
<p>I get a message sayingåÊ</p>
<pre>Traceback (most recent call last):
  File &#34;classifier_template.py&#34;, line 8, in &lt;module&gt;
    from sklearn.tree import export_graphviz<br />ImportError: No module named sklearn.tree</pre>
<p>How do I fix this? Do the files need to be in a specific directory?</p>",No Module named sklearn.tree,1
940847370,4/26/2016 17:39:37,false,1969464626,,4/26/2016 17:38:56,false,neodev,0.8889,33568303,VEN,23,Cabimas,190.77.7.36,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>When I try to run the commandåÊ</p>
<pre>python classifier_template.py articles </pre>
<p>I get a message sayingåÊ</p>
<pre>Traceback (most recent call last):
  File &#34;classifier_template.py&#34;, line 8, in &lt;module&gt;
    from sklearn.tree import export_graphviz<br />ImportError: No module named sklearn.tree</pre>
<p>How do I fix this? Do the files need to be in a specific directory?</p>",No Module named sklearn.tree,1
940847370,4/26/2016 17:49:32,false,1969470399,,4/26/2016 17:45:16,false,clixsense,1.0,35444326,BRA,07,Brasília,177.15.130.106,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>When I try to run the commandåÊ</p>
<pre>python classifier_template.py articles </pre>
<p>I get a message sayingåÊ</p>
<pre>Traceback (most recent call last):
  File &#34;classifier_template.py&#34;, line 8, in &lt;module&gt;
    from sklearn.tree import export_graphviz<br />ImportError: No module named sklearn.tree</pre>
<p>How do I fix this? Do the files need to be in a specific directory?</p>",No Module named sklearn.tree,1
940847370,4/26/2016 18:04:05,false,1969477769,,4/26/2016 17:46:51,false,clixsense,0.8889,35338593,ITA,14,Cagliari,151.56.132.145,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>When I try to run the commandåÊ</p>
<pre>python classifier_template.py articles </pre>
<p>I get a message sayingåÊ</p>
<pre>Traceback (most recent call last):
  File &#34;classifier_template.py&#34;, line 8, in &lt;module&gt;
    from sklearn.tree import export_graphviz<br />ImportError: No module named sklearn.tree</pre>
<p>How do I fix this? Do the files need to be in a specific directory?</p>",No Module named sklearn.tree,1
940847371,4/26/2016 15:59:59,false,1969403529,,4/26/2016 15:58:26,false,elite,1.0,33243069,IND,10,Faridabad,116.203.79.150,0,,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p>I&#39;m unable to complete the survey because attempting to visit the URL <a href=""http://www.crowd-workers.com/track"">www.crowd-workers.com/track</a>åÊbrings me to a 404 Error.</p>",www.crowd-workers.com/track ERROR 404,"<p>I keep getting the following error when I try to runåÊ</p>
<p>cat list_of_urls.txt | python get_clean_text.py &gt; articles_and_urls.txt and I&#39;m not sure why. Any ideas?åÊ</p>
<p></p>
<pre>Traceback (most recent call last):
  File &#34;get_clean_text.py&#34;, line 25, in &lt;module&gt;
    txt = get_text(url)
  File &#34;get_clean_text.py&#34;, line 14, in get_text
    soup = BeautifulSoup(urllib2.urlopen(url).read(), &#39;html.parser&#39;)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 127, in urlopen
    return _opener.open(url, data, timeout)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 404, in open
    response = self._open(req, data)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 422, in _open
    &#39;_open&#39;, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1214, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1187, in do_open
    r = h.getresponse(buffering=True)
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 1067, in getresponse
    response.begin()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 409, in begin
    version, status, reason = self._read_status()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 365, in _read_status
    line = self.fp.readline(_MAXLINE &#43; 1)
  File &#34;/usr/lib64/python2.7/socket.py&#34;, line 476, in readline
    data = self._sock.recv(self._rbufsize)
socket.error: [Errno 104] Connection reset by peer</pre>
<p></p>",Connection Reset by Peer error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,4
940847371,4/26/2016 16:01:46,false,1969404613,,4/26/2016 16:01:11,false,personaly,1.0,33663352,ARG,01,Mar Del Plata,181.168.213.227,0,,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p>I&#39;m unable to complete the survey because attempting to visit the URL <a href=""http://www.crowd-workers.com/track"">www.crowd-workers.com/track</a>åÊbrings me to a 404 Error.</p>",www.crowd-workers.com/track ERROR 404,"<p>I keep getting the following error when I try to runåÊ</p>
<p>cat list_of_urls.txt | python get_clean_text.py &gt; articles_and_urls.txt and I&#39;m not sure why. Any ideas?åÊ</p>
<p></p>
<pre>Traceback (most recent call last):
  File &#34;get_clean_text.py&#34;, line 25, in &lt;module&gt;
    txt = get_text(url)
  File &#34;get_clean_text.py&#34;, line 14, in get_text
    soup = BeautifulSoup(urllib2.urlopen(url).read(), &#39;html.parser&#39;)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 127, in urlopen
    return _opener.open(url, data, timeout)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 404, in open
    response = self._open(req, data)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 422, in _open
    &#39;_open&#39;, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1214, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1187, in do_open
    r = h.getresponse(buffering=True)
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 1067, in getresponse
    response.begin()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 409, in begin
    version, status, reason = self._read_status()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 365, in _read_status
    line = self.fp.readline(_MAXLINE &#43; 1)
  File &#34;/usr/lib64/python2.7/socket.py&#34;, line 476, in readline
    data = self._sock.recv(self._rbufsize)
socket.error: [Errno 104] Connection reset by peer</pre>
<p></p>",Connection Reset by Peer error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,4
940847371,4/26/2016 16:09:03,false,1969408350,,4/26/2016 16:03:36,false,clixsense,0.8889,8057247,PRT,17,Póvoa De Varzim,144.64.25.68,"1
2
3
5",,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p>I&#39;m unable to complete the survey because attempting to visit the URL <a href=""http://www.crowd-workers.com/track"">www.crowd-workers.com/track</a>åÊbrings me to a 404 Error.</p>",www.crowd-workers.com/track ERROR 404,"<p>I keep getting the following error when I try to runåÊ</p>
<p>cat list_of_urls.txt | python get_clean_text.py &gt; articles_and_urls.txt and I&#39;m not sure why. Any ideas?åÊ</p>
<p></p>
<pre>Traceback (most recent call last):
  File &#34;get_clean_text.py&#34;, line 25, in &lt;module&gt;
    txt = get_text(url)
  File &#34;get_clean_text.py&#34;, line 14, in get_text
    soup = BeautifulSoup(urllib2.urlopen(url).read(), &#39;html.parser&#39;)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 127, in urlopen
    return _opener.open(url, data, timeout)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 404, in open
    response = self._open(req, data)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 422, in _open
    &#39;_open&#39;, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1214, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1187, in do_open
    r = h.getresponse(buffering=True)
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 1067, in getresponse
    response.begin()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 409, in begin
    version, status, reason = self._read_status()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 365, in _read_status
    line = self.fp.readline(_MAXLINE &#43; 1)
  File &#34;/usr/lib64/python2.7/socket.py&#34;, line 476, in readline
    data = self._sock.recv(self._rbufsize)
socket.error: [Errno 104] Connection reset by peer</pre>
<p></p>",Connection Reset by Peer error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,4
940847371,4/26/2016 16:24:40,false,1969419133,,4/26/2016 16:04:24,false,neodev,0.8889,21971187,TTO,08,Valsayn,190.213.132.190,0,,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p>I&#39;m unable to complete the survey because attempting to visit the URL <a href=""http://www.crowd-workers.com/track"">www.crowd-workers.com/track</a>åÊbrings me to a 404 Error.</p>",www.crowd-workers.com/track ERROR 404,"<p>I keep getting the following error when I try to runåÊ</p>
<p>cat list_of_urls.txt | python get_clean_text.py &gt; articles_and_urls.txt and I&#39;m not sure why. Any ideas?åÊ</p>
<p></p>
<pre>Traceback (most recent call last):
  File &#34;get_clean_text.py&#34;, line 25, in &lt;module&gt;
    txt = get_text(url)
  File &#34;get_clean_text.py&#34;, line 14, in get_text
    soup = BeautifulSoup(urllib2.urlopen(url).read(), &#39;html.parser&#39;)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 127, in urlopen
    return _opener.open(url, data, timeout)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 404, in open
    response = self._open(req, data)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 422, in _open
    &#39;_open&#39;, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1214, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1187, in do_open
    r = h.getresponse(buffering=True)
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 1067, in getresponse
    response.begin()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 409, in begin
    version, status, reason = self._read_status()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 365, in _read_status
    line = self.fp.readline(_MAXLINE &#43; 1)
  File &#34;/usr/lib64/python2.7/socket.py&#34;, line 476, in readline
    data = self._sock.recv(self._rbufsize)
socket.error: [Errno 104] Connection reset by peer</pre>
<p></p>",Connection Reset by Peer error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,4
940847371,4/26/2016 16:40:10,false,1969429583,,4/26/2016 16:37:31,false,neodev,0.7778,32569659,USA,MN,Minneapolis,97.127.88.224,0,,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p>I&#39;m unable to complete the survey because attempting to visit the URL <a href=""http://www.crowd-workers.com/track"">www.crowd-workers.com/track</a>åÊbrings me to a 404 Error.</p>",www.crowd-workers.com/track ERROR 404,"<p>I keep getting the following error when I try to runåÊ</p>
<p>cat list_of_urls.txt | python get_clean_text.py &gt; articles_and_urls.txt and I&#39;m not sure why. Any ideas?åÊ</p>
<p></p>
<pre>Traceback (most recent call last):
  File &#34;get_clean_text.py&#34;, line 25, in &lt;module&gt;
    txt = get_text(url)
  File &#34;get_clean_text.py&#34;, line 14, in get_text
    soup = BeautifulSoup(urllib2.urlopen(url).read(), &#39;html.parser&#39;)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 127, in urlopen
    return _opener.open(url, data, timeout)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 404, in open
    response = self._open(req, data)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 422, in _open
    &#39;_open&#39;, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1214, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1187, in do_open
    r = h.getresponse(buffering=True)
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 1067, in getresponse
    response.begin()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 409, in begin
    version, status, reason = self._read_status()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 365, in _read_status
    line = self.fp.readline(_MAXLINE &#43; 1)
  File &#34;/usr/lib64/python2.7/socket.py&#34;, line 476, in readline
    data = self._sock.recv(self._rbufsize)
socket.error: [Errno 104] Connection reset by peer</pre>
<p></p>",Connection Reset by Peer error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,4
940847372,4/26/2016 17:32:34,false,1969460928,,4/26/2016 17:32:18,false,neodev,0.8889,33131546,IDN,04,Jakarta,139.194.89.60,0,,<p>How can we findåÊhourly wage through CrowdFlower? Do we need to estimate it or there is a section that gives a precise number?</p>,Hourly Wage on CrowdFlower,"<p>Based on the instructions, the sample.txt file that we upload to CloudFlower should contain 500 positively labelled articles. In the screenshot at step 4 however the urls have a mix of both 0 and 1 labels. Which one is correct? And more generally, why are we having only the positive articles being labelled by crowdworkers?</p>",CrowdFlower &#34;sample.txt&#34; Clarification,<p>My crowdflower account doesn&#39;t have any funds yet ÛÓ is it possible to share accounts just for this assignment?</p>,sharing crowdflower,No one has completed my request in the last 5 hours and it is still only 20% completed. Is there any possible way to update my task such that it becomes more popular to contributors?,No More Contributors in CrowdFlower,"<p>Reminder: if you don&#39;t sign up for CrowdFlower credits WITHIN THE NEXT HOUR, then you will have to fund your account yourself.åÊ</p>
<p></p>
<p>See details below.åÊ</p>
<p></p>
<p>--CCB</p>
<p></p>
<p>------</p>
<p></p>
<p>If you would like ~$100 of free Credit on CrowdFlower, please sign up for a CrowdFlower Customer (aka Requester) now. åÊYou may have already completed this step as part of HW1. åÊIf not, then please sign up here:åÊhttps://make.crowdflower.com/users/new</p>
<div></div>
<div>Once that is done, please fill out this Google form before class on Wednesday:åÊ</div>
<div>https://docs.google.com/forms/d/1shp2S5Jl3r5bEx6hT_as8xQJZ5piiy2KRIEYQS_8U74/viewform</div>
<div></div>
<div>If you do not submit the Google form before Wednesday at 2pm, then you will not receive the free credit from CrowdFlower, and you will have to fund your account with your own money.</div>
<div></div>
<div>ÛÓChris</div>
<div></div>
<div><br /></div>
<p></p>",last chance to sign up for CrowdFlower credits,should we submit the csvs as well?,crowdflower csvs,5
940847372,4/26/2016 17:36:01,false,1969462739,,4/26/2016 17:35:20,false,clixsense,1.0,30712378,ROU,21,Deva,79.119.241.200,0,,<p>How can we findåÊhourly wage through CrowdFlower? Do we need to estimate it or there is a section that gives a precise number?</p>,Hourly Wage on CrowdFlower,"<p>Based on the instructions, the sample.txt file that we upload to CloudFlower should contain 500 positively labelled articles. In the screenshot at step 4 however the urls have a mix of both 0 and 1 labels. Which one is correct? And more generally, why are we having only the positive articles being labelled by crowdworkers?</p>",CrowdFlower &#34;sample.txt&#34; Clarification,<p>My crowdflower account doesn&#39;t have any funds yet ÛÓ is it possible to share accounts just for this assignment?</p>,sharing crowdflower,No one has completed my request in the last 5 hours and it is still only 20% completed. Is there any possible way to update my task such that it becomes more popular to contributors?,No More Contributors in CrowdFlower,"<p>Reminder: if you don&#39;t sign up for CrowdFlower credits WITHIN THE NEXT HOUR, then you will have to fund your account yourself.åÊ</p>
<p></p>
<p>See details below.åÊ</p>
<p></p>
<p>--CCB</p>
<p></p>
<p>------</p>
<p></p>
<p>If you would like ~$100 of free Credit on CrowdFlower, please sign up for a CrowdFlower Customer (aka Requester) now. åÊYou may have already completed this step as part of HW1. åÊIf not, then please sign up here:åÊhttps://make.crowdflower.com/users/new</p>
<div></div>
<div>Once that is done, please fill out this Google form before class on Wednesday:åÊ</div>
<div>https://docs.google.com/forms/d/1shp2S5Jl3r5bEx6hT_as8xQJZ5piiy2KRIEYQS_8U74/viewform</div>
<div></div>
<div>If you do not submit the Google form before Wednesday at 2pm, then you will not receive the free credit from CrowdFlower, and you will have to fund your account with your own money.</div>
<div></div>
<div>ÛÓChris</div>
<div></div>
<div><br /></div>
<p></p>",last chance to sign up for CrowdFlower credits,should we submit the csvs as well?,crowdflower csvs,5
940847372,4/26/2016 17:38:55,false,1969464252,,4/26/2016 17:36:08,false,neodev,0.8889,33568303,VEN,23,Cabimas,190.77.7.36,0,,<p>How can we findåÊhourly wage through CrowdFlower? Do we need to estimate it or there is a section that gives a precise number?</p>,Hourly Wage on CrowdFlower,"<p>Based on the instructions, the sample.txt file that we upload to CloudFlower should contain 500 positively labelled articles. In the screenshot at step 4 however the urls have a mix of both 0 and 1 labels. Which one is correct? And more generally, why are we having only the positive articles being labelled by crowdworkers?</p>",CrowdFlower &#34;sample.txt&#34; Clarification,<p>My crowdflower account doesn&#39;t have any funds yet ÛÓ is it possible to share accounts just for this assignment?</p>,sharing crowdflower,No one has completed my request in the last 5 hours and it is still only 20% completed. Is there any possible way to update my task such that it becomes more popular to contributors?,No More Contributors in CrowdFlower,"<p>Reminder: if you don&#39;t sign up for CrowdFlower credits WITHIN THE NEXT HOUR, then you will have to fund your account yourself.åÊ</p>
<p></p>
<p>See details below.åÊ</p>
<p></p>
<p>--CCB</p>
<p></p>
<p>------</p>
<p></p>
<p>If you would like ~$100 of free Credit on CrowdFlower, please sign up for a CrowdFlower Customer (aka Requester) now. åÊYou may have already completed this step as part of HW1. åÊIf not, then please sign up here:åÊhttps://make.crowdflower.com/users/new</p>
<div></div>
<div>Once that is done, please fill out this Google form before class on Wednesday:åÊ</div>
<div>https://docs.google.com/forms/d/1shp2S5Jl3r5bEx6hT_as8xQJZ5piiy2KRIEYQS_8U74/viewform</div>
<div></div>
<div>If you do not submit the Google form before Wednesday at 2pm, then you will not receive the free credit from CrowdFlower, and you will have to fund your account with your own money.</div>
<div></div>
<div>ÛÓChris</div>
<div></div>
<div><br /></div>
<p></p>",last chance to sign up for CrowdFlower credits,should we submit the csvs as well?,crowdflower csvs,5
940847372,4/26/2016 17:45:14,false,1969468154,,4/26/2016 17:39:40,false,clixsense,1.0,35444326,BRA,07,Brasília,177.15.130.106,0,,<p>How can we findåÊhourly wage through CrowdFlower? Do we need to estimate it or there is a section that gives a precise number?</p>,Hourly Wage on CrowdFlower,"<p>Based on the instructions, the sample.txt file that we upload to CloudFlower should contain 500 positively labelled articles. In the screenshot at step 4 however the urls have a mix of both 0 and 1 labels. Which one is correct? And more generally, why are we having only the positive articles being labelled by crowdworkers?</p>",CrowdFlower &#34;sample.txt&#34; Clarification,<p>My crowdflower account doesn&#39;t have any funds yet ÛÓ is it possible to share accounts just for this assignment?</p>,sharing crowdflower,No one has completed my request in the last 5 hours and it is still only 20% completed. Is there any possible way to update my task such that it becomes more popular to contributors?,No More Contributors in CrowdFlower,"<p>Reminder: if you don&#39;t sign up for CrowdFlower credits WITHIN THE NEXT HOUR, then you will have to fund your account yourself.åÊ</p>
<p></p>
<p>See details below.åÊ</p>
<p></p>
<p>--CCB</p>
<p></p>
<p>------</p>
<p></p>
<p>If you would like ~$100 of free Credit on CrowdFlower, please sign up for a CrowdFlower Customer (aka Requester) now. åÊYou may have already completed this step as part of HW1. åÊIf not, then please sign up here:åÊhttps://make.crowdflower.com/users/new</p>
<div></div>
<div>Once that is done, please fill out this Google form before class on Wednesday:åÊ</div>
<div>https://docs.google.com/forms/d/1shp2S5Jl3r5bEx6hT_as8xQJZ5piiy2KRIEYQS_8U74/viewform</div>
<div></div>
<div>If you do not submit the Google form before Wednesday at 2pm, then you will not receive the free credit from CrowdFlower, and you will have to fund your account with your own money.</div>
<div></div>
<div>ÛÓChris</div>
<div></div>
<div><br /></div>
<p></p>",last chance to sign up for CrowdFlower credits,should we submit the csvs as well?,crowdflower csvs,5
940847372,4/26/2016 18:19:09,false,1969486219,,4/26/2016 18:16:24,false,neodev,0.8889,35550011,VEN,07,Valencia,190.204.238.112,0,,<p>How can we findåÊhourly wage through CrowdFlower? Do we need to estimate it or there is a section that gives a precise number?</p>,Hourly Wage on CrowdFlower,"<p>Based on the instructions, the sample.txt file that we upload to CloudFlower should contain 500 positively labelled articles. In the screenshot at step 4 however the urls have a mix of both 0 and 1 labels. Which one is correct? And more generally, why are we having only the positive articles being labelled by crowdworkers?</p>",CrowdFlower &#34;sample.txt&#34; Clarification,<p>My crowdflower account doesn&#39;t have any funds yet ÛÓ is it possible to share accounts just for this assignment?</p>,sharing crowdflower,No one has completed my request in the last 5 hours and it is still only 20% completed. Is there any possible way to update my task such that it becomes more popular to contributors?,No More Contributors in CrowdFlower,"<p>Reminder: if you don&#39;t sign up for CrowdFlower credits WITHIN THE NEXT HOUR, then you will have to fund your account yourself.åÊ</p>
<p></p>
<p>See details below.åÊ</p>
<p></p>
<p>--CCB</p>
<p></p>
<p>------</p>
<p></p>
<p>If you would like ~$100 of free Credit on CrowdFlower, please sign up for a CrowdFlower Customer (aka Requester) now. åÊYou may have already completed this step as part of HW1. åÊIf not, then please sign up here:åÊhttps://make.crowdflower.com/users/new</p>
<div></div>
<div>Once that is done, please fill out this Google form before class on Wednesday:åÊ</div>
<div>https://docs.google.com/forms/d/1shp2S5Jl3r5bEx6hT_as8xQJZ5piiy2KRIEYQS_8U74/viewform</div>
<div></div>
<div>If you do not submit the Google form before Wednesday at 2pm, then you will not receive the free credit from CrowdFlower, and you will have to fund your account with your own money.</div>
<div></div>
<div>ÛÓChris</div>
<div></div>
<div><br /></div>
<p></p>",last chance to sign up for CrowdFlower credits,should we submit the csvs as well?,crowdflower csvs,5
940847373,4/26/2016 17:41:58,false,1969466123,,4/26/2016 17:40:30,false,neodev,0.8889,33568303,VEN,23,Cabimas,190.77.7.36,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>When our group tried to upload HITsåÊto the MTurk sandbox, we saw that we got a couple of failures. The number of failures went down after a little while, but we still ended up with a couple with the error as &#34;throttled&#34;. Is this normal behavior for the sandbox? I.e. do we need to worry about this when we actually upload our HITs (not to the sandbox)?</p>",Failures on MTurk,0
940847373,4/26/2016 17:51:16,false,1969471400,,4/26/2016 17:50:22,false,clixsense,1.0,35444326,BRA,07,Brasília,177.15.130.106,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>When our group tried to upload HITsåÊto the MTurk sandbox, we saw that we got a couple of failures. The number of failures went down after a little while, but we still ended up with a couple with the error as &#34;throttled&#34;. Is this normal behavior for the sandbox? I.e. do we need to worry about this when we actually upload our HITs (not to the sandbox)?</p>",Failures on MTurk,0
940847373,4/26/2016 18:21:14,false,1969487260,,4/26/2016 18:20:20,false,neodev,0.8889,35550011,VEN,07,Valencia,190.204.238.112,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>When our group tried to upload HITsåÊto the MTurk sandbox, we saw that we got a couple of failures. The number of failures went down after a little while, but we still ended up with a couple with the error as &#34;throttled&#34;. Is this normal behavior for the sandbox? I.e. do we need to worry about this when we actually upload our HITs (not to the sandbox)?</p>",Failures on MTurk,0
940847373,4/26/2016 18:25:34,false,1969489419,,4/26/2016 18:24:15,false,elite,1.0,30128662,BGR,50,Pleven,212.233.177.195,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>When our group tried to upload HITsåÊto the MTurk sandbox, we saw that we got a couple of failures. The number of failures went down after a little while, but we still ended up with a couple with the error as &#34;throttled&#34;. Is this normal behavior for the sandbox? I.e. do we need to worry about this when we actually upload our HITs (not to the sandbox)?</p>",Failures on MTurk,0
940847373,4/26/2016 18:56:00,false,1969504147,,4/26/2016 18:55:19,false,neodev,1.0,29879245,RUS,69,Smolensk,37.144.124.118,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>When our group tried to upload HITsåÊto the MTurk sandbox, we saw that we got a couple of failures. The number of failures went down after a little while, but we still ended up with a couple with the error as &#34;throttled&#34;. Is this normal behavior for the sandbox? I.e. do we need to worry about this when we actually upload our HITs (not to the sandbox)?</p>",Failures on MTurk,0
940847374,4/26/2016 17:23:10,false,1969455469,,4/26/2016 17:16:53,false,neodev,1.0,33973110,VEN,23,Maracaibo,186.94.238.104,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,<p>One of the video links for the peer review assignment we are supposed to do is broken (iStockPhoto). What should I do?</p>,Video link broken,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>I&#39;m working on the current homework. My groupåÊwould like to do the experiment &#34;Financial Incentives and the Performance of Crowds&#34;, but we need a large dataset of images. Do you have any suggestions for where we could get such a dataset?</p>",Image datasets?,3
940847374,4/26/2016 17:24:11,false,1969455979,,4/26/2016 17:22:14,false,elite,1.0,25411289,HRV,"","",31.147.119.175,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,<p>One of the video links for the peer review assignment we are supposed to do is broken (iStockPhoto). What should I do?</p>,Video link broken,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>I&#39;m working on the current homework. My groupåÊwould like to do the experiment &#34;Financial Incentives and the Performance of Crowds&#34;, but we need a large dataset of images. Do you have any suggestions for where we could get such a dataset?</p>",Image datasets?,3
940847374,4/26/2016 17:31:56,false,1969460622,,4/26/2016 17:31:26,false,neodev,0.8889,33131546,IDN,04,Jakarta,139.194.89.60,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,<p>One of the video links for the peer review assignment we are supposed to do is broken (iStockPhoto). What should I do?</p>,Video link broken,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>I&#39;m working on the current homework. My groupåÊwould like to do the experiment &#34;Financial Incentives and the Performance of Crowds&#34;, but we need a large dataset of images. Do you have any suggestions for where we could get such a dataset?</p>",Image datasets?,3
940847374,4/26/2016 17:41:50,false,1969466058,,4/26/2016 17:32:10,false,neodev,0.8889,19625264,DZA,41,Chlef,41.102.7.217,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,<p>One of the video links for the peer review assignment we are supposed to do is broken (iStockPhoto). What should I do?</p>,Video link broken,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>I&#39;m working on the current homework. My groupåÊwould like to do the experiment &#34;Financial Incentives and the Performance of Crowds&#34;, but we need a large dataset of images. Do you have any suggestions for where we could get such a dataset?</p>",Image datasets?,3
940847374,4/26/2016 17:46:50,false,1969469057,,4/26/2016 17:32:15,false,clixsense,0.8889,35338593,ITA,14,Cagliari,151.56.132.145,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,<p>One of the video links for the peer review assignment we are supposed to do is broken (iStockPhoto). What should I do?</p>,Video link broken,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>I&#39;m working on the current homework. My groupåÊwould like to do the experiment &#34;Financial Incentives and the Performance of Crowds&#34;, but we need a large dataset of images. Do you have any suggestions for where we could get such a dataset?</p>",Image datasets?,3
940847375,4/26/2016 16:26:27,false,1969420528,,4/26/2016 16:25:32,false,neodev,1.0,29175140,VEN,25,Caracas,190.72.125.134,0,,<p>How can we findåÊhourly wage through CrowdFlower? Do we need to estimate it or there is a section that gives a precise number?</p>,Hourly Wage on CrowdFlower,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,"<p>Based on the instructions, the sample.txt file that we upload to CloudFlower should contain 500 positively labelled articles. In the screenshot at step 4 however the urls have a mix of both 0 and 1 labels. Which one is correct? And more generally, why are we having only the positive articles being labelled by crowdworkers?</p>",CrowdFlower &#34;sample.txt&#34; Clarification,<p>My crowdflower account doesn&#39;t have any funds yet ÛÓ is it possible to share accounts just for this assignment?</p>,sharing crowdflower,"<p>Reminder: if you don&#39;t sign up for CrowdFlower credits WITHIN THE NEXT HOUR, then you will have to fund your account yourself.åÊ</p>
<p></p>
<p>See details below.åÊ</p>
<p></p>
<p>--CCB</p>
<p></p>
<p>------</p>
<p></p>
<p>If you would like ~$100 of free Credit on CrowdFlower, please sign up for a CrowdFlower Customer (aka Requester) now. åÊYou may have already completed this step as part of HW1. åÊIf not, then please sign up here:åÊhttps://make.crowdflower.com/users/new</p>
<div></div>
<div>Once that is done, please fill out this Google form before class on Wednesday:åÊ</div>
<div>https://docs.google.com/forms/d/1shp2S5Jl3r5bEx6hT_as8xQJZ5piiy2KRIEYQS_8U74/viewform</div>
<div></div>
<div>If you do not submit the Google form before Wednesday at 2pm, then you will not receive the free credit from CrowdFlower, and you will have to fund your account with your own money.</div>
<div></div>
<div>ÛÓChris</div>
<div></div>
<div><br /></div>
<p></p>",last chance to sign up for CrowdFlower credits,No one has completed my request in the last 5 hours and it is still only 20% completed. Is there any possible way to update my task such that it becomes more popular to contributors?,No More Contributors in CrowdFlower,5
940847375,4/26/2016 16:45:07,false,1969432484,,4/26/2016 16:44:04,false,clixsense,1.0,6329782,IDN,10,Sleman,202.67.40.222,0,,<p>How can we findåÊhourly wage through CrowdFlower? Do we need to estimate it or there is a section that gives a precise number?</p>,Hourly Wage on CrowdFlower,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,"<p>Based on the instructions, the sample.txt file that we upload to CloudFlower should contain 500 positively labelled articles. In the screenshot at step 4 however the urls have a mix of both 0 and 1 labels. Which one is correct? And more generally, why are we having only the positive articles being labelled by crowdworkers?</p>",CrowdFlower &#34;sample.txt&#34; Clarification,<p>My crowdflower account doesn&#39;t have any funds yet ÛÓ is it possible to share accounts just for this assignment?</p>,sharing crowdflower,"<p>Reminder: if you don&#39;t sign up for CrowdFlower credits WITHIN THE NEXT HOUR, then you will have to fund your account yourself.åÊ</p>
<p></p>
<p>See details below.åÊ</p>
<p></p>
<p>--CCB</p>
<p></p>
<p>------</p>
<p></p>
<p>If you would like ~$100 of free Credit on CrowdFlower, please sign up for a CrowdFlower Customer (aka Requester) now. åÊYou may have already completed this step as part of HW1. åÊIf not, then please sign up here:åÊhttps://make.crowdflower.com/users/new</p>
<div></div>
<div>Once that is done, please fill out this Google form before class on Wednesday:åÊ</div>
<div>https://docs.google.com/forms/d/1shp2S5Jl3r5bEx6hT_as8xQJZ5piiy2KRIEYQS_8U74/viewform</div>
<div></div>
<div>If you do not submit the Google form before Wednesday at 2pm, then you will not receive the free credit from CrowdFlower, and you will have to fund your account with your own money.</div>
<div></div>
<div>ÛÓChris</div>
<div></div>
<div><br /></div>
<p></p>",last chance to sign up for CrowdFlower credits,No one has completed my request in the last 5 hours and it is still only 20% completed. Is there any possible way to update my task such that it becomes more popular to contributors?,No More Contributors in CrowdFlower,5
940847375,4/26/2016 17:01:22,false,1969442591,,4/26/2016 17:00:04,false,clixsense,1.0,21408115,IDN,07,Semarang,36.79.23.180,0,,<p>How can we findåÊhourly wage through CrowdFlower? Do we need to estimate it or there is a section that gives a precise number?</p>,Hourly Wage on CrowdFlower,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,"<p>Based on the instructions, the sample.txt file that we upload to CloudFlower should contain 500 positively labelled articles. In the screenshot at step 4 however the urls have a mix of both 0 and 1 labels. Which one is correct? And more generally, why are we having only the positive articles being labelled by crowdworkers?</p>",CrowdFlower &#34;sample.txt&#34; Clarification,<p>My crowdflower account doesn&#39;t have any funds yet ÛÓ is it possible to share accounts just for this assignment?</p>,sharing crowdflower,"<p>Reminder: if you don&#39;t sign up for CrowdFlower credits WITHIN THE NEXT HOUR, then you will have to fund your account yourself.åÊ</p>
<p></p>
<p>See details below.åÊ</p>
<p></p>
<p>--CCB</p>
<p></p>
<p>------</p>
<p></p>
<p>If you would like ~$100 of free Credit on CrowdFlower, please sign up for a CrowdFlower Customer (aka Requester) now. åÊYou may have already completed this step as part of HW1. åÊIf not, then please sign up here:åÊhttps://make.crowdflower.com/users/new</p>
<div></div>
<div>Once that is done, please fill out this Google form before class on Wednesday:åÊ</div>
<div>https://docs.google.com/forms/d/1shp2S5Jl3r5bEx6hT_as8xQJZ5piiy2KRIEYQS_8U74/viewform</div>
<div></div>
<div>If you do not submit the Google form before Wednesday at 2pm, then you will not receive the free credit from CrowdFlower, and you will have to fund your account with your own money.</div>
<div></div>
<div>ÛÓChris</div>
<div></div>
<div><br /></div>
<p></p>",last chance to sign up for CrowdFlower credits,No one has completed my request in the last 5 hours and it is still only 20% completed. Is there any possible way to update my task such that it becomes more popular to contributors?,No More Contributors in CrowdFlower,5
940847375,4/26/2016 17:05:47,false,1969445078,,4/26/2016 17:04:18,false,neodev,1.0,36167043,GBR,G6,Hull,77.86.101.69,0,,<p>How can we findåÊhourly wage through CrowdFlower? Do we need to estimate it or there is a section that gives a precise number?</p>,Hourly Wage on CrowdFlower,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,"<p>Based on the instructions, the sample.txt file that we upload to CloudFlower should contain 500 positively labelled articles. In the screenshot at step 4 however the urls have a mix of both 0 and 1 labels. Which one is correct? And more generally, why are we having only the positive articles being labelled by crowdworkers?</p>",CrowdFlower &#34;sample.txt&#34; Clarification,<p>My crowdflower account doesn&#39;t have any funds yet ÛÓ is it possible to share accounts just for this assignment?</p>,sharing crowdflower,"<p>Reminder: if you don&#39;t sign up for CrowdFlower credits WITHIN THE NEXT HOUR, then you will have to fund your account yourself.åÊ</p>
<p></p>
<p>See details below.åÊ</p>
<p></p>
<p>--CCB</p>
<p></p>
<p>------</p>
<p></p>
<p>If you would like ~$100 of free Credit on CrowdFlower, please sign up for a CrowdFlower Customer (aka Requester) now. åÊYou may have already completed this step as part of HW1. åÊIf not, then please sign up here:åÊhttps://make.crowdflower.com/users/new</p>
<div></div>
<div>Once that is done, please fill out this Google form before class on Wednesday:åÊ</div>
<div>https://docs.google.com/forms/d/1shp2S5Jl3r5bEx6hT_as8xQJZ5piiy2KRIEYQS_8U74/viewform</div>
<div></div>
<div>If you do not submit the Google form before Wednesday at 2pm, then you will not receive the free credit from CrowdFlower, and you will have to fund your account with your own money.</div>
<div></div>
<div>ÛÓChris</div>
<div></div>
<div><br /></div>
<p></p>",last chance to sign up for CrowdFlower credits,No one has completed my request in the last 5 hours and it is still only 20% completed. Is there any possible way to update my task such that it becomes more popular to contributors?,No More Contributors in CrowdFlower,5
940847375,4/26/2016 17:34:24,false,1969461908,,4/26/2016 17:33:46,false,clixsense,1.0,30712378,ROU,21,Deva,79.119.241.200,0,,<p>How can we findåÊhourly wage through CrowdFlower? Do we need to estimate it or there is a section that gives a precise number?</p>,Hourly Wage on CrowdFlower,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,"<p>Based on the instructions, the sample.txt file that we upload to CloudFlower should contain 500 positively labelled articles. In the screenshot at step 4 however the urls have a mix of both 0 and 1 labels. Which one is correct? And more generally, why are we having only the positive articles being labelled by crowdworkers?</p>",CrowdFlower &#34;sample.txt&#34; Clarification,<p>My crowdflower account doesn&#39;t have any funds yet ÛÓ is it possible to share accounts just for this assignment?</p>,sharing crowdflower,"<p>Reminder: if you don&#39;t sign up for CrowdFlower credits WITHIN THE NEXT HOUR, then you will have to fund your account yourself.åÊ</p>
<p></p>
<p>See details below.åÊ</p>
<p></p>
<p>--CCB</p>
<p></p>
<p>------</p>
<p></p>
<p>If you would like ~$100 of free Credit on CrowdFlower, please sign up for a CrowdFlower Customer (aka Requester) now. åÊYou may have already completed this step as part of HW1. åÊIf not, then please sign up here:åÊhttps://make.crowdflower.com/users/new</p>
<div></div>
<div>Once that is done, please fill out this Google form before class on Wednesday:åÊ</div>
<div>https://docs.google.com/forms/d/1shp2S5Jl3r5bEx6hT_as8xQJZ5piiy2KRIEYQS_8U74/viewform</div>
<div></div>
<div>If you do not submit the Google form before Wednesday at 2pm, then you will not receive the free credit from CrowdFlower, and you will have to fund your account with your own money.</div>
<div></div>
<div>ÛÓChris</div>
<div></div>
<div><br /></div>
<p></p>",last chance to sign up for CrowdFlower credits,No one has completed my request in the last 5 hours and it is still only 20% completed. Is there any possible way to update my task such that it becomes more popular to contributors?,No More Contributors in CrowdFlower,5
940847376,4/26/2016 17:32:48,false,1969461081,,4/26/2016 17:32:36,false,neodev,0.8889,33131546,IDN,04,Jakarta,139.194.89.60,0,,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p></p><pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p>When I run the code, I keep getting this error. I have tried looking at my API Key and seeing if I have an extra space in the API key field, but it looks perfectly fine to me.åÊ</p>",Error on running Bing API.py,"<p>I&#39;m unable to complete the survey because attempting to visit the URL <a href=""http://www.crowd-workers.com/track"">www.crowd-workers.com/track</a>åÊbrings me to a 404 Error.</p>",www.crowd-workers.com/track ERROR 404,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,2
940847376,4/26/2016 17:36:20,false,1969462857,,4/26/2016 17:36:03,false,clixsense,1.0,30712378,ROU,21,Deva,79.119.241.200,0,,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p></p><pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p>When I run the code, I keep getting this error. I have tried looking at my API Key and seeing if I have an extra space in the API key field, but it looks perfectly fine to me.åÊ</p>",Error on running Bing API.py,"<p>I&#39;m unable to complete the survey because attempting to visit the URL <a href=""http://www.crowd-workers.com/track"">www.crowd-workers.com/track</a>åÊbrings me to a 404 Error.</p>",www.crowd-workers.com/track ERROR 404,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,2
940847376,4/26/2016 17:39:37,false,1969464632,,4/26/2016 17:38:56,false,neodev,0.8889,33568303,VEN,23,Cabimas,190.77.7.36,0,,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p></p><pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p>When I run the code, I keep getting this error. I have tried looking at my API Key and seeing if I have an extra space in the API key field, but it looks perfectly fine to me.åÊ</p>",Error on running Bing API.py,"<p>I&#39;m unable to complete the survey because attempting to visit the URL <a href=""http://www.crowd-workers.com/track"">www.crowd-workers.com/track</a>åÊbrings me to a 404 Error.</p>",www.crowd-workers.com/track ERROR 404,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,2
940847376,4/26/2016 17:49:32,false,1969470401,,4/26/2016 17:45:16,false,clixsense,1.0,35444326,BRA,07,Brasília,177.15.130.106,0,,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p></p><pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p>When I run the code, I keep getting this error. I have tried looking at my API Key and seeing if I have an extra space in the API key field, but it looks perfectly fine to me.åÊ</p>",Error on running Bing API.py,"<p>I&#39;m unable to complete the survey because attempting to visit the URL <a href=""http://www.crowd-workers.com/track"">www.crowd-workers.com/track</a>åÊbrings me to a 404 Error.</p>",www.crowd-workers.com/track ERROR 404,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,2
940847376,4/26/2016 18:04:05,false,1969477776,,4/26/2016 17:46:51,false,clixsense,0.8889,35338593,ITA,14,Cagliari,151.56.132.145,0,,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p></p><pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p>When I run the code, I keep getting this error. I have tried looking at my API Key and seeing if I have an extra space in the API key field, but it looks perfectly fine to me.åÊ</p>",Error on running Bing API.py,"<p>I&#39;m unable to complete the survey because attempting to visit the URL <a href=""http://www.crowd-workers.com/track"">www.crowd-workers.com/track</a>åÊbrings me to a 404 Error.</p>",www.crowd-workers.com/track ERROR 404,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,2
940847377,4/26/2016 17:27:57,false,1969458136,,4/26/2016 17:23:11,false,neodev,1.0,33973110,VEN,23,Maracaibo,186.94.238.104,0,,"<p>Hi everyone,åÊ</p>
<p></p>
<p>We&#39;ve had a chance to look through submissions for HW3 and while it&#39;ll be a few days before grades are ready to be sent out, we wanted to offer some feedback and clarification. Most of you had really high accuracies and many of you noted that accuracy was &#34;higher&#34; when you eliminated cross-validation because of overfitting (in the same way scores are higher when the midterm matches the practice test very closely).åÊ</p>
<p></p>
<p>X represented the full set of articles. y represented the labels. There were hundreds of thousands of unigram features, because features corresponded to each word that appeared (repeats did not contribute to # of features -- see &#39;that guy shot that other guy&#39; for an example).åÊ</p>
<p></p>
<p>This week, you are re-training your classifier on the full set of articles that we gave you last week (&#39;articles&#39; is the name of the text file) and then output predictions on the new URLs you collected.åÊYou&#39;ll want to modify the code for your statistical classifier to generate the labels for crowdworkers to check - the file on the assignment page is a great start towards that.åÊ</p>
<p></p>
<p>If you have any questions, please come to office hours sooner rather than later and post to Piazza if your question hasn&#39;t already been asked. Some parts of this assignment just take time to finish -- crawling, waiting for crowd worker judgements, the questionnaire -- and it&#39;s better to leave yourself time to debug.åÊ</p>",Using your classifier in HW4: Becoming a Requester,"<p>Hi all!</p>
<p></p>
<p>We have finished grading your classifier assignments, and you should get your grades shortly if you have not already (look for an email from Kate!). You guys did very well overall. It was not an easy assignment and there wereåÊa lot of new concepts that you had to take in (and new packages you had to install) all at once, so very well done.åÊ</p>
<p></p>
<p>There were twoåÊcommon points of confusion. Please feel free to ask questions here or in OHs if you are having trouble with these concepts, since they are important if you intend to do more ML in the future.</p>
<p></p>
<p><strong>What exactly areåÊX and y? What are their dimensions?åÊ</strong></p>
<p>In short, y is a vector of labels (one label per article). X is a matrix of features (one row per article, one column per feature). (You mightåÊpreferåÊto think of X as aåÊlist of vectors of features).åÊA key point is thatåÊ<span style=""text-decoration:underline"">every article has a value for every feature</span>-- so every article (row) has the same number of features (columns) associated with it. What changes areåÊthe values of those features (e.g. 0 or 1 in our assignment). Since the &#34;features&#34; in our caseåÊwere words, <span style=""text-decoration:underline"">the number of features (number of columns of X) is equal to the number of words in our vocabulary</span> (this was probably ~300,000-500,000 in our dataset, depending on your preprocessing). If theåÊword appeared in the article, theåÊvalue for theåÊword is 1 for that article, and if it did not appear, theåÊvalue is 0.åÊ</p>
<p></p>
<p>You are not alone if you find this concept a bit abstract. The biggest hurdle to overcome when learning about ML is getting comfortable thinking about your data as a feature matrix. Once you have internalized this notion, the rest of ML is reasonably straightforward. Please, ask us questions!</p>
<p></p>
<p><strong>False positives and false negativesåÊ</strong></p>
<p>Many of you were confused by the concept of false positives and false negatives. <span style=""text-decoration:underline"">A *false positive* in our case is an article that should have been labeled &#34;not gun related&#34; (0) but was falsely labeled &#34;gun related&#34; (1).</span> These were likely articles with words like &#34;shoot&#34; but which referred to sports or movies instead of violent crimes. A *false negative* isåÊan article that should have been labeled &#34;gun related&#34; (1) but was falsely labeled &#34;not gun related&#34; (0). Its good to think about these different types of errors when you are working on prediction tasks-- often different types of errors have different costs, or one type of error is worse than another.åÊåÊ(E.g. in medicine, tests that produce many false negativesåÊare usually considered worse that ones that produce false positives. I&#39;m sure you can think of some reasons why?)</p>
<p>åÊ</p>
<p><strong>But what now? My life has been so empty since turning in that classifier assignment...åÊ</strong></p>
<p>Many of you have expressed interest in learning more about machine learning, and you should absolutely consider enrolling in <a href=""https://alliance.seas.upenn.edu/~cis520/wiki/"" target=""_blank"">penn&#39;s ML course</a>åÊor <a href=""http://www.seas.upenn.edu/~cis519/fall2015/"" target=""_blank"">penn&#39;s intro ML course</a>. Or check out the <a href=""https://www.coursera.org/learn/machine-learning"" target=""_blank"">Coursera course</a>åÊ(crowdsourcing woot woot!). If you can&#39;t wait that long,åÊyou have all the freedom in the world to build an ML component into your final project, and we are more than happy to help you do so!</p>
<p></p>
<p>Hope everyone hasåÊa relaxing, happy weekend!åÊ</p>
<p></p>
<p>#pin</p>",Take-aways from you classifier assignments,"<p>When I run the classifier, I get the error:åÊ</p>
<p>Reading raw data</p>
<p>Loading training data</p>
<p>Traceback (most recent call last):</p>
<p>åÊ File &#34;predict_unlabelled.py&#34;, line 65, in &lt;module&gt;</p>
<p>åÊ åÊ y, X, texts, dv, le = get_matricies(training_data)</p>
<p>åÊ File &#34;predict_unlabelled.py&#34;, line 31, in get_matricies</p>
<p>åÊ åÊ texts = [d[1] for d in data]</p>
<p>IndexError: list index out of range</p>
<p></p>
<p>Which is an error indicated in part of the code we were given. Does anyone know why this is?åÊ</p>",Error when running classifier,<p>ShouldåÊwe have a single if-and-or statement with all of the keywords we use? Or should we have a series of if statements when checking keywords?åÊ</p>,rule-based classifier,<p>Resolved!</p>,Error when running classifier template,"<p>After testing a bunch of words in the rule based classifier, we saw that using only a few words gave us the highest accuracy of .94572. Is this a high enough accuracy?</p>
<p></p>
<p>Adding other words just lowered the accuracy so we aren&#39;t sure if we should continue given the fact we arent using a lot of words. We tried but most individual words we added had pretty low accuracies alone. Here is the snippet and the uncommented code is the one that we are using.</p>
<p></p>
<p>Just want to make sure being drawing out the tree and adding to the decision tree classifier, thanks!</p>
<p></p>
<pre>		# if &#34;gun&#34; in text : prediction = &#39;1&#39;
		# if &#34;shooting&#34; in text : prediction = &#39;1&#39;
		# if &#34;shot&#34; in text : prediction = &#39;1&#39;
		# if &#34;killed&#34; in text : prediction = &#39;1&#39;
		# if &#34;violence&#34; in text : prediction = &#39;1&#39;
		# if &#34;murder&#34; in text : prediction = &#39;1&#39;
		# if &#34;crime&#34; in text : prediction = &#39;1&#39;
		# if &#34;sentence&#34; in text : prediction = &#39;1&#39;
		# if &#34;death&#34; in text : prediction = &#39;1&#39;
		# if &#34;gun&#34; and  &#34;shooting&#34; in text : prediction = &#39;1&#39;
		# if &#34;firearm&#34; in text : prediction = &#39;1&#39;
		# if &#34;weapon&#34; in text : prediction = &#39;1&#39;
		# if &#34;shooting&#34; in text and &#34;missile&#34; not in text : prediction = &#39;1&#39;
		# if &#34;gun&#34; in text and &#34;violence&#34; in text : prediction = &#39;1&#39;
		# if &#34;shooting&#34; in text and &#34;policy&#34; not in text and &#34;ball&#34; not in text and &#34;missile&#34; not in text : prediction = &#39;1&#39;
		# if &#34;firearm&#34; in text and &#34;shooting&#34; in text : prediction = &#39;1&#39;
		#if &#34;shooter&#34; in text and &#34;shooting&#34; in text : prediction = &#39;1&#39;
		if &#34;shooting&#34; in text and &#34;basketball&#34; not in text and &#34;policy&#34; not in text and &#39;missile&#39; not in text:
			prediction = &#39;1&#39;
		elif &#34;gun&#34; in text and &#34;shot&#34; in text and &#34;roses&#34; not in text:
			prediction = &#39;1&#39;</pre>
<p></p>",Accuracy of rule based classifier,5
940847377,4/26/2016 17:32:16,false,1969460790,,4/26/2016 17:31:58,false,neodev,0.8889,33131546,IDN,04,Jakarta,139.194.89.60,0,,"<p>Hi everyone,åÊ</p>
<p></p>
<p>We&#39;ve had a chance to look through submissions for HW3 and while it&#39;ll be a few days before grades are ready to be sent out, we wanted to offer some feedback and clarification. Most of you had really high accuracies and many of you noted that accuracy was &#34;higher&#34; when you eliminated cross-validation because of overfitting (in the same way scores are higher when the midterm matches the practice test very closely).åÊ</p>
<p></p>
<p>X represented the full set of articles. y represented the labels. There were hundreds of thousands of unigram features, because features corresponded to each word that appeared (repeats did not contribute to # of features -- see &#39;that guy shot that other guy&#39; for an example).åÊ</p>
<p></p>
<p>This week, you are re-training your classifier on the full set of articles that we gave you last week (&#39;articles&#39; is the name of the text file) and then output predictions on the new URLs you collected.åÊYou&#39;ll want to modify the code for your statistical classifier to generate the labels for crowdworkers to check - the file on the assignment page is a great start towards that.åÊ</p>
<p></p>
<p>If you have any questions, please come to office hours sooner rather than later and post to Piazza if your question hasn&#39;t already been asked. Some parts of this assignment just take time to finish -- crawling, waiting for crowd worker judgements, the questionnaire -- and it&#39;s better to leave yourself time to debug.åÊ</p>",Using your classifier in HW4: Becoming a Requester,"<p>Hi all!</p>
<p></p>
<p>We have finished grading your classifier assignments, and you should get your grades shortly if you have not already (look for an email from Kate!). You guys did very well overall. It was not an easy assignment and there wereåÊa lot of new concepts that you had to take in (and new packages you had to install) all at once, so very well done.åÊ</p>
<p></p>
<p>There were twoåÊcommon points of confusion. Please feel free to ask questions here or in OHs if you are having trouble with these concepts, since they are important if you intend to do more ML in the future.</p>
<p></p>
<p><strong>What exactly areåÊX and y? What are their dimensions?åÊ</strong></p>
<p>In short, y is a vector of labels (one label per article). X is a matrix of features (one row per article, one column per feature). (You mightåÊpreferåÊto think of X as aåÊlist of vectors of features).åÊA key point is thatåÊ<span style=""text-decoration:underline"">every article has a value for every feature</span>-- so every article (row) has the same number of features (columns) associated with it. What changes areåÊthe values of those features (e.g. 0 or 1 in our assignment). Since the &#34;features&#34; in our caseåÊwere words, <span style=""text-decoration:underline"">the number of features (number of columns of X) is equal to the number of words in our vocabulary</span> (this was probably ~300,000-500,000 in our dataset, depending on your preprocessing). If theåÊword appeared in the article, theåÊvalue for theåÊword is 1 for that article, and if it did not appear, theåÊvalue is 0.åÊ</p>
<p></p>
<p>You are not alone if you find this concept a bit abstract. The biggest hurdle to overcome when learning about ML is getting comfortable thinking about your data as a feature matrix. Once you have internalized this notion, the rest of ML is reasonably straightforward. Please, ask us questions!</p>
<p></p>
<p><strong>False positives and false negativesåÊ</strong></p>
<p>Many of you were confused by the concept of false positives and false negatives. <span style=""text-decoration:underline"">A *false positive* in our case is an article that should have been labeled &#34;not gun related&#34; (0) but was falsely labeled &#34;gun related&#34; (1).</span> These were likely articles with words like &#34;shoot&#34; but which referred to sports or movies instead of violent crimes. A *false negative* isåÊan article that should have been labeled &#34;gun related&#34; (1) but was falsely labeled &#34;not gun related&#34; (0). Its good to think about these different types of errors when you are working on prediction tasks-- often different types of errors have different costs, or one type of error is worse than another.åÊåÊ(E.g. in medicine, tests that produce many false negativesåÊare usually considered worse that ones that produce false positives. I&#39;m sure you can think of some reasons why?)</p>
<p>åÊ</p>
<p><strong>But what now? My life has been so empty since turning in that classifier assignment...åÊ</strong></p>
<p>Many of you have expressed interest in learning more about machine learning, and you should absolutely consider enrolling in <a href=""https://alliance.seas.upenn.edu/~cis520/wiki/"" target=""_blank"">penn&#39;s ML course</a>åÊor <a href=""http://www.seas.upenn.edu/~cis519/fall2015/"" target=""_blank"">penn&#39;s intro ML course</a>. Or check out the <a href=""https://www.coursera.org/learn/machine-learning"" target=""_blank"">Coursera course</a>åÊ(crowdsourcing woot woot!). If you can&#39;t wait that long,åÊyou have all the freedom in the world to build an ML component into your final project, and we are more than happy to help you do so!</p>
<p></p>
<p>Hope everyone hasåÊa relaxing, happy weekend!åÊ</p>
<p></p>
<p>#pin</p>",Take-aways from you classifier assignments,"<p>When I run the classifier, I get the error:åÊ</p>
<p>Reading raw data</p>
<p>Loading training data</p>
<p>Traceback (most recent call last):</p>
<p>åÊ File &#34;predict_unlabelled.py&#34;, line 65, in &lt;module&gt;</p>
<p>åÊ åÊ y, X, texts, dv, le = get_matricies(training_data)</p>
<p>åÊ File &#34;predict_unlabelled.py&#34;, line 31, in get_matricies</p>
<p>åÊ åÊ texts = [d[1] for d in data]</p>
<p>IndexError: list index out of range</p>
<p></p>
<p>Which is an error indicated in part of the code we were given. Does anyone know why this is?åÊ</p>",Error when running classifier,<p>ShouldåÊwe have a single if-and-or statement with all of the keywords we use? Or should we have a series of if statements when checking keywords?åÊ</p>,rule-based classifier,<p>Resolved!</p>,Error when running classifier template,"<p>After testing a bunch of words in the rule based classifier, we saw that using only a few words gave us the highest accuracy of .94572. Is this a high enough accuracy?</p>
<p></p>
<p>Adding other words just lowered the accuracy so we aren&#39;t sure if we should continue given the fact we arent using a lot of words. We tried but most individual words we added had pretty low accuracies alone. Here is the snippet and the uncommented code is the one that we are using.</p>
<p></p>
<p>Just want to make sure being drawing out the tree and adding to the decision tree classifier, thanks!</p>
<p></p>
<pre>		# if &#34;gun&#34; in text : prediction = &#39;1&#39;
		# if &#34;shooting&#34; in text : prediction = &#39;1&#39;
		# if &#34;shot&#34; in text : prediction = &#39;1&#39;
		# if &#34;killed&#34; in text : prediction = &#39;1&#39;
		# if &#34;violence&#34; in text : prediction = &#39;1&#39;
		# if &#34;murder&#34; in text : prediction = &#39;1&#39;
		# if &#34;crime&#34; in text : prediction = &#39;1&#39;
		# if &#34;sentence&#34; in text : prediction = &#39;1&#39;
		# if &#34;death&#34; in text : prediction = &#39;1&#39;
		# if &#34;gun&#34; and  &#34;shooting&#34; in text : prediction = &#39;1&#39;
		# if &#34;firearm&#34; in text : prediction = &#39;1&#39;
		# if &#34;weapon&#34; in text : prediction = &#39;1&#39;
		# if &#34;shooting&#34; in text and &#34;missile&#34; not in text : prediction = &#39;1&#39;
		# if &#34;gun&#34; in text and &#34;violence&#34; in text : prediction = &#39;1&#39;
		# if &#34;shooting&#34; in text and &#34;policy&#34; not in text and &#34;ball&#34; not in text and &#34;missile&#34; not in text : prediction = &#39;1&#39;
		# if &#34;firearm&#34; in text and &#34;shooting&#34; in text : prediction = &#39;1&#39;
		#if &#34;shooter&#34; in text and &#34;shooting&#34; in text : prediction = &#39;1&#39;
		if &#34;shooting&#34; in text and &#34;basketball&#34; not in text and &#34;policy&#34; not in text and &#39;missile&#39; not in text:
			prediction = &#39;1&#39;
		elif &#34;gun&#34; in text and &#34;shot&#34; in text and &#34;roses&#34; not in text:
			prediction = &#39;1&#39;</pre>
<p></p>",Accuracy of rule based classifier,5
940847377,4/26/2016 17:35:19,false,1969462332,,4/26/2016 17:34:26,false,clixsense,1.0,30712378,ROU,21,Deva,79.119.241.200,0,,"<p>Hi everyone,åÊ</p>
<p></p>
<p>We&#39;ve had a chance to look through submissions for HW3 and while it&#39;ll be a few days before grades are ready to be sent out, we wanted to offer some feedback and clarification. Most of you had really high accuracies and many of you noted that accuracy was &#34;higher&#34; when you eliminated cross-validation because of overfitting (in the same way scores are higher when the midterm matches the practice test very closely).åÊ</p>
<p></p>
<p>X represented the full set of articles. y represented the labels. There were hundreds of thousands of unigram features, because features corresponded to each word that appeared (repeats did not contribute to # of features -- see &#39;that guy shot that other guy&#39; for an example).åÊ</p>
<p></p>
<p>This week, you are re-training your classifier on the full set of articles that we gave you last week (&#39;articles&#39; is the name of the text file) and then output predictions on the new URLs you collected.åÊYou&#39;ll want to modify the code for your statistical classifier to generate the labels for crowdworkers to check - the file on the assignment page is a great start towards that.åÊ</p>
<p></p>
<p>If you have any questions, please come to office hours sooner rather than later and post to Piazza if your question hasn&#39;t already been asked. Some parts of this assignment just take time to finish -- crawling, waiting for crowd worker judgements, the questionnaire -- and it&#39;s better to leave yourself time to debug.åÊ</p>",Using your classifier in HW4: Becoming a Requester,"<p>Hi all!</p>
<p></p>
<p>We have finished grading your classifier assignments, and you should get your grades shortly if you have not already (look for an email from Kate!). You guys did very well overall. It was not an easy assignment and there wereåÊa lot of new concepts that you had to take in (and new packages you had to install) all at once, so very well done.åÊ</p>
<p></p>
<p>There were twoåÊcommon points of confusion. Please feel free to ask questions here or in OHs if you are having trouble with these concepts, since they are important if you intend to do more ML in the future.</p>
<p></p>
<p><strong>What exactly areåÊX and y? What are their dimensions?åÊ</strong></p>
<p>In short, y is a vector of labels (one label per article). X is a matrix of features (one row per article, one column per feature). (You mightåÊpreferåÊto think of X as aåÊlist of vectors of features).åÊA key point is thatåÊ<span style=""text-decoration:underline"">every article has a value for every feature</span>-- so every article (row) has the same number of features (columns) associated with it. What changes areåÊthe values of those features (e.g. 0 or 1 in our assignment). Since the &#34;features&#34; in our caseåÊwere words, <span style=""text-decoration:underline"">the number of features (number of columns of X) is equal to the number of words in our vocabulary</span> (this was probably ~300,000-500,000 in our dataset, depending on your preprocessing). If theåÊword appeared in the article, theåÊvalue for theåÊword is 1 for that article, and if it did not appear, theåÊvalue is 0.åÊ</p>
<p></p>
<p>You are not alone if you find this concept a bit abstract. The biggest hurdle to overcome when learning about ML is getting comfortable thinking about your data as a feature matrix. Once you have internalized this notion, the rest of ML is reasonably straightforward. Please, ask us questions!</p>
<p></p>
<p><strong>False positives and false negativesåÊ</strong></p>
<p>Many of you were confused by the concept of false positives and false negatives. <span style=""text-decoration:underline"">A *false positive* in our case is an article that should have been labeled &#34;not gun related&#34; (0) but was falsely labeled &#34;gun related&#34; (1).</span> These were likely articles with words like &#34;shoot&#34; but which referred to sports or movies instead of violent crimes. A *false negative* isåÊan article that should have been labeled &#34;gun related&#34; (1) but was falsely labeled &#34;not gun related&#34; (0). Its good to think about these different types of errors when you are working on prediction tasks-- often different types of errors have different costs, or one type of error is worse than another.åÊåÊ(E.g. in medicine, tests that produce many false negativesåÊare usually considered worse that ones that produce false positives. I&#39;m sure you can think of some reasons why?)</p>
<p>åÊ</p>
<p><strong>But what now? My life has been so empty since turning in that classifier assignment...åÊ</strong></p>
<p>Many of you have expressed interest in learning more about machine learning, and you should absolutely consider enrolling in <a href=""https://alliance.seas.upenn.edu/~cis520/wiki/"" target=""_blank"">penn&#39;s ML course</a>åÊor <a href=""http://www.seas.upenn.edu/~cis519/fall2015/"" target=""_blank"">penn&#39;s intro ML course</a>. Or check out the <a href=""https://www.coursera.org/learn/machine-learning"" target=""_blank"">Coursera course</a>åÊ(crowdsourcing woot woot!). If you can&#39;t wait that long,åÊyou have all the freedom in the world to build an ML component into your final project, and we are more than happy to help you do so!</p>
<p></p>
<p>Hope everyone hasåÊa relaxing, happy weekend!åÊ</p>
<p></p>
<p>#pin</p>",Take-aways from you classifier assignments,"<p>When I run the classifier, I get the error:åÊ</p>
<p>Reading raw data</p>
<p>Loading training data</p>
<p>Traceback (most recent call last):</p>
<p>åÊ File &#34;predict_unlabelled.py&#34;, line 65, in &lt;module&gt;</p>
<p>åÊ åÊ y, X, texts, dv, le = get_matricies(training_data)</p>
<p>åÊ File &#34;predict_unlabelled.py&#34;, line 31, in get_matricies</p>
<p>åÊ åÊ texts = [d[1] for d in data]</p>
<p>IndexError: list index out of range</p>
<p></p>
<p>Which is an error indicated in part of the code we were given. Does anyone know why this is?åÊ</p>",Error when running classifier,<p>ShouldåÊwe have a single if-and-or statement with all of the keywords we use? Or should we have a series of if statements when checking keywords?åÊ</p>,rule-based classifier,<p>Resolved!</p>,Error when running classifier template,"<p>After testing a bunch of words in the rule based classifier, we saw that using only a few words gave us the highest accuracy of .94572. Is this a high enough accuracy?</p>
<p></p>
<p>Adding other words just lowered the accuracy so we aren&#39;t sure if we should continue given the fact we arent using a lot of words. We tried but most individual words we added had pretty low accuracies alone. Here is the snippet and the uncommented code is the one that we are using.</p>
<p></p>
<p>Just want to make sure being drawing out the tree and adding to the decision tree classifier, thanks!</p>
<p></p>
<pre>		# if &#34;gun&#34; in text : prediction = &#39;1&#39;
		# if &#34;shooting&#34; in text : prediction = &#39;1&#39;
		# if &#34;shot&#34; in text : prediction = &#39;1&#39;
		# if &#34;killed&#34; in text : prediction = &#39;1&#39;
		# if &#34;violence&#34; in text : prediction = &#39;1&#39;
		# if &#34;murder&#34; in text : prediction = &#39;1&#39;
		# if &#34;crime&#34; in text : prediction = &#39;1&#39;
		# if &#34;sentence&#34; in text : prediction = &#39;1&#39;
		# if &#34;death&#34; in text : prediction = &#39;1&#39;
		# if &#34;gun&#34; and  &#34;shooting&#34; in text : prediction = &#39;1&#39;
		# if &#34;firearm&#34; in text : prediction = &#39;1&#39;
		# if &#34;weapon&#34; in text : prediction = &#39;1&#39;
		# if &#34;shooting&#34; in text and &#34;missile&#34; not in text : prediction = &#39;1&#39;
		# if &#34;gun&#34; in text and &#34;violence&#34; in text : prediction = &#39;1&#39;
		# if &#34;shooting&#34; in text and &#34;policy&#34; not in text and &#34;ball&#34; not in text and &#34;missile&#34; not in text : prediction = &#39;1&#39;
		# if &#34;firearm&#34; in text and &#34;shooting&#34; in text : prediction = &#39;1&#39;
		#if &#34;shooter&#34; in text and &#34;shooting&#34; in text : prediction = &#39;1&#39;
		if &#34;shooting&#34; in text and &#34;basketball&#34; not in text and &#34;policy&#34; not in text and &#39;missile&#39; not in text:
			prediction = &#39;1&#39;
		elif &#34;gun&#34; in text and &#34;shot&#34; in text and &#34;roses&#34; not in text:
			prediction = &#39;1&#39;</pre>
<p></p>",Accuracy of rule based classifier,5
940847377,4/26/2016 18:27:49,false,1969490430,,4/26/2016 18:04:06,false,clixsense,0.8889,35338593,ITA,14,Cagliari,151.56.132.145,0,,"<p>Hi everyone,åÊ</p>
<p></p>
<p>We&#39;ve had a chance to look through submissions for HW3 and while it&#39;ll be a few days before grades are ready to be sent out, we wanted to offer some feedback and clarification. Most of you had really high accuracies and many of you noted that accuracy was &#34;higher&#34; when you eliminated cross-validation because of overfitting (in the same way scores are higher when the midterm matches the practice test very closely).åÊ</p>
<p></p>
<p>X represented the full set of articles. y represented the labels. There were hundreds of thousands of unigram features, because features corresponded to each word that appeared (repeats did not contribute to # of features -- see &#39;that guy shot that other guy&#39; for an example).åÊ</p>
<p></p>
<p>This week, you are re-training your classifier on the full set of articles that we gave you last week (&#39;articles&#39; is the name of the text file) and then output predictions on the new URLs you collected.åÊYou&#39;ll want to modify the code for your statistical classifier to generate the labels for crowdworkers to check - the file on the assignment page is a great start towards that.åÊ</p>
<p></p>
<p>If you have any questions, please come to office hours sooner rather than later and post to Piazza if your question hasn&#39;t already been asked. Some parts of this assignment just take time to finish -- crawling, waiting for crowd worker judgements, the questionnaire -- and it&#39;s better to leave yourself time to debug.åÊ</p>",Using your classifier in HW4: Becoming a Requester,"<p>Hi all!</p>
<p></p>
<p>We have finished grading your classifier assignments, and you should get your grades shortly if you have not already (look for an email from Kate!). You guys did very well overall. It was not an easy assignment and there wereåÊa lot of new concepts that you had to take in (and new packages you had to install) all at once, so very well done.åÊ</p>
<p></p>
<p>There were twoåÊcommon points of confusion. Please feel free to ask questions here or in OHs if you are having trouble with these concepts, since they are important if you intend to do more ML in the future.</p>
<p></p>
<p><strong>What exactly areåÊX and y? What are their dimensions?åÊ</strong></p>
<p>In short, y is a vector of labels (one label per article). X is a matrix of features (one row per article, one column per feature). (You mightåÊpreferåÊto think of X as aåÊlist of vectors of features).åÊA key point is thatåÊ<span style=""text-decoration:underline"">every article has a value for every feature</span>-- so every article (row) has the same number of features (columns) associated with it. What changes areåÊthe values of those features (e.g. 0 or 1 in our assignment). Since the &#34;features&#34; in our caseåÊwere words, <span style=""text-decoration:underline"">the number of features (number of columns of X) is equal to the number of words in our vocabulary</span> (this was probably ~300,000-500,000 in our dataset, depending on your preprocessing). If theåÊword appeared in the article, theåÊvalue for theåÊword is 1 for that article, and if it did not appear, theåÊvalue is 0.åÊ</p>
<p></p>
<p>You are not alone if you find this concept a bit abstract. The biggest hurdle to overcome when learning about ML is getting comfortable thinking about your data as a feature matrix. Once you have internalized this notion, the rest of ML is reasonably straightforward. Please, ask us questions!</p>
<p></p>
<p><strong>False positives and false negativesåÊ</strong></p>
<p>Many of you were confused by the concept of false positives and false negatives. <span style=""text-decoration:underline"">A *false positive* in our case is an article that should have been labeled &#34;not gun related&#34; (0) but was falsely labeled &#34;gun related&#34; (1).</span> These were likely articles with words like &#34;shoot&#34; but which referred to sports or movies instead of violent crimes. A *false negative* isåÊan article that should have been labeled &#34;gun related&#34; (1) but was falsely labeled &#34;not gun related&#34; (0). Its good to think about these different types of errors when you are working on prediction tasks-- often different types of errors have different costs, or one type of error is worse than another.åÊåÊ(E.g. in medicine, tests that produce many false negativesåÊare usually considered worse that ones that produce false positives. I&#39;m sure you can think of some reasons why?)</p>
<p>åÊ</p>
<p><strong>But what now? My life has been so empty since turning in that classifier assignment...åÊ</strong></p>
<p>Many of you have expressed interest in learning more about machine learning, and you should absolutely consider enrolling in <a href=""https://alliance.seas.upenn.edu/~cis520/wiki/"" target=""_blank"">penn&#39;s ML course</a>åÊor <a href=""http://www.seas.upenn.edu/~cis519/fall2015/"" target=""_blank"">penn&#39;s intro ML course</a>. Or check out the <a href=""https://www.coursera.org/learn/machine-learning"" target=""_blank"">Coursera course</a>åÊ(crowdsourcing woot woot!). If you can&#39;t wait that long,åÊyou have all the freedom in the world to build an ML component into your final project, and we are more than happy to help you do so!</p>
<p></p>
<p>Hope everyone hasåÊa relaxing, happy weekend!åÊ</p>
<p></p>
<p>#pin</p>",Take-aways from you classifier assignments,"<p>When I run the classifier, I get the error:åÊ</p>
<p>Reading raw data</p>
<p>Loading training data</p>
<p>Traceback (most recent call last):</p>
<p>åÊ File &#34;predict_unlabelled.py&#34;, line 65, in &lt;module&gt;</p>
<p>åÊ åÊ y, X, texts, dv, le = get_matricies(training_data)</p>
<p>åÊ File &#34;predict_unlabelled.py&#34;, line 31, in get_matricies</p>
<p>åÊ åÊ texts = [d[1] for d in data]</p>
<p>IndexError: list index out of range</p>
<p></p>
<p>Which is an error indicated in part of the code we were given. Does anyone know why this is?åÊ</p>",Error when running classifier,<p>ShouldåÊwe have a single if-and-or statement with all of the keywords we use? Or should we have a series of if statements when checking keywords?åÊ</p>,rule-based classifier,<p>Resolved!</p>,Error when running classifier template,"<p>After testing a bunch of words in the rule based classifier, we saw that using only a few words gave us the highest accuracy of .94572. Is this a high enough accuracy?</p>
<p></p>
<p>Adding other words just lowered the accuracy so we aren&#39;t sure if we should continue given the fact we arent using a lot of words. We tried but most individual words we added had pretty low accuracies alone. Here is the snippet and the uncommented code is the one that we are using.</p>
<p></p>
<p>Just want to make sure being drawing out the tree and adding to the decision tree classifier, thanks!</p>
<p></p>
<pre>		# if &#34;gun&#34; in text : prediction = &#39;1&#39;
		# if &#34;shooting&#34; in text : prediction = &#39;1&#39;
		# if &#34;shot&#34; in text : prediction = &#39;1&#39;
		# if &#34;killed&#34; in text : prediction = &#39;1&#39;
		# if &#34;violence&#34; in text : prediction = &#39;1&#39;
		# if &#34;murder&#34; in text : prediction = &#39;1&#39;
		# if &#34;crime&#34; in text : prediction = &#39;1&#39;
		# if &#34;sentence&#34; in text : prediction = &#39;1&#39;
		# if &#34;death&#34; in text : prediction = &#39;1&#39;
		# if &#34;gun&#34; and  &#34;shooting&#34; in text : prediction = &#39;1&#39;
		# if &#34;firearm&#34; in text : prediction = &#39;1&#39;
		# if &#34;weapon&#34; in text : prediction = &#39;1&#39;
		# if &#34;shooting&#34; in text and &#34;missile&#34; not in text : prediction = &#39;1&#39;
		# if &#34;gun&#34; in text and &#34;violence&#34; in text : prediction = &#39;1&#39;
		# if &#34;shooting&#34; in text and &#34;policy&#34; not in text and &#34;ball&#34; not in text and &#34;missile&#34; not in text : prediction = &#39;1&#39;
		# if &#34;firearm&#34; in text and &#34;shooting&#34; in text : prediction = &#39;1&#39;
		#if &#34;shooter&#34; in text and &#34;shooting&#34; in text : prediction = &#39;1&#39;
		if &#34;shooting&#34; in text and &#34;basketball&#34; not in text and &#34;policy&#34; not in text and &#39;missile&#39; not in text:
			prediction = &#39;1&#39;
		elif &#34;gun&#34; in text and &#34;shot&#34; in text and &#34;roses&#34; not in text:
			prediction = &#39;1&#39;</pre>
<p></p>",Accuracy of rule based classifier,5
940847377,4/26/2016 18:41:46,false,1969497897,,4/26/2016 18:39:26,false,neodev,1.0,29879245,RUS,69,Smolensk,37.144.124.118,0,,"<p>Hi everyone,åÊ</p>
<p></p>
<p>We&#39;ve had a chance to look through submissions for HW3 and while it&#39;ll be a few days before grades are ready to be sent out, we wanted to offer some feedback and clarification. Most of you had really high accuracies and many of you noted that accuracy was &#34;higher&#34; when you eliminated cross-validation because of overfitting (in the same way scores are higher when the midterm matches the practice test very closely).åÊ</p>
<p></p>
<p>X represented the full set of articles. y represented the labels. There were hundreds of thousands of unigram features, because features corresponded to each word that appeared (repeats did not contribute to # of features -- see &#39;that guy shot that other guy&#39; for an example).åÊ</p>
<p></p>
<p>This week, you are re-training your classifier on the full set of articles that we gave you last week (&#39;articles&#39; is the name of the text file) and then output predictions on the new URLs you collected.åÊYou&#39;ll want to modify the code for your statistical classifier to generate the labels for crowdworkers to check - the file on the assignment page is a great start towards that.åÊ</p>
<p></p>
<p>If you have any questions, please come to office hours sooner rather than later and post to Piazza if your question hasn&#39;t already been asked. Some parts of this assignment just take time to finish -- crawling, waiting for crowd worker judgements, the questionnaire -- and it&#39;s better to leave yourself time to debug.åÊ</p>",Using your classifier in HW4: Becoming a Requester,"<p>Hi all!</p>
<p></p>
<p>We have finished grading your classifier assignments, and you should get your grades shortly if you have not already (look for an email from Kate!). You guys did very well overall. It was not an easy assignment and there wereåÊa lot of new concepts that you had to take in (and new packages you had to install) all at once, so very well done.åÊ</p>
<p></p>
<p>There were twoåÊcommon points of confusion. Please feel free to ask questions here or in OHs if you are having trouble with these concepts, since they are important if you intend to do more ML in the future.</p>
<p></p>
<p><strong>What exactly areåÊX and y? What are their dimensions?åÊ</strong></p>
<p>In short, y is a vector of labels (one label per article). X is a matrix of features (one row per article, one column per feature). (You mightåÊpreferåÊto think of X as aåÊlist of vectors of features).åÊA key point is thatåÊ<span style=""text-decoration:underline"">every article has a value for every feature</span>-- so every article (row) has the same number of features (columns) associated with it. What changes areåÊthe values of those features (e.g. 0 or 1 in our assignment). Since the &#34;features&#34; in our caseåÊwere words, <span style=""text-decoration:underline"">the number of features (number of columns of X) is equal to the number of words in our vocabulary</span> (this was probably ~300,000-500,000 in our dataset, depending on your preprocessing). If theåÊword appeared in the article, theåÊvalue for theåÊword is 1 for that article, and if it did not appear, theåÊvalue is 0.åÊ</p>
<p></p>
<p>You are not alone if you find this concept a bit abstract. The biggest hurdle to overcome when learning about ML is getting comfortable thinking about your data as a feature matrix. Once you have internalized this notion, the rest of ML is reasonably straightforward. Please, ask us questions!</p>
<p></p>
<p><strong>False positives and false negativesåÊ</strong></p>
<p>Many of you were confused by the concept of false positives and false negatives. <span style=""text-decoration:underline"">A *false positive* in our case is an article that should have been labeled &#34;not gun related&#34; (0) but was falsely labeled &#34;gun related&#34; (1).</span> These were likely articles with words like &#34;shoot&#34; but which referred to sports or movies instead of violent crimes. A *false negative* isåÊan article that should have been labeled &#34;gun related&#34; (1) but was falsely labeled &#34;not gun related&#34; (0). Its good to think about these different types of errors when you are working on prediction tasks-- often different types of errors have different costs, or one type of error is worse than another.åÊåÊ(E.g. in medicine, tests that produce many false negativesåÊare usually considered worse that ones that produce false positives. I&#39;m sure you can think of some reasons why?)</p>
<p>åÊ</p>
<p><strong>But what now? My life has been so empty since turning in that classifier assignment...åÊ</strong></p>
<p>Many of you have expressed interest in learning more about machine learning, and you should absolutely consider enrolling in <a href=""https://alliance.seas.upenn.edu/~cis520/wiki/"" target=""_blank"">penn&#39;s ML course</a>åÊor <a href=""http://www.seas.upenn.edu/~cis519/fall2015/"" target=""_blank"">penn&#39;s intro ML course</a>. Or check out the <a href=""https://www.coursera.org/learn/machine-learning"" target=""_blank"">Coursera course</a>åÊ(crowdsourcing woot woot!). If you can&#39;t wait that long,åÊyou have all the freedom in the world to build an ML component into your final project, and we are more than happy to help you do so!</p>
<p></p>
<p>Hope everyone hasåÊa relaxing, happy weekend!åÊ</p>
<p></p>
<p>#pin</p>",Take-aways from you classifier assignments,"<p>When I run the classifier, I get the error:åÊ</p>
<p>Reading raw data</p>
<p>Loading training data</p>
<p>Traceback (most recent call last):</p>
<p>åÊ File &#34;predict_unlabelled.py&#34;, line 65, in &lt;module&gt;</p>
<p>åÊ åÊ y, X, texts, dv, le = get_matricies(training_data)</p>
<p>åÊ File &#34;predict_unlabelled.py&#34;, line 31, in get_matricies</p>
<p>åÊ åÊ texts = [d[1] for d in data]</p>
<p>IndexError: list index out of range</p>
<p></p>
<p>Which is an error indicated in part of the code we were given. Does anyone know why this is?åÊ</p>",Error when running classifier,<p>ShouldåÊwe have a single if-and-or statement with all of the keywords we use? Or should we have a series of if statements when checking keywords?åÊ</p>,rule-based classifier,<p>Resolved!</p>,Error when running classifier template,"<p>After testing a bunch of words in the rule based classifier, we saw that using only a few words gave us the highest accuracy of .94572. Is this a high enough accuracy?</p>
<p></p>
<p>Adding other words just lowered the accuracy so we aren&#39;t sure if we should continue given the fact we arent using a lot of words. We tried but most individual words we added had pretty low accuracies alone. Here is the snippet and the uncommented code is the one that we are using.</p>
<p></p>
<p>Just want to make sure being drawing out the tree and adding to the decision tree classifier, thanks!</p>
<p></p>
<pre>		# if &#34;gun&#34; in text : prediction = &#39;1&#39;
		# if &#34;shooting&#34; in text : prediction = &#39;1&#39;
		# if &#34;shot&#34; in text : prediction = &#39;1&#39;
		# if &#34;killed&#34; in text : prediction = &#39;1&#39;
		# if &#34;violence&#34; in text : prediction = &#39;1&#39;
		# if &#34;murder&#34; in text : prediction = &#39;1&#39;
		# if &#34;crime&#34; in text : prediction = &#39;1&#39;
		# if &#34;sentence&#34; in text : prediction = &#39;1&#39;
		# if &#34;death&#34; in text : prediction = &#39;1&#39;
		# if &#34;gun&#34; and  &#34;shooting&#34; in text : prediction = &#39;1&#39;
		# if &#34;firearm&#34; in text : prediction = &#39;1&#39;
		# if &#34;weapon&#34; in text : prediction = &#39;1&#39;
		# if &#34;shooting&#34; in text and &#34;missile&#34; not in text : prediction = &#39;1&#39;
		# if &#34;gun&#34; in text and &#34;violence&#34; in text : prediction = &#39;1&#39;
		# if &#34;shooting&#34; in text and &#34;policy&#34; not in text and &#34;ball&#34; not in text and &#34;missile&#34; not in text : prediction = &#39;1&#39;
		# if &#34;firearm&#34; in text and &#34;shooting&#34; in text : prediction = &#39;1&#39;
		#if &#34;shooter&#34; in text and &#34;shooting&#34; in text : prediction = &#39;1&#39;
		if &#34;shooting&#34; in text and &#34;basketball&#34; not in text and &#34;policy&#34; not in text and &#39;missile&#39; not in text:
			prediction = &#39;1&#39;
		elif &#34;gun&#34; in text and &#34;shot&#34; in text and &#34;roses&#34; not in text:
			prediction = &#39;1&#39;</pre>
<p></p>",Accuracy of rule based classifier,5
940847378,4/26/2016 17:40:29,false,1969465250,,4/26/2016 17:39:38,false,neodev,0.8889,33568303,VEN,23,Cabimas,190.77.7.36,0,,"<p>When I try to submit the homework, it does this marvelous thing where it tells me that net213 is an invalid config file.</p>
<p></p>
<p>What to do now? Somewhat concerned.</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hqh2r7yc6p926r/ikaaipxq9b3g/image.png"" /></p>",Invalid config file,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>InåÊthe bing_api.py, I am trying to print out the xml response that we receive.</p>
<p>Any ideas how to do this?</p>",Print out xml file,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,"<p>I want to do queries with operators, which do work in the user interface that we used in Part 1, Step 2.</p>
<p>e.g:åÊget_urls(&#39;gun shooting shot -basketball -&#34;photo shoot&#34; -&#34;shooting range&#34; -&#34;water gun&#34; -&#34;toy gun&#34; -vaccine&#39;)</p>
<p></p>
<p>However, when I run this, I get aåÊHTTP Error 400: Bad Request.åÊ</p>
<p></p>
<p>If I replace &#34; &#34;&#39;s with &#34;%&#34;s, and remove all of the &#34;-&#34;&#39;s, then it works (I think...I get results at least).</p>
<p></p>
<p>However, when I replace the &#34; &#34;&#39;s with &#34;%&#34;s, and leave the &#34;-&#34;s, I don&#39;t get any results (and this query does work inåÊ<a href=""https://datamarket.azure.com/dataset/explore/bing/search"">https://datamarket.azure.com/dataset/explore/bing/search</a>)</p>
<p></p>
<p>Do you have any suggestions on how to do a multi-term operator query?</p>
<p></p>
<p>Thanks!</p>",Query with Operators,5
940847378,4/26/2016 17:50:21,false,1969470893,,4/26/2016 17:49:33,false,clixsense,1.0,35444326,BRA,07,Brasília,177.15.130.106,0,,"<p>When I try to submit the homework, it does this marvelous thing where it tells me that net213 is an invalid config file.</p>
<p></p>
<p>What to do now? Somewhat concerned.</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hqh2r7yc6p926r/ikaaipxq9b3g/image.png"" /></p>",Invalid config file,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>InåÊthe bing_api.py, I am trying to print out the xml response that we receive.</p>
<p>Any ideas how to do this?</p>",Print out xml file,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,"<p>I want to do queries with operators, which do work in the user interface that we used in Part 1, Step 2.</p>
<p>e.g:åÊget_urls(&#39;gun shooting shot -basketball -&#34;photo shoot&#34; -&#34;shooting range&#34; -&#34;water gun&#34; -&#34;toy gun&#34; -vaccine&#39;)</p>
<p></p>
<p>However, when I run this, I get aåÊHTTP Error 400: Bad Request.åÊ</p>
<p></p>
<p>If I replace &#34; &#34;&#39;s with &#34;%&#34;s, and remove all of the &#34;-&#34;&#39;s, then it works (I think...I get results at least).</p>
<p></p>
<p>However, when I replace the &#34; &#34;&#39;s with &#34;%&#34;s, and leave the &#34;-&#34;s, I don&#39;t get any results (and this query does work inåÊ<a href=""https://datamarket.azure.com/dataset/explore/bing/search"">https://datamarket.azure.com/dataset/explore/bing/search</a>)</p>
<p></p>
<p>Do you have any suggestions on how to do a multi-term operator query?</p>
<p></p>
<p>Thanks!</p>",Query with Operators,5
940847378,4/26/2016 18:20:19,false,1969486841,,4/26/2016 18:19:10,false,neodev,0.8889,35550011,VEN,07,Valencia,190.204.238.112,0,,"<p>When I try to submit the homework, it does this marvelous thing where it tells me that net213 is an invalid config file.</p>
<p></p>
<p>What to do now? Somewhat concerned.</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hqh2r7yc6p926r/ikaaipxq9b3g/image.png"" /></p>",Invalid config file,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>InåÊthe bing_api.py, I am trying to print out the xml response that we receive.</p>
<p>Any ideas how to do this?</p>",Print out xml file,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,"<p>I want to do queries with operators, which do work in the user interface that we used in Part 1, Step 2.</p>
<p>e.g:åÊget_urls(&#39;gun shooting shot -basketball -&#34;photo shoot&#34; -&#34;shooting range&#34; -&#34;water gun&#34; -&#34;toy gun&#34; -vaccine&#39;)</p>
<p></p>
<p>However, when I run this, I get aåÊHTTP Error 400: Bad Request.åÊ</p>
<p></p>
<p>If I replace &#34; &#34;&#39;s with &#34;%&#34;s, and remove all of the &#34;-&#34;&#39;s, then it works (I think...I get results at least).</p>
<p></p>
<p>However, when I replace the &#34; &#34;&#39;s with &#34;%&#34;s, and leave the &#34;-&#34;s, I don&#39;t get any results (and this query does work inåÊ<a href=""https://datamarket.azure.com/dataset/explore/bing/search"">https://datamarket.azure.com/dataset/explore/bing/search</a>)</p>
<p></p>
<p>Do you have any suggestions on how to do a multi-term operator query?</p>
<p></p>
<p>Thanks!</p>",Query with Operators,5
940847378,4/26/2016 18:24:14,false,1969488806,,4/26/2016 18:22:47,false,elite,1.0,30128662,BGR,50,Pleven,212.233.177.195,0,,"<p>When I try to submit the homework, it does this marvelous thing where it tells me that net213 is an invalid config file.</p>
<p></p>
<p>What to do now? Somewhat concerned.</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hqh2r7yc6p926r/ikaaipxq9b3g/image.png"" /></p>",Invalid config file,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>InåÊthe bing_api.py, I am trying to print out the xml response that we receive.</p>
<p>Any ideas how to do this?</p>",Print out xml file,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,"<p>I want to do queries with operators, which do work in the user interface that we used in Part 1, Step 2.</p>
<p>e.g:åÊget_urls(&#39;gun shooting shot -basketball -&#34;photo shoot&#34; -&#34;shooting range&#34; -&#34;water gun&#34; -&#34;toy gun&#34; -vaccine&#39;)</p>
<p></p>
<p>However, when I run this, I get aåÊHTTP Error 400: Bad Request.åÊ</p>
<p></p>
<p>If I replace &#34; &#34;&#39;s with &#34;%&#34;s, and remove all of the &#34;-&#34;&#39;s, then it works (I think...I get results at least).</p>
<p></p>
<p>However, when I replace the &#34; &#34;&#39;s with &#34;%&#34;s, and leave the &#34;-&#34;s, I don&#39;t get any results (and this query does work inåÊ<a href=""https://datamarket.azure.com/dataset/explore/bing/search"">https://datamarket.azure.com/dataset/explore/bing/search</a>)</p>
<p></p>
<p>Do you have any suggestions on how to do a multi-term operator query?</p>
<p></p>
<p>Thanks!</p>",Query with Operators,5
940847378,4/26/2016 18:45:01,false,1969499306,,4/26/2016 18:41:05,false,neodev,1.0,35974955,VEN,17,Porlamar,190.198.232.239,0,,"<p>When I try to submit the homework, it does this marvelous thing where it tells me that net213 is an invalid config file.</p>
<p></p>
<p>What to do now? Somewhat concerned.</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hqh2r7yc6p926r/ikaaipxq9b3g/image.png"" /></p>",Invalid config file,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>InåÊthe bing_api.py, I am trying to print out the xml response that we receive.</p>
<p>Any ideas how to do this?</p>",Print out xml file,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,"<p>I want to do queries with operators, which do work in the user interface that we used in Part 1, Step 2.</p>
<p>e.g:åÊget_urls(&#39;gun shooting shot -basketball -&#34;photo shoot&#34; -&#34;shooting range&#34; -&#34;water gun&#34; -&#34;toy gun&#34; -vaccine&#39;)</p>
<p></p>
<p>However, when I run this, I get aåÊHTTP Error 400: Bad Request.åÊ</p>
<p></p>
<p>If I replace &#34; &#34;&#39;s with &#34;%&#34;s, and remove all of the &#34;-&#34;&#39;s, then it works (I think...I get results at least).</p>
<p></p>
<p>However, when I replace the &#34; &#34;&#39;s with &#34;%&#34;s, and leave the &#34;-&#34;s, I don&#39;t get any results (and this query does work inåÊ<a href=""https://datamarket.azure.com/dataset/explore/bing/search"">https://datamarket.azure.com/dataset/explore/bing/search</a>)</p>
<p></p>
<p>Do you have any suggestions on how to do a multi-term operator query?</p>
<p></p>
<p>Thanks!</p>",Query with Operators,5
940847379,4/26/2016 18:22:36,false,1969487898,,4/26/2016 18:21:16,false,neodev,0.8889,35550011,VEN,07,Valencia,190.204.238.112,0,,"<p>Hi everyone,</p>
<p></p>
<p>I&#39;m looking for a partner toåÊcollaborate with on upcoming assignments and/or the final project. E-mail me at <a href=""mailto:hcutler&#64;seas.upenn.edu"">hcutler&#64;seas.upenn.edu</a>åÊif you&#39;re interested!</p>
<p></p>
<p>Have a great day.</p>
<p></p>
<p>Hannah Cutler</p>
<p>SEAS &#39;17</p>",Looking for a partner for homeworks and/or final project?,<p></p>,Confirming: separate late day policy for the final group project?,<p>What if we need to use a late day for Friday?</p>,How do late days work for the final project?,"<p>Are we allowed to modify one of the past final project ideas shown to us in class? We have an interesting ideaåÊsimilar to PictureThis and wanted to make sure it&#39;s OK that it&#39;s similar.</p>
<p></p>
<p>Thanks!</p>",Final project idea similar to PictureThis,<p>This is the link:åÊhttps://github.com/benpleitner/NETS213-Final-Project</p>,Group Project,"<p>FYI, the second part of the final project was released last night:åÊ<a href=""http://crowdsourcing-class.org/final-project-part2.html"">http://crowdsourcing-class.org/final-project-part2.html</a></p>
<p></p>
<p>There are two deliverables. åÊThe first one is due on Saturday before 11:59pm.</p>",Final Project Part 2 has been released,1
940847379,4/26/2016 18:27:26,false,1969490294,,4/26/2016 18:25:35,false,elite,1.0,30128662,BGR,50,Pleven,212.233.177.195,0,,"<p>Hi everyone,</p>
<p></p>
<p>I&#39;m looking for a partner toåÊcollaborate with on upcoming assignments and/or the final project. E-mail me at <a href=""mailto:hcutler&#64;seas.upenn.edu"">hcutler&#64;seas.upenn.edu</a>åÊif you&#39;re interested!</p>
<p></p>
<p>Have a great day.</p>
<p></p>
<p>Hannah Cutler</p>
<p>SEAS &#39;17</p>",Looking for a partner for homeworks and/or final project?,<p></p>,Confirming: separate late day policy for the final group project?,<p>What if we need to use a late day for Friday?</p>,How do late days work for the final project?,"<p>Are we allowed to modify one of the past final project ideas shown to us in class? We have an interesting ideaåÊsimilar to PictureThis and wanted to make sure it&#39;s OK that it&#39;s similar.</p>
<p></p>
<p>Thanks!</p>",Final project idea similar to PictureThis,<p>This is the link:åÊhttps://github.com/benpleitner/NETS213-Final-Project</p>,Group Project,"<p>FYI, the second part of the final project was released last night:åÊ<a href=""http://crowdsourcing-class.org/final-project-part2.html"">http://crowdsourcing-class.org/final-project-part2.html</a></p>
<p></p>
<p>There are two deliverables. åÊThe first one is due on Saturday before 11:59pm.</p>",Final Project Part 2 has been released,1
940847379,4/26/2016 18:37:13,false,1969495299,,4/26/2016 18:33:39,false,neodev,1.0,29879245,RUS,69,Smolensk,37.144.124.118,0,,"<p>Hi everyone,</p>
<p></p>
<p>I&#39;m looking for a partner toåÊcollaborate with on upcoming assignments and/or the final project. E-mail me at <a href=""mailto:hcutler&#64;seas.upenn.edu"">hcutler&#64;seas.upenn.edu</a>åÊif you&#39;re interested!</p>
<p></p>
<p>Have a great day.</p>
<p></p>
<p>Hannah Cutler</p>
<p>SEAS &#39;17</p>",Looking for a partner for homeworks and/or final project?,<p></p>,Confirming: separate late day policy for the final group project?,<p>What if we need to use a late day for Friday?</p>,How do late days work for the final project?,"<p>Are we allowed to modify one of the past final project ideas shown to us in class? We have an interesting ideaåÊsimilar to PictureThis and wanted to make sure it&#39;s OK that it&#39;s similar.</p>
<p></p>
<p>Thanks!</p>",Final project idea similar to PictureThis,<p>This is the link:åÊhttps://github.com/benpleitner/NETS213-Final-Project</p>,Group Project,"<p>FYI, the second part of the final project was released last night:åÊ<a href=""http://crowdsourcing-class.org/final-project-part2.html"">http://crowdsourcing-class.org/final-project-part2.html</a></p>
<p></p>
<p>There are two deliverables. åÊThe first one is due on Saturday before 11:59pm.</p>",Final Project Part 2 has been released,1
940847379,4/26/2016 19:02:49,false,1969507176,,4/26/2016 18:44:58,false,neodev,1.0,11172894,IND,28,Champdani,117.194.5.117,0,,"<p>Hi everyone,</p>
<p></p>
<p>I&#39;m looking for a partner toåÊcollaborate with on upcoming assignments and/or the final project. E-mail me at <a href=""mailto:hcutler&#64;seas.upenn.edu"">hcutler&#64;seas.upenn.edu</a>åÊif you&#39;re interested!</p>
<p></p>
<p>Have a great day.</p>
<p></p>
<p>Hannah Cutler</p>
<p>SEAS &#39;17</p>",Looking for a partner for homeworks and/or final project?,<p></p>,Confirming: separate late day policy for the final group project?,<p>What if we need to use a late day for Friday?</p>,How do late days work for the final project?,"<p>Are we allowed to modify one of the past final project ideas shown to us in class? We have an interesting ideaåÊsimilar to PictureThis and wanted to make sure it&#39;s OK that it&#39;s similar.</p>
<p></p>
<p>Thanks!</p>",Final project idea similar to PictureThis,<p>This is the link:åÊhttps://github.com/benpleitner/NETS213-Final-Project</p>,Group Project,"<p>FYI, the second part of the final project was released last night:åÊ<a href=""http://crowdsourcing-class.org/final-project-part2.html"">http://crowdsourcing-class.org/final-project-part2.html</a></p>
<p></p>
<p>There are two deliverables. åÊThe first one is due on Saturday before 11:59pm.</p>",Final Project Part 2 has been released,1
940847379,4/26/2016 19:25:38,false,1969519500,,4/26/2016 19:25:12,false,tremorgames,1.0,25197223,HRV,15,Split,94.253.234.240,0,,"<p>Hi everyone,</p>
<p></p>
<p>I&#39;m looking for a partner toåÊcollaborate with on upcoming assignments and/or the final project. E-mail me at <a href=""mailto:hcutler&#64;seas.upenn.edu"">hcutler&#64;seas.upenn.edu</a>åÊif you&#39;re interested!</p>
<p></p>
<p>Have a great day.</p>
<p></p>
<p>Hannah Cutler</p>
<p>SEAS &#39;17</p>",Looking for a partner for homeworks and/or final project?,<p></p>,Confirming: separate late day policy for the final group project?,<p>What if we need to use a late day for Friday?</p>,How do late days work for the final project?,"<p>Are we allowed to modify one of the past final project ideas shown to us in class? We have an interesting ideaåÊsimilar to PictureThis and wanted to make sure it&#39;s OK that it&#39;s similar.</p>
<p></p>
<p>Thanks!</p>",Final project idea similar to PictureThis,<p>This is the link:åÊhttps://github.com/benpleitner/NETS213-Final-Project</p>,Group Project,"<p>FYI, the second part of the final project was released last night:åÊ<a href=""http://crowdsourcing-class.org/final-project-part2.html"">http://crowdsourcing-class.org/final-project-part2.html</a></p>
<p></p>
<p>There are two deliverables. åÊThe first one is due on Saturday before 11:59pm.</p>",Final Project Part 2 has been released,1
940847380,4/26/2016 15:28:06,false,1969389966,,4/26/2016 15:27:30,false,instagc,0.8889,13581319,USA,IL,Waltonville,208.70.36.12,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,<p>Could you provide some resources for learning how to build web apps from scratch?åÊ</p>,Tips for Building Web Apps,0
940847380,4/26/2016 15:30:54,false,1969391257,,4/26/2016 15:28:59,false,elite,1.0,30280423,ITA,15,Siracusa,151.54.84.121,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,<p>Could you provide some resources for learning how to build web apps from scratch?åÊ</p>,Tips for Building Web Apps,0
940847380,4/26/2016 15:35:58,false,1969392977,,4/26/2016 15:30:06,false,clixsense,0.8889,36052512,PHL,F2,Quezon City,49.149.150.150,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,<p>Could you provide some resources for learning how to build web apps from scratch?åÊ</p>,Tips for Building Web Apps,0
940847380,4/26/2016 15:45:41,false,1969397209,,4/26/2016 15:39:57,false,clixsense,1.0,21875134,GBR,H9,London,87.112.158.81,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,<p>Could you provide some resources for learning how to build web apps from scratch?åÊ</p>,Tips for Building Web Apps,0
940847380,4/26/2016 15:47:45,false,1969398095,,4/26/2016 15:41:08,false,neodev,0.7778,32569659,USA,MN,Minneapolis,97.127.88.224,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,<p>Could you provide some resources for learning how to build web apps from scratch?åÊ</p>,Tips for Building Web Apps,0
940847381,4/26/2016 15:12:15,false,1969364820,,4/26/2016 15:12:00,false,tremorgames,1.0,32635967,LTU,60,Panevezys,78.63.38.165,0,,<p>One of the video links for the peer review assignment we are supposed to do is broken (iStockPhoto). What should I do?</p>,Video link broken,"<p>I created a Powerpoint and recorded my voice to go along with it, but for some reason, when I export it as a .mov it gets corrupted and doesn&#39;t play the audio. I think it may be my inclusion of embedded screen recordings within the powerpoint itself, but I&#39;ve been unable to find a solution. Would you accept a Dropbox or Drive link to the powerpoint and when you click &#34;Start Presentation,&#34; it&#39;ll run on its own accord with audio and all? Thanks!</p>",Video submission,"<p>How should we submit the link to our video presentation on Vimeo? Should we email the link to the Professor, or will a survey be posted for submission?</p>",Submitting Video Presentation,"<p>Hi, one of the vimeo links in the email sent to me has no video when I click on it. What do?</p>",Link has No Video,"<p>Hi,</p>
<p></p>
<p>I have been trying for about an hour but have not been able to successfully export my video. I have not used iMovie before but it seems relatively straight forward. However, my computer will not make the video file for me to upload to Vimeo. What should I do?</p>",Exporting video,"<p>Please submit a link to your company profile videoåÊ<a href=""https://docs.google.com/forms/d/1y2ObY-Vvgc-_3r8HG8SyTIP5ofA35xxZcwNl1RJf4nc/viewform"">using this questionnaire</a>åÊbefore class on Friday. åÊThe link includes aåÊquestionnaire likeåÊusual.</p>",Submit your company profile video,1
940847381,4/26/2016 15:20:55,false,1969378550,,4/26/2016 15:20:31,false,neodev,1.0,19132694,LKA,36,Colombo,123.231.124.170,0,,<p>One of the video links for the peer review assignment we are supposed to do is broken (iStockPhoto). What should I do?</p>,Video link broken,"<p>I created a Powerpoint and recorded my voice to go along with it, but for some reason, when I export it as a .mov it gets corrupted and doesn&#39;t play the audio. I think it may be my inclusion of embedded screen recordings within the powerpoint itself, but I&#39;ve been unable to find a solution. Would you accept a Dropbox or Drive link to the powerpoint and when you click &#34;Start Presentation,&#34; it&#39;ll run on its own accord with audio and all? Thanks!</p>",Video submission,"<p>How should we submit the link to our video presentation on Vimeo? Should we email the link to the Professor, or will a survey be posted for submission?</p>",Submitting Video Presentation,"<p>Hi, one of the vimeo links in the email sent to me has no video when I click on it. What do?</p>",Link has No Video,"<p>Hi,</p>
<p></p>
<p>I have been trying for about an hour but have not been able to successfully export my video. I have not used iMovie before but it seems relatively straight forward. However, my computer will not make the video file for me to upload to Vimeo. What should I do?</p>",Exporting video,"<p>Please submit a link to your company profile videoåÊ<a href=""https://docs.google.com/forms/d/1y2ObY-Vvgc-_3r8HG8SyTIP5ofA35xxZcwNl1RJf4nc/viewform"">using this questionnaire</a>åÊbefore class on Friday. åÊThe link includes aåÊquestionnaire likeåÊusual.</p>",Submit your company profile video,1
940847381,4/26/2016 15:23:37,false,1969383219,,4/26/2016 15:20:30,false,clixsense,1.0,24287706,TWN,04,Keelung,61.231.195.173,0,,<p>One of the video links for the peer review assignment we are supposed to do is broken (iStockPhoto). What should I do?</p>,Video link broken,"<p>I created a Powerpoint and recorded my voice to go along with it, but for some reason, when I export it as a .mov it gets corrupted and doesn&#39;t play the audio. I think it may be my inclusion of embedded screen recordings within the powerpoint itself, but I&#39;ve been unable to find a solution. Would you accept a Dropbox or Drive link to the powerpoint and when you click &#34;Start Presentation,&#34; it&#39;ll run on its own accord with audio and all? Thanks!</p>",Video submission,"<p>How should we submit the link to our video presentation on Vimeo? Should we email the link to the Professor, or will a survey be posted for submission?</p>",Submitting Video Presentation,"<p>Hi, one of the vimeo links in the email sent to me has no video when I click on it. What do?</p>",Link has No Video,"<p>Hi,</p>
<p></p>
<p>I have been trying for about an hour but have not been able to successfully export my video. I have not used iMovie before but it seems relatively straight forward. However, my computer will not make the video file for me to upload to Vimeo. What should I do?</p>",Exporting video,"<p>Please submit a link to your company profile videoåÊ<a href=""https://docs.google.com/forms/d/1y2ObY-Vvgc-_3r8HG8SyTIP5ofA35xxZcwNl1RJf4nc/viewform"">using this questionnaire</a>åÊbefore class on Friday. åÊThe link includes aåÊquestionnaire likeåÊusual.</p>",Submit your company profile video,1
940847381,4/26/2016 15:26:30,false,1969388021,,4/26/2016 15:24:24,false,clixsense,1.0,7837812,SRB,00,Belgrade,79.101.254.233,0,,<p>One of the video links for the peer review assignment we are supposed to do is broken (iStockPhoto). What should I do?</p>,Video link broken,"<p>I created a Powerpoint and recorded my voice to go along with it, but for some reason, when I export it as a .mov it gets corrupted and doesn&#39;t play the audio. I think it may be my inclusion of embedded screen recordings within the powerpoint itself, but I&#39;ve been unable to find a solution. Would you accept a Dropbox or Drive link to the powerpoint and when you click &#34;Start Presentation,&#34; it&#39;ll run on its own accord with audio and all? Thanks!</p>",Video submission,"<p>How should we submit the link to our video presentation on Vimeo? Should we email the link to the Professor, or will a survey be posted for submission?</p>",Submitting Video Presentation,"<p>Hi, one of the vimeo links in the email sent to me has no video when I click on it. What do?</p>",Link has No Video,"<p>Hi,</p>
<p></p>
<p>I have been trying for about an hour but have not been able to successfully export my video. I have not used iMovie before but it seems relatively straight forward. However, my computer will not make the video file for me to upload to Vimeo. What should I do?</p>",Exporting video,"<p>Please submit a link to your company profile videoåÊ<a href=""https://docs.google.com/forms/d/1y2ObY-Vvgc-_3r8HG8SyTIP5ofA35xxZcwNl1RJf4nc/viewform"">using this questionnaire</a>åÊbefore class on Friday. åÊThe link includes aåÊquestionnaire likeåÊusual.</p>",Submit your company profile video,1
940847381,4/26/2016 15:26:56,false,1969388616,,4/26/2016 15:26:14,false,instagc,0.8889,13581319,USA,IL,Waltonville,208.70.36.12,0,,<p>One of the video links for the peer review assignment we are supposed to do is broken (iStockPhoto). What should I do?</p>,Video link broken,"<p>I created a Powerpoint and recorded my voice to go along with it, but for some reason, when I export it as a .mov it gets corrupted and doesn&#39;t play the audio. I think it may be my inclusion of embedded screen recordings within the powerpoint itself, but I&#39;ve been unable to find a solution. Would you accept a Dropbox or Drive link to the powerpoint and when you click &#34;Start Presentation,&#34; it&#39;ll run on its own accord with audio and all? Thanks!</p>",Video submission,"<p>How should we submit the link to our video presentation on Vimeo? Should we email the link to the Professor, or will a survey be posted for submission?</p>",Submitting Video Presentation,"<p>Hi, one of the vimeo links in the email sent to me has no video when I click on it. What do?</p>",Link has No Video,"<p>Hi,</p>
<p></p>
<p>I have been trying for about an hour but have not been able to successfully export my video. I have not used iMovie before but it seems relatively straight forward. However, my computer will not make the video file for me to upload to Vimeo. What should I do?</p>",Exporting video,"<p>Please submit a link to your company profile videoåÊ<a href=""https://docs.google.com/forms/d/1y2ObY-Vvgc-_3r8HG8SyTIP5ofA35xxZcwNl1RJf4nc/viewform"">using this questionnaire</a>åÊbefore class on Friday. åÊThe link includes aåÊquestionnaire likeåÊusual.</p>",Submit your company profile video,1
940847382,4/26/2016 17:41:58,false,1969466125,,4/26/2016 17:40:30,false,neodev,0.8889,33568303,VEN,23,Cabimas,190.77.7.36,0,,"<p>When I try to submit the homework, it does this marvelous thing where it tells me that net213 is an invalid config file.</p>
<p></p>
<p>What to do now? Somewhat concerned.</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hqh2r7yc6p926r/ikaaipxq9b3g/image.png"" /></p>",Invalid config file,"<p>InåÊthe bing_api.py, I am trying to print out the xml response that we receive.</p>
<p>Any ideas how to do this?</p>",Print out xml file,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>The course selection period ends today. åÊIf you were shopping NETS 213 and do not intend to take it, please drop it today so that one of the 10 students on the waitlist can take your place.</p>
<p></p>
<p></p>",Course selection period ends today,2
940847382,4/26/2016 17:51:16,false,1969471399,,4/26/2016 17:50:22,false,clixsense,1.0,35444326,BRA,07,Brasília,177.15.130.106,0,,"<p>When I try to submit the homework, it does this marvelous thing where it tells me that net213 is an invalid config file.</p>
<p></p>
<p>What to do now? Somewhat concerned.</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hqh2r7yc6p926r/ikaaipxq9b3g/image.png"" /></p>",Invalid config file,"<p>InåÊthe bing_api.py, I am trying to print out the xml response that we receive.</p>
<p>Any ideas how to do this?</p>",Print out xml file,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>The course selection period ends today. åÊIf you were shopping NETS 213 and do not intend to take it, please drop it today so that one of the 10 students on the waitlist can take your place.</p>
<p></p>
<p></p>",Course selection period ends today,2
940847382,4/26/2016 18:21:14,false,1969487261,,4/26/2016 18:20:20,false,neodev,0.8889,35550011,VEN,07,Valencia,190.204.238.112,0,,"<p>When I try to submit the homework, it does this marvelous thing where it tells me that net213 is an invalid config file.</p>
<p></p>
<p>What to do now? Somewhat concerned.</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hqh2r7yc6p926r/ikaaipxq9b3g/image.png"" /></p>",Invalid config file,"<p>InåÊthe bing_api.py, I am trying to print out the xml response that we receive.</p>
<p>Any ideas how to do this?</p>",Print out xml file,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>The course selection period ends today. åÊIf you were shopping NETS 213 and do not intend to take it, please drop it today so that one of the 10 students on the waitlist can take your place.</p>
<p></p>
<p></p>",Course selection period ends today,2
940847382,4/26/2016 18:25:34,false,1969489422,,4/26/2016 18:24:15,false,elite,1.0,30128662,BGR,50,Pleven,212.233.177.195,0,,"<p>When I try to submit the homework, it does this marvelous thing where it tells me that net213 is an invalid config file.</p>
<p></p>
<p>What to do now? Somewhat concerned.</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hqh2r7yc6p926r/ikaaipxq9b3g/image.png"" /></p>",Invalid config file,"<p>InåÊthe bing_api.py, I am trying to print out the xml response that we receive.</p>
<p>Any ideas how to do this?</p>",Print out xml file,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>The course selection period ends today. åÊIf you were shopping NETS 213 and do not intend to take it, please drop it today so that one of the 10 students on the waitlist can take your place.</p>
<p></p>
<p></p>",Course selection period ends today,2
940847382,4/26/2016 18:56:00,false,1969504148,,4/26/2016 18:55:19,false,neodev,1.0,29879245,RUS,69,Smolensk,37.144.124.118,0,,"<p>When I try to submit the homework, it does this marvelous thing where it tells me that net213 is an invalid config file.</p>
<p></p>
<p>What to do now? Somewhat concerned.</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hqh2r7yc6p926r/ikaaipxq9b3g/image.png"" /></p>",Invalid config file,"<p>InåÊthe bing_api.py, I am trying to print out the xml response that we receive.</p>
<p>Any ideas how to do this?</p>",Print out xml file,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>The course selection period ends today. åÊIf you were shopping NETS 213 and do not intend to take it, please drop it today so that one of the 10 students on the waitlist can take your place.</p>
<p></p>
<p></p>",Course selection period ends today,2
940847383,4/26/2016 17:40:29,false,1969465257,,4/26/2016 17:39:38,false,neodev,0.8889,33568303,VEN,23,Cabimas,190.77.7.36,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The slides for today&#39;s lecture topics are now online:</p>
<p><a href=""http://crowdsourcing-class.org/slides/machine-learning-part-2.pdf"">Machine Learning - part 2</a></p>
<p><a href=""http://crowdsourcing-class.org/slides/amazon-mechanical-turk.pdf"">The Amazon Mechanical Turk crowdsourcing platform</a></p>
<p></p>
<p>Please post any questions that you have about either topic to this thread.åÊ</p>",Slides for today&#39;s lecture,"<p>Hi Everyone!</p>
<p>åÊ</p>
<p>We just sent everyone emails about your IPython assignment. As a reminder it was graded for participation credit if you turned it in. No late days were deduced on this assignment.åÊ</p>
<p></p>
<p>The email was sent from &#34;nets213&#64;seas.upenn.edu&#34;. Please check your spam in case you don&#39;t find the email in your inbox.</p>",IPython Bootcamp (participation) score report mailed,1
940847383,4/26/2016 17:50:21,false,1969470895,,4/26/2016 17:49:33,false,clixsense,1.0,35444326,BRA,07,Brasília,177.15.130.106,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The slides for today&#39;s lecture topics are now online:</p>
<p><a href=""http://crowdsourcing-class.org/slides/machine-learning-part-2.pdf"">Machine Learning - part 2</a></p>
<p><a href=""http://crowdsourcing-class.org/slides/amazon-mechanical-turk.pdf"">The Amazon Mechanical Turk crowdsourcing platform</a></p>
<p></p>
<p>Please post any questions that you have about either topic to this thread.åÊ</p>",Slides for today&#39;s lecture,"<p>Hi Everyone!</p>
<p>åÊ</p>
<p>We just sent everyone emails about your IPython assignment. As a reminder it was graded for participation credit if you turned it in. No late days were deduced on this assignment.åÊ</p>
<p></p>
<p>The email was sent from &#34;nets213&#64;seas.upenn.edu&#34;. Please check your spam in case you don&#39;t find the email in your inbox.</p>",IPython Bootcamp (participation) score report mailed,1
940847383,4/26/2016 18:20:19,false,1969486836,,4/26/2016 18:19:10,false,neodev,0.8889,35550011,VEN,07,Valencia,190.204.238.112,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The slides for today&#39;s lecture topics are now online:</p>
<p><a href=""http://crowdsourcing-class.org/slides/machine-learning-part-2.pdf"">Machine Learning - part 2</a></p>
<p><a href=""http://crowdsourcing-class.org/slides/amazon-mechanical-turk.pdf"">The Amazon Mechanical Turk crowdsourcing platform</a></p>
<p></p>
<p>Please post any questions that you have about either topic to this thread.åÊ</p>",Slides for today&#39;s lecture,"<p>Hi Everyone!</p>
<p>åÊ</p>
<p>We just sent everyone emails about your IPython assignment. As a reminder it was graded for participation credit if you turned it in. No late days were deduced on this assignment.åÊ</p>
<p></p>
<p>The email was sent from &#34;nets213&#64;seas.upenn.edu&#34;. Please check your spam in case you don&#39;t find the email in your inbox.</p>",IPython Bootcamp (participation) score report mailed,1
940847383,4/26/2016 18:24:14,false,1969488813,,4/26/2016 18:22:47,false,elite,1.0,30128662,BGR,50,Pleven,212.233.177.195,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The slides for today&#39;s lecture topics are now online:</p>
<p><a href=""http://crowdsourcing-class.org/slides/machine-learning-part-2.pdf"">Machine Learning - part 2</a></p>
<p><a href=""http://crowdsourcing-class.org/slides/amazon-mechanical-turk.pdf"">The Amazon Mechanical Turk crowdsourcing platform</a></p>
<p></p>
<p>Please post any questions that you have about either topic to this thread.åÊ</p>",Slides for today&#39;s lecture,"<p>Hi Everyone!</p>
<p>åÊ</p>
<p>We just sent everyone emails about your IPython assignment. As a reminder it was graded for participation credit if you turned it in. No late days were deduced on this assignment.åÊ</p>
<p></p>
<p>The email was sent from &#34;nets213&#64;seas.upenn.edu&#34;. Please check your spam in case you don&#39;t find the email in your inbox.</p>",IPython Bootcamp (participation) score report mailed,1
940847383,4/26/2016 18:45:01,false,1969499310,,4/26/2016 18:41:05,false,neodev,1.0,35974955,VEN,17,Porlamar,190.198.232.239,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The slides for today&#39;s lecture topics are now online:</p>
<p><a href=""http://crowdsourcing-class.org/slides/machine-learning-part-2.pdf"">Machine Learning - part 2</a></p>
<p><a href=""http://crowdsourcing-class.org/slides/amazon-mechanical-turk.pdf"">The Amazon Mechanical Turk crowdsourcing platform</a></p>
<p></p>
<p>Please post any questions that you have about either topic to this thread.åÊ</p>",Slides for today&#39;s lecture,"<p>Hi Everyone!</p>
<p>åÊ</p>
<p>We just sent everyone emails about your IPython assignment. As a reminder it was graded for participation credit if you turned it in. No late days were deduced on this assignment.åÊ</p>
<p></p>
<p>The email was sent from &#34;nets213&#64;seas.upenn.edu&#34;. Please check your spam in case you don&#39;t find the email in your inbox.</p>",IPython Bootcamp (participation) score report mailed,1
940847384,4/26/2016 16:03:21,false,1969405343,,4/26/2016 16:02:52,false,personaly,1.0,33663352,ARG,01,Mar Del Plata,181.168.213.227,0,,"<p>Hello, I submit my hw five prior to class on friday but have just realized that one of the code files was not in the submission directory I submit. Specifically it was predict_unlabelled.py. I have not changed the file at since submission and the only changes made to the original skeleton were using my old get_features from the prior hw assignment. Should I resubmit the entire assignment? I really dont want to lose 4 or 5 late days for this mistake, is there something I can do?</p>",HW5 submission error,"<p>I am really unsure how to check that I turned in my files. I triedåÊ</p>
<pre>ameliagoodman$ turnin -c nets213 -p gun-classifier -v hw4submission</pre>
<p>because I found that on Piazza but then I gotåÊ</p>
<p></p>
<pre>drwxr-x--- ameliago/ameliago 0 2016-02-05 15:28 hw4submission/<br />-rw-r--r-- ameliago/ameliago 1248629 2016-02-05 15:26 hw4submission/Amelia Goodman.zip<br />Compressing submitted files... please wait<br />Your files have been submitted to nets213, gun-classifier for grading.</pre>
<p>which is the same thing I got when I submitted. So I&#39;m not sure if now my timestamp is going to be updated and counted as late, and I&#39;m still not sure if I submitted the right thing. Any suggestions?</p>
<p></p>
<p></p>",How to check my submission,"<p>Hi Everyone,</p>
<p></p>
<p>Many of you had trouble with turnin for the bootcamp, and sent us email submissions. As a general policy, we cannot accept submissions via email. For this assignment, we will not penalize late submissions, so if you have not yet submitted via turnin, or you are still having trouble using scp/ssh, reach out to us and let&#39;s get it figured out.åÊ</p>
<p></p>
<p>For future assignments, we will only consider an assignment to be submitted if it is submitted via turnin before the deadline. Keep in mind our late-day policy: you are allowed 5 total late days during the semester with no penalty. After that, we deduct one point per day late.</p>
<p></p>
<p>Everyone have a wonderful weekend!</p>
<p>Ellie</p>",Late submissions and turnin,"Our submission yesterday had one pennkey which was incorrect, how can I go about fixing this?",incorrect pennkey in submission,"<p>Just checked with turnin -c nets213 -l. Would you mind opening it up? Thank you!</p>
<p></p>
<p>EDIT as of 3:14: working now, thanks.</p>",Submission is turned off for crowdflower assignment,Can you try to please post the submission link soon? (A place where we can send you links to our Github repositories).,Project Part 2 Submission,4
940847384,4/26/2016 16:24:34,false,1969419056,,4/26/2016 16:23:47,false,neodev,1.0,29175140,VEN,25,Caracas,190.72.125.134,0,,"<p>Hello, I submit my hw five prior to class on friday but have just realized that one of the code files was not in the submission directory I submit. Specifically it was predict_unlabelled.py. I have not changed the file at since submission and the only changes made to the original skeleton were using my old get_features from the prior hw assignment. Should I resubmit the entire assignment? I really dont want to lose 4 or 5 late days for this mistake, is there something I can do?</p>",HW5 submission error,"<p>I am really unsure how to check that I turned in my files. I triedåÊ</p>
<pre>ameliagoodman$ turnin -c nets213 -p gun-classifier -v hw4submission</pre>
<p>because I found that on Piazza but then I gotåÊ</p>
<p></p>
<pre>drwxr-x--- ameliago/ameliago 0 2016-02-05 15:28 hw4submission/<br />-rw-r--r-- ameliago/ameliago 1248629 2016-02-05 15:26 hw4submission/Amelia Goodman.zip<br />Compressing submitted files... please wait<br />Your files have been submitted to nets213, gun-classifier for grading.</pre>
<p>which is the same thing I got when I submitted. So I&#39;m not sure if now my timestamp is going to be updated and counted as late, and I&#39;m still not sure if I submitted the right thing. Any suggestions?</p>
<p></p>
<p></p>",How to check my submission,"<p>Hi Everyone,</p>
<p></p>
<p>Many of you had trouble with turnin for the bootcamp, and sent us email submissions. As a general policy, we cannot accept submissions via email. For this assignment, we will not penalize late submissions, so if you have not yet submitted via turnin, or you are still having trouble using scp/ssh, reach out to us and let&#39;s get it figured out.åÊ</p>
<p></p>
<p>For future assignments, we will only consider an assignment to be submitted if it is submitted via turnin before the deadline. Keep in mind our late-day policy: you are allowed 5 total late days during the semester with no penalty. After that, we deduct one point per day late.</p>
<p></p>
<p>Everyone have a wonderful weekend!</p>
<p>Ellie</p>",Late submissions and turnin,"Our submission yesterday had one pennkey which was incorrect, how can I go about fixing this?",incorrect pennkey in submission,"<p>Just checked with turnin -c nets213 -l. Would you mind opening it up? Thank you!</p>
<p></p>
<p>EDIT as of 3:14: working now, thanks.</p>",Submission is turned off for crowdflower assignment,Can you try to please post the submission link soon? (A place where we can send you links to our Github repositories).,Project Part 2 Submission,4
940847384,4/26/2016 16:34:40,false,1969426093,,4/26/2016 16:32:37,false,clixsense,0.8889,8057247,PRT,17,Póvoa De Varzim,144.64.25.68,0,,"<p>Hello, I submit my hw five prior to class on friday but have just realized that one of the code files was not in the submission directory I submit. Specifically it was predict_unlabelled.py. I have not changed the file at since submission and the only changes made to the original skeleton were using my old get_features from the prior hw assignment. Should I resubmit the entire assignment? I really dont want to lose 4 or 5 late days for this mistake, is there something I can do?</p>",HW5 submission error,"<p>I am really unsure how to check that I turned in my files. I triedåÊ</p>
<pre>ameliagoodman$ turnin -c nets213 -p gun-classifier -v hw4submission</pre>
<p>because I found that on Piazza but then I gotåÊ</p>
<p></p>
<pre>drwxr-x--- ameliago/ameliago 0 2016-02-05 15:28 hw4submission/<br />-rw-r--r-- ameliago/ameliago 1248629 2016-02-05 15:26 hw4submission/Amelia Goodman.zip<br />Compressing submitted files... please wait<br />Your files have been submitted to nets213, gun-classifier for grading.</pre>
<p>which is the same thing I got when I submitted. So I&#39;m not sure if now my timestamp is going to be updated and counted as late, and I&#39;m still not sure if I submitted the right thing. Any suggestions?</p>
<p></p>
<p></p>",How to check my submission,"<p>Hi Everyone,</p>
<p></p>
<p>Many of you had trouble with turnin for the bootcamp, and sent us email submissions. As a general policy, we cannot accept submissions via email. For this assignment, we will not penalize late submissions, so if you have not yet submitted via turnin, or you are still having trouble using scp/ssh, reach out to us and let&#39;s get it figured out.åÊ</p>
<p></p>
<p>For future assignments, we will only consider an assignment to be submitted if it is submitted via turnin before the deadline. Keep in mind our late-day policy: you are allowed 5 total late days during the semester with no penalty. After that, we deduct one point per day late.</p>
<p></p>
<p>Everyone have a wonderful weekend!</p>
<p>Ellie</p>",Late submissions and turnin,"Our submission yesterday had one pennkey which was incorrect, how can I go about fixing this?",incorrect pennkey in submission,"<p>Just checked with turnin -c nets213 -l. Would you mind opening it up? Thank you!</p>
<p></p>
<p>EDIT as of 3:14: working now, thanks.</p>",Submission is turned off for crowdflower assignment,Can you try to please post the submission link soon? (A place where we can send you links to our Github repositories).,Project Part 2 Submission,4
940847384,4/26/2016 16:41:30,false,1969430402,,4/26/2016 16:40:27,false,clixsense,1.0,6329782,IDN,07,Bekonang,202.67.40.31,0,,"<p>Hello, I submit my hw five prior to class on friday but have just realized that one of the code files was not in the submission directory I submit. Specifically it was predict_unlabelled.py. I have not changed the file at since submission and the only changes made to the original skeleton were using my old get_features from the prior hw assignment. Should I resubmit the entire assignment? I really dont want to lose 4 or 5 late days for this mistake, is there something I can do?</p>",HW5 submission error,"<p>I am really unsure how to check that I turned in my files. I triedåÊ</p>
<pre>ameliagoodman$ turnin -c nets213 -p gun-classifier -v hw4submission</pre>
<p>because I found that on Piazza but then I gotåÊ</p>
<p></p>
<pre>drwxr-x--- ameliago/ameliago 0 2016-02-05 15:28 hw4submission/<br />-rw-r--r-- ameliago/ameliago 1248629 2016-02-05 15:26 hw4submission/Amelia Goodman.zip<br />Compressing submitted files... please wait<br />Your files have been submitted to nets213, gun-classifier for grading.</pre>
<p>which is the same thing I got when I submitted. So I&#39;m not sure if now my timestamp is going to be updated and counted as late, and I&#39;m still not sure if I submitted the right thing. Any suggestions?</p>
<p></p>
<p></p>",How to check my submission,"<p>Hi Everyone,</p>
<p></p>
<p>Many of you had trouble with turnin for the bootcamp, and sent us email submissions. As a general policy, we cannot accept submissions via email. For this assignment, we will not penalize late submissions, so if you have not yet submitted via turnin, or you are still having trouble using scp/ssh, reach out to us and let&#39;s get it figured out.åÊ</p>
<p></p>
<p>For future assignments, we will only consider an assignment to be submitted if it is submitted via turnin before the deadline. Keep in mind our late-day policy: you are allowed 5 total late days during the semester with no penalty. After that, we deduct one point per day late.</p>
<p></p>
<p>Everyone have a wonderful weekend!</p>
<p>Ellie</p>",Late submissions and turnin,"Our submission yesterday had one pennkey which was incorrect, how can I go about fixing this?",incorrect pennkey in submission,"<p>Just checked with turnin -c nets213 -l. Would you mind opening it up? Thank you!</p>
<p></p>
<p>EDIT as of 3:14: working now, thanks.</p>",Submission is turned off for crowdflower assignment,Can you try to please post the submission link soon? (A place where we can send you links to our Github repositories).,Project Part 2 Submission,4
940847384,4/26/2016 16:58:38,false,1969440699,,4/26/2016 16:56:43,false,clixsense,1.0,21408115,IDN,07,Semarang,36.79.23.180,0,,"<p>Hello, I submit my hw five prior to class on friday but have just realized that one of the code files was not in the submission directory I submit. Specifically it was predict_unlabelled.py. I have not changed the file at since submission and the only changes made to the original skeleton were using my old get_features from the prior hw assignment. Should I resubmit the entire assignment? I really dont want to lose 4 or 5 late days for this mistake, is there something I can do?</p>",HW5 submission error,"<p>I am really unsure how to check that I turned in my files. I triedåÊ</p>
<pre>ameliagoodman$ turnin -c nets213 -p gun-classifier -v hw4submission</pre>
<p>because I found that on Piazza but then I gotåÊ</p>
<p></p>
<pre>drwxr-x--- ameliago/ameliago 0 2016-02-05 15:28 hw4submission/<br />-rw-r--r-- ameliago/ameliago 1248629 2016-02-05 15:26 hw4submission/Amelia Goodman.zip<br />Compressing submitted files... please wait<br />Your files have been submitted to nets213, gun-classifier for grading.</pre>
<p>which is the same thing I got when I submitted. So I&#39;m not sure if now my timestamp is going to be updated and counted as late, and I&#39;m still not sure if I submitted the right thing. Any suggestions?</p>
<p></p>
<p></p>",How to check my submission,"<p>Hi Everyone,</p>
<p></p>
<p>Many of you had trouble with turnin for the bootcamp, and sent us email submissions. As a general policy, we cannot accept submissions via email. For this assignment, we will not penalize late submissions, so if you have not yet submitted via turnin, or you are still having trouble using scp/ssh, reach out to us and let&#39;s get it figured out.åÊ</p>
<p></p>
<p>For future assignments, we will only consider an assignment to be submitted if it is submitted via turnin before the deadline. Keep in mind our late-day policy: you are allowed 5 total late days during the semester with no penalty. After that, we deduct one point per day late.</p>
<p></p>
<p>Everyone have a wonderful weekend!</p>
<p>Ellie</p>",Late submissions and turnin,"Our submission yesterday had one pennkey which was incorrect, how can I go about fixing this?",incorrect pennkey in submission,"<p>Just checked with turnin -c nets213 -l. Would you mind opening it up? Thank you!</p>
<p></p>
<p>EDIT as of 3:14: working now, thanks.</p>",Submission is turned off for crowdflower assignment,Can you try to please post the submission link soon? (A place where we can send you links to our Github repositories).,Project Part 2 Submission,4
940847385,4/26/2016 17:02:29,false,1969443197,,4/26/2016 17:01:24,false,clixsense,1.0,21408115,IDN,07,Semarang,36.79.23.180,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The slides for today&#39;s lecture topics are now online:</p>
<p><a href=""http://crowdsourcing-class.org/slides/machine-learning-part-2.pdf"">Machine Learning - part 2</a></p>
<p><a href=""http://crowdsourcing-class.org/slides/amazon-mechanical-turk.pdf"">The Amazon Mechanical Turk crowdsourcing platform</a></p>
<p></p>
<p>Please post any questions that you have about either topic to this thread.åÊ</p>",Slides for today&#39;s lecture,"<p>Here are the slides from today&#39;s lecture:åÊ<a href=""http://crowdsourcing-class.org/slides/machine-learning.pdf"">Machine Learning part 1</a>.</p>
<p></p>
<p>There areåÊa nice set of OreillyåÊvideos by Hilary Mason on <a href=""http://shop.oreilly.com/product/0636920017493.do"" target=""_blank"">Machine Learning in Python</a>. åÊIf anyone can figure out if we can watch them for free via the Penn Library, please post instructions here.</p>
<p></p>
<p>As always, please use this thread for any questions or discussion topics relating to today&#39;s lecture.åÊ</p>",Slides for Machine Learning lecture,4
940847385,4/26/2016 17:07:43,false,1969446212,,4/26/2016 17:05:48,false,neodev,1.0,36167043,GBR,G6,Hull,77.86.101.69,5,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The slides for today&#39;s lecture topics are now online:</p>
<p><a href=""http://crowdsourcing-class.org/slides/machine-learning-part-2.pdf"">Machine Learning - part 2</a></p>
<p><a href=""http://crowdsourcing-class.org/slides/amazon-mechanical-turk.pdf"">The Amazon Mechanical Turk crowdsourcing platform</a></p>
<p></p>
<p>Please post any questions that you have about either topic to this thread.åÊ</p>",Slides for today&#39;s lecture,"<p>Here are the slides from today&#39;s lecture:åÊ<a href=""http://crowdsourcing-class.org/slides/machine-learning.pdf"">Machine Learning part 1</a>.</p>
<p></p>
<p>There areåÊa nice set of OreillyåÊvideos by Hilary Mason on <a href=""http://shop.oreilly.com/product/0636920017493.do"" target=""_blank"">Machine Learning in Python</a>. åÊIf anyone can figure out if we can watch them for free via the Penn Library, please post instructions here.</p>
<p></p>
<p>As always, please use this thread for any questions or discussion topics relating to today&#39;s lecture.åÊ</p>",Slides for Machine Learning lecture,4
940847385,4/26/2016 17:08:43,false,1969446808,,4/26/2016 17:06:43,false,neodev,1.0,33973110,VEN,23,Maracaibo,186.94.238.104,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The slides for today&#39;s lecture topics are now online:</p>
<p><a href=""http://crowdsourcing-class.org/slides/machine-learning-part-2.pdf"">Machine Learning - part 2</a></p>
<p><a href=""http://crowdsourcing-class.org/slides/amazon-mechanical-turk.pdf"">The Amazon Mechanical Turk crowdsourcing platform</a></p>
<p></p>
<p>Please post any questions that you have about either topic to this thread.åÊ</p>",Slides for today&#39;s lecture,"<p>Here are the slides from today&#39;s lecture:åÊ<a href=""http://crowdsourcing-class.org/slides/machine-learning.pdf"">Machine Learning part 1</a>.</p>
<p></p>
<p>There areåÊa nice set of OreillyåÊvideos by Hilary Mason on <a href=""http://shop.oreilly.com/product/0636920017493.do"" target=""_blank"">Machine Learning in Python</a>. åÊIf anyone can figure out if we can watch them for free via the Penn Library, please post instructions here.</p>
<p></p>
<p>As always, please use this thread for any questions or discussion topics relating to today&#39;s lecture.åÊ</p>",Slides for Machine Learning lecture,4
940847385,4/26/2016 17:16:41,false,1969451754,,4/26/2016 17:14:02,false,elite,1.0,25411289,HRV,"","",31.147.119.175,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The slides for today&#39;s lecture topics are now online:</p>
<p><a href=""http://crowdsourcing-class.org/slides/machine-learning-part-2.pdf"">Machine Learning - part 2</a></p>
<p><a href=""http://crowdsourcing-class.org/slides/amazon-mechanical-turk.pdf"">The Amazon Mechanical Turk crowdsourcing platform</a></p>
<p></p>
<p>Please post any questions that you have about either topic to this thread.åÊ</p>",Slides for today&#39;s lecture,"<p>Here are the slides from today&#39;s lecture:åÊ<a href=""http://crowdsourcing-class.org/slides/machine-learning.pdf"">Machine Learning part 1</a>.</p>
<p></p>
<p>There areåÊa nice set of OreillyåÊvideos by Hilary Mason on <a href=""http://shop.oreilly.com/product/0636920017493.do"" target=""_blank"">Machine Learning in Python</a>. åÊIf anyone can figure out if we can watch them for free via the Penn Library, please post instructions here.</p>
<p></p>
<p>As always, please use this thread for any questions or discussion topics relating to today&#39;s lecture.åÊ</p>",Slides for Machine Learning lecture,4
940847385,4/26/2016 17:30:30,false,1969459772,,4/26/2016 17:11:11,false,neodev,0.8889,19625264,DZA,41,Chlef,41.102.7.217,5,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The slides for today&#39;s lecture topics are now online:</p>
<p><a href=""http://crowdsourcing-class.org/slides/machine-learning-part-2.pdf"">Machine Learning - part 2</a></p>
<p><a href=""http://crowdsourcing-class.org/slides/amazon-mechanical-turk.pdf"">The Amazon Mechanical Turk crowdsourcing platform</a></p>
<p></p>
<p>Please post any questions that you have about either topic to this thread.åÊ</p>",Slides for today&#39;s lecture,"<p>Here are the slides from today&#39;s lecture:åÊ<a href=""http://crowdsourcing-class.org/slides/machine-learning.pdf"">Machine Learning part 1</a>.</p>
<p></p>
<p>There areåÊa nice set of OreillyåÊvideos by Hilary Mason on <a href=""http://shop.oreilly.com/product/0636920017493.do"" target=""_blank"">Machine Learning in Python</a>. åÊIf anyone can figure out if we can watch them for free via the Penn Library, please post instructions here.</p>
<p></p>
<p>As always, please use this thread for any questions or discussion topics relating to today&#39;s lecture.åÊ</p>",Slides for Machine Learning lecture,4
940847386,4/26/2016 15:11:45,false,1969364130,,4/26/2016 15:11:33,false,tremorgames,1.0,32635967,LTU,60,Panevezys,78.63.38.165,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>If a task on crowdflower is complete and I want to do the same task again, how can I repost it?</p>",Repost the same task,0
940847386,4/26/2016 15:19:14,false,1969375917,,4/26/2016 15:15:44,false,clixsense,1.0,24287706,TWN,04,Keelung,61.231.195.173,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>If a task on crowdflower is complete and I want to do the same task again, how can I repost it?</p>",Repost the same task,0
940847386,4/26/2016 15:20:09,false,1969377313,,4/26/2016 15:19:55,false,neodev,1.0,19132694,LKA,36,Colombo,123.231.124.170,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>If a task on crowdflower is complete and I want to do the same task again, how can I repost it?</p>",Repost the same task,0
940847386,4/26/2016 15:21:37,false,1969379714,,4/26/2016 15:19:32,false,clixsense,1.0,7837812,SRB,00,Belgrade,79.101.254.233,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>If a task on crowdflower is complete and I want to do the same task again, how can I repost it?</p>",Repost the same task,0
940847386,4/26/2016 15:26:18,false,1969387714,,4/26/2016 15:21:33,false,clixsense,0.8889,36052512,PHL,F2,Quezon City,49.149.150.150,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>If a task on crowdflower is complete and I want to do the same task again, how can I repost it?</p>",Repost the same task,0
940847387,4/26/2016 15:59:59,false,1969403528,,4/26/2016 15:58:26,false,elite,1.0,33243069,IND,10,Faridabad,116.203.79.150,0,,<p></p>,Confirming: separate late day policy for the final group project?,"<p>Hello, I&#39;m a little confused as to how late days work for groups. Is it that if the group wants to use a late day, all x members of the group must use their late day or is it that only one person of the group uses one late day. This question is only geared for regular homework assignments. I understand that there is a separate late day policy for the final group project. Thanks so much for the help!</p>",Late days for groups,"<p>This is a friendly reminder that <a href=""http://crowdsourcing-class.org/assignment1.html"" target=""_blank"">homework 1</a> is due tomorrow before class. åÊ</p>
<p></p>
<p>I would also like to draw your attention to the course&#39;s late day policy (given on the <a href=""http://crowdsourcing-class.org"" target=""_blank"">course web page</a>).</p>
<blockquote>
<p dir=""ltr"">Everyone can have 5 free late days without penalty. After you have used your free late days, you will lose 20% per day (or fraction thereof) that your assignment is submitted late. The final project will have its own late day policy.</p>
</blockquote>
<p>You do not need to ask permission to use a late day. åÊNote that I have a strict policy of not granting additional late days foråÊanyåÊreason, because I do not want to adjudicate what constitutes a fair excuse. åÊSo if you exhaust your 5 free late days early, please do not ask for more.</p>",Reminder: HW1 is due tomorrow by 2pm (and late day policy),<p>What if we need to use a late day for Friday?</p>,How do late days work for the final project?,<p>I just submitted my assignment for HW2. Does this count as two or three late days?</p>,Late days,Are we allowed to use late days on the first part of the final project (the homework due Friday)?,late days,4
940847387,4/26/2016 16:01:46,false,1969404612,,4/26/2016 16:01:11,false,personaly,1.0,33663352,ARG,01,Mar Del Plata,181.168.213.227,0,,<p></p>,Confirming: separate late day policy for the final group project?,"<p>Hello, I&#39;m a little confused as to how late days work for groups. Is it that if the group wants to use a late day, all x members of the group must use their late day or is it that only one person of the group uses one late day. This question is only geared for regular homework assignments. I understand that there is a separate late day policy for the final group project. Thanks so much for the help!</p>",Late days for groups,"<p>This is a friendly reminder that <a href=""http://crowdsourcing-class.org/assignment1.html"" target=""_blank"">homework 1</a> is due tomorrow before class. åÊ</p>
<p></p>
<p>I would also like to draw your attention to the course&#39;s late day policy (given on the <a href=""http://crowdsourcing-class.org"" target=""_blank"">course web page</a>).</p>
<blockquote>
<p dir=""ltr"">Everyone can have 5 free late days without penalty. After you have used your free late days, you will lose 20% per day (or fraction thereof) that your assignment is submitted late. The final project will have its own late day policy.</p>
</blockquote>
<p>You do not need to ask permission to use a late day. åÊNote that I have a strict policy of not granting additional late days foråÊanyåÊreason, because I do not want to adjudicate what constitutes a fair excuse. åÊSo if you exhaust your 5 free late days early, please do not ask for more.</p>",Reminder: HW1 is due tomorrow by 2pm (and late day policy),<p>What if we need to use a late day for Friday?</p>,How do late days work for the final project?,<p>I just submitted my assignment for HW2. Does this count as two or three late days?</p>,Late days,Are we allowed to use late days on the first part of the final project (the homework due Friday)?,late days,4
940847387,4/26/2016 16:09:03,false,1969408349,,4/26/2016 16:03:36,false,clixsense,0.8889,8057247,PRT,17,Póvoa De Varzim,144.64.25.68,0,,<p></p>,Confirming: separate late day policy for the final group project?,"<p>Hello, I&#39;m a little confused as to how late days work for groups. Is it that if the group wants to use a late day, all x members of the group must use their late day or is it that only one person of the group uses one late day. This question is only geared for regular homework assignments. I understand that there is a separate late day policy for the final group project. Thanks so much for the help!</p>",Late days for groups,"<p>This is a friendly reminder that <a href=""http://crowdsourcing-class.org/assignment1.html"" target=""_blank"">homework 1</a> is due tomorrow before class. åÊ</p>
<p></p>
<p>I would also like to draw your attention to the course&#39;s late day policy (given on the <a href=""http://crowdsourcing-class.org"" target=""_blank"">course web page</a>).</p>
<blockquote>
<p dir=""ltr"">Everyone can have 5 free late days without penalty. After you have used your free late days, you will lose 20% per day (or fraction thereof) that your assignment is submitted late. The final project will have its own late day policy.</p>
</blockquote>
<p>You do not need to ask permission to use a late day. åÊNote that I have a strict policy of not granting additional late days foråÊanyåÊreason, because I do not want to adjudicate what constitutes a fair excuse. åÊSo if you exhaust your 5 free late days early, please do not ask for more.</p>",Reminder: HW1 is due tomorrow by 2pm (and late day policy),<p>What if we need to use a late day for Friday?</p>,How do late days work for the final project?,<p>I just submitted my assignment for HW2. Does this count as two or three late days?</p>,Late days,Are we allowed to use late days on the first part of the final project (the homework due Friday)?,late days,4
940847387,4/26/2016 16:24:40,false,1969419123,,4/26/2016 16:04:24,false,neodev,0.8889,21971187,TTO,08,Valsayn,190.213.132.190,0,,<p></p>,Confirming: separate late day policy for the final group project?,"<p>Hello, I&#39;m a little confused as to how late days work for groups. Is it that if the group wants to use a late day, all x members of the group must use their late day or is it that only one person of the group uses one late day. This question is only geared for regular homework assignments. I understand that there is a separate late day policy for the final group project. Thanks so much for the help!</p>",Late days for groups,"<p>This is a friendly reminder that <a href=""http://crowdsourcing-class.org/assignment1.html"" target=""_blank"">homework 1</a> is due tomorrow before class. åÊ</p>
<p></p>
<p>I would also like to draw your attention to the course&#39;s late day policy (given on the <a href=""http://crowdsourcing-class.org"" target=""_blank"">course web page</a>).</p>
<blockquote>
<p dir=""ltr"">Everyone can have 5 free late days without penalty. After you have used your free late days, you will lose 20% per day (or fraction thereof) that your assignment is submitted late. The final project will have its own late day policy.</p>
</blockquote>
<p>You do not need to ask permission to use a late day. åÊNote that I have a strict policy of not granting additional late days foråÊanyåÊreason, because I do not want to adjudicate what constitutes a fair excuse. åÊSo if you exhaust your 5 free late days early, please do not ask for more.</p>",Reminder: HW1 is due tomorrow by 2pm (and late day policy),<p>What if we need to use a late day for Friday?</p>,How do late days work for the final project?,<p>I just submitted my assignment for HW2. Does this count as two or three late days?</p>,Late days,Are we allowed to use late days on the first part of the final project (the homework due Friday)?,late days,4
940847387,4/26/2016 16:40:10,false,1969429578,,4/26/2016 16:37:31,false,neodev,0.7778,32569659,USA,MN,Minneapolis,97.127.88.224,"3
4",,<p></p>,Confirming: separate late day policy for the final group project?,"<p>Hello, I&#39;m a little confused as to how late days work for groups. Is it that if the group wants to use a late day, all x members of the group must use their late day or is it that only one person of the group uses one late day. This question is only geared for regular homework assignments. I understand that there is a separate late day policy for the final group project. Thanks so much for the help!</p>",Late days for groups,"<p>This is a friendly reminder that <a href=""http://crowdsourcing-class.org/assignment1.html"" target=""_blank"">homework 1</a> is due tomorrow before class. åÊ</p>
<p></p>
<p>I would also like to draw your attention to the course&#39;s late day policy (given on the <a href=""http://crowdsourcing-class.org"" target=""_blank"">course web page</a>).</p>
<blockquote>
<p dir=""ltr"">Everyone can have 5 free late days without penalty. After you have used your free late days, you will lose 20% per day (or fraction thereof) that your assignment is submitted late. The final project will have its own late day policy.</p>
</blockquote>
<p>You do not need to ask permission to use a late day. åÊNote that I have a strict policy of not granting additional late days foråÊanyåÊreason, because I do not want to adjudicate what constitutes a fair excuse. åÊSo if you exhaust your 5 free late days early, please do not ask for more.</p>",Reminder: HW1 is due tomorrow by 2pm (and late day policy),<p>What if we need to use a late day for Friday?</p>,How do late days work for the final project?,<p>I just submitted my assignment for HW2. Does this count as two or three late days?</p>,Late days,Are we allowed to use late days on the first part of the final project (the homework due Friday)?,late days,4
940847388,4/26/2016 17:41:58,false,1969466131,,4/26/2016 17:40:30,false,neodev,0.8889,33568303,VEN,23,Cabimas,190.77.7.36,0,,"<p>I&#39;ve already started my OH in the GRW bump space so feel free to come on by early if you&#39;d like. I&#39;ll be here till 5 unless it get&#39;s crowded in which case find me near the study space next to the elevators on the 5th floor of Levine.</p>
<p></p>",Starting OH early,"<p>Hey everyone,åÊI&#39;m moving my office hours tonight to Moore 100 instead of the bump space.</p>",OH moved to Moore,"<p>Hi everyone,åÊ</p>
<p></p>
<p>I&#39;m not feeling well, so I&#39;m canceling the office hours I was planning to hold today from 4-6pm. However, Sierra will still be holding her office hours at the same time. I&#39;ll try to hold make-up OH soon.åÊ</p>
<p></p>
<p>Best,</p>
<p>Kate</p>",OH Today,"<p>Hi all,</p>
<p></p>
<p>I have to cancel my office hours today from 12-2. If you needed help on somethingåÊandåÊwere planning to come, send me an email or a private message here and we can set up a time to talk either in person or over video chat today or this weekend.åÊ</p>
<p></p>
<p>Sorry for the inconvenience!</p>
<p>Ellie</p>",Canceling OH for today,Will there be any OH over the weekend since this assignment is due on Monday instead of the usual Friday?,OH over the weekend?,<p>Sorry everyone I&#39;m running a little late. Will start my OH at 3:15 and go till 5:15</p>,Pushing back OH by 15,1
940847388,4/26/2016 17:51:16,false,1969471398,,4/26/2016 17:50:22,false,clixsense,1.0,35444326,BRA,07,Brasília,177.15.130.106,0,,"<p>I&#39;ve already started my OH in the GRW bump space so feel free to come on by early if you&#39;d like. I&#39;ll be here till 5 unless it get&#39;s crowded in which case find me near the study space next to the elevators on the 5th floor of Levine.</p>
<p></p>",Starting OH early,"<p>Hey everyone,åÊI&#39;m moving my office hours tonight to Moore 100 instead of the bump space.</p>",OH moved to Moore,"<p>Hi everyone,åÊ</p>
<p></p>
<p>I&#39;m not feeling well, so I&#39;m canceling the office hours I was planning to hold today from 4-6pm. However, Sierra will still be holding her office hours at the same time. I&#39;ll try to hold make-up OH soon.åÊ</p>
<p></p>
<p>Best,</p>
<p>Kate</p>",OH Today,"<p>Hi all,</p>
<p></p>
<p>I have to cancel my office hours today from 12-2. If you needed help on somethingåÊandåÊwere planning to come, send me an email or a private message here and we can set up a time to talk either in person or over video chat today or this weekend.åÊ</p>
<p></p>
<p>Sorry for the inconvenience!</p>
<p>Ellie</p>",Canceling OH for today,Will there be any OH over the weekend since this assignment is due on Monday instead of the usual Friday?,OH over the weekend?,<p>Sorry everyone I&#39;m running a little late. Will start my OH at 3:15 and go till 5:15</p>,Pushing back OH by 15,1
940847388,4/26/2016 18:21:14,false,1969487255,,4/26/2016 18:20:20,false,neodev,0.8889,35550011,VEN,07,Valencia,190.204.238.112,0,,"<p>I&#39;ve already started my OH in the GRW bump space so feel free to come on by early if you&#39;d like. I&#39;ll be here till 5 unless it get&#39;s crowded in which case find me near the study space next to the elevators on the 5th floor of Levine.</p>
<p></p>",Starting OH early,"<p>Hey everyone,åÊI&#39;m moving my office hours tonight to Moore 100 instead of the bump space.</p>",OH moved to Moore,"<p>Hi everyone,åÊ</p>
<p></p>
<p>I&#39;m not feeling well, so I&#39;m canceling the office hours I was planning to hold today from 4-6pm. However, Sierra will still be holding her office hours at the same time. I&#39;ll try to hold make-up OH soon.åÊ</p>
<p></p>
<p>Best,</p>
<p>Kate</p>",OH Today,"<p>Hi all,</p>
<p></p>
<p>I have to cancel my office hours today from 12-2. If you needed help on somethingåÊandåÊwere planning to come, send me an email or a private message here and we can set up a time to talk either in person or over video chat today or this weekend.åÊ</p>
<p></p>
<p>Sorry for the inconvenience!</p>
<p>Ellie</p>",Canceling OH for today,Will there be any OH over the weekend since this assignment is due on Monday instead of the usual Friday?,OH over the weekend?,<p>Sorry everyone I&#39;m running a little late. Will start my OH at 3:15 and go till 5:15</p>,Pushing back OH by 15,1
940847388,4/26/2016 18:25:34,false,1969489415,,4/26/2016 18:24:15,false,elite,1.0,30128662,BGR,50,Pleven,212.233.177.195,0,,"<p>I&#39;ve already started my OH in the GRW bump space so feel free to come on by early if you&#39;d like. I&#39;ll be here till 5 unless it get&#39;s crowded in which case find me near the study space next to the elevators on the 5th floor of Levine.</p>
<p></p>",Starting OH early,"<p>Hey everyone,åÊI&#39;m moving my office hours tonight to Moore 100 instead of the bump space.</p>",OH moved to Moore,"<p>Hi everyone,åÊ</p>
<p></p>
<p>I&#39;m not feeling well, so I&#39;m canceling the office hours I was planning to hold today from 4-6pm. However, Sierra will still be holding her office hours at the same time. I&#39;ll try to hold make-up OH soon.åÊ</p>
<p></p>
<p>Best,</p>
<p>Kate</p>",OH Today,"<p>Hi all,</p>
<p></p>
<p>I have to cancel my office hours today from 12-2. If you needed help on somethingåÊandåÊwere planning to come, send me an email or a private message here and we can set up a time to talk either in person or over video chat today or this weekend.åÊ</p>
<p></p>
<p>Sorry for the inconvenience!</p>
<p>Ellie</p>",Canceling OH for today,Will there be any OH over the weekend since this assignment is due on Monday instead of the usual Friday?,OH over the weekend?,<p>Sorry everyone I&#39;m running a little late. Will start my OH at 3:15 and go till 5:15</p>,Pushing back OH by 15,1
940847388,4/26/2016 18:56:00,false,1969504145,,4/26/2016 18:55:19,false,neodev,1.0,29879245,RUS,69,Smolensk,37.144.124.118,0,,"<p>I&#39;ve already started my OH in the GRW bump space so feel free to come on by early if you&#39;d like. I&#39;ll be here till 5 unless it get&#39;s crowded in which case find me near the study space next to the elevators on the 5th floor of Levine.</p>
<p></p>",Starting OH early,"<p>Hey everyone,åÊI&#39;m moving my office hours tonight to Moore 100 instead of the bump space.</p>",OH moved to Moore,"<p>Hi everyone,åÊ</p>
<p></p>
<p>I&#39;m not feeling well, so I&#39;m canceling the office hours I was planning to hold today from 4-6pm. However, Sierra will still be holding her office hours at the same time. I&#39;ll try to hold make-up OH soon.åÊ</p>
<p></p>
<p>Best,</p>
<p>Kate</p>",OH Today,"<p>Hi all,</p>
<p></p>
<p>I have to cancel my office hours today from 12-2. If you needed help on somethingåÊandåÊwere planning to come, send me an email or a private message here and we can set up a time to talk either in person or over video chat today or this weekend.åÊ</p>
<p></p>
<p>Sorry for the inconvenience!</p>
<p>Ellie</p>",Canceling OH for today,Will there be any OH over the weekend since this assignment is due on Monday instead of the usual Friday?,OH over the weekend?,<p>Sorry everyone I&#39;m running a little late. Will start my OH at 3:15 and go till 5:15</p>,Pushing back OH by 15,1
940847389,4/26/2016 15:52:33,false,1969400267,,4/26/2016 15:50:49,false,elite,0.8889,36575101,IND,07,New Delhi,112.196.144.2,0,,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"We&#39;re just supposed to use &#34;print&#34; statements in our ipython file for this, right?",prints the answer to standard output,3
940847389,4/26/2016 15:53:45,false,1969400807,,4/26/2016 15:51:13,false,neodev,1.0,28875937,PAK,08,Islamabad,119.153.105.50,0,,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"We&#39;re just supposed to use &#34;print&#34; statements in our ipython file for this, right?",prints the answer to standard output,3
940847389,4/26/2016 15:56:15,false,1969401848,,4/26/2016 15:48:57,false,neodev,0.8889,21971187,TTO,08,Valsayn,190.213.132.190,0,,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"We&#39;re just supposed to use &#34;print&#34; statements in our ipython file for this, right?",prints the answer to standard output,3
940847389,4/26/2016 15:56:23,false,1969401929,,4/26/2016 15:53:18,false,neodev,1.0,13396426,VEN,15,Santa Teresa,190.38.163.149,0,,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"We&#39;re just supposed to use &#34;print&#34; statements in our ipython file for this, right?",prints the answer to standard output,3
940847389,4/26/2016 16:00:16,false,1969403682,,4/26/2016 15:56:11,false,clixsense,1.0,21875134,GBR,H9,London,87.112.158.81,0,,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Following the bash commands given on the hw page leaves us with a file of the form, 
1, articleurl
is this the format needed? There are no articles that were labeled as not gun related by our classifier in our sample.txt. Also do we need to have all of the articles we found classified on mturk or only 500?",CSV and 1&#39;s for crowdflower,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"We&#39;re just supposed to use &#34;print&#34; statements in our ipython file for this, right?",prints the answer to standard output,3
940847390,4/26/2016 15:54:11,false,1969401046,,4/26/2016 15:52:34,false,elite,0.8889,36575101,IND,07,New Delhi,112.196.144.2,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>When my program gets to the Statistical Analysis bit, it makes it through all the words and creates wordcount dictionaries that are appended to the list of features. Then it hangs for a bit until the terminal just prints &#34;Killed&#34; and program ends. Any idea what might be causing this? Not enough memory?</p>",Statistical Analysis &#34;Killed&#34;,2
940847390,4/26/2016 15:56:34,false,1969401963,,4/26/2016 15:53:47,false,neodev,1.0,28875937,PAK,08,Islamabad,119.153.105.50,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>When my program gets to the Statistical Analysis bit, it makes it through all the words and creates wordcount dictionaries that are appended to the list of features. Then it hangs for a bit until the terminal just prints &#34;Killed&#34; and program ends. Any idea what might be causing this? Not enough memory?</p>",Statistical Analysis &#34;Killed&#34;,2
940847390,4/26/2016 15:58:11,false,1969402631,,4/26/2016 15:56:24,false,neodev,1.0,13396426,VEN,15,Santa Teresa,190.38.163.149,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>When my program gets to the Statistical Analysis bit, it makes it through all the words and creates wordcount dictionaries that are appended to the list of features. Then it hangs for a bit until the terminal just prints &#34;Killed&#34; and program ends. Any idea what might be causing this? Not enough memory?</p>",Statistical Analysis &#34;Killed&#34;,2
940847390,4/26/2016 16:04:22,false,1969405946,,4/26/2016 15:56:16,false,neodev,0.8889,21971187,TTO,08,Valsayn,190.213.132.190,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>When my program gets to the Statistical Analysis bit, it makes it through all the words and creates wordcount dictionaries that are appended to the list of features. Then it hangs for a bit until the terminal just prints &#34;Killed&#34; and program ends. Any idea what might be causing this? Not enough memory?</p>",Statistical Analysis &#34;Killed&#34;,2
940847390,4/26/2016 16:19:52,false,1969415130,,4/26/2016 15:56:52,false,neodev,0.7778,32569659,USA,MN,Minneapolis,97.127.88.224,0,,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>When my program gets to the Statistical Analysis bit, it makes it through all the words and creates wordcount dictionaries that are appended to the list of features. Then it hangs for a bit until the terminal just prints &#34;Killed&#34; and program ends. Any idea what might be causing this? Not enough memory?</p>",Statistical Analysis &#34;Killed&#34;,2
940848369,4/26/2016 15:10:31,true,1969362385,,4/26/2016 15:08:05,false,tremorgames,1.0,32635967,LTU,60,Panevezys,78.63.38.165,0,0,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"I had about 3200 urls when I ran the get_clean_text.py on the biglab machines, but when I counted the number of lines of the file obtained after running BeautifulSoup, I only have 1379 lines (i.e articles and urls). Is this normal, or should I scrape the urls again? I believe a lot of urls&#39; articles didn&#39;t get scraped because of exception handling in get_clean_text.py",Number of articles and urls is significantly lesser than urls,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,1
940848369,4/26/2016 15:14:04,true,1969367417,,4/26/2016 15:11:16,false,clixsense,1.0,24287706,TWN,04,Keelung,61.231.195.173,0,0,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"I had about 3200 urls when I ran the get_clean_text.py on the biglab machines, but when I counted the number of lines of the file obtained after running BeautifulSoup, I only have 1379 lines (i.e articles and urls). Is this normal, or should I scrape the urls again? I believe a lot of urls&#39; articles didn&#39;t get scraped because of exception handling in get_clean_text.py",Number of articles and urls is significantly lesser than urls,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,1
940848369,4/26/2016 15:20:09,true,1969377308,,4/26/2016 15:19:55,false,neodev,1.0,19132694,LKA,36,Colombo,123.231.124.170,0,0,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"I had about 3200 urls when I ran the get_clean_text.py on the biglab machines, but when I counted the number of lines of the file obtained after running BeautifulSoup, I only have 1379 lines (i.e articles and urls). Is this normal, or should I scrape the urls again? I believe a lot of urls&#39; articles didn&#39;t get scraped because of exception handling in get_clean_text.py",Number of articles and urls is significantly lesser than urls,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,1
940848369,4/26/2016 15:24:23,true,1969384426,,4/26/2016 15:21:38,false,clixsense,1.0,7837812,SRB,00,Belgrade,79.101.254.233,0,0,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"I had about 3200 urls when I ran the get_clean_text.py on the biglab machines, but when I counted the number of lines of the file obtained after running BeautifulSoup, I only have 1379 lines (i.e articles and urls). Is this normal, or should I scrape the urls again? I believe a lot of urls&#39; articles didn&#39;t get scraped because of exception handling in get_clean_text.py",Number of articles and urls is significantly lesser than urls,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,1
940848369,4/26/2016 15:26:18,true,1969387709,,4/26/2016 15:21:33,false,clixsense,0.8889,36052512,PHL,F2,Quezon City,49.149.150.150,0,0,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"I had about 3200 urls when I ran the get_clean_text.py on the biglab machines, but when I counted the number of lines of the file obtained after running BeautifulSoup, I only have 1379 lines (i.e articles and urls). Is this normal, or should I scrape the urls again? I believe a lot of urls&#39; articles didn&#39;t get scraped because of exception handling in get_clean_text.py",Number of articles and urls is significantly lesser than urls,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,1
940848369,4/26/2016 15:27:29,true,1969389355,,4/26/2016 15:26:56,false,instagc,0.8889,13581319,USA,IL,Waltonville,208.70.36.12,0,0,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"I had about 3200 urls when I ran the get_clean_text.py on the biglab machines, but when I counted the number of lines of the file obtained after running BeautifulSoup, I only have 1379 lines (i.e articles and urls). Is this normal, or should I scrape the urls again? I believe a lot of urls&#39; articles didn&#39;t get scraped because of exception handling in get_clean_text.py",Number of articles and urls is significantly lesser than urls,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,1
940848369,4/26/2016 15:28:58,true,1969390422,,4/26/2016 15:27:17,false,elite,1.0,30280423,ITA,15,Siracusa,151.54.84.121,0,0,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"I had about 3200 urls when I ran the get_clean_text.py on the biglab machines, but when I counted the number of lines of the file obtained after running BeautifulSoup, I only have 1379 lines (i.e articles and urls). Is this normal, or should I scrape the urls again? I believe a lot of urls&#39; articles didn&#39;t get scraped because of exception handling in get_clean_text.py",Number of articles and urls is significantly lesser than urls,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,1
940848369,4/26/2016 15:39:56,true,1969394929,,4/26/2016 15:27:12,false,clixsense,1.0,21875134,GBR,H9,London,87.112.158.81,0,0,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"I had about 3200 urls when I ran the get_clean_text.py on the biglab machines, but when I counted the number of lines of the file obtained after running BeautifulSoup, I only have 1379 lines (i.e articles and urls). Is this normal, or should I scrape the urls again? I believe a lot of urls&#39; articles didn&#39;t get scraped because of exception handling in get_clean_text.py",Number of articles and urls is significantly lesser than urls,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,1
940848369,4/26/2016 15:53:17,true,1969400604,,4/26/2016 15:51:23,false,neodev,1.0,13396426,VEN,15,Santa Teresa,190.38.163.149,0,0,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"I had about 3200 urls when I ran the get_clean_text.py on the biglab machines, but when I counted the number of lines of the file obtained after running BeautifulSoup, I only have 1379 lines (i.e articles and urls). Is this normal, or should I scrape the urls again? I believe a lot of urls&#39; articles didn&#39;t get scraped because of exception handling in get_clean_text.py",Number of articles and urls is significantly lesser than urls,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,1
940848369,4/26/2016 15:54:11,true,1969401044,,4/26/2016 15:52:34,false,elite,0.8889,36575101,IND,07,New Delhi,112.196.144.2,0,0,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"I had about 3200 urls when I ran the get_clean_text.py on the biglab machines, but when I counted the number of lines of the file obtained after running BeautifulSoup, I only have 1379 lines (i.e articles and urls). Is this normal, or should I scrape the urls again? I believe a lot of urls&#39; articles didn&#39;t get scraped because of exception handling in get_clean_text.py",Number of articles and urls is significantly lesser than urls,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,1
940848369,4/26/2016 15:54:59,true,1969401303,,4/26/2016 15:53:17,false,elite,1.0,33243069,IND,10,Faridabad,116.203.79.150,0,0,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"I had about 3200 urls when I ran the get_clean_text.py on the biglab machines, but when I counted the number of lines of the file obtained after running BeautifulSoup, I only have 1379 lines (i.e articles and urls). Is this normal, or should I scrape the urls again? I believe a lot of urls&#39; articles didn&#39;t get scraped because of exception handling in get_clean_text.py",Number of articles and urls is significantly lesser than urls,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,1
940848369,4/26/2016 15:56:34,true,1969401960,,4/26/2016 15:53:47,false,neodev,1.0,28875937,PAK,08,Islamabad,119.153.105.50,0,0,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"I had about 3200 urls when I ran the get_clean_text.py on the biglab machines, but when I counted the number of lines of the file obtained after running BeautifulSoup, I only have 1379 lines (i.e articles and urls). Is this normal, or should I scrape the urls again? I believe a lot of urls&#39; articles didn&#39;t get scraped because of exception handling in get_clean_text.py",Number of articles and urls is significantly lesser than urls,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,1
940848369,4/26/2016 16:01:09,true,1969404225,,4/26/2016 16:00:06,false,personaly,1.0,33663352,ARG,01,Mar Del Plata,181.168.213.227,0,0,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"I had about 3200 urls when I ran the get_clean_text.py on the biglab machines, but when I counted the number of lines of the file obtained after running BeautifulSoup, I only have 1379 lines (i.e articles and urls). Is this normal, or should I scrape the urls again? I believe a lot of urls&#39; articles didn&#39;t get scraped because of exception handling in get_clean_text.py",Number of articles and urls is significantly lesser than urls,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,1
940848369,4/26/2016 16:03:35,true,1969405541,,4/26/2016 15:48:02,false,clixsense,0.8889,8057247,PRT,17,Póvoa De Varzim,144.64.25.68,0,0,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"I had about 3200 urls when I ran the get_clean_text.py on the biglab machines, but when I counted the number of lines of the file obtained after running BeautifulSoup, I only have 1379 lines (i.e articles and urls). Is this normal, or should I scrape the urls again? I believe a lot of urls&#39; articles didn&#39;t get scraped because of exception handling in get_clean_text.py",Number of articles and urls is significantly lesser than urls,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,1
940848369,4/26/2016 16:15:36,true,1969411451,,4/26/2016 16:12:15,false,elite,1.0,30128662,BGR,50,Pleven,212.233.177.195,0,0,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"I had about 3200 urls when I ran the get_clean_text.py on the biglab machines, but when I counted the number of lines of the file obtained after running BeautifulSoup, I only have 1379 lines (i.e articles and urls). Is this normal, or should I scrape the urls again? I believe a lot of urls&#39; articles didn&#39;t get scraped because of exception handling in get_clean_text.py",Number of articles and urls is significantly lesser than urls,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,1
940848369,4/26/2016 16:19:52,true,1969415134,,4/26/2016 15:56:52,false,neodev,0.7778,32569659,USA,MN,Minneapolis,97.127.88.224,0,0,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"I had about 3200 urls when I ran the get_clean_text.py on the biglab machines, but when I counted the number of lines of the file obtained after running BeautifulSoup, I only have 1379 lines (i.e articles and urls). Is this normal, or should I scrape the urls again? I believe a lot of urls&#39; articles didn&#39;t get scraped because of exception handling in get_clean_text.py",Number of articles and urls is significantly lesser than urls,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,1
940848369,4/26/2016 16:24:34,true,1969419057,,4/26/2016 16:23:47,false,neodev,1.0,29175140,VEN,25,Caracas,190.72.125.134,0,0,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"I had about 3200 urls when I ran the get_clean_text.py on the biglab machines, but when I counted the number of lines of the file obtained after running BeautifulSoup, I only have 1379 lines (i.e articles and urls). Is this normal, or should I scrape the urls again? I believe a lot of urls&#39; articles didn&#39;t get scraped because of exception handling in get_clean_text.py",Number of articles and urls is significantly lesser than urls,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,1
940848369,4/26/2016 16:24:40,true,1969419125,,4/26/2016 16:04:24,false,neodev,0.8889,21971187,TTO,08,Valsayn,190.213.132.190,0,0,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"I had about 3200 urls when I ran the get_clean_text.py on the biglab machines, but when I counted the number of lines of the file obtained after running BeautifulSoup, I only have 1379 lines (i.e articles and urls). Is this normal, or should I scrape the urls again? I believe a lot of urls&#39; articles didn&#39;t get scraped because of exception handling in get_clean_text.py",Number of articles and urls is significantly lesser than urls,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,1
940848369,4/26/2016 16:39:18,true,1969429042,,4/26/2016 16:30:39,false,clixsense,1.0,6329782,IDN,07,Bekonang,202.67.40.31,0,0,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"I had about 3200 urls when I ran the get_clean_text.py on the biglab machines, but when I counted the number of lines of the file obtained after running BeautifulSoup, I only have 1379 lines (i.e articles and urls). Is this normal, or should I scrape the urls again? I believe a lot of urls&#39; articles didn&#39;t get scraped because of exception handling in get_clean_text.py",Number of articles and urls is significantly lesser than urls,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,1
940848369,4/26/2016 16:40:52,true,1969429998,,4/26/2016 16:36:32,false,prodege,1.0,22387641,USA,PA,Jeannette,208.94.42.131,0,0,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"I had about 3200 urls when I ran the get_clean_text.py on the biglab machines, but when I counted the number of lines of the file obtained after running BeautifulSoup, I only have 1379 lines (i.e articles and urls). Is this normal, or should I scrape the urls again? I believe a lot of urls&#39; articles didn&#39;t get scraped because of exception handling in get_clean_text.py",Number of articles and urls is significantly lesser than urls,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,1
940848369,4/26/2016 17:02:13,true,1969443061,,4/26/2016 16:59:19,false,neodev,1.0,36167043,GBR,G6,Hull,77.86.101.69,0,0,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"I had about 3200 urls when I ran the get_clean_text.py on the biglab machines, but when I counted the number of lines of the file obtained after running BeautifulSoup, I only have 1379 lines (i.e articles and urls). Is this normal, or should I scrape the urls again? I believe a lot of urls&#39; articles didn&#39;t get scraped because of exception handling in get_clean_text.py",Number of articles and urls is significantly lesser than urls,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,1
940848369,4/26/2016 17:02:29,true,1969443187,,4/26/2016 17:01:24,false,clixsense,1.0,21408115,IDN,07,Semarang,36.79.23.180,0,0,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"I had about 3200 urls when I ran the get_clean_text.py on the biglab machines, but when I counted the number of lines of the file obtained after running BeautifulSoup, I only have 1379 lines (i.e articles and urls). Is this normal, or should I scrape the urls again? I believe a lot of urls&#39; articles didn&#39;t get scraped because of exception handling in get_clean_text.py",Number of articles and urls is significantly lesser than urls,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,1
940848369,4/26/2016 17:11:04,true,1969448325,true,4/26/2016 17:01:16,false,neodev,0.8889,19625264,DZA,41,Chlef,41.102.7.217,5,0,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"I had about 3200 urls when I ran the get_clean_text.py on the biglab machines, but when I counted the number of lines of the file obtained after running BeautifulSoup, I only have 1379 lines (i.e articles and urls). Is this normal, or should I scrape the urls again? I believe a lot of urls&#39; articles didn&#39;t get scraped because of exception handling in get_clean_text.py",Number of articles and urls is significantly lesser than urls,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,1
940848369,4/26/2016 17:14:00,true,1969450319,,4/26/2016 17:10:16,false,elite,1.0,25411289,HRV,"","",31.147.119.175,0,0,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"I had about 3200 urls when I ran the get_clean_text.py on the biglab machines, but when I counted the number of lines of the file obtained after running BeautifulSoup, I only have 1379 lines (i.e articles and urls). Is this normal, or should I scrape the urls again? I believe a lot of urls&#39; articles didn&#39;t get scraped because of exception handling in get_clean_text.py",Number of articles and urls is significantly lesser than urls,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,1
940848369,4/26/2016 17:15:16,true,1969451006,,4/26/2016 17:01:58,false,clixsense,0.8889,35338593,ITA,14,Cagliari,151.56.132.145,0,0,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"I had about 3200 urls when I ran the get_clean_text.py on the biglab machines, but when I counted the number of lines of the file obtained after running BeautifulSoup, I only have 1379 lines (i.e articles and urls). Is this normal, or should I scrape the urls again? I believe a lot of urls&#39; articles didn&#39;t get scraped because of exception handling in get_clean_text.py",Number of articles and urls is significantly lesser than urls,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,1
940848369,4/26/2016 17:16:52,true,1969451845,,4/26/2016 17:08:45,false,neodev,1.0,33973110,VEN,23,Maracaibo,186.94.238.104,0,0,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"I had about 3200 urls when I ran the get_clean_text.py on the biglab machines, but when I counted the number of lines of the file obtained after running BeautifulSoup, I only have 1379 lines (i.e articles and urls). Is this normal, or should I scrape the urls again? I believe a lot of urls&#39; articles didn&#39;t get scraped because of exception handling in get_clean_text.py",Number of articles and urls is significantly lesser than urls,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,1
940848369,4/26/2016 17:32:21,true,1969460820,,4/26/2016 17:30:36,false,neodev,1.0,11172894,IND,28,Champdani,117.194.5.117,0,0,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"I had about 3200 urls when I ran the get_clean_text.py on the biglab machines, but when I counted the number of lines of the file obtained after running BeautifulSoup, I only have 1379 lines (i.e articles and urls). Is this normal, or should I scrape the urls again? I believe a lot of urls&#39; articles didn&#39;t get scraped because of exception handling in get_clean_text.py",Number of articles and urls is significantly lesser than urls,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,1
940848369,4/26/2016 17:32:48,true,1969461085,,4/26/2016 17:32:36,false,neodev,0.8889,33131546,IDN,04,Jakarta,139.194.89.60,0,0,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"I had about 3200 urls when I ran the get_clean_text.py on the biglab machines, but when I counted the number of lines of the file obtained after running BeautifulSoup, I only have 1379 lines (i.e articles and urls). Is this normal, or should I scrape the urls again? I believe a lot of urls&#39; articles didn&#39;t get scraped because of exception handling in get_clean_text.py",Number of articles and urls is significantly lesser than urls,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,1
940848369,4/26/2016 17:34:24,true,1969461906,,4/26/2016 17:33:46,false,clixsense,1.0,30712378,ROU,21,Deva,79.119.241.200,0,0,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"I had about 3200 urls when I ran the get_clean_text.py on the biglab machines, but when I counted the number of lines of the file obtained after running BeautifulSoup, I only have 1379 lines (i.e articles and urls). Is this normal, or should I scrape the urls again? I believe a lot of urls&#39; articles didn&#39;t get scraped because of exception handling in get_clean_text.py",Number of articles and urls is significantly lesser than urls,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,1
940848369,4/26/2016 17:35:56,true,1969462691,,4/26/2016 17:30:54,false,neodev,0.8889,33568303,VEN,23,Cabimas,190.77.7.36,0,0,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"I had about 3200 urls when I ran the get_clean_text.py on the biglab machines, but when I counted the number of lines of the file obtained after running BeautifulSoup, I only have 1379 lines (i.e articles and urls). Is this normal, or should I scrape the urls again? I believe a lot of urls&#39; articles didn&#39;t get scraped because of exception handling in get_clean_text.py",Number of articles and urls is significantly lesser than urls,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,1
940848369,4/26/2016 17:45:14,true,1969468161,,4/26/2016 17:39:40,false,clixsense,1.0,35444326,BRA,07,Brasília,177.15.130.106,0,0,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"I had about 3200 urls when I ran the get_clean_text.py on the biglab machines, but when I counted the number of lines of the file obtained after running BeautifulSoup, I only have 1379 lines (i.e articles and urls). Is this normal, or should I scrape the urls again? I believe a lot of urls&#39; articles didn&#39;t get scraped because of exception handling in get_clean_text.py",Number of articles and urls is significantly lesser than urls,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,1
940848369,4/26/2016 18:16:16,true,1969484747,true,4/26/2016 18:07:59,false,neodev,0.8889,35550011,VEN,07,Valencia,190.204.238.112,5,0,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"I had about 3200 urls when I ran the get_clean_text.py on the biglab machines, but when I counted the number of lines of the file obtained after running BeautifulSoup, I only have 1379 lines (i.e articles and urls). Is this normal, or should I scrape the urls again? I believe a lot of urls&#39; articles didn&#39;t get scraped because of exception handling in get_clean_text.py",Number of articles and urls is significantly lesser than urls,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,1
940848369,4/26/2016 18:41:46,true,1969497899,,4/26/2016 18:39:26,false,neodev,1.0,29879245,RUS,69,Smolensk,37.144.124.118,0,0,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"I had about 3200 urls when I ran the get_clean_text.py on the biglab machines, but when I counted the number of lines of the file obtained after running BeautifulSoup, I only have 1379 lines (i.e articles and urls). Is this normal, or should I scrape the urls again? I believe a lot of urls&#39; articles didn&#39;t get scraped because of exception handling in get_clean_text.py",Number of articles and urls is significantly lesser than urls,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,1
940848369,4/26/2016 19:25:11,true,1969519071,,4/26/2016 19:21:41,false,tremorgames,1.0,25197223,HRV,15,Split,94.253.234.240,0,0,"<p>I&#39;m having trouble using xpath to get the URL, title, and date parsed out from the Bing API results. I know that using doc.xpath(&#34;//entry//url&#34;) will give me a list of items with the URLs, and doc.xpath(&#34;//entry//date&#34;) will similarly give me a list of items with the dates, but I&#39;d like to do both in the same loop (without having to iterate through two lists) ÛÓ is there a way I can do this?</p>
<p></p>
<p>Thanks!</p>","Bing API: url, title, and date",<p>Should we include these in our submission?</p>,Nocera URLs,"<p>What can I enter in the company URL section of the survey if my company is Silk Road? It doesn&#39;t really have a URL. Its .onion URL (the legitimate one at least) is hard to find.åÊ</p>
<p></p>
<p>Can I just leave this one blank?</p>",Nonexistent URL,<p>How many judgments per urlåÊshould we have for crowdflower on the low qualityåÊopen-ended HIT? Is 1 enough?</p>,Number of judgements per url,"I had about 3200 urls when I ran the get_clean_text.py on the biglab machines, but when I counted the number of lines of the file obtained after running BeautifulSoup, I only have 1379 lines (i.e articles and urls). Is this normal, or should I scrape the urls again? I believe a lot of urls&#39; articles didn&#39;t get scraped because of exception handling in get_clean_text.py",Number of articles and urls is significantly lesser than urls,"<p>The downloaded text file contains 100 urls, but the instructions say &#34;WeÛªve pulled together 400 of the urls your workers called ÛÏgun relatedÛ that you will use for this assignment.&#34; Are we supposed to have 400 or just the 100 given to us?</p>",Number of URLs,1
940848513,4/26/2016 15:10:31,true,1969362381,,4/26/2016 15:08:05,false,tremorgames,1.0,32635967,LTU,60,Panevezys,78.63.38.165,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p></p><pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p>When I run the code, I keep getting this error. I have tried looking at my API Key and seeing if I have an extra space in the API key field, but it looks perfectly fine to me.åÊ</p>",Error on running Bing API.py,"<p>I wanted to resubmit my project to include the .md file. But whenever I try to submit I get this error:</p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8n7u3ifw4mc/il5eikvnqd55/Screen_Shot_20160227_at_12.27.14_PM.png"" /></p>
<p></p>
<p>Is it ok if I email my directory to a TA? I don&#39;t really know how to fix this issue.</p>
<p></p>
<p>Best,</p>
<p>Ben</p>
<p></p>",Error on resubmission,3
940848513,4/26/2016 15:14:04,true,1969367421,,4/26/2016 15:11:16,false,clixsense,1.0,24287706,TWN,04,Keelung,61.231.195.173,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p></p><pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p>When I run the code, I keep getting this error. I have tried looking at my API Key and seeing if I have an extra space in the API key field, but it looks perfectly fine to me.åÊ</p>",Error on running Bing API.py,"<p>I wanted to resubmit my project to include the .md file. But whenever I try to submit I get this error:</p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8n7u3ifw4mc/il5eikvnqd55/Screen_Shot_20160227_at_12.27.14_PM.png"" /></p>
<p></p>
<p>Is it ok if I email my directory to a TA? I don&#39;t really know how to fix this issue.</p>
<p></p>
<p>Best,</p>
<p>Ben</p>
<p></p>",Error on resubmission,3
940848513,4/26/2016 15:17:20,true,1969372592,,4/26/2016 15:12:10,false,clixsense,1.0,7837812,SRB,00,Belgrade,79.101.254.233,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p></p><pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p>When I run the code, I keep getting this error. I have tried looking at my API Key and seeing if I have an extra space in the API key field, but it looks perfectly fine to me.åÊ</p>",Error on running Bing API.py,"<p>I wanted to resubmit my project to include the .md file. But whenever I try to submit I get this error:</p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8n7u3ifw4mc/il5eikvnqd55/Screen_Shot_20160227_at_12.27.14_PM.png"" /></p>
<p></p>
<p>Is it ok if I email my directory to a TA? I don&#39;t really know how to fix this issue.</p>
<p></p>
<p>Best,</p>
<p>Ben</p>
<p></p>",Error on resubmission,3
940848513,4/26/2016 15:19:29,true,1969376231,,4/26/2016 15:14:50,false,neodev,1.0,19132694,LKA,36,Colombo,123.231.124.170,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p></p><pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p>When I run the code, I keep getting this error. I have tried looking at my API Key and seeing if I have an extra space in the API key field, but it looks perfectly fine to me.åÊ</p>",Error on running Bing API.py,"<p>I wanted to resubmit my project to include the .md file. But whenever I try to submit I get this error:</p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8n7u3ifw4mc/il5eikvnqd55/Screen_Shot_20160227_at_12.27.14_PM.png"" /></p>
<p></p>
<p>Is it ok if I email my directory to a TA? I don&#39;t really know how to fix this issue.</p>
<p></p>
<p>Best,</p>
<p>Ben</p>
<p></p>",Error on resubmission,3
940848513,4/26/2016 15:21:31,true,1969379541,,4/26/2016 15:12:37,false,clixsense,0.8889,36052512,PHL,F2,Quezon City,49.149.150.150,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p></p><pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p>When I run the code, I keep getting this error. I have tried looking at my API Key and seeing if I have an extra space in the API key field, but it looks perfectly fine to me.åÊ</p>",Error on running Bing API.py,"<p>I wanted to resubmit my project to include the .md file. But whenever I try to submit I get this error:</p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8n7u3ifw4mc/il5eikvnqd55/Screen_Shot_20160227_at_12.27.14_PM.png"" /></p>
<p></p>
<p>Is it ok if I email my directory to a TA? I don&#39;t really know how to fix this issue.</p>
<p></p>
<p>Best,</p>
<p>Ben</p>
<p></p>",Error on resubmission,3
940848513,4/26/2016 15:26:03,true,1969387317,,4/26/2016 15:24:14,false,instagc,0.8889,13581319,USA,IL,Waltonville,208.70.36.12,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p></p><pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p>When I run the code, I keep getting this error. I have tried looking at my API Key and seeing if I have an extra space in the API key field, but it looks perfectly fine to me.åÊ</p>",Error on running Bing API.py,"<p>I wanted to resubmit my project to include the .md file. But whenever I try to submit I get this error:</p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8n7u3ifw4mc/il5eikvnqd55/Screen_Shot_20160227_at_12.27.14_PM.png"" /></p>
<p></p>
<p>Is it ok if I email my directory to a TA? I don&#39;t really know how to fix this issue.</p>
<p></p>
<p>Best,</p>
<p>Ben</p>
<p></p>",Error on resubmission,3
940848513,4/26/2016 15:30:54,true,1969391259,,4/26/2016 15:28:59,false,elite,1.0,30280423,ITA,15,Siracusa,151.54.84.121,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p></p><pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p>When I run the code, I keep getting this error. I have tried looking at my API Key and seeing if I have an extra space in the API key field, but it looks perfectly fine to me.åÊ</p>",Error on running Bing API.py,"<p>I wanted to resubmit my project to include the .md file. But whenever I try to submit I get this error:</p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8n7u3ifw4mc/il5eikvnqd55/Screen_Shot_20160227_at_12.27.14_PM.png"" /></p>
<p></p>
<p>Is it ok if I email my directory to a TA? I don&#39;t really know how to fix this issue.</p>
<p></p>
<p>Best,</p>
<p>Ben</p>
<p></p>",Error on resubmission,3
940848513,4/26/2016 15:39:56,true,1969394938,,4/26/2016 15:27:12,false,clixsense,1.0,21875134,GBR,H9,London,87.112.158.81,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p></p><pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p>When I run the code, I keep getting this error. I have tried looking at my API Key and seeing if I have an extra space in the API key field, but it looks perfectly fine to me.åÊ</p>",Error on running Bing API.py,"<p>I wanted to resubmit my project to include the .md file. But whenever I try to submit I get this error:</p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8n7u3ifw4mc/il5eikvnqd55/Screen_Shot_20160227_at_12.27.14_PM.png"" /></p>
<p></p>
<p>Is it ok if I email my directory to a TA? I don&#39;t really know how to fix this issue.</p>
<p></p>
<p>Best,</p>
<p>Ben</p>
<p></p>",Error on resubmission,3
940848513,4/26/2016 15:40:37,true,1969395290,,4/26/2016 15:34:58,false,neodev,0.7778,32569659,USA,MN,Minneapolis,97.127.88.224,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p></p><pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p>When I run the code, I keep getting this error. I have tried looking at my API Key and seeing if I have an extra space in the API key field, but it looks perfectly fine to me.åÊ</p>",Error on running Bing API.py,"<p>I wanted to resubmit my project to include the .md file. But whenever I try to submit I get this error:</p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8n7u3ifw4mc/il5eikvnqd55/Screen_Shot_20160227_at_12.27.14_PM.png"" /></p>
<p></p>
<p>Is it ok if I email my directory to a TA? I don&#39;t really know how to fix this issue.</p>
<p></p>
<p>Best,</p>
<p>Ben</p>
<p></p>",Error on resubmission,3
940848513,4/26/2016 15:45:33,true,1969397152,,4/26/2016 15:43:24,false,neodev,0.8889,21971187,TTO,08,Valsayn,190.213.132.190,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p></p><pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p>When I run the code, I keep getting this error. I have tried looking at my API Key and seeing if I have an extra space in the API key field, but it looks perfectly fine to me.åÊ</p>",Error on running Bing API.py,"<p>I wanted to resubmit my project to include the .md file. But whenever I try to submit I get this error:</p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8n7u3ifw4mc/il5eikvnqd55/Screen_Shot_20160227_at_12.27.14_PM.png"" /></p>
<p></p>
<p>Is it ok if I email my directory to a TA? I don&#39;t really know how to fix this issue.</p>
<p></p>
<p>Best,</p>
<p>Ben</p>
<p></p>",Error on resubmission,3
940848513,4/26/2016 15:47:54,true,1969398214,,4/26/2016 15:44:26,false,neodev,1.0,28875937,PAK,05,Karachi,182.180.125.133,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p></p><pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p>When I run the code, I keep getting this error. I have tried looking at my API Key and seeing if I have an extra space in the API key field, but it looks perfectly fine to me.åÊ</p>",Error on running Bing API.py,"<p>I wanted to resubmit my project to include the .md file. But whenever I try to submit I get this error:</p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8n7u3ifw4mc/il5eikvnqd55/Screen_Shot_20160227_at_12.27.14_PM.png"" /></p>
<p></p>
<p>Is it ok if I email my directory to a TA? I don&#39;t really know how to fix this issue.</p>
<p></p>
<p>Best,</p>
<p>Ben</p>
<p></p>",Error on resubmission,3
940848513,4/26/2016 15:52:33,true,1969400271,,4/26/2016 15:50:49,false,elite,0.8889,36575101,IND,07,New Delhi,112.196.144.2,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p></p><pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p>When I run the code, I keep getting this error. I have tried looking at my API Key and seeing if I have an extra space in the API key field, but it looks perfectly fine to me.åÊ</p>",Error on running Bing API.py,"<p>I wanted to resubmit my project to include the .md file. But whenever I try to submit I get this error:</p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8n7u3ifw4mc/il5eikvnqd55/Screen_Shot_20160227_at_12.27.14_PM.png"" /></p>
<p></p>
<p>Is it ok if I email my directory to a TA? I don&#39;t really know how to fix this issue.</p>
<p></p>
<p>Best,</p>
<p>Ben</p>
<p></p>",Error on resubmission,3
940848513,4/26/2016 15:54:59,true,1969401308,,4/26/2016 15:53:17,false,elite,1.0,33243069,IND,10,Faridabad,116.203.79.150,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p></p><pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p>When I run the code, I keep getting this error. I have tried looking at my API Key and seeing if I have an extra space in the API key field, but it looks perfectly fine to me.åÊ</p>",Error on running Bing API.py,"<p>I wanted to resubmit my project to include the .md file. But whenever I try to submit I get this error:</p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8n7u3ifw4mc/il5eikvnqd55/Screen_Shot_20160227_at_12.27.14_PM.png"" /></p>
<p></p>
<p>Is it ok if I email my directory to a TA? I don&#39;t really know how to fix this issue.</p>
<p></p>
<p>Best,</p>
<p>Ben</p>
<p></p>",Error on resubmission,3
940848513,4/26/2016 15:56:23,true,1969401928,,4/26/2016 15:53:18,false,neodev,1.0,13396426,VEN,15,Santa Teresa,190.38.163.149,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p></p><pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p>When I run the code, I keep getting this error. I have tried looking at my API Key and seeing if I have an extra space in the API key field, but it looks perfectly fine to me.åÊ</p>",Error on running Bing API.py,"<p>I wanted to resubmit my project to include the .md file. But whenever I try to submit I get this error:</p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8n7u3ifw4mc/il5eikvnqd55/Screen_Shot_20160227_at_12.27.14_PM.png"" /></p>
<p></p>
<p>Is it ok if I email my directory to a TA? I don&#39;t really know how to fix this issue.</p>
<p></p>
<p>Best,</p>
<p>Ben</p>
<p></p>",Error on resubmission,3
940848513,4/26/2016 16:01:09,true,1969404227,,4/26/2016 16:00:06,false,personaly,1.0,33663352,ARG,01,Mar Del Plata,181.168.213.227,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p></p><pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p>When I run the code, I keep getting this error. I have tried looking at my API Key and seeing if I have an extra space in the API key field, but it looks perfectly fine to me.åÊ</p>",Error on running Bing API.py,"<p>I wanted to resubmit my project to include the .md file. But whenever I try to submit I get this error:</p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8n7u3ifw4mc/il5eikvnqd55/Screen_Shot_20160227_at_12.27.14_PM.png"" /></p>
<p></p>
<p>Is it ok if I email my directory to a TA? I don&#39;t really know how to fix this issue.</p>
<p></p>
<p>Best,</p>
<p>Ben</p>
<p></p>",Error on resubmission,3
940848513,4/26/2016 16:09:03,true,1969408351,true,4/26/2016 16:03:36,false,clixsense,0.8889,8057247,PRT,17,Póvoa De Varzim,144.64.25.68,"1
2
3
4",0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p></p><pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p>When I run the code, I keep getting this error. I have tried looking at my API Key and seeing if I have an extra space in the API key field, but it looks perfectly fine to me.åÊ</p>",Error on running Bing API.py,"<p>I wanted to resubmit my project to include the .md file. But whenever I try to submit I get this error:</p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8n7u3ifw4mc/il5eikvnqd55/Screen_Shot_20160227_at_12.27.14_PM.png"" /></p>
<p></p>
<p>Is it ok if I email my directory to a TA? I don&#39;t really know how to fix this issue.</p>
<p></p>
<p>Best,</p>
<p>Ben</p>
<p></p>",Error on resubmission,3
940848513,4/26/2016 16:15:36,true,1969411462,,4/26/2016 16:12:15,false,elite,1.0,30128662,BGR,50,Pleven,212.233.177.195,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p></p><pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p>When I run the code, I keep getting this error. I have tried looking at my API Key and seeing if I have an extra space in the API key field, but it looks perfectly fine to me.åÊ</p>",Error on running Bing API.py,"<p>I wanted to resubmit my project to include the .md file. But whenever I try to submit I get this error:</p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8n7u3ifw4mc/il5eikvnqd55/Screen_Shot_20160227_at_12.27.14_PM.png"" /></p>
<p></p>
<p>Is it ok if I email my directory to a TA? I don&#39;t really know how to fix this issue.</p>
<p></p>
<p>Best,</p>
<p>Ben</p>
<p></p>",Error on resubmission,3
940848513,4/26/2016 16:22:17,true,1969417143,,4/26/2016 16:18:27,false,neodev,1.0,29175140,VEN,25,Caracas,190.72.125.134,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p></p><pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p>When I run the code, I keep getting this error. I have tried looking at my API Key and seeing if I have an extra space in the API key field, but it looks perfectly fine to me.åÊ</p>",Error on running Bing API.py,"<p>I wanted to resubmit my project to include the .md file. But whenever I try to submit I get this error:</p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8n7u3ifw4mc/il5eikvnqd55/Screen_Shot_20160227_at_12.27.14_PM.png"" /></p>
<p></p>
<p>Is it ok if I email my directory to a TA? I don&#39;t really know how to fix this issue.</p>
<p></p>
<p>Best,</p>
<p>Ben</p>
<p></p>",Error on resubmission,3
940848513,4/26/2016 16:40:52,true,1969430000,,4/26/2016 16:36:32,false,prodege,1.0,22387641,USA,PA,Jeannette,208.94.42.131,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p></p><pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p>When I run the code, I keep getting this error. I have tried looking at my API Key and seeing if I have an extra space in the API key field, but it looks perfectly fine to me.åÊ</p>",Error on running Bing API.py,"<p>I wanted to resubmit my project to include the .md file. But whenever I try to submit I get this error:</p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8n7u3ifw4mc/il5eikvnqd55/Screen_Shot_20160227_at_12.27.14_PM.png"" /></p>
<p></p>
<p>Is it ok if I email my directory to a TA? I don&#39;t really know how to fix this issue.</p>
<p></p>
<p>Best,</p>
<p>Ben</p>
<p></p>",Error on resubmission,3
940848513,4/26/2016 16:45:07,true,1969432482,,4/26/2016 16:44:04,false,clixsense,1.0,6329782,IDN,10,Sleman,202.67.40.222,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p></p><pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p>When I run the code, I keep getting this error. I have tried looking at my API Key and seeing if I have an extra space in the API key field, but it looks perfectly fine to me.åÊ</p>",Error on running Bing API.py,"<p>I wanted to resubmit my project to include the .md file. But whenever I try to submit I get this error:</p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8n7u3ifw4mc/il5eikvnqd55/Screen_Shot_20160227_at_12.27.14_PM.png"" /></p>
<p></p>
<p>Is it ok if I email my directory to a TA? I don&#39;t really know how to fix this issue.</p>
<p></p>
<p>Best,</p>
<p>Ben</p>
<p></p>",Error on resubmission,3
940848513,4/26/2016 16:56:40,true,1969439487,,4/26/2016 16:53:35,false,clixsense,1.0,21408115,IDN,07,Semarang,36.79.23.180,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p></p><pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p>When I run the code, I keep getting this error. I have tried looking at my API Key and seeing if I have an extra space in the API key field, but it looks perfectly fine to me.åÊ</p>",Error on running Bing API.py,"<p>I wanted to resubmit my project to include the .md file. But whenever I try to submit I get this error:</p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8n7u3ifw4mc/il5eikvnqd55/Screen_Shot_20160227_at_12.27.14_PM.png"" /></p>
<p></p>
<p>Is it ok if I email my directory to a TA? I don&#39;t really know how to fix this issue.</p>
<p></p>
<p>Best,</p>
<p>Ben</p>
<p></p>",Error on resubmission,3
940848513,4/26/2016 17:02:13,true,1969443065,,4/26/2016 16:59:19,false,neodev,1.0,36167043,GBR,G6,Hull,77.86.101.69,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p></p><pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p>When I run the code, I keep getting this error. I have tried looking at my API Key and seeing if I have an extra space in the API key field, but it looks perfectly fine to me.åÊ</p>",Error on running Bing API.py,"<p>I wanted to resubmit my project to include the .md file. But whenever I try to submit I get this error:</p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8n7u3ifw4mc/il5eikvnqd55/Screen_Shot_20160227_at_12.27.14_PM.png"" /></p>
<p></p>
<p>Is it ok if I email my directory to a TA? I don&#39;t really know how to fix this issue.</p>
<p></p>
<p>Best,</p>
<p>Ben</p>
<p></p>",Error on resubmission,3
940848513,4/26/2016 17:06:41,true,1969445566,,4/26/2016 17:00:39,false,neodev,1.0,33973110,VEN,23,Maracaibo,186.94.238.104,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p></p><pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p>When I run the code, I keep getting this error. I have tried looking at my API Key and seeing if I have an extra space in the API key field, but it looks perfectly fine to me.åÊ</p>",Error on running Bing API.py,"<p>I wanted to resubmit my project to include the .md file. But whenever I try to submit I get this error:</p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8n7u3ifw4mc/il5eikvnqd55/Screen_Shot_20160227_at_12.27.14_PM.png"" /></p>
<p></p>
<p>Is it ok if I email my directory to a TA? I don&#39;t really know how to fix this issue.</p>
<p></p>
<p>Best,</p>
<p>Ben</p>
<p></p>",Error on resubmission,3
940848513,4/26/2016 17:11:04,true,1969448313,,4/26/2016 17:01:16,false,neodev,0.8889,19625264,DZA,41,Chlef,41.102.7.217,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p></p><pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p>When I run the code, I keep getting this error. I have tried looking at my API Key and seeing if I have an extra space in the API key field, but it looks perfectly fine to me.åÊ</p>",Error on running Bing API.py,"<p>I wanted to resubmit my project to include the .md file. But whenever I try to submit I get this error:</p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8n7u3ifw4mc/il5eikvnqd55/Screen_Shot_20160227_at_12.27.14_PM.png"" /></p>
<p></p>
<p>Is it ok if I email my directory to a TA? I don&#39;t really know how to fix this issue.</p>
<p></p>
<p>Best,</p>
<p>Ben</p>
<p></p>",Error on resubmission,3
940848513,4/26/2016 17:14:00,true,1969450324,,4/26/2016 17:10:16,false,elite,1.0,25411289,HRV,"","",31.147.119.175,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p></p><pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p>When I run the code, I keep getting this error. I have tried looking at my API Key and seeing if I have an extra space in the API key field, but it looks perfectly fine to me.åÊ</p>",Error on running Bing API.py,"<p>I wanted to resubmit my project to include the .md file. But whenever I try to submit I get this error:</p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8n7u3ifw4mc/il5eikvnqd55/Screen_Shot_20160227_at_12.27.14_PM.png"" /></p>
<p></p>
<p>Is it ok if I email my directory to a TA? I don&#39;t really know how to fix this issue.</p>
<p></p>
<p>Best,</p>
<p>Ben</p>
<p></p>",Error on resubmission,3
940848513,4/26/2016 17:15:16,true,1969451000,,4/26/2016 17:01:58,false,clixsense,0.8889,35338593,ITA,14,Cagliari,151.56.132.145,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p></p><pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p>When I run the code, I keep getting this error. I have tried looking at my API Key and seeing if I have an extra space in the API key field, but it looks perfectly fine to me.åÊ</p>",Error on running Bing API.py,"<p>I wanted to resubmit my project to include the .md file. But whenever I try to submit I get this error:</p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8n7u3ifw4mc/il5eikvnqd55/Screen_Shot_20160227_at_12.27.14_PM.png"" /></p>
<p></p>
<p>Is it ok if I email my directory to a TA? I don&#39;t really know how to fix this issue.</p>
<p></p>
<p>Best,</p>
<p>Ben</p>
<p></p>",Error on resubmission,3
940848513,4/26/2016 17:31:21,true,1969460301,,4/26/2016 17:29:58,false,neodev,0.8889,33131546,IDN,04,Jakarta,139.194.89.60,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p></p><pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p>When I run the code, I keep getting this error. I have tried looking at my API Key and seeing if I have an extra space in the API key field, but it looks perfectly fine to me.åÊ</p>",Error on running Bing API.py,"<p>I wanted to resubmit my project to include the .md file. But whenever I try to submit I get this error:</p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8n7u3ifw4mc/il5eikvnqd55/Screen_Shot_20160227_at_12.27.14_PM.png"" /></p>
<p></p>
<p>Is it ok if I email my directory to a TA? I don&#39;t really know how to fix this issue.</p>
<p></p>
<p>Best,</p>
<p>Ben</p>
<p></p>",Error on resubmission,3
940848513,4/26/2016 17:32:21,true,1969460821,,4/26/2016 17:30:36,false,neodev,1.0,11172894,IND,28,Champdani,117.194.5.117,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p></p><pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p>When I run the code, I keep getting this error. I have tried looking at my API Key and seeing if I have an extra space in the API key field, but it looks perfectly fine to me.åÊ</p>",Error on running Bing API.py,"<p>I wanted to resubmit my project to include the .md file. But whenever I try to submit I get this error:</p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8n7u3ifw4mc/il5eikvnqd55/Screen_Shot_20160227_at_12.27.14_PM.png"" /></p>
<p></p>
<p>Is it ok if I email my directory to a TA? I don&#39;t really know how to fix this issue.</p>
<p></p>
<p>Best,</p>
<p>Ben</p>
<p></p>",Error on resubmission,3
940848513,4/26/2016 17:33:45,true,1969461569,,4/26/2016 17:33:17,false,clixsense,1.0,30712378,ROU,21,Deva,79.119.241.200,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p></p><pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p>When I run the code, I keep getting this error. I have tried looking at my API Key and seeing if I have an extra space in the API key field, but it looks perfectly fine to me.åÊ</p>",Error on running Bing API.py,"<p>I wanted to resubmit my project to include the .md file. But whenever I try to submit I get this error:</p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8n7u3ifw4mc/il5eikvnqd55/Screen_Shot_20160227_at_12.27.14_PM.png"" /></p>
<p></p>
<p>Is it ok if I email my directory to a TA? I don&#39;t really know how to fix this issue.</p>
<p></p>
<p>Best,</p>
<p>Ben</p>
<p></p>",Error on resubmission,3
940848513,4/26/2016 17:35:56,true,1969462696,,4/26/2016 17:30:54,false,neodev,0.8889,33568303,VEN,23,Cabimas,190.77.7.36,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p></p><pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p>When I run the code, I keep getting this error. I have tried looking at my API Key and seeing if I have an extra space in the API key field, but it looks perfectly fine to me.åÊ</p>",Error on running Bing API.py,"<p>I wanted to resubmit my project to include the .md file. But whenever I try to submit I get this error:</p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8n7u3ifw4mc/il5eikvnqd55/Screen_Shot_20160227_at_12.27.14_PM.png"" /></p>
<p></p>
<p>Is it ok if I email my directory to a TA? I don&#39;t really know how to fix this issue.</p>
<p></p>
<p>Best,</p>
<p>Ben</p>
<p></p>",Error on resubmission,3
940848513,4/26/2016 17:39:39,true,1969464644,,4/26/2016 17:35:23,false,clixsense,1.0,35444326,BRA,07,Brasília,177.15.130.106,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p></p><pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p>When I run the code, I keep getting this error. I have tried looking at my API Key and seeing if I have an extra space in the API key field, but it looks perfectly fine to me.åÊ</p>",Error on running Bing API.py,"<p>I wanted to resubmit my project to include the .md file. But whenever I try to submit I get this error:</p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8n7u3ifw4mc/il5eikvnqd55/Screen_Shot_20160227_at_12.27.14_PM.png"" /></p>
<p></p>
<p>Is it ok if I email my directory to a TA? I don&#39;t really know how to fix this issue.</p>
<p></p>
<p>Best,</p>
<p>Ben</p>
<p></p>",Error on resubmission,3
940848513,4/26/2016 18:21:14,true,1969487251,,4/26/2016 18:20:20,false,neodev,0.8889,35550011,VEN,07,Valencia,190.204.238.112,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p></p><pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p>When I run the code, I keep getting this error. I have tried looking at my API Key and seeing if I have an extra space in the API key field, but it looks perfectly fine to me.åÊ</p>",Error on running Bing API.py,"<p>I wanted to resubmit my project to include the .md file. But whenever I try to submit I get this error:</p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8n7u3ifw4mc/il5eikvnqd55/Screen_Shot_20160227_at_12.27.14_PM.png"" /></p>
<p></p>
<p>Is it ok if I email my directory to a TA? I don&#39;t really know how to fix this issue.</p>
<p></p>
<p>Best,</p>
<p>Ben</p>
<p></p>",Error on resubmission,3
940848513,4/26/2016 18:33:38,true,1969493275,,4/26/2016 18:19:15,false,neodev,1.0,29879245,RUS,69,Smolensk,37.144.124.118,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p></p><pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p>When I run the code, I keep getting this error. I have tried looking at my API Key and seeing if I have an extra space in the API key field, but it looks perfectly fine to me.åÊ</p>",Error on running Bing API.py,"<p>I wanted to resubmit my project to include the .md file. But whenever I try to submit I get this error:</p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8n7u3ifw4mc/il5eikvnqd55/Screen_Shot_20160227_at_12.27.14_PM.png"" /></p>
<p></p>
<p>Is it ok if I email my directory to a TA? I don&#39;t really know how to fix this issue.</p>
<p></p>
<p>Best,</p>
<p>Ben</p>
<p></p>",Error on resubmission,3
940848513,4/26/2016 18:41:04,true,1969497528,,4/26/2016 18:31:56,false,neodev,1.0,35974955,VEN,17,Porlamar,190.198.232.239,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p></p><pre>Querying Bing (iteration 0 out of 10)
Traceback (most recent call last):
  File &#34;bing_api.py&#34;, line 49, in &lt;module&gt;
    get_urls(&#34;shooting&#34;)
  File &#34;bing_api.py&#34;, line 36, in get_urls
    response = requestOpener.open(request).read()
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 410, in open
    response = meth(req, response)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 523, in http_response
    &#39;http&#39;, request, response, code, msg, hdrs)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 448, in error
    return self._call_chain(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 531, in http_error_default
    raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)
urllib2.HTTPError: HTTP Error 401: The authorization type you provided is not supported.  Only Basic and OAuth are supported</pre>
<p>When I run the code, I keep getting this error. I have tried looking at my API Key and seeing if I have an extra space in the API key field, but it looks perfectly fine to me.åÊ</p>",Error on running Bing API.py,"<p>I wanted to resubmit my project to include the .md file. But whenever I try to submit I get this error:</p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwl8n7u3ifw4mc/il5eikvnqd55/Screen_Shot_20160227_at_12.27.14_PM.png"" /></p>
<p></p>
<p>Is it ok if I email my directory to a TA? I don&#39;t really know how to fix this issue.</p>
<p></p>
<p>Best,</p>
<p>Ben</p>
<p></p>",Error on resubmission,3
940848593,4/26/2016 15:10:31,true,1969362378,,4/26/2016 15:08:05,false,tremorgames,1.0,32635967,LTU,60,Panevezys,78.63.38.165,0,0,"<p>I&#39;ve already started my OH in the GRW bump space so feel free to come on by early if you&#39;d like. I&#39;ll be here till 5 unless it get&#39;s crowded in which case find me near the study space next to the elevators on the 5th floor of Levine.</p>
<p></p>",Starting OH early,"<p>Hey everyone,åÊI&#39;m moving my office hours tonight to Moore 100 instead of the bump space.</p>",OH moved to Moore,"<p>Hi everyone,åÊ</p>
<p></p>
<p>I&#39;m not feeling well, so I&#39;m canceling the office hours I was planning to hold today from 4-6pm. However, Sierra will still be holding her office hours at the same time. I&#39;ll try to hold make-up OH soon.åÊ</p>
<p></p>
<p>Best,</p>
<p>Kate</p>",OH Today,"<p>Hi all,</p>
<p></p>
<p>I have to cancel my office hours today from 12-2. If you needed help on somethingåÊandåÊwere planning to come, send me an email or a private message here and we can set up a time to talk either in person or over video chat today or this weekend.åÊ</p>
<p></p>
<p>Sorry for the inconvenience!</p>
<p>Ellie</p>",Canceling OH for today,Will there be any OH over the weekend since this assignment is due on Monday instead of the usual Friday?,OH over the weekend?,<p>Sorry everyone I&#39;m running a little late. Will start my OH at 3:15 and go till 5:15</p>,Pushing back OH by 15,1
940848593,4/26/2016 15:14:04,true,1969367412,,4/26/2016 15:11:16,false,clixsense,1.0,24287706,TWN,04,Keelung,61.231.195.173,0,0,"<p>I&#39;ve already started my OH in the GRW bump space so feel free to come on by early if you&#39;d like. I&#39;ll be here till 5 unless it get&#39;s crowded in which case find me near the study space next to the elevators on the 5th floor of Levine.</p>
<p></p>",Starting OH early,"<p>Hey everyone,åÊI&#39;m moving my office hours tonight to Moore 100 instead of the bump space.</p>",OH moved to Moore,"<p>Hi everyone,åÊ</p>
<p></p>
<p>I&#39;m not feeling well, so I&#39;m canceling the office hours I was planning to hold today from 4-6pm. However, Sierra will still be holding her office hours at the same time. I&#39;ll try to hold make-up OH soon.åÊ</p>
<p></p>
<p>Best,</p>
<p>Kate</p>",OH Today,"<p>Hi all,</p>
<p></p>
<p>I have to cancel my office hours today from 12-2. If you needed help on somethingåÊandåÊwere planning to come, send me an email or a private message here and we can set up a time to talk either in person or over video chat today or this weekend.åÊ</p>
<p></p>
<p>Sorry for the inconvenience!</p>
<p>Ellie</p>",Canceling OH for today,Will there be any OH over the weekend since this assignment is due on Monday instead of the usual Friday?,OH over the weekend?,<p>Sorry everyone I&#39;m running a little late. Will start my OH at 3:15 and go till 5:15</p>,Pushing back OH by 15,1
940848593,4/26/2016 15:19:29,true,1969376227,,4/26/2016 15:14:50,false,neodev,1.0,19132694,LKA,36,Colombo,123.231.124.170,0,0,"<p>I&#39;ve already started my OH in the GRW bump space so feel free to come on by early if you&#39;d like. I&#39;ll be here till 5 unless it get&#39;s crowded in which case find me near the study space next to the elevators on the 5th floor of Levine.</p>
<p></p>",Starting OH early,"<p>Hey everyone,åÊI&#39;m moving my office hours tonight to Moore 100 instead of the bump space.</p>",OH moved to Moore,"<p>Hi everyone,åÊ</p>
<p></p>
<p>I&#39;m not feeling well, so I&#39;m canceling the office hours I was planning to hold today from 4-6pm. However, Sierra will still be holding her office hours at the same time. I&#39;ll try to hold make-up OH soon.åÊ</p>
<p></p>
<p>Best,</p>
<p>Kate</p>",OH Today,"<p>Hi all,</p>
<p></p>
<p>I have to cancel my office hours today from 12-2. If you needed help on somethingåÊandåÊwere planning to come, send me an email or a private message here and we can set up a time to talk either in person or over video chat today or this weekend.åÊ</p>
<p></p>
<p>Sorry for the inconvenience!</p>
<p>Ellie</p>",Canceling OH for today,Will there be any OH over the weekend since this assignment is due on Monday instead of the usual Friday?,OH over the weekend?,<p>Sorry everyone I&#39;m running a little late. Will start my OH at 3:15 and go till 5:15</p>,Pushing back OH by 15,1
940848593,4/26/2016 15:19:31,true,1969376278,,4/26/2016 15:17:21,false,clixsense,1.0,7837812,SRB,00,Belgrade,79.101.254.233,0,0,"<p>I&#39;ve already started my OH in the GRW bump space so feel free to come on by early if you&#39;d like. I&#39;ll be here till 5 unless it get&#39;s crowded in which case find me near the study space next to the elevators on the 5th floor of Levine.</p>
<p></p>",Starting OH early,"<p>Hey everyone,åÊI&#39;m moving my office hours tonight to Moore 100 instead of the bump space.</p>",OH moved to Moore,"<p>Hi everyone,åÊ</p>
<p></p>
<p>I&#39;m not feeling well, so I&#39;m canceling the office hours I was planning to hold today from 4-6pm. However, Sierra will still be holding her office hours at the same time. I&#39;ll try to hold make-up OH soon.åÊ</p>
<p></p>
<p>Best,</p>
<p>Kate</p>",OH Today,"<p>Hi all,</p>
<p></p>
<p>I have to cancel my office hours today from 12-2. If you needed help on somethingåÊandåÊwere planning to come, send me an email or a private message here and we can set up a time to talk either in person or over video chat today or this weekend.åÊ</p>
<p></p>
<p>Sorry for the inconvenience!</p>
<p>Ellie</p>",Canceling OH for today,Will there be any OH over the weekend since this assignment is due on Monday instead of the usual Friday?,OH over the weekend?,<p>Sorry everyone I&#39;m running a little late. Will start my OH at 3:15 and go till 5:15</p>,Pushing back OH by 15,1
940848593,4/26/2016 15:21:05,true,1969378826,,4/26/2016 15:15:48,false,elite,1.0,30280423,ITA,15,Siracusa,151.54.84.121,0,0,"<p>I&#39;ve already started my OH in the GRW bump space so feel free to come on by early if you&#39;d like. I&#39;ll be here till 5 unless it get&#39;s crowded in which case find me near the study space next to the elevators on the 5th floor of Levine.</p>
<p></p>",Starting OH early,"<p>Hey everyone,åÊI&#39;m moving my office hours tonight to Moore 100 instead of the bump space.</p>",OH moved to Moore,"<p>Hi everyone,åÊ</p>
<p></p>
<p>I&#39;m not feeling well, so I&#39;m canceling the office hours I was planning to hold today from 4-6pm. However, Sierra will still be holding her office hours at the same time. I&#39;ll try to hold make-up OH soon.åÊ</p>
<p></p>
<p>Best,</p>
<p>Kate</p>",OH Today,"<p>Hi all,</p>
<p></p>
<p>I have to cancel my office hours today from 12-2. If you needed help on somethingåÊandåÊwere planning to come, send me an email or a private message here and we can set up a time to talk either in person or over video chat today or this weekend.åÊ</p>
<p></p>
<p>Sorry for the inconvenience!</p>
<p>Ellie</p>",Canceling OH for today,Will there be any OH over the weekend since this assignment is due on Monday instead of the usual Friday?,OH over the weekend?,<p>Sorry everyone I&#39;m running a little late. Will start my OH at 3:15 and go till 5:15</p>,Pushing back OH by 15,1
940848593,4/26/2016 15:26:03,true,1969387322,,4/26/2016 15:24:14,false,instagc,0.8889,13581319,USA,IL,Waltonville,208.70.36.12,0,0,"<p>I&#39;ve already started my OH in the GRW bump space so feel free to come on by early if you&#39;d like. I&#39;ll be here till 5 unless it get&#39;s crowded in which case find me near the study space next to the elevators on the 5th floor of Levine.</p>
<p></p>",Starting OH early,"<p>Hey everyone,åÊI&#39;m moving my office hours tonight to Moore 100 instead of the bump space.</p>",OH moved to Moore,"<p>Hi everyone,åÊ</p>
<p></p>
<p>I&#39;m not feeling well, so I&#39;m canceling the office hours I was planning to hold today from 4-6pm. However, Sierra will still be holding her office hours at the same time. I&#39;ll try to hold make-up OH soon.åÊ</p>
<p></p>
<p>Best,</p>
<p>Kate</p>",OH Today,"<p>Hi all,</p>
<p></p>
<p>I have to cancel my office hours today from 12-2. If you needed help on somethingåÊandåÊwere planning to come, send me an email or a private message here and we can set up a time to talk either in person or over video chat today or this weekend.åÊ</p>
<p></p>
<p>Sorry for the inconvenience!</p>
<p>Ellie</p>",Canceling OH for today,Will there be any OH over the weekend since this assignment is due on Monday instead of the usual Friday?,OH over the weekend?,<p>Sorry everyone I&#39;m running a little late. Will start my OH at 3:15 and go till 5:15</p>,Pushing back OH by 15,1
940848593,4/26/2016 15:35:58,true,1969392975,true,4/26/2016 15:30:06,false,clixsense,0.8889,36052512,PHL,F2,Quezon City,49.149.150.150,1,0,"<p>I&#39;ve already started my OH in the GRW bump space so feel free to come on by early if you&#39;d like. I&#39;ll be here till 5 unless it get&#39;s crowded in which case find me near the study space next to the elevators on the 5th floor of Levine.</p>
<p></p>",Starting OH early,"<p>Hey everyone,åÊI&#39;m moving my office hours tonight to Moore 100 instead of the bump space.</p>",OH moved to Moore,"<p>Hi everyone,åÊ</p>
<p></p>
<p>I&#39;m not feeling well, so I&#39;m canceling the office hours I was planning to hold today from 4-6pm. However, Sierra will still be holding her office hours at the same time. I&#39;ll try to hold make-up OH soon.åÊ</p>
<p></p>
<p>Best,</p>
<p>Kate</p>",OH Today,"<p>Hi all,</p>
<p></p>
<p>I have to cancel my office hours today from 12-2. If you needed help on somethingåÊandåÊwere planning to come, send me an email or a private message here and we can set up a time to talk either in person or over video chat today or this weekend.åÊ</p>
<p></p>
<p>Sorry for the inconvenience!</p>
<p>Ellie</p>",Canceling OH for today,Will there be any OH over the weekend since this assignment is due on Monday instead of the usual Friday?,OH over the weekend?,<p>Sorry everyone I&#39;m running a little late. Will start my OH at 3:15 and go till 5:15</p>,Pushing back OH by 15,1
940848593,4/26/2016 15:44:24,true,1969396767,,4/26/2016 15:31:20,false,neodev,1.0,28875937,PAK,04,Lahore,119.153.154.137,0,0,"<p>I&#39;ve already started my OH in the GRW bump space so feel free to come on by early if you&#39;d like. I&#39;ll be here till 5 unless it get&#39;s crowded in which case find me near the study space next to the elevators on the 5th floor of Levine.</p>
<p></p>",Starting OH early,"<p>Hey everyone,åÊI&#39;m moving my office hours tonight to Moore 100 instead of the bump space.</p>",OH moved to Moore,"<p>Hi everyone,åÊ</p>
<p></p>
<p>I&#39;m not feeling well, so I&#39;m canceling the office hours I was planning to hold today from 4-6pm. However, Sierra will still be holding her office hours at the same time. I&#39;ll try to hold make-up OH soon.åÊ</p>
<p></p>
<p>Best,</p>
<p>Kate</p>",OH Today,"<p>Hi all,</p>
<p></p>
<p>I have to cancel my office hours today from 12-2. If you needed help on somethingåÊandåÊwere planning to come, send me an email or a private message here and we can set up a time to talk either in person or over video chat today or this weekend.åÊ</p>
<p></p>
<p>Sorry for the inconvenience!</p>
<p>Ellie</p>",Canceling OH for today,Will there be any OH over the weekend since this assignment is due on Monday instead of the usual Friday?,OH over the weekend?,<p>Sorry everyone I&#39;m running a little late. Will start my OH at 3:15 and go till 5:15</p>,Pushing back OH by 15,1
940848593,4/26/2016 15:45:33,true,1969397158,true,4/26/2016 15:43:24,false,neodev,0.8889,21971187,TTO,08,Valsayn,190.213.132.190,1,0,"<p>I&#39;ve already started my OH in the GRW bump space so feel free to come on by early if you&#39;d like. I&#39;ll be here till 5 unless it get&#39;s crowded in which case find me near the study space next to the elevators on the 5th floor of Levine.</p>
<p></p>",Starting OH early,"<p>Hey everyone,åÊI&#39;m moving my office hours tonight to Moore 100 instead of the bump space.</p>",OH moved to Moore,"<p>Hi everyone,åÊ</p>
<p></p>
<p>I&#39;m not feeling well, so I&#39;m canceling the office hours I was planning to hold today from 4-6pm. However, Sierra will still be holding her office hours at the same time. I&#39;ll try to hold make-up OH soon.åÊ</p>
<p></p>
<p>Best,</p>
<p>Kate</p>",OH Today,"<p>Hi all,</p>
<p></p>
<p>I have to cancel my office hours today from 12-2. If you needed help on somethingåÊandåÊwere planning to come, send me an email or a private message here and we can set up a time to talk either in person or over video chat today or this weekend.åÊ</p>
<p></p>
<p>Sorry for the inconvenience!</p>
<p>Ellie</p>",Canceling OH for today,Will there be any OH over the weekend since this assignment is due on Monday instead of the usual Friday?,OH over the weekend?,<p>Sorry everyone I&#39;m running a little late. Will start my OH at 3:15 and go till 5:15</p>,Pushing back OH by 15,1
940848593,4/26/2016 15:45:34,true,1969397181,true,4/26/2016 15:37:11,false,elite,0.8889,36575101,IND,07,New Delhi,112.196.144.2,1,0,"<p>I&#39;ve already started my OH in the GRW bump space so feel free to come on by early if you&#39;d like. I&#39;ll be here till 5 unless it get&#39;s crowded in which case find me near the study space next to the elevators on the 5th floor of Levine.</p>
<p></p>",Starting OH early,"<p>Hey everyone,åÊI&#39;m moving my office hours tonight to Moore 100 instead of the bump space.</p>",OH moved to Moore,"<p>Hi everyone,åÊ</p>
<p></p>
<p>I&#39;m not feeling well, so I&#39;m canceling the office hours I was planning to hold today from 4-6pm. However, Sierra will still be holding her office hours at the same time. I&#39;ll try to hold make-up OH soon.åÊ</p>
<p></p>
<p>Best,</p>
<p>Kate</p>",OH Today,"<p>Hi all,</p>
<p></p>
<p>I have to cancel my office hours today from 12-2. If you needed help on somethingåÊandåÊwere planning to come, send me an email or a private message here and we can set up a time to talk either in person or over video chat today or this weekend.åÊ</p>
<p></p>
<p>Sorry for the inconvenience!</p>
<p>Ellie</p>",Canceling OH for today,Will there be any OH over the weekend since this assignment is due on Monday instead of the usual Friday?,OH over the weekend?,<p>Sorry everyone I&#39;m running a little late. Will start my OH at 3:15 and go till 5:15</p>,Pushing back OH by 15,1
940848593,4/26/2016 15:51:22,true,1969399816,,4/26/2016 15:36:03,false,neodev,1.0,13396426,VEN,15,Santa Teresa,190.38.163.149,0,0,"<p>I&#39;ve already started my OH in the GRW bump space so feel free to come on by early if you&#39;d like. I&#39;ll be here till 5 unless it get&#39;s crowded in which case find me near the study space next to the elevators on the 5th floor of Levine.</p>
<p></p>",Starting OH early,"<p>Hey everyone,åÊI&#39;m moving my office hours tonight to Moore 100 instead of the bump space.</p>",OH moved to Moore,"<p>Hi everyone,åÊ</p>
<p></p>
<p>I&#39;m not feeling well, so I&#39;m canceling the office hours I was planning to hold today from 4-6pm. However, Sierra will still be holding her office hours at the same time. I&#39;ll try to hold make-up OH soon.åÊ</p>
<p></p>
<p>Best,</p>
<p>Kate</p>",OH Today,"<p>Hi all,</p>
<p></p>
<p>I have to cancel my office hours today from 12-2. If you needed help on somethingåÊandåÊwere planning to come, send me an email or a private message here and we can set up a time to talk either in person or over video chat today or this weekend.åÊ</p>
<p></p>
<p>Sorry for the inconvenience!</p>
<p>Ellie</p>",Canceling OH for today,Will there be any OH over the weekend since this assignment is due on Monday instead of the usual Friday?,OH over the weekend?,<p>Sorry everyone I&#39;m running a little late. Will start my OH at 3:15 and go till 5:15</p>,Pushing back OH by 15,1
940848593,4/26/2016 15:54:59,true,1969401304,,4/26/2016 15:53:17,false,elite,1.0,33243069,IND,10,Faridabad,116.203.79.150,0,0,"<p>I&#39;ve already started my OH in the GRW bump space so feel free to come on by early if you&#39;d like. I&#39;ll be here till 5 unless it get&#39;s crowded in which case find me near the study space next to the elevators on the 5th floor of Levine.</p>
<p></p>",Starting OH early,"<p>Hey everyone,åÊI&#39;m moving my office hours tonight to Moore 100 instead of the bump space.</p>",OH moved to Moore,"<p>Hi everyone,åÊ</p>
<p></p>
<p>I&#39;m not feeling well, so I&#39;m canceling the office hours I was planning to hold today from 4-6pm. However, Sierra will still be holding her office hours at the same time. I&#39;ll try to hold make-up OH soon.åÊ</p>
<p></p>
<p>Best,</p>
<p>Kate</p>",OH Today,"<p>Hi all,</p>
<p></p>
<p>I have to cancel my office hours today from 12-2. If you needed help on somethingåÊandåÊwere planning to come, send me an email or a private message here and we can set up a time to talk either in person or over video chat today or this weekend.åÊ</p>
<p></p>
<p>Sorry for the inconvenience!</p>
<p>Ellie</p>",Canceling OH for today,Will there be any OH over the weekend since this assignment is due on Monday instead of the usual Friday?,OH over the weekend?,<p>Sorry everyone I&#39;m running a little late. Will start my OH at 3:15 and go till 5:15</p>,Pushing back OH by 15,1
940848593,4/26/2016 16:00:16,true,1969403678,,4/26/2016 15:56:11,false,clixsense,1.0,21875134,GBR,H9,London,87.112.158.81,0,0,"<p>I&#39;ve already started my OH in the GRW bump space so feel free to come on by early if you&#39;d like. I&#39;ll be here till 5 unless it get&#39;s crowded in which case find me near the study space next to the elevators on the 5th floor of Levine.</p>
<p></p>",Starting OH early,"<p>Hey everyone,åÊI&#39;m moving my office hours tonight to Moore 100 instead of the bump space.</p>",OH moved to Moore,"<p>Hi everyone,åÊ</p>
<p></p>
<p>I&#39;m not feeling well, so I&#39;m canceling the office hours I was planning to hold today from 4-6pm. However, Sierra will still be holding her office hours at the same time. I&#39;ll try to hold make-up OH soon.åÊ</p>
<p></p>
<p>Best,</p>
<p>Kate</p>",OH Today,"<p>Hi all,</p>
<p></p>
<p>I have to cancel my office hours today from 12-2. If you needed help on somethingåÊandåÊwere planning to come, send me an email or a private message here and we can set up a time to talk either in person or over video chat today or this weekend.åÊ</p>
<p></p>
<p>Sorry for the inconvenience!</p>
<p>Ellie</p>",Canceling OH for today,Will there be any OH over the weekend since this assignment is due on Monday instead of the usual Friday?,OH over the weekend?,<p>Sorry everyone I&#39;m running a little late. Will start my OH at 3:15 and go till 5:15</p>,Pushing back OH by 15,1
940848593,4/26/2016 16:01:09,true,1969404226,,4/26/2016 16:00:06,false,personaly,1.0,33663352,ARG,01,Mar Del Plata,181.168.213.227,0,0,"<p>I&#39;ve already started my OH in the GRW bump space so feel free to come on by early if you&#39;d like. I&#39;ll be here till 5 unless it get&#39;s crowded in which case find me near the study space next to the elevators on the 5th floor of Levine.</p>
<p></p>",Starting OH early,"<p>Hey everyone,åÊI&#39;m moving my office hours tonight to Moore 100 instead of the bump space.</p>",OH moved to Moore,"<p>Hi everyone,åÊ</p>
<p></p>
<p>I&#39;m not feeling well, so I&#39;m canceling the office hours I was planning to hold today from 4-6pm. However, Sierra will still be holding her office hours at the same time. I&#39;ll try to hold make-up OH soon.åÊ</p>
<p></p>
<p>Best,</p>
<p>Kate</p>",OH Today,"<p>Hi all,</p>
<p></p>
<p>I have to cancel my office hours today from 12-2. If you needed help on somethingåÊandåÊwere planning to come, send me an email or a private message here and we can set up a time to talk either in person or over video chat today or this weekend.åÊ</p>
<p></p>
<p>Sorry for the inconvenience!</p>
<p>Ellie</p>",Canceling OH for today,Will there be any OH over the weekend since this assignment is due on Monday instead of the usual Friday?,OH over the weekend?,<p>Sorry everyone I&#39;m running a little late. Will start my OH at 3:15 and go till 5:15</p>,Pushing back OH by 15,1
940848593,4/26/2016 16:03:35,true,1969405535,,4/26/2016 15:48:02,false,clixsense,0.8889,8057247,PRT,17,Póvoa De Varzim,144.64.25.68,0,0,"<p>I&#39;ve already started my OH in the GRW bump space so feel free to come on by early if you&#39;d like. I&#39;ll be here till 5 unless it get&#39;s crowded in which case find me near the study space next to the elevators on the 5th floor of Levine.</p>
<p></p>",Starting OH early,"<p>Hey everyone,åÊI&#39;m moving my office hours tonight to Moore 100 instead of the bump space.</p>",OH moved to Moore,"<p>Hi everyone,åÊ</p>
<p></p>
<p>I&#39;m not feeling well, so I&#39;m canceling the office hours I was planning to hold today from 4-6pm. However, Sierra will still be holding her office hours at the same time. I&#39;ll try to hold make-up OH soon.åÊ</p>
<p></p>
<p>Best,</p>
<p>Kate</p>",OH Today,"<p>Hi all,</p>
<p></p>
<p>I have to cancel my office hours today from 12-2. If you needed help on somethingåÊandåÊwere planning to come, send me an email or a private message here and we can set up a time to talk either in person or over video chat today or this weekend.åÊ</p>
<p></p>
<p>Sorry for the inconvenience!</p>
<p>Ellie</p>",Canceling OH for today,Will there be any OH over the weekend since this assignment is due on Monday instead of the usual Friday?,OH over the weekend?,<p>Sorry everyone I&#39;m running a little late. Will start my OH at 3:15 and go till 5:15</p>,Pushing back OH by 15,1
940848593,4/26/2016 16:22:17,true,1969417137,,4/26/2016 16:18:27,false,neodev,1.0,29175140,VEN,25,Caracas,190.72.125.134,0,0,"<p>I&#39;ve already started my OH in the GRW bump space so feel free to come on by early if you&#39;d like. I&#39;ll be here till 5 unless it get&#39;s crowded in which case find me near the study space next to the elevators on the 5th floor of Levine.</p>
<p></p>",Starting OH early,"<p>Hey everyone,åÊI&#39;m moving my office hours tonight to Moore 100 instead of the bump space.</p>",OH moved to Moore,"<p>Hi everyone,åÊ</p>
<p></p>
<p>I&#39;m not feeling well, so I&#39;m canceling the office hours I was planning to hold today from 4-6pm. However, Sierra will still be holding her office hours at the same time. I&#39;ll try to hold make-up OH soon.åÊ</p>
<p></p>
<p>Best,</p>
<p>Kate</p>",OH Today,"<p>Hi all,</p>
<p></p>
<p>I have to cancel my office hours today from 12-2. If you needed help on somethingåÊandåÊwere planning to come, send me an email or a private message here and we can set up a time to talk either in person or over video chat today or this weekend.åÊ</p>
<p></p>
<p>Sorry for the inconvenience!</p>
<p>Ellie</p>",Canceling OH for today,Will there be any OH over the weekend since this assignment is due on Monday instead of the usual Friday?,OH over the weekend?,<p>Sorry everyone I&#39;m running a little late. Will start my OH at 3:15 and go till 5:15</p>,Pushing back OH by 15,1
940848593,4/26/2016 16:31:08,true,1969424243,true,4/26/2016 16:19:53,false,neodev,0.7778,32569659,USA,MN,Minneapolis,97.127.88.224,"1
5",0,"<p>I&#39;ve already started my OH in the GRW bump space so feel free to come on by early if you&#39;d like. I&#39;ll be here till 5 unless it get&#39;s crowded in which case find me near the study space next to the elevators on the 5th floor of Levine.</p>
<p></p>",Starting OH early,"<p>Hey everyone,åÊI&#39;m moving my office hours tonight to Moore 100 instead of the bump space.</p>",OH moved to Moore,"<p>Hi everyone,åÊ</p>
<p></p>
<p>I&#39;m not feeling well, so I&#39;m canceling the office hours I was planning to hold today from 4-6pm. However, Sierra will still be holding her office hours at the same time. I&#39;ll try to hold make-up OH soon.åÊ</p>
<p></p>
<p>Best,</p>
<p>Kate</p>",OH Today,"<p>Hi all,</p>
<p></p>
<p>I have to cancel my office hours today from 12-2. If you needed help on somethingåÊandåÊwere planning to come, send me an email or a private message here and we can set up a time to talk either in person or over video chat today or this weekend.åÊ</p>
<p></p>
<p>Sorry for the inconvenience!</p>
<p>Ellie</p>",Canceling OH for today,Will there be any OH over the weekend since this assignment is due on Monday instead of the usual Friday?,OH over the weekend?,<p>Sorry everyone I&#39;m running a little late. Will start my OH at 3:15 and go till 5:15</p>,Pushing back OH by 15,1
940848593,4/26/2016 16:39:18,true,1969429035,,4/26/2016 16:30:39,false,clixsense,1.0,6329782,IDN,07,Bekonang,202.67.40.31,0,0,"<p>I&#39;ve already started my OH in the GRW bump space so feel free to come on by early if you&#39;d like. I&#39;ll be here till 5 unless it get&#39;s crowded in which case find me near the study space next to the elevators on the 5th floor of Levine.</p>
<p></p>",Starting OH early,"<p>Hey everyone,åÊI&#39;m moving my office hours tonight to Moore 100 instead of the bump space.</p>",OH moved to Moore,"<p>Hi everyone,åÊ</p>
<p></p>
<p>I&#39;m not feeling well, so I&#39;m canceling the office hours I was planning to hold today from 4-6pm. However, Sierra will still be holding her office hours at the same time. I&#39;ll try to hold make-up OH soon.åÊ</p>
<p></p>
<p>Best,</p>
<p>Kate</p>",OH Today,"<p>Hi all,</p>
<p></p>
<p>I have to cancel my office hours today from 12-2. If you needed help on somethingåÊandåÊwere planning to come, send me an email or a private message here and we can set up a time to talk either in person or over video chat today or this weekend.åÊ</p>
<p></p>
<p>Sorry for the inconvenience!</p>
<p>Ellie</p>",Canceling OH for today,Will there be any OH over the weekend since this assignment is due on Monday instead of the usual Friday?,OH over the weekend?,<p>Sorry everyone I&#39;m running a little late. Will start my OH at 3:15 and go till 5:15</p>,Pushing back OH by 15,1
940848593,4/26/2016 17:01:22,true,1969442592,,4/26/2016 17:00:04,false,clixsense,1.0,21408115,IDN,07,Semarang,36.79.23.180,0,0,"<p>I&#39;ve already started my OH in the GRW bump space so feel free to come on by early if you&#39;d like. I&#39;ll be here till 5 unless it get&#39;s crowded in which case find me near the study space next to the elevators on the 5th floor of Levine.</p>
<p></p>",Starting OH early,"<p>Hey everyone,åÊI&#39;m moving my office hours tonight to Moore 100 instead of the bump space.</p>",OH moved to Moore,"<p>Hi everyone,åÊ</p>
<p></p>
<p>I&#39;m not feeling well, so I&#39;m canceling the office hours I was planning to hold today from 4-6pm. However, Sierra will still be holding her office hours at the same time. I&#39;ll try to hold make-up OH soon.åÊ</p>
<p></p>
<p>Best,</p>
<p>Kate</p>",OH Today,"<p>Hi all,</p>
<p></p>
<p>I have to cancel my office hours today from 12-2. If you needed help on somethingåÊandåÊwere planning to come, send me an email or a private message here and we can set up a time to talk either in person or over video chat today or this weekend.åÊ</p>
<p></p>
<p>Sorry for the inconvenience!</p>
<p>Ellie</p>",Canceling OH for today,Will there be any OH over the weekend since this assignment is due on Monday instead of the usual Friday?,OH over the weekend?,<p>Sorry everyone I&#39;m running a little late. Will start my OH at 3:15 and go till 5:15</p>,Pushing back OH by 15,1
940848593,4/26/2016 17:06:41,true,1969445558,,4/26/2016 17:00:39,false,neodev,1.0,33973110,VEN,23,Maracaibo,186.94.238.104,0,0,"<p>I&#39;ve already started my OH in the GRW bump space so feel free to come on by early if you&#39;d like. I&#39;ll be here till 5 unless it get&#39;s crowded in which case find me near the study space next to the elevators on the 5th floor of Levine.</p>
<p></p>",Starting OH early,"<p>Hey everyone,åÊI&#39;m moving my office hours tonight to Moore 100 instead of the bump space.</p>",OH moved to Moore,"<p>Hi everyone,åÊ</p>
<p></p>
<p>I&#39;m not feeling well, so I&#39;m canceling the office hours I was planning to hold today from 4-6pm. However, Sierra will still be holding her office hours at the same time. I&#39;ll try to hold make-up OH soon.åÊ</p>
<p></p>
<p>Best,</p>
<p>Kate</p>",OH Today,"<p>Hi all,</p>
<p></p>
<p>I have to cancel my office hours today from 12-2. If you needed help on somethingåÊandåÊwere planning to come, send me an email or a private message here and we can set up a time to talk either in person or over video chat today or this weekend.åÊ</p>
<p></p>
<p>Sorry for the inconvenience!</p>
<p>Ellie</p>",Canceling OH for today,Will there be any OH over the weekend since this assignment is due on Monday instead of the usual Friday?,OH over the weekend?,<p>Sorry everyone I&#39;m running a little late. Will start my OH at 3:15 and go till 5:15</p>,Pushing back OH by 15,1
940848593,4/26/2016 17:08:54,true,1969446919,,4/26/2016 17:07:44,false,neodev,1.0,36167043,GBR,G6,Hull,77.86.101.69,0,0,"<p>I&#39;ve already started my OH in the GRW bump space so feel free to come on by early if you&#39;d like. I&#39;ll be here till 5 unless it get&#39;s crowded in which case find me near the study space next to the elevators on the 5th floor of Levine.</p>
<p></p>",Starting OH early,"<p>Hey everyone,åÊI&#39;m moving my office hours tonight to Moore 100 instead of the bump space.</p>",OH moved to Moore,"<p>Hi everyone,åÊ</p>
<p></p>
<p>I&#39;m not feeling well, so I&#39;m canceling the office hours I was planning to hold today from 4-6pm. However, Sierra will still be holding her office hours at the same time. I&#39;ll try to hold make-up OH soon.åÊ</p>
<p></p>
<p>Best,</p>
<p>Kate</p>",OH Today,"<p>Hi all,</p>
<p></p>
<p>I have to cancel my office hours today from 12-2. If you needed help on somethingåÊandåÊwere planning to come, send me an email or a private message here and we can set up a time to talk either in person or over video chat today or this weekend.åÊ</p>
<p></p>
<p>Sorry for the inconvenience!</p>
<p>Ellie</p>",Canceling OH for today,Will there be any OH over the weekend since this assignment is due on Monday instead of the usual Friday?,OH over the weekend?,<p>Sorry everyone I&#39;m running a little late. Will start my OH at 3:15 and go till 5:15</p>,Pushing back OH by 15,1
940848593,4/26/2016 17:14:00,true,1969450325,,4/26/2016 17:10:16,false,elite,1.0,25411289,HRV,"","",31.147.119.175,0,0,"<p>I&#39;ve already started my OH in the GRW bump space so feel free to come on by early if you&#39;d like. I&#39;ll be here till 5 unless it get&#39;s crowded in which case find me near the study space next to the elevators on the 5th floor of Levine.</p>
<p></p>",Starting OH early,"<p>Hey everyone,åÊI&#39;m moving my office hours tonight to Moore 100 instead of the bump space.</p>",OH moved to Moore,"<p>Hi everyone,åÊ</p>
<p></p>
<p>I&#39;m not feeling well, so I&#39;m canceling the office hours I was planning to hold today from 4-6pm. However, Sierra will still be holding her office hours at the same time. I&#39;ll try to hold make-up OH soon.åÊ</p>
<p></p>
<p>Best,</p>
<p>Kate</p>",OH Today,"<p>Hi all,</p>
<p></p>
<p>I have to cancel my office hours today from 12-2. If you needed help on somethingåÊandåÊwere planning to come, send me an email or a private message here and we can set up a time to talk either in person or over video chat today or this weekend.åÊ</p>
<p></p>
<p>Sorry for the inconvenience!</p>
<p>Ellie</p>",Canceling OH for today,Will there be any OH over the weekend since this assignment is due on Monday instead of the usual Friday?,OH over the weekend?,<p>Sorry everyone I&#39;m running a little late. Will start my OH at 3:15 and go till 5:15</p>,Pushing back OH by 15,1
940848593,4/26/2016 17:15:16,true,1969451003,,4/26/2016 17:01:58,false,clixsense,0.8889,35338593,ITA,14,Cagliari,151.56.132.145,0,0,"<p>I&#39;ve already started my OH in the GRW bump space so feel free to come on by early if you&#39;d like. I&#39;ll be here till 5 unless it get&#39;s crowded in which case find me near the study space next to the elevators on the 5th floor of Levine.</p>
<p></p>",Starting OH early,"<p>Hey everyone,åÊI&#39;m moving my office hours tonight to Moore 100 instead of the bump space.</p>",OH moved to Moore,"<p>Hi everyone,åÊ</p>
<p></p>
<p>I&#39;m not feeling well, so I&#39;m canceling the office hours I was planning to hold today from 4-6pm. However, Sierra will still be holding her office hours at the same time. I&#39;ll try to hold make-up OH soon.åÊ</p>
<p></p>
<p>Best,</p>
<p>Kate</p>",OH Today,"<p>Hi all,</p>
<p></p>
<p>I have to cancel my office hours today from 12-2. If you needed help on somethingåÊandåÊwere planning to come, send me an email or a private message here and we can set up a time to talk either in person or over video chat today or this weekend.åÊ</p>
<p></p>
<p>Sorry for the inconvenience!</p>
<p>Ellie</p>",Canceling OH for today,Will there be any OH over the weekend since this assignment is due on Monday instead of the usual Friday?,OH over the weekend?,<p>Sorry everyone I&#39;m running a little late. Will start my OH at 3:15 and go till 5:15</p>,Pushing back OH by 15,1
940848593,4/26/2016 17:31:21,true,1969460299,true,4/26/2016 17:29:58,false,neodev,0.8889,33131546,IDN,04,Jakarta,139.194.89.60,"1
2",0,"<p>I&#39;ve already started my OH in the GRW bump space so feel free to come on by early if you&#39;d like. I&#39;ll be here till 5 unless it get&#39;s crowded in which case find me near the study space next to the elevators on the 5th floor of Levine.</p>
<p></p>",Starting OH early,"<p>Hey everyone,åÊI&#39;m moving my office hours tonight to Moore 100 instead of the bump space.</p>",OH moved to Moore,"<p>Hi everyone,åÊ</p>
<p></p>
<p>I&#39;m not feeling well, so I&#39;m canceling the office hours I was planning to hold today from 4-6pm. However, Sierra will still be holding her office hours at the same time. I&#39;ll try to hold make-up OH soon.åÊ</p>
<p></p>
<p>Best,</p>
<p>Kate</p>",OH Today,"<p>Hi all,</p>
<p></p>
<p>I have to cancel my office hours today from 12-2. If you needed help on somethingåÊandåÊwere planning to come, send me an email or a private message here and we can set up a time to talk either in person or over video chat today or this weekend.åÊ</p>
<p></p>
<p>Sorry for the inconvenience!</p>
<p>Ellie</p>",Canceling OH for today,Will there be any OH over the weekend since this assignment is due on Monday instead of the usual Friday?,OH over the weekend?,<p>Sorry everyone I&#39;m running a little late. Will start my OH at 3:15 and go till 5:15</p>,Pushing back OH by 15,1
940848593,4/26/2016 17:35:56,true,1969462692,true,4/26/2016 17:30:54,false,neodev,0.8889,33568303,VEN,23,Cabimas,190.77.7.36,1,0,"<p>I&#39;ve already started my OH in the GRW bump space so feel free to come on by early if you&#39;d like. I&#39;ll be here till 5 unless it get&#39;s crowded in which case find me near the study space next to the elevators on the 5th floor of Levine.</p>
<p></p>",Starting OH early,"<p>Hey everyone,åÊI&#39;m moving my office hours tonight to Moore 100 instead of the bump space.</p>",OH moved to Moore,"<p>Hi everyone,åÊ</p>
<p></p>
<p>I&#39;m not feeling well, so I&#39;m canceling the office hours I was planning to hold today from 4-6pm. However, Sierra will still be holding her office hours at the same time. I&#39;ll try to hold make-up OH soon.åÊ</p>
<p></p>
<p>Best,</p>
<p>Kate</p>",OH Today,"<p>Hi all,</p>
<p></p>
<p>I have to cancel my office hours today from 12-2. If you needed help on somethingåÊandåÊwere planning to come, send me an email or a private message here and we can set up a time to talk either in person or over video chat today or this weekend.åÊ</p>
<p></p>
<p>Sorry for the inconvenience!</p>
<p>Ellie</p>",Canceling OH for today,Will there be any OH over the weekend since this assignment is due on Monday instead of the usual Friday?,OH over the weekend?,<p>Sorry everyone I&#39;m running a little late. Will start my OH at 3:15 and go till 5:15</p>,Pushing back OH by 15,1
940848593,4/26/2016 17:36:20,true,1969462848,,4/26/2016 17:36:03,false,clixsense,1.0,30712378,ROU,21,Deva,79.119.241.200,0,0,"<p>I&#39;ve already started my OH in the GRW bump space so feel free to come on by early if you&#39;d like. I&#39;ll be here till 5 unless it get&#39;s crowded in which case find me near the study space next to the elevators on the 5th floor of Levine.</p>
<p></p>",Starting OH early,"<p>Hey everyone,åÊI&#39;m moving my office hours tonight to Moore 100 instead of the bump space.</p>",OH moved to Moore,"<p>Hi everyone,åÊ</p>
<p></p>
<p>I&#39;m not feeling well, so I&#39;m canceling the office hours I was planning to hold today from 4-6pm. However, Sierra will still be holding her office hours at the same time. I&#39;ll try to hold make-up OH soon.åÊ</p>
<p></p>
<p>Best,</p>
<p>Kate</p>",OH Today,"<p>Hi all,</p>
<p></p>
<p>I have to cancel my office hours today from 12-2. If you needed help on somethingåÊandåÊwere planning to come, send me an email or a private message here and we can set up a time to talk either in person or over video chat today or this weekend.åÊ</p>
<p></p>
<p>Sorry for the inconvenience!</p>
<p>Ellie</p>",Canceling OH for today,Will there be any OH over the weekend since this assignment is due on Monday instead of the usual Friday?,OH over the weekend?,<p>Sorry everyone I&#39;m running a little late. Will start my OH at 3:15 and go till 5:15</p>,Pushing back OH by 15,1
940848593,4/26/2016 17:39:39,true,1969464643,,4/26/2016 17:35:23,false,clixsense,1.0,35444326,BRA,07,Brasília,177.15.130.106,0,0,"<p>I&#39;ve already started my OH in the GRW bump space so feel free to come on by early if you&#39;d like. I&#39;ll be here till 5 unless it get&#39;s crowded in which case find me near the study space next to the elevators on the 5th floor of Levine.</p>
<p></p>",Starting OH early,"<p>Hey everyone,åÊI&#39;m moving my office hours tonight to Moore 100 instead of the bump space.</p>",OH moved to Moore,"<p>Hi everyone,åÊ</p>
<p></p>
<p>I&#39;m not feeling well, so I&#39;m canceling the office hours I was planning to hold today from 4-6pm. However, Sierra will still be holding her office hours at the same time. I&#39;ll try to hold make-up OH soon.åÊ</p>
<p></p>
<p>Best,</p>
<p>Kate</p>",OH Today,"<p>Hi all,</p>
<p></p>
<p>I have to cancel my office hours today from 12-2. If you needed help on somethingåÊandåÊwere planning to come, send me an email or a private message here and we can set up a time to talk either in person or over video chat today or this weekend.åÊ</p>
<p></p>
<p>Sorry for the inconvenience!</p>
<p>Ellie</p>",Canceling OH for today,Will there be any OH over the weekend since this assignment is due on Monday instead of the usual Friday?,OH over the weekend?,<p>Sorry everyone I&#39;m running a little late. Will start my OH at 3:15 and go till 5:15</p>,Pushing back OH by 15,1
940848593,4/26/2016 17:41:50,true,1969466064,,4/26/2016 17:32:10,false,neodev,0.8889,19625264,DZA,41,Chlef,41.102.7.217,0,0,"<p>I&#39;ve already started my OH in the GRW bump space so feel free to come on by early if you&#39;d like. I&#39;ll be here till 5 unless it get&#39;s crowded in which case find me near the study space next to the elevators on the 5th floor of Levine.</p>
<p></p>",Starting OH early,"<p>Hey everyone,åÊI&#39;m moving my office hours tonight to Moore 100 instead of the bump space.</p>",OH moved to Moore,"<p>Hi everyone,åÊ</p>
<p></p>
<p>I&#39;m not feeling well, so I&#39;m canceling the office hours I was planning to hold today from 4-6pm. However, Sierra will still be holding her office hours at the same time. I&#39;ll try to hold make-up OH soon.åÊ</p>
<p></p>
<p>Best,</p>
<p>Kate</p>",OH Today,"<p>Hi all,</p>
<p></p>
<p>I have to cancel my office hours today from 12-2. If you needed help on somethingåÊandåÊwere planning to come, send me an email or a private message here and we can set up a time to talk either in person or over video chat today or this weekend.åÊ</p>
<p></p>
<p>Sorry for the inconvenience!</p>
<p>Ellie</p>",Canceling OH for today,Will there be any OH over the weekend since this assignment is due on Monday instead of the usual Friday?,OH over the weekend?,<p>Sorry everyone I&#39;m running a little late. Will start my OH at 3:15 and go till 5:15</p>,Pushing back OH by 15,1
940848593,4/26/2016 18:20:19,true,1969486839,,4/26/2016 18:19:10,false,neodev,0.8889,35550011,VEN,07,Valencia,190.204.238.112,0,0,"<p>I&#39;ve already started my OH in the GRW bump space so feel free to come on by early if you&#39;d like. I&#39;ll be here till 5 unless it get&#39;s crowded in which case find me near the study space next to the elevators on the 5th floor of Levine.</p>
<p></p>",Starting OH early,"<p>Hey everyone,åÊI&#39;m moving my office hours tonight to Moore 100 instead of the bump space.</p>",OH moved to Moore,"<p>Hi everyone,åÊ</p>
<p></p>
<p>I&#39;m not feeling well, so I&#39;m canceling the office hours I was planning to hold today from 4-6pm. However, Sierra will still be holding her office hours at the same time. I&#39;ll try to hold make-up OH soon.åÊ</p>
<p></p>
<p>Best,</p>
<p>Kate</p>",OH Today,"<p>Hi all,</p>
<p></p>
<p>I have to cancel my office hours today from 12-2. If you needed help on somethingåÊandåÊwere planning to come, send me an email or a private message here and we can set up a time to talk either in person or over video chat today or this weekend.åÊ</p>
<p></p>
<p>Sorry for the inconvenience!</p>
<p>Ellie</p>",Canceling OH for today,Will there be any OH over the weekend since this assignment is due on Monday instead of the usual Friday?,OH over the weekend?,<p>Sorry everyone I&#39;m running a little late. Will start my OH at 3:15 and go till 5:15</p>,Pushing back OH by 15,1
940848593,4/26/2016 18:27:26,true,1969490295,,4/26/2016 18:25:35,false,elite,1.0,30128662,BGR,50,Pleven,212.233.177.195,0,0,"<p>I&#39;ve already started my OH in the GRW bump space so feel free to come on by early if you&#39;d like. I&#39;ll be here till 5 unless it get&#39;s crowded in which case find me near the study space next to the elevators on the 5th floor of Levine.</p>
<p></p>",Starting OH early,"<p>Hey everyone,åÊI&#39;m moving my office hours tonight to Moore 100 instead of the bump space.</p>",OH moved to Moore,"<p>Hi everyone,åÊ</p>
<p></p>
<p>I&#39;m not feeling well, so I&#39;m canceling the office hours I was planning to hold today from 4-6pm. However, Sierra will still be holding her office hours at the same time. I&#39;ll try to hold make-up OH soon.åÊ</p>
<p></p>
<p>Best,</p>
<p>Kate</p>",OH Today,"<p>Hi all,</p>
<p></p>
<p>I have to cancel my office hours today from 12-2. If you needed help on somethingåÊandåÊwere planning to come, send me an email or a private message here and we can set up a time to talk either in person or over video chat today or this weekend.åÊ</p>
<p></p>
<p>Sorry for the inconvenience!</p>
<p>Ellie</p>",Canceling OH for today,Will there be any OH over the weekend since this assignment is due on Monday instead of the usual Friday?,OH over the weekend?,<p>Sorry everyone I&#39;m running a little late. Will start my OH at 3:15 and go till 5:15</p>,Pushing back OH by 15,1
940848593,4/26/2016 18:33:38,true,1969493273,,4/26/2016 18:19:15,false,neodev,1.0,29879245,RUS,69,Smolensk,37.144.124.118,0,0,"<p>I&#39;ve already started my OH in the GRW bump space so feel free to come on by early if you&#39;d like. I&#39;ll be here till 5 unless it get&#39;s crowded in which case find me near the study space next to the elevators on the 5th floor of Levine.</p>
<p></p>",Starting OH early,"<p>Hey everyone,åÊI&#39;m moving my office hours tonight to Moore 100 instead of the bump space.</p>",OH moved to Moore,"<p>Hi everyone,åÊ</p>
<p></p>
<p>I&#39;m not feeling well, so I&#39;m canceling the office hours I was planning to hold today from 4-6pm. However, Sierra will still be holding her office hours at the same time. I&#39;ll try to hold make-up OH soon.åÊ</p>
<p></p>
<p>Best,</p>
<p>Kate</p>",OH Today,"<p>Hi all,</p>
<p></p>
<p>I have to cancel my office hours today from 12-2. If you needed help on somethingåÊandåÊwere planning to come, send me an email or a private message here and we can set up a time to talk either in person or over video chat today or this weekend.åÊ</p>
<p></p>
<p>Sorry for the inconvenience!</p>
<p>Ellie</p>",Canceling OH for today,Will there be any OH over the weekend since this assignment is due on Monday instead of the usual Friday?,OH over the weekend?,<p>Sorry everyone I&#39;m running a little late. Will start my OH at 3:15 and go till 5:15</p>,Pushing back OH by 15,1
940848593,4/26/2016 18:45:01,true,1969499308,,4/26/2016 18:41:05,false,neodev,1.0,35974955,VEN,17,Porlamar,190.198.232.239,0,0,"<p>I&#39;ve already started my OH in the GRW bump space so feel free to come on by early if you&#39;d like. I&#39;ll be here till 5 unless it get&#39;s crowded in which case find me near the study space next to the elevators on the 5th floor of Levine.</p>
<p></p>",Starting OH early,"<p>Hey everyone,åÊI&#39;m moving my office hours tonight to Moore 100 instead of the bump space.</p>",OH moved to Moore,"<p>Hi everyone,åÊ</p>
<p></p>
<p>I&#39;m not feeling well, so I&#39;m canceling the office hours I was planning to hold today from 4-6pm. However, Sierra will still be holding her office hours at the same time. I&#39;ll try to hold make-up OH soon.åÊ</p>
<p></p>
<p>Best,</p>
<p>Kate</p>",OH Today,"<p>Hi all,</p>
<p></p>
<p>I have to cancel my office hours today from 12-2. If you needed help on somethingåÊandåÊwere planning to come, send me an email or a private message here and we can set up a time to talk either in person or over video chat today or this weekend.åÊ</p>
<p></p>
<p>Sorry for the inconvenience!</p>
<p>Ellie</p>",Canceling OH for today,Will there be any OH over the weekend since this assignment is due on Monday instead of the usual Friday?,OH over the weekend?,<p>Sorry everyone I&#39;m running a little late. Will start my OH at 3:15 and go till 5:15</p>,Pushing back OH by 15,1
940848593,4/26/2016 19:25:11,true,1969519070,,4/26/2016 19:21:41,false,tremorgames,1.0,25197223,HRV,15,Split,94.253.234.240,0,0,"<p>I&#39;ve already started my OH in the GRW bump space so feel free to come on by early if you&#39;d like. I&#39;ll be here till 5 unless it get&#39;s crowded in which case find me near the study space next to the elevators on the 5th floor of Levine.</p>
<p></p>",Starting OH early,"<p>Hey everyone,åÊI&#39;m moving my office hours tonight to Moore 100 instead of the bump space.</p>",OH moved to Moore,"<p>Hi everyone,åÊ</p>
<p></p>
<p>I&#39;m not feeling well, so I&#39;m canceling the office hours I was planning to hold today from 4-6pm. However, Sierra will still be holding her office hours at the same time. I&#39;ll try to hold make-up OH soon.åÊ</p>
<p></p>
<p>Best,</p>
<p>Kate</p>",OH Today,"<p>Hi all,</p>
<p></p>
<p>I have to cancel my office hours today from 12-2. If you needed help on somethingåÊandåÊwere planning to come, send me an email or a private message here and we can set up a time to talk either in person or over video chat today or this weekend.åÊ</p>
<p></p>
<p>Sorry for the inconvenience!</p>
<p>Ellie</p>",Canceling OH for today,Will there be any OH over the weekend since this assignment is due on Monday instead of the usual Friday?,OH over the weekend?,<p>Sorry everyone I&#39;m running a little late. Will start my OH at 3:15 and go till 5:15</p>,Pushing back OH by 15,1
940848892,4/26/2016 15:11:31,true,1969363829,,4/26/2016 15:10:32,false,tremorgames,1.0,32635967,LTU,60,Panevezys,78.63.38.165,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>When our group tried to upload HITsåÊto the MTurk sandbox, we saw that we got a couple of failures. The number of failures went down after a little while, but we still ended up with a couple with the error as &#34;throttled&#34;. Is this normal behavior for the sandbox? I.e. do we need to worry about this when we actually upload our HITs (not to the sandbox)?</p>",Failures on MTurk,0
940848892,4/26/2016 15:14:04,true,1969367420,,4/26/2016 15:11:16,false,clixsense,1.0,24287706,TWN,04,Keelung,61.231.195.173,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>When our group tried to upload HITsåÊto the MTurk sandbox, we saw that we got a couple of failures. The number of failures went down after a little while, but we still ended up with a couple with the error as &#34;throttled&#34;. Is this normal behavior for the sandbox? I.e. do we need to worry about this when we actually upload our HITs (not to the sandbox)?</p>",Failures on MTurk,0
940848892,4/26/2016 15:17:20,true,1969372591,,4/26/2016 15:12:10,false,clixsense,1.0,7837812,SRB,00,Belgrade,79.101.254.233,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>When our group tried to upload HITsåÊto the MTurk sandbox, we saw that we got a couple of failures. The number of failures went down after a little while, but we still ended up with a couple with the error as &#34;throttled&#34;. Is this normal behavior for the sandbox? I.e. do we need to worry about this when we actually upload our HITs (not to the sandbox)?</p>",Failures on MTurk,0
940848892,4/26/2016 15:19:53,true,1969376822,,4/26/2016 15:19:30,false,neodev,1.0,19132694,LKA,36,Colombo,123.231.124.170,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>When our group tried to upload HITsåÊto the MTurk sandbox, we saw that we got a couple of failures. The number of failures went down after a little while, but we still ended up with a couple with the error as &#34;throttled&#34;. Is this normal behavior for the sandbox? I.e. do we need to worry about this when we actually upload our HITs (not to the sandbox)?</p>",Failures on MTurk,0
940848892,4/26/2016 15:21:05,true,1969378832,,4/26/2016 15:15:48,false,elite,1.0,30280423,ITA,15,Siracusa,151.54.84.121,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>When our group tried to upload HITsåÊto the MTurk sandbox, we saw that we got a couple of failures. The number of failures went down after a little while, but we still ended up with a couple with the error as &#34;throttled&#34;. Is this normal behavior for the sandbox? I.e. do we need to worry about this when we actually upload our HITs (not to the sandbox)?</p>",Failures on MTurk,0
940848892,4/26/2016 15:21:31,true,1969379540,,4/26/2016 15:12:37,false,clixsense,0.8889,36052512,PHL,F2,Quezon City,49.149.150.150,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>When our group tried to upload HITsåÊto the MTurk sandbox, we saw that we got a couple of failures. The number of failures went down after a little while, but we still ended up with a couple with the error as &#34;throttled&#34;. Is this normal behavior for the sandbox? I.e. do we need to worry about this when we actually upload our HITs (not to the sandbox)?</p>",Failures on MTurk,0
940848892,4/26/2016 15:26:03,true,1969387312,,4/26/2016 15:24:14,false,instagc,0.8889,13581319,USA,IL,Waltonville,208.70.36.12,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>When our group tried to upload HITsåÊto the MTurk sandbox, we saw that we got a couple of failures. The number of failures went down after a little while, but we still ended up with a couple with the error as &#34;throttled&#34;. Is this normal behavior for the sandbox? I.e. do we need to worry about this when we actually upload our HITs (not to the sandbox)?</p>",Failures on MTurk,0
940848892,4/26/2016 15:36:01,true,1969392990,,4/26/2016 15:30:10,false,neodev,1.0,13396426,VEN,15,Santa Teresa,190.38.163.149,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>When our group tried to upload HITsåÊto the MTurk sandbox, we saw that we got a couple of failures. The number of failures went down after a little while, but we still ended up with a couple with the error as &#34;throttled&#34;. Is this normal behavior for the sandbox? I.e. do we need to worry about this when we actually upload our HITs (not to the sandbox)?</p>",Failures on MTurk,0
940848892,4/26/2016 15:40:37,true,1969395295,,4/26/2016 15:34:58,false,neodev,0.7778,32569659,USA,MN,Minneapolis,97.127.88.224,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>When our group tried to upload HITsåÊto the MTurk sandbox, we saw that we got a couple of failures. The number of failures went down after a little while, but we still ended up with a couple with the error as &#34;throttled&#34;. Is this normal behavior for the sandbox? I.e. do we need to worry about this when we actually upload our HITs (not to the sandbox)?</p>",Failures on MTurk,0
940848892,4/26/2016 15:45:34,true,1969397182,,4/26/2016 15:37:11,false,elite,0.8889,36575101,IND,07,New Delhi,112.196.144.2,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>When our group tried to upload HITsåÊto the MTurk sandbox, we saw that we got a couple of failures. The number of failures went down after a little while, but we still ended up with a couple with the error as &#34;throttled&#34;. Is this normal behavior for the sandbox? I.e. do we need to worry about this when we actually upload our HITs (not to the sandbox)?</p>",Failures on MTurk,0
940848892,4/26/2016 15:48:55,true,1969398737,,4/26/2016 15:46:06,false,neodev,0.8889,21971187,TTO,08,Valsayn,190.213.132.190,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>When our group tried to upload HITsåÊto the MTurk sandbox, we saw that we got a couple of failures. The number of failures went down after a little while, but we still ended up with a couple with the error as &#34;throttled&#34;. Is this normal behavior for the sandbox? I.e. do we need to worry about this when we actually upload our HITs (not to the sandbox)?</p>",Failures on MTurk,0
940848892,4/26/2016 15:51:11,true,1969399728,,4/26/2016 15:47:56,false,neodev,1.0,28875937,PAK,08,Islamabad,119.153.105.50,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>When our group tried to upload HITsåÊto the MTurk sandbox, we saw that we got a couple of failures. The number of failures went down after a little while, but we still ended up with a couple with the error as &#34;throttled&#34;. Is this normal behavior for the sandbox? I.e. do we need to worry about this when we actually upload our HITs (not to the sandbox)?</p>",Failures on MTurk,0
940848892,4/26/2016 15:56:46,true,1969402045,,4/26/2016 15:55:02,false,elite,1.0,33243069,IND,10,Faridabad,116.203.79.150,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>When our group tried to upload HITsåÊto the MTurk sandbox, we saw that we got a couple of failures. The number of failures went down after a little while, but we still ended up with a couple with the error as &#34;throttled&#34;. Is this normal behavior for the sandbox? I.e. do we need to worry about this when we actually upload our HITs (not to the sandbox)?</p>",Failures on MTurk,0
940848892,4/26/2016 16:02:20,true,1969404866,,4/26/2016 16:01:47,false,personaly,1.0,33663352,ARG,01,Mar Del Plata,181.168.213.227,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>When our group tried to upload HITsåÊto the MTurk sandbox, we saw that we got a couple of failures. The number of failures went down after a little while, but we still ended up with a couple with the error as &#34;throttled&#34;. Is this normal behavior for the sandbox? I.e. do we need to worry about this when we actually upload our HITs (not to the sandbox)?</p>",Failures on MTurk,0
940848892,4/26/2016 16:22:17,true,1969417141,,4/26/2016 16:18:27,false,neodev,1.0,29175140,VEN,25,Caracas,190.72.125.134,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>When our group tried to upload HITsåÊto the MTurk sandbox, we saw that we got a couple of failures. The number of failures went down after a little while, but we still ended up with a couple with the error as &#34;throttled&#34;. Is this normal behavior for the sandbox? I.e. do we need to worry about this when we actually upload our HITs (not to the sandbox)?</p>",Failures on MTurk,0
940848892,4/26/2016 16:34:40,true,1969426098,,4/26/2016 16:32:37,false,clixsense,0.8889,8057247,PRT,17,Póvoa De Varzim,144.64.25.68,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>When our group tried to upload HITsåÊto the MTurk sandbox, we saw that we got a couple of failures. The number of failures went down after a little while, but we still ended up with a couple with the error as &#34;throttled&#34;. Is this normal behavior for the sandbox? I.e. do we need to worry about this when we actually upload our HITs (not to the sandbox)?</p>",Failures on MTurk,0
940848892,4/26/2016 16:39:18,true,1969429031,,4/26/2016 16:30:39,false,clixsense,1.0,6329782,IDN,07,Bekonang,202.67.40.31,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>When our group tried to upload HITsåÊto the MTurk sandbox, we saw that we got a couple of failures. The number of failures went down after a little while, but we still ended up with a couple with the error as &#34;throttled&#34;. Is this normal behavior for the sandbox? I.e. do we need to worry about this when we actually upload our HITs (not to the sandbox)?</p>",Failures on MTurk,0
940848892,4/26/2016 16:58:38,true,1969440700,,4/26/2016 16:56:43,false,clixsense,1.0,21408115,IDN,07,Semarang,36.79.23.180,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>When our group tried to upload HITsåÊto the MTurk sandbox, we saw that we got a couple of failures. The number of failures went down after a little while, but we still ended up with a couple with the error as &#34;throttled&#34;. Is this normal behavior for the sandbox? I.e. do we need to worry about this when we actually upload our HITs (not to the sandbox)?</p>",Failures on MTurk,0
940848892,4/26/2016 17:07:43,true,1969446217,,4/26/2016 17:05:48,false,neodev,1.0,36167043,GBR,G6,Hull,77.86.101.69,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>When our group tried to upload HITsåÊto the MTurk sandbox, we saw that we got a couple of failures. The number of failures went down after a little while, but we still ended up with a couple with the error as &#34;throttled&#34;. Is this normal behavior for the sandbox? I.e. do we need to worry about this when we actually upload our HITs (not to the sandbox)?</p>",Failures on MTurk,0
940848892,4/26/2016 17:19:05,true,1969453025,,4/26/2016 17:16:43,false,elite,1.0,25411289,HRV,"","",31.147.119.175,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>When our group tried to upload HITsåÊto the MTurk sandbox, we saw that we got a couple of failures. The number of failures went down after a little while, but we still ended up with a couple with the error as &#34;throttled&#34;. Is this normal behavior for the sandbox? I.e. do we need to worry about this when we actually upload our HITs (not to the sandbox)?</p>",Failures on MTurk,0
940848892,4/26/2016 17:27:57,true,1969458134,,4/26/2016 17:23:11,false,neodev,1.0,33973110,VEN,23,Maracaibo,186.94.238.104,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>When our group tried to upload HITsåÊto the MTurk sandbox, we saw that we got a couple of failures. The number of failures went down after a little while, but we still ended up with a couple with the error as &#34;throttled&#34;. Is this normal behavior for the sandbox? I.e. do we need to worry about this when we actually upload our HITs (not to the sandbox)?</p>",Failures on MTurk,0
940848892,4/26/2016 17:30:30,true,1969459778,,4/26/2016 17:11:11,false,neodev,0.8889,19625264,DZA,41,Chlef,41.102.7.217,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>When our group tried to upload HITsåÊto the MTurk sandbox, we saw that we got a couple of failures. The number of failures went down after a little while, but we still ended up with a couple with the error as &#34;throttled&#34;. Is this normal behavior for the sandbox? I.e. do we need to worry about this when we actually upload our HITs (not to the sandbox)?</p>",Failures on MTurk,0
940848892,4/26/2016 17:31:21,true,1969460303,,4/26/2016 17:29:58,false,neodev,0.8889,33131546,IDN,04,Jakarta,139.194.89.60,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>When our group tried to upload HITsåÊto the MTurk sandbox, we saw that we got a couple of failures. The number of failures went down after a little while, but we still ended up with a couple with the error as &#34;throttled&#34;. Is this normal behavior for the sandbox? I.e. do we need to worry about this when we actually upload our HITs (not to the sandbox)?</p>",Failures on MTurk,0
940848892,4/26/2016 17:33:45,true,1969461567,,4/26/2016 17:33:17,false,clixsense,1.0,30712378,ROU,21,Deva,79.119.241.200,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>When our group tried to upload HITsåÊto the MTurk sandbox, we saw that we got a couple of failures. The number of failures went down after a little while, but we still ended up with a couple with the error as &#34;throttled&#34;. Is this normal behavior for the sandbox? I.e. do we need to worry about this when we actually upload our HITs (not to the sandbox)?</p>",Failures on MTurk,0
940848892,4/26/2016 17:35:56,true,1969462693,,4/26/2016 17:30:54,false,neodev,0.8889,33568303,VEN,23,Cabimas,190.77.7.36,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>When our group tried to upload HITsåÊto the MTurk sandbox, we saw that we got a couple of failures. The number of failures went down after a little while, but we still ended up with a couple with the error as &#34;throttled&#34;. Is this normal behavior for the sandbox? I.e. do we need to worry about this when we actually upload our HITs (not to the sandbox)?</p>",Failures on MTurk,0
940848892,4/26/2016 17:50:21,true,1969470888,,4/26/2016 17:49:33,false,clixsense,1.0,35444326,BRA,07,Brasília,177.15.130.106,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>When our group tried to upload HITsåÊto the MTurk sandbox, we saw that we got a couple of failures. The number of failures went down after a little while, but we still ended up with a couple with the error as &#34;throttled&#34;. Is this normal behavior for the sandbox? I.e. do we need to worry about this when we actually upload our HITs (not to the sandbox)?</p>",Failures on MTurk,0
940848892,4/26/2016 18:16:16,true,1969484744,,4/26/2016 18:07:59,false,neodev,0.8889,35550011,VEN,07,Valencia,190.204.238.112,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>When our group tried to upload HITsåÊto the MTurk sandbox, we saw that we got a couple of failures. The number of failures went down after a little while, but we still ended up with a couple with the error as &#34;throttled&#34;. Is this normal behavior for the sandbox? I.e. do we need to worry about this when we actually upload our HITs (not to the sandbox)?</p>",Failures on MTurk,0
940848892,4/26/2016 18:25:34,true,1969489418,,4/26/2016 18:24:15,false,elite,1.0,30128662,BGR,50,Pleven,212.233.177.195,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>When our group tried to upload HITsåÊto the MTurk sandbox, we saw that we got a couple of failures. The number of failures went down after a little while, but we still ended up with a couple with the error as &#34;throttled&#34;. Is this normal behavior for the sandbox? I.e. do we need to worry about this when we actually upload our HITs (not to the sandbox)?</p>",Failures on MTurk,0
940848892,4/26/2016 18:27:49,true,1969490434,,4/26/2016 18:04:06,false,clixsense,0.8889,35338593,ITA,14,Cagliari,151.56.132.145,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>When our group tried to upload HITsåÊto the MTurk sandbox, we saw that we got a couple of failures. The number of failures went down after a little while, but we still ended up with a couple with the error as &#34;throttled&#34;. Is this normal behavior for the sandbox? I.e. do we need to worry about this when we actually upload our HITs (not to the sandbox)?</p>",Failures on MTurk,0
940848892,4/26/2016 18:37:13,true,1969495292,,4/26/2016 18:33:39,false,neodev,1.0,29879245,RUS,69,Smolensk,37.144.124.118,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>When our group tried to upload HITsåÊto the MTurk sandbox, we saw that we got a couple of failures. The number of failures went down after a little while, but we still ended up with a couple with the error as &#34;throttled&#34;. Is this normal behavior for the sandbox? I.e. do we need to worry about this when we actually upload our HITs (not to the sandbox)?</p>",Failures on MTurk,0
940848892,4/26/2016 18:41:04,true,1969497533,,4/26/2016 18:31:56,false,neodev,1.0,35974955,VEN,17,Porlamar,190.198.232.239,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>When our group tried to upload HITsåÊto the MTurk sandbox, we saw that we got a couple of failures. The number of failures went down after a little while, but we still ended up with a couple with the error as &#34;throttled&#34;. Is this normal behavior for the sandbox? I.e. do we need to worry about this when we actually upload our HITs (not to the sandbox)?</p>",Failures on MTurk,0
940848892,4/26/2016 19:25:11,true,1969519073,,4/26/2016 19:21:41,false,tremorgames,1.0,25197223,HRV,15,Split,94.253.234.240,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,<p>I&#39;m having trouble determining the xpath for the material we want. Can someone point me in the right direction on how we should go about doing this? Thank you!</p>,XPath,"<p>When our group tried to upload HITsåÊto the MTurk sandbox, we saw that we got a couple of failures. The number of failures went down after a little while, but we still ended up with a couple with the error as &#34;throttled&#34;. Is this normal behavior for the sandbox? I.e. do we need to worry about this when we actually upload our HITs (not to the sandbox)?</p>",Failures on MTurk,0
940849047,4/26/2016 15:10:31,true,1969362380,,4/26/2016 15:08:05,false,tremorgames,1.0,32635967,LTU,60,Panevezys,78.63.38.165,0,0,"<p>When I use the command</p>
<pre><code> cat articles_and_urls.txt | cut -f 1 &gt; urls.txt 
 cat articles_and_urls.txt | cut -f 2 &gt; unlabelled_articles.txt </code></pre>
<p></p>
<p>I get this error</p>
<p>cut: stdin: Illegal byte sequence</p>
<p></p>
<p>I tried looking it up but nothing has really helped.</p>",Error preparing input data for part 2,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hlicfcesNVC/im1ikvuvripg/Screen_Shot_20160321_at_12.50.01_AM.png"" /></p>
<p></p>
<p>I&#39;m getting this error - the aggregated csv for CrowdFlower for this link predicts that theåÊurlåÊisåÊbroken. How mightåÊthis be resolved? Thanks!åÊ</p>",Error with kappa.py,2
940849047,4/26/2016 15:14:04,true,1969367418,,4/26/2016 15:11:16,false,clixsense,1.0,24287706,TWN,04,Keelung,61.231.195.173,0,0,"<p>When I use the command</p>
<pre><code> cat articles_and_urls.txt | cut -f 1 &gt; urls.txt 
 cat articles_and_urls.txt | cut -f 2 &gt; unlabelled_articles.txt </code></pre>
<p></p>
<p>I get this error</p>
<p>cut: stdin: Illegal byte sequence</p>
<p></p>
<p>I tried looking it up but nothing has really helped.</p>",Error preparing input data for part 2,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hlicfcesNVC/im1ikvuvripg/Screen_Shot_20160321_at_12.50.01_AM.png"" /></p>
<p></p>
<p>I&#39;m getting this error - the aggregated csv for CrowdFlower for this link predicts that theåÊurlåÊisåÊbroken. How mightåÊthis be resolved? Thanks!åÊ</p>",Error with kappa.py,2
940849047,4/26/2016 15:17:20,true,1969372587,,4/26/2016 15:12:10,false,clixsense,1.0,7837812,SRB,00,Belgrade,79.101.254.233,0,0,"<p>When I use the command</p>
<pre><code> cat articles_and_urls.txt | cut -f 1 &gt; urls.txt 
 cat articles_and_urls.txt | cut -f 2 &gt; unlabelled_articles.txt </code></pre>
<p></p>
<p>I get this error</p>
<p>cut: stdin: Illegal byte sequence</p>
<p></p>
<p>I tried looking it up but nothing has really helped.</p>",Error preparing input data for part 2,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hlicfcesNVC/im1ikvuvripg/Screen_Shot_20160321_at_12.50.01_AM.png"" /></p>
<p></p>
<p>I&#39;m getting this error - the aggregated csv for CrowdFlower for this link predicts that theåÊurlåÊisåÊbroken. How mightåÊthis be resolved? Thanks!åÊ</p>",Error with kappa.py,2
940849047,4/26/2016 15:19:29,true,1969376229,,4/26/2016 15:14:50,false,neodev,1.0,19132694,LKA,36,Colombo,123.231.124.170,0,0,"<p>When I use the command</p>
<pre><code> cat articles_and_urls.txt | cut -f 1 &gt; urls.txt 
 cat articles_and_urls.txt | cut -f 2 &gt; unlabelled_articles.txt </code></pre>
<p></p>
<p>I get this error</p>
<p>cut: stdin: Illegal byte sequence</p>
<p></p>
<p>I tried looking it up but nothing has really helped.</p>",Error preparing input data for part 2,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hlicfcesNVC/im1ikvuvripg/Screen_Shot_20160321_at_12.50.01_AM.png"" /></p>
<p></p>
<p>I&#39;m getting this error - the aggregated csv for CrowdFlower for this link predicts that theåÊurlåÊisåÊbroken. How mightåÊthis be resolved? Thanks!åÊ</p>",Error with kappa.py,2
940849047,4/26/2016 15:21:05,true,1969378836,,4/26/2016 15:15:48,false,elite,1.0,30280423,ITA,15,Siracusa,151.54.84.121,0,0,"<p>When I use the command</p>
<pre><code> cat articles_and_urls.txt | cut -f 1 &gt; urls.txt 
 cat articles_and_urls.txt | cut -f 2 &gt; unlabelled_articles.txt </code></pre>
<p></p>
<p>I get this error</p>
<p>cut: stdin: Illegal byte sequence</p>
<p></p>
<p>I tried looking it up but nothing has really helped.</p>",Error preparing input data for part 2,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hlicfcesNVC/im1ikvuvripg/Screen_Shot_20160321_at_12.50.01_AM.png"" /></p>
<p></p>
<p>I&#39;m getting this error - the aggregated csv for CrowdFlower for this link predicts that theåÊurlåÊisåÊbroken. How mightåÊthis be resolved? Thanks!åÊ</p>",Error with kappa.py,2
940849047,4/26/2016 15:26:03,true,1969387309,true,4/26/2016 15:24:14,false,instagc,0.8889,13581319,USA,IL,Waltonville,208.70.36.12,1,0,"<p>When I use the command</p>
<pre><code> cat articles_and_urls.txt | cut -f 1 &gt; urls.txt 
 cat articles_and_urls.txt | cut -f 2 &gt; unlabelled_articles.txt </code></pre>
<p></p>
<p>I get this error</p>
<p>cut: stdin: Illegal byte sequence</p>
<p></p>
<p>I tried looking it up but nothing has really helped.</p>",Error preparing input data for part 2,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hlicfcesNVC/im1ikvuvripg/Screen_Shot_20160321_at_12.50.01_AM.png"" /></p>
<p></p>
<p>I&#39;m getting this error - the aggregated csv for CrowdFlower for this link predicts that theåÊurlåÊisåÊbroken. How mightåÊthis be resolved? Thanks!åÊ</p>",Error with kappa.py,2
940849047,4/26/2016 15:39:31,true,1969394679,,4/26/2016 15:36:13,false,clixsense,0.8889,36052512,PHL,F2,Quezon City,49.149.150.150,0,0,"<p>When I use the command</p>
<pre><code> cat articles_and_urls.txt | cut -f 1 &gt; urls.txt 
 cat articles_and_urls.txt | cut -f 2 &gt; unlabelled_articles.txt </code></pre>
<p></p>
<p>I get this error</p>
<p>cut: stdin: Illegal byte sequence</p>
<p></p>
<p>I tried looking it up but nothing has really helped.</p>",Error preparing input data for part 2,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hlicfcesNVC/im1ikvuvripg/Screen_Shot_20160321_at_12.50.01_AM.png"" /></p>
<p></p>
<p>I&#39;m getting this error - the aggregated csv for CrowdFlower for this link predicts that theåÊurlåÊisåÊbroken. How mightåÊthis be resolved? Thanks!åÊ</p>",Error with kappa.py,2
940849047,4/26/2016 15:39:56,true,1969394930,,4/26/2016 15:27:12,false,clixsense,1.0,21875134,GBR,H9,London,87.112.158.81,0,0,"<p>When I use the command</p>
<pre><code> cat articles_and_urls.txt | cut -f 1 &gt; urls.txt 
 cat articles_and_urls.txt | cut -f 2 &gt; unlabelled_articles.txt </code></pre>
<p></p>
<p>I get this error</p>
<p>cut: stdin: Illegal byte sequence</p>
<p></p>
<p>I tried looking it up but nothing has really helped.</p>",Error preparing input data for part 2,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hlicfcesNVC/im1ikvuvripg/Screen_Shot_20160321_at_12.50.01_AM.png"" /></p>
<p></p>
<p>I&#39;m getting this error - the aggregated csv for CrowdFlower for this link predicts that theåÊurlåÊisåÊbroken. How mightåÊthis be resolved? Thanks!åÊ</p>",Error with kappa.py,2
940849047,4/26/2016 15:44:24,true,1969396766,,4/26/2016 15:31:20,false,neodev,1.0,28875937,PAK,04,Lahore,119.153.154.137,0,0,"<p>When I use the command</p>
<pre><code> cat articles_and_urls.txt | cut -f 1 &gt; urls.txt 
 cat articles_and_urls.txt | cut -f 2 &gt; unlabelled_articles.txt </code></pre>
<p></p>
<p>I get this error</p>
<p>cut: stdin: Illegal byte sequence</p>
<p></p>
<p>I tried looking it up but nothing has really helped.</p>",Error preparing input data for part 2,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hlicfcesNVC/im1ikvuvripg/Screen_Shot_20160321_at_12.50.01_AM.png"" /></p>
<p></p>
<p>I&#39;m getting this error - the aggregated csv for CrowdFlower for this link predicts that theåÊurlåÊisåÊbroken. How mightåÊthis be resolved? Thanks!åÊ</p>",Error with kappa.py,2
940849047,4/26/2016 15:49:11,true,1969398837,,4/26/2016 15:45:52,false,elite,0.8889,36575101,IND,07,New Delhi,112.196.144.2,0,0,"<p>When I use the command</p>
<pre><code> cat articles_and_urls.txt | cut -f 1 &gt; urls.txt 
 cat articles_and_urls.txt | cut -f 2 &gt; unlabelled_articles.txt </code></pre>
<p></p>
<p>I get this error</p>
<p>cut: stdin: Illegal byte sequence</p>
<p></p>
<p>I tried looking it up but nothing has really helped.</p>",Error preparing input data for part 2,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hlicfcesNVC/im1ikvuvripg/Screen_Shot_20160321_at_12.50.01_AM.png"" /></p>
<p></p>
<p>I&#39;m getting this error - the aggregated csv for CrowdFlower for this link predicts that theåÊurlåÊisåÊbroken. How mightåÊthis be resolved? Thanks!åÊ</p>",Error with kappa.py,2
940849047,4/26/2016 15:58:11,true,1969402630,,4/26/2016 15:56:24,false,neodev,1.0,13396426,VEN,15,Santa Teresa,190.38.163.149,0,0,"<p>When I use the command</p>
<pre><code> cat articles_and_urls.txt | cut -f 1 &gt; urls.txt 
 cat articles_and_urls.txt | cut -f 2 &gt; unlabelled_articles.txt </code></pre>
<p></p>
<p>I get this error</p>
<p>cut: stdin: Illegal byte sequence</p>
<p></p>
<p>I tried looking it up but nothing has really helped.</p>",Error preparing input data for part 2,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hlicfcesNVC/im1ikvuvripg/Screen_Shot_20160321_at_12.50.01_AM.png"" /></p>
<p></p>
<p>I&#39;m getting this error - the aggregated csv for CrowdFlower for this link predicts that theåÊurlåÊisåÊbroken. How mightåÊthis be resolved? Thanks!åÊ</p>",Error with kappa.py,2
940849047,4/26/2016 16:01:09,true,1969404228,,4/26/2016 16:00:06,false,personaly,1.0,33663352,ARG,01,Mar Del Plata,181.168.213.227,0,0,"<p>When I use the command</p>
<pre><code> cat articles_and_urls.txt | cut -f 1 &gt; urls.txt 
 cat articles_and_urls.txt | cut -f 2 &gt; unlabelled_articles.txt </code></pre>
<p></p>
<p>I get this error</p>
<p>cut: stdin: Illegal byte sequence</p>
<p></p>
<p>I tried looking it up but nothing has really helped.</p>",Error preparing input data for part 2,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hlicfcesNVC/im1ikvuvripg/Screen_Shot_20160321_at_12.50.01_AM.png"" /></p>
<p></p>
<p>I&#39;m getting this error - the aggregated csv for CrowdFlower for this link predicts that theåÊurlåÊisåÊbroken. How mightåÊthis be resolved? Thanks!åÊ</p>",Error with kappa.py,2
940849047,4/26/2016 16:01:37,true,1969404544,,4/26/2016 16:00:00,false,elite,1.0,33243069,IND,10,Faridabad,116.203.79.150,0,0,"<p>When I use the command</p>
<pre><code> cat articles_and_urls.txt | cut -f 1 &gt; urls.txt 
 cat articles_and_urls.txt | cut -f 2 &gt; unlabelled_articles.txt </code></pre>
<p></p>
<p>I get this error</p>
<p>cut: stdin: Illegal byte sequence</p>
<p></p>
<p>I tried looking it up but nothing has really helped.</p>",Error preparing input data for part 2,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hlicfcesNVC/im1ikvuvripg/Screen_Shot_20160321_at_12.50.01_AM.png"" /></p>
<p></p>
<p>I&#39;m getting this error - the aggregated csv for CrowdFlower for this link predicts that theåÊurlåÊisåÊbroken. How mightåÊthis be resolved? Thanks!åÊ</p>",Error with kappa.py,2
940849047,4/26/2016 16:04:22,true,1969405942,,4/26/2016 15:56:16,false,neodev,0.8889,21971187,TTO,08,Valsayn,190.213.132.190,0,0,"<p>When I use the command</p>
<pre><code> cat articles_and_urls.txt | cut -f 1 &gt; urls.txt 
 cat articles_and_urls.txt | cut -f 2 &gt; unlabelled_articles.txt </code></pre>
<p></p>
<p>I get this error</p>
<p>cut: stdin: Illegal byte sequence</p>
<p></p>
<p>I tried looking it up but nothing has really helped.</p>",Error preparing input data for part 2,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hlicfcesNVC/im1ikvuvripg/Screen_Shot_20160321_at_12.50.01_AM.png"" /></p>
<p></p>
<p>I&#39;m getting this error - the aggregated csv for CrowdFlower for this link predicts that theåÊurlåÊisåÊbroken. How mightåÊthis be resolved? Thanks!åÊ</p>",Error with kappa.py,2
940849047,4/26/2016 16:12:58,true,1969410161,,4/26/2016 16:10:21,false,clixsense,0.8889,8057247,PRT,17,Póvoa De Varzim,144.64.25.68,0,0,"<p>When I use the command</p>
<pre><code> cat articles_and_urls.txt | cut -f 1 &gt; urls.txt 
 cat articles_and_urls.txt | cut -f 2 &gt; unlabelled_articles.txt </code></pre>
<p></p>
<p>I get this error</p>
<p>cut: stdin: Illegal byte sequence</p>
<p></p>
<p>I tried looking it up but nothing has really helped.</p>",Error preparing input data for part 2,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hlicfcesNVC/im1ikvuvripg/Screen_Shot_20160321_at_12.50.01_AM.png"" /></p>
<p></p>
<p>I&#39;m getting this error - the aggregated csv for CrowdFlower for this link predicts that theåÊurlåÊisåÊbroken. How mightåÊthis be resolved? Thanks!åÊ</p>",Error with kappa.py,2
940849047,4/26/2016 16:25:31,true,1969419713,,4/26/2016 16:24:35,false,neodev,1.0,29175140,VEN,25,Caracas,190.72.125.134,0,0,"<p>When I use the command</p>
<pre><code> cat articles_and_urls.txt | cut -f 1 &gt; urls.txt 
 cat articles_and_urls.txt | cut -f 2 &gt; unlabelled_articles.txt </code></pre>
<p></p>
<p>I get this error</p>
<p>cut: stdin: Illegal byte sequence</p>
<p></p>
<p>I tried looking it up but nothing has really helped.</p>",Error preparing input data for part 2,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hlicfcesNVC/im1ikvuvripg/Screen_Shot_20160321_at_12.50.01_AM.png"" /></p>
<p></p>
<p>I&#39;m getting this error - the aggregated csv for CrowdFlower for this link predicts that theåÊurlåÊisåÊbroken. How mightåÊthis be resolved? Thanks!åÊ</p>",Error with kappa.py,2
940849047,4/26/2016 16:40:10,true,1969429581,,4/26/2016 16:37:31,false,neodev,0.7778,32569659,USA,MN,Minneapolis,97.127.88.224,0,0,"<p>When I use the command</p>
<pre><code> cat articles_and_urls.txt | cut -f 1 &gt; urls.txt 
 cat articles_and_urls.txt | cut -f 2 &gt; unlabelled_articles.txt </code></pre>
<p></p>
<p>I get this error</p>
<p>cut: stdin: Illegal byte sequence</p>
<p></p>
<p>I tried looking it up but nothing has really helped.</p>",Error preparing input data for part 2,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hlicfcesNVC/im1ikvuvripg/Screen_Shot_20160321_at_12.50.01_AM.png"" /></p>
<p></p>
<p>I&#39;m getting this error - the aggregated csv for CrowdFlower for this link predicts that theåÊurlåÊisåÊbroken. How mightåÊthis be resolved? Thanks!åÊ</p>",Error with kappa.py,2
940849047,4/26/2016 16:40:25,true,1969429694,,4/26/2016 16:39:20,false,clixsense,1.0,6329782,IDN,07,Bekonang,202.67.40.31,0,0,"<p>When I use the command</p>
<pre><code> cat articles_and_urls.txt | cut -f 1 &gt; urls.txt 
 cat articles_and_urls.txt | cut -f 2 &gt; unlabelled_articles.txt </code></pre>
<p></p>
<p>I get this error</p>
<p>cut: stdin: Illegal byte sequence</p>
<p></p>
<p>I tried looking it up but nothing has really helped.</p>",Error preparing input data for part 2,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hlicfcesNVC/im1ikvuvripg/Screen_Shot_20160321_at_12.50.01_AM.png"" /></p>
<p></p>
<p>I&#39;m getting this error - the aggregated csv for CrowdFlower for this link predicts that theåÊurlåÊisåÊbroken. How mightåÊthis be resolved? Thanks!åÊ</p>",Error with kappa.py,2
940849047,4/26/2016 16:56:40,true,1969439480,,4/26/2016 16:53:35,false,clixsense,1.0,21408115,IDN,07,Semarang,36.79.23.180,0,0,"<p>When I use the command</p>
<pre><code> cat articles_and_urls.txt | cut -f 1 &gt; urls.txt 
 cat articles_and_urls.txt | cut -f 2 &gt; unlabelled_articles.txt </code></pre>
<p></p>
<p>I get this error</p>
<p>cut: stdin: Illegal byte sequence</p>
<p></p>
<p>I tried looking it up but nothing has really helped.</p>",Error preparing input data for part 2,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hlicfcesNVC/im1ikvuvripg/Screen_Shot_20160321_at_12.50.01_AM.png"" /></p>
<p></p>
<p>I&#39;m getting this error - the aggregated csv for CrowdFlower for this link predicts that theåÊurlåÊisåÊbroken. How mightåÊthis be resolved? Thanks!åÊ</p>",Error with kappa.py,2
940849047,4/26/2016 17:02:13,true,1969443064,,4/26/2016 16:59:19,false,neodev,1.0,36167043,GBR,G6,Hull,77.86.101.69,0,0,"<p>When I use the command</p>
<pre><code> cat articles_and_urls.txt | cut -f 1 &gt; urls.txt 
 cat articles_and_urls.txt | cut -f 2 &gt; unlabelled_articles.txt </code></pre>
<p></p>
<p>I get this error</p>
<p>cut: stdin: Illegal byte sequence</p>
<p></p>
<p>I tried looking it up but nothing has really helped.</p>",Error preparing input data for part 2,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hlicfcesNVC/im1ikvuvripg/Screen_Shot_20160321_at_12.50.01_AM.png"" /></p>
<p></p>
<p>I&#39;m getting this error - the aggregated csv for CrowdFlower for this link predicts that theåÊurlåÊisåÊbroken. How mightåÊthis be resolved? Thanks!åÊ</p>",Error with kappa.py,2
940849047,4/26/2016 17:06:41,true,1969445560,,4/26/2016 17:00:39,false,neodev,1.0,33973110,VEN,23,Maracaibo,186.94.238.104,0,0,"<p>When I use the command</p>
<pre><code> cat articles_and_urls.txt | cut -f 1 &gt; urls.txt 
 cat articles_and_urls.txt | cut -f 2 &gt; unlabelled_articles.txt </code></pre>
<p></p>
<p>I get this error</p>
<p>cut: stdin: Illegal byte sequence</p>
<p></p>
<p>I tried looking it up but nothing has really helped.</p>",Error preparing input data for part 2,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hlicfcesNVC/im1ikvuvripg/Screen_Shot_20160321_at_12.50.01_AM.png"" /></p>
<p></p>
<p>I&#39;m getting this error - the aggregated csv for CrowdFlower for this link predicts that theåÊurlåÊisåÊbroken. How mightåÊthis be resolved? Thanks!åÊ</p>",Error with kappa.py,2
940849047,4/26/2016 17:11:04,true,1969448317,,4/26/2016 17:01:16,false,neodev,0.8889,19625264,DZA,41,Chlef,41.102.7.217,0,0,"<p>When I use the command</p>
<pre><code> cat articles_and_urls.txt | cut -f 1 &gt; urls.txt 
 cat articles_and_urls.txt | cut -f 2 &gt; unlabelled_articles.txt </code></pre>
<p></p>
<p>I get this error</p>
<p>cut: stdin: Illegal byte sequence</p>
<p></p>
<p>I tried looking it up but nothing has really helped.</p>",Error preparing input data for part 2,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hlicfcesNVC/im1ikvuvripg/Screen_Shot_20160321_at_12.50.01_AM.png"" /></p>
<p></p>
<p>I&#39;m getting this error - the aggregated csv for CrowdFlower for this link predicts that theåÊurlåÊisåÊbroken. How mightåÊthis be resolved? Thanks!åÊ</p>",Error with kappa.py,2
940849047,4/26/2016 17:14:00,true,1969450328,,4/26/2016 17:10:16,false,elite,1.0,25411289,HRV,"","",31.147.119.175,0,0,"<p>When I use the command</p>
<pre><code> cat articles_and_urls.txt | cut -f 1 &gt; urls.txt 
 cat articles_and_urls.txt | cut -f 2 &gt; unlabelled_articles.txt </code></pre>
<p></p>
<p>I get this error</p>
<p>cut: stdin: Illegal byte sequence</p>
<p></p>
<p>I tried looking it up but nothing has really helped.</p>",Error preparing input data for part 2,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hlicfcesNVC/im1ikvuvripg/Screen_Shot_20160321_at_12.50.01_AM.png"" /></p>
<p></p>
<p>I&#39;m getting this error - the aggregated csv for CrowdFlower for this link predicts that theåÊurlåÊisåÊbroken. How mightåÊthis be resolved? Thanks!åÊ</p>",Error with kappa.py,2
940849047,4/26/2016 17:31:21,true,1969460302,,4/26/2016 17:29:58,false,neodev,0.8889,33131546,IDN,04,Jakarta,139.194.89.60,0,0,"<p>When I use the command</p>
<pre><code> cat articles_and_urls.txt | cut -f 1 &gt; urls.txt 
 cat articles_and_urls.txt | cut -f 2 &gt; unlabelled_articles.txt </code></pre>
<p></p>
<p>I get this error</p>
<p>cut: stdin: Illegal byte sequence</p>
<p></p>
<p>I tried looking it up but nothing has really helped.</p>",Error preparing input data for part 2,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hlicfcesNVC/im1ikvuvripg/Screen_Shot_20160321_at_12.50.01_AM.png"" /></p>
<p></p>
<p>I&#39;m getting this error - the aggregated csv for CrowdFlower for this link predicts that theåÊurlåÊisåÊbroken. How mightåÊthis be resolved? Thanks!åÊ</p>",Error with kappa.py,2
940849047,4/26/2016 17:33:45,true,1969461565,,4/26/2016 17:33:17,false,clixsense,1.0,30712378,ROU,21,Deva,79.119.241.200,0,0,"<p>When I use the command</p>
<pre><code> cat articles_and_urls.txt | cut -f 1 &gt; urls.txt 
 cat articles_and_urls.txt | cut -f 2 &gt; unlabelled_articles.txt </code></pre>
<p></p>
<p>I get this error</p>
<p>cut: stdin: Illegal byte sequence</p>
<p></p>
<p>I tried looking it up but nothing has really helped.</p>",Error preparing input data for part 2,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hlicfcesNVC/im1ikvuvripg/Screen_Shot_20160321_at_12.50.01_AM.png"" /></p>
<p></p>
<p>I&#39;m getting this error - the aggregated csv for CrowdFlower for this link predicts that theåÊurlåÊisåÊbroken. How mightåÊthis be resolved? Thanks!åÊ</p>",Error with kappa.py,2
940849047,4/26/2016 17:38:55,true,1969464254,,4/26/2016 17:36:08,false,neodev,0.8889,33568303,VEN,23,Cabimas,190.77.7.36,0,0,"<p>When I use the command</p>
<pre><code> cat articles_and_urls.txt | cut -f 1 &gt; urls.txt 
 cat articles_and_urls.txt | cut -f 2 &gt; unlabelled_articles.txt </code></pre>
<p></p>
<p>I get this error</p>
<p>cut: stdin: Illegal byte sequence</p>
<p></p>
<p>I tried looking it up but nothing has really helped.</p>",Error preparing input data for part 2,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hlicfcesNVC/im1ikvuvripg/Screen_Shot_20160321_at_12.50.01_AM.png"" /></p>
<p></p>
<p>I&#39;m getting this error - the aggregated csv for CrowdFlower for this link predicts that theåÊurlåÊisåÊbroken. How mightåÊthis be resolved? Thanks!åÊ</p>",Error with kappa.py,2
940849047,4/26/2016 17:46:50,true,1969469060,,4/26/2016 17:32:15,false,clixsense,0.8889,35338593,ITA,14,Cagliari,151.56.132.145,0,0,"<p>When I use the command</p>
<pre><code> cat articles_and_urls.txt | cut -f 1 &gt; urls.txt 
 cat articles_and_urls.txt | cut -f 2 &gt; unlabelled_articles.txt </code></pre>
<p></p>
<p>I get this error</p>
<p>cut: stdin: Illegal byte sequence</p>
<p></p>
<p>I tried looking it up but nothing has really helped.</p>",Error preparing input data for part 2,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hlicfcesNVC/im1ikvuvripg/Screen_Shot_20160321_at_12.50.01_AM.png"" /></p>
<p></p>
<p>I&#39;m getting this error - the aggregated csv for CrowdFlower for this link predicts that theåÊurlåÊisåÊbroken. How mightåÊthis be resolved? Thanks!åÊ</p>",Error with kappa.py,2
940849047,4/26/2016 17:49:32,true,1969470402,,4/26/2016 17:45:16,false,clixsense,1.0,35444326,BRA,07,Brasília,177.15.130.106,0,0,"<p>When I use the command</p>
<pre><code> cat articles_and_urls.txt | cut -f 1 &gt; urls.txt 
 cat articles_and_urls.txt | cut -f 2 &gt; unlabelled_articles.txt </code></pre>
<p></p>
<p>I get this error</p>
<p>cut: stdin: Illegal byte sequence</p>
<p></p>
<p>I tried looking it up but nothing has really helped.</p>",Error preparing input data for part 2,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hlicfcesNVC/im1ikvuvripg/Screen_Shot_20160321_at_12.50.01_AM.png"" /></p>
<p></p>
<p>I&#39;m getting this error - the aggregated csv for CrowdFlower for this link predicts that theåÊurlåÊisåÊbroken. How mightåÊthis be resolved? Thanks!åÊ</p>",Error with kappa.py,2
940849047,4/26/2016 18:16:16,true,1969484749,,4/26/2016 18:07:59,false,neodev,0.8889,35550011,VEN,07,Valencia,190.204.238.112,0,0,"<p>When I use the command</p>
<pre><code> cat articles_and_urls.txt | cut -f 1 &gt; urls.txt 
 cat articles_and_urls.txt | cut -f 2 &gt; unlabelled_articles.txt </code></pre>
<p></p>
<p>I get this error</p>
<p>cut: stdin: Illegal byte sequence</p>
<p></p>
<p>I tried looking it up but nothing has really helped.</p>",Error preparing input data for part 2,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hlicfcesNVC/im1ikvuvripg/Screen_Shot_20160321_at_12.50.01_AM.png"" /></p>
<p></p>
<p>I&#39;m getting this error - the aggregated csv for CrowdFlower for this link predicts that theåÊurlåÊisåÊbroken. How mightåÊthis be resolved? Thanks!åÊ</p>",Error with kappa.py,2
940849047,4/26/2016 18:24:14,true,1969488803,,4/26/2016 18:22:47,false,elite,1.0,30128662,BGR,50,Pleven,212.233.177.195,0,0,"<p>When I use the command</p>
<pre><code> cat articles_and_urls.txt | cut -f 1 &gt; urls.txt 
 cat articles_and_urls.txt | cut -f 2 &gt; unlabelled_articles.txt </code></pre>
<p></p>
<p>I get this error</p>
<p>cut: stdin: Illegal byte sequence</p>
<p></p>
<p>I tried looking it up but nothing has really helped.</p>",Error preparing input data for part 2,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hlicfcesNVC/im1ikvuvripg/Screen_Shot_20160321_at_12.50.01_AM.png"" /></p>
<p></p>
<p>I&#39;m getting this error - the aggregated csv for CrowdFlower for this link predicts that theåÊurlåÊisåÊbroken. How mightåÊthis be resolved? Thanks!åÊ</p>",Error with kappa.py,2
940849047,4/26/2016 18:33:38,true,1969493271,,4/26/2016 18:19:15,false,neodev,1.0,29879245,RUS,69,Smolensk,37.144.124.118,0,0,"<p>When I use the command</p>
<pre><code> cat articles_and_urls.txt | cut -f 1 &gt; urls.txt 
 cat articles_and_urls.txt | cut -f 2 &gt; unlabelled_articles.txt </code></pre>
<p></p>
<p>I get this error</p>
<p>cut: stdin: Illegal byte sequence</p>
<p></p>
<p>I tried looking it up but nothing has really helped.</p>",Error preparing input data for part 2,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hlicfcesNVC/im1ikvuvripg/Screen_Shot_20160321_at_12.50.01_AM.png"" /></p>
<p></p>
<p>I&#39;m getting this error - the aggregated csv for CrowdFlower for this link predicts that theåÊurlåÊisåÊbroken. How mightåÊthis be resolved? Thanks!åÊ</p>",Error with kappa.py,2
940849047,4/26/2016 19:25:38,true,1969519497,,4/26/2016 19:25:12,false,tremorgames,1.0,25197223,HRV,15,Split,94.253.234.240,0,0,"<p>When I use the command</p>
<pre><code> cat articles_and_urls.txt | cut -f 1 &gt; urls.txt 
 cat articles_and_urls.txt | cut -f 2 &gt; unlabelled_articles.txt </code></pre>
<p></p>
<p>I get this error</p>
<p>cut: stdin: Illegal byte sequence</p>
<p></p>
<p>I tried looking it up but nothing has really helped.</p>",Error preparing input data for part 2,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hlicfcesNVC/im1ikvuvripg/Screen_Shot_20160321_at_12.50.01_AM.png"" /></p>
<p></p>
<p>I&#39;m getting this error - the aggregated csv for CrowdFlower for this link predicts that theåÊurlåÊisåÊbroken. How mightåÊthis be resolved? Thanks!åÊ</p>",Error with kappa.py,2
940849303,4/26/2016 15:11:59,true,1969364460,,4/26/2016 15:11:46,false,tremorgames,1.0,32635967,LTU,60,Panevezys,78.63.38.165,0,0,"<p>Hi, so I read though the other piazza post about how the statistical analysis takes a long time, but I am still having some trouble with my code.</p>
<p></p>
<p>I have already tried waiting 10-15 minutes and removing all stopwords, but for some reason the statistical analysis refuses to complete in a reasonable amount of time</p>
<p></p>
<p>Below is my code:</p>
<pre>def isStopWord(word):
	with open(&#39;stopwords.txt&#39;, &#39;r&#39;) as stopwords:
		for line in stopwords:
			if word in line:
				return True
		return False

def get_features(X) : 
	features = []
	for x in X : 
		f = {}
		#TODO replace this dummy feature function with a unigram model, like we did in class
		for word in x.split():
			if isStopWord(word):
				continue
			if word in f:
				f[word] = f[word] &#43; 1.0
			else:
				f[word] =  1.0
		features.append(f)
	return features</pre>
<p>Not really sure what to do at this point because of this. Also might this be related to using the VM?</p>",Statistical Analysis Taking A Very Long Time,It takes a really long time. Is that expected?,statistical classification runtime,"<p>My script runs forever on my virtual machine, so I&#39;m running it off of biglab. I got the statistical analysis to work once (took about 2 minutes), but after un-commenting get_top_features and get_misclassified, my script gets killed before finishing the initial statistical analysis. I&#39;ve tried several times and I&#39;m unsure what the issue is; I&#39;ve copied my code below.</p>
<p></p>
<pre>#!/bin/python<br /><br />import os<br />import sys<br />import string<br />import random<br />import operator<br />from sklearn.tree import export_graphviz<br />from sklearn.tree import DecisionTreeClassifier<br />from sklearn.naive_bayes import MultinomialNB<br />from sklearn.linear_model import LogisticRegression<br />from sklearn.preprocessing import LabelEncoder<br />from sklearn.feature_extraction import DictVectorizer<br />from sklearn.cross_validation import train_test_split<br />from sklearn.externals.six import StringIO  <br /><br />#read in raw data from file and return a list of (label, article) tuples<br />def get_data(filename): <br />	data = [line.strip().split(&#39;\t&#39;) for line in open(filename).readlines()]<br />	random.shuffle(data)<br />	return data<br /><br />#this function builds the feature matrix for the Decision Tree.<br />def get_dtree_features(X) :<br />	features = []<br />	#TODO : Add the features you would like to use to train the Decision Tree here.<br />	feature_list = [&#39;gun&#39;, &#39;shooter&#39;, &#39;shot&#39;, &#39;shooting&#39;, &#39;player&#39;, &#39;coach&#39;, &#39;game&#39;, &#39;movie&#39;, &#39;actor&#39;, &#39;film&#39;, &#39;bullet&#39;, &#39;photo&#39;, &#39;model&#39;, &#39;point&#39;, &#39;suspect&#39;, &#39;victim&#39;, &#39;police&#39;, &#39;fired&#39;]<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w in feature_list : <br />				f[w] = 1.0<br />		features.append(f)<br />	return features<br /><br /><br />#this is the main function you care about; pack all the cleverest features you can think of into here.<br />def get_features(X) :<br />	stopwords = open(&#39;stopwords.txt&#39;).read()<br />	features = []<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w not in stopwords:            <br />				if w not in f : <br />					f[w] = 0<br />				f[w] &#43;= 1                <br />		features.append(f)<br />	return features<br /><br />#vectorize feature dictionaries and return feature and label matricies<br />def get_matricies(data, typ=&#34;unigram&#34;) : <br />	dv = DictVectorizer(sparse=True) <br />	le = LabelEncoder()<br />	y = [d[0] for d in data]<br />	texts = [d[1] for d in data]<br />	if typ == &#34;tree&#34;:<br />		X = get_dtree_features(texts)<br />	else :<br />		X = get_features(texts)<br />	#Here we are returning 5 things, the label vector y and feature matrix X, but also the texts from which the features were extracted and the <br />	#objects that were used to encode them. These will come in handy for your analysis, but you can ignore them for the initial parts of the assignment<br />	return le.fit_transform(y), dv.fit_transform(X), texts, dv, le<br /><br />#train and multinomial naive bayes classifier<br />def train_classifier(X, y):<br />	clf = LogisticRegression()<br />	clf.fit(X,y)<br />	return clf <br /><br />#train a Decision Tree classifier<br />def train_dtree_classifier(X, y):<br />	clf = DecisionTreeClassifier(max_depth=10)<br />	clf.fit(X,y)<br />	return clf<br /><br />#test the classifier<br />def test_classifier(clf, X, y):<br />	return clf.score(X,y)<br /><br />#cross validation	<br />def cross_validate(X, y, dv=None, typ=&#34;unigram&#34;, numfolds=5,):<br />	test_accs = []<br />	split = 1.0 / numfolds<br />	for i in range(numfolds):<br />		x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=i)<br />		if typ == &#34;tree&#34; :<br />			clf = train_dtree_classifier(x_train, y_train)<br />		else :<br />			clf = train_classifier(x_train, y_train)<br />		test_acc = test_classifier(clf, x_test, y_test)<br />		test_accs.append(test_acc)<br />		print &#39;Fold %d : %.05f&#39;%(i,test_acc)<br />	test_average = float(sum(test_accs))/ numfolds<br />	if typ == &#34;tree&#34; :<br />		with open(&#34;output.dot&#34;, &#39;w&#39;) as f:<br />			f = export_graphviz(clf, out_file=f, feature_names=dv.get_feature_names(), class_names=[&#39;Non Gun Related&#39;,&#39;Gun Related&#39;])<br />		create_graph(&#34;decision-tree.png&#34;)<br />	print &#39;Test Average : %.05f&#39;%(test_average)<br />	print<br />	return test_average<br /><br />#run a rule based classifier and calculate the accuracy<br />def rule_based_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO add more keywords, see how well they do alone and in combination<br />		if &#34;shooter&#34; in text or &#34;shot&#34; in text or &#34;shooting&#34; in text :<br />			if &#34;player&#34; in text or &#34;coach&#34; in text or &#34;game&#34; in text: <br />				prediction = &#39;0&#39;<br />			elif &#34;movie&#34; in text or &#34;film&#34; in text or &#34;actor&#34; in text:<br />				prediction = &#39;0&#39;<br />			else : prediction = &#39;1&#39;<br />		if &#34;gun&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Rule-based classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#Extra Credit Rule Based Classifier<br />def extra_credit_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO develop your conditional statements here<br />		if &#34;shooting&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Reverse Engineered classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#train and multinomial naive bayes classifier<br />def get_top_features(X, y, dv):<br />	clf = train_classifier(X, y)<br />	#the DictVectorizer object remembers which column number corresponds to which feature, and return the feature names in the correct order<br />	feature_names = dv.get_feature_names() <br /><br />	#The below code will get the weights from the classifier, and print out the weights of the features you are interested in<br />	features = [] #this will be a list of (feature_idx, weight) tuples<br />	for i,w in enumerate(clf.coef_[0]): <br />		features.append((i,w))<br />	#Sort the list by values, with the largest ones first<br />	features = sorted(features, key=lambda e: e[1], reverse=True)<br /><br />        #Print out the feature names and thier weights<br />	for i,w in features:<br />	  print &#39;%s\t%s&#39;%(feature_names[i], w)<br /><br />def get_misclassified_examples(y, X, texts) :<br />	x_train, x_test, y_train, y_test, train_texts, test_texts = train_test_split(X, y, texts)<br />	clf = train_classifier(x_train, y_train)<br /><br />	#TODO: You will have to write some code to call your classifier on each of the test examples, and check whether its prediction was right or wrong<br /><br />def create_graph(file_name) :<br />	os.system(&#34;dot -Tpng output.dot -o &#34; &#43; file_name)<br />	os.unlink(&#34;output.dot&#34;)<br /><br />if __name__ == &#39;__main__&#39; : <br /><br />	raw_data = get_data(sys.argv[1])<br />	<br />	print &#39;\nRule-based classification&#39;<br />	rule_based_classifier(raw_data)<br /><br />################ Decision Tree ################<br /><br /># print &#39;\nDecision Tree classification&#39;<br /># y, X, texts, dv, le = get_matricies(raw_data, &#34;tree&#34;)<br /># cross_validate(X,y,dv,&#34;tree&#34;)<br /><br />################ Statistical Classification ################<br />print &#39;\nStatistical classification&#39;<br />y, X, texts, dv, le = get_matricies(raw_data)<br />cross_validate(X,y)<br /><br />get_top_features(X, y, dv)<br />get_misclassified_examples(y, X, texts)<br />
</pre>
<p>Thanks in advance for any help you can give.</p>",Statistical Analysis &#34;Killed&#34; - even on biglab,"<p>When my program gets to the Statistical Analysis bit, it makes it through all the words and creates wordcount dictionaries that are appended to the list of features. Then it hangs for a bit until the terminal just prints &#34;Killed&#34; and program ends. Any idea what might be causing this? Not enough memory?</p>",Statistical Analysis &#34;Killed&#34;,,,"<p>I&#39;m confused on what exactly we&#39;re supposed to do for part 6. Are the y dimensions the same as the number of articles? Also, how is logistic regression supposed to be used here?</p>",Statistical Analysis Part 6,0
940849303,4/26/2016 15:15:42,true,1969369978,,4/26/2016 15:14:06,false,clixsense,1.0,24287706,TWN,04,Keelung,61.231.195.173,0,0,"<p>Hi, so I read though the other piazza post about how the statistical analysis takes a long time, but I am still having some trouble with my code.</p>
<p></p>
<p>I have already tried waiting 10-15 minutes and removing all stopwords, but for some reason the statistical analysis refuses to complete in a reasonable amount of time</p>
<p></p>
<p>Below is my code:</p>
<pre>def isStopWord(word):
	with open(&#39;stopwords.txt&#39;, &#39;r&#39;) as stopwords:
		for line in stopwords:
			if word in line:
				return True
		return False

def get_features(X) : 
	features = []
	for x in X : 
		f = {}
		#TODO replace this dummy feature function with a unigram model, like we did in class
		for word in x.split():
			if isStopWord(word):
				continue
			if word in f:
				f[word] = f[word] &#43; 1.0
			else:
				f[word] =  1.0
		features.append(f)
	return features</pre>
<p>Not really sure what to do at this point because of this. Also might this be related to using the VM?</p>",Statistical Analysis Taking A Very Long Time,It takes a really long time. Is that expected?,statistical classification runtime,"<p>My script runs forever on my virtual machine, so I&#39;m running it off of biglab. I got the statistical analysis to work once (took about 2 minutes), but after un-commenting get_top_features and get_misclassified, my script gets killed before finishing the initial statistical analysis. I&#39;ve tried several times and I&#39;m unsure what the issue is; I&#39;ve copied my code below.</p>
<p></p>
<pre>#!/bin/python<br /><br />import os<br />import sys<br />import string<br />import random<br />import operator<br />from sklearn.tree import export_graphviz<br />from sklearn.tree import DecisionTreeClassifier<br />from sklearn.naive_bayes import MultinomialNB<br />from sklearn.linear_model import LogisticRegression<br />from sklearn.preprocessing import LabelEncoder<br />from sklearn.feature_extraction import DictVectorizer<br />from sklearn.cross_validation import train_test_split<br />from sklearn.externals.six import StringIO  <br /><br />#read in raw data from file and return a list of (label, article) tuples<br />def get_data(filename): <br />	data = [line.strip().split(&#39;\t&#39;) for line in open(filename).readlines()]<br />	random.shuffle(data)<br />	return data<br /><br />#this function builds the feature matrix for the Decision Tree.<br />def get_dtree_features(X) :<br />	features = []<br />	#TODO : Add the features you would like to use to train the Decision Tree here.<br />	feature_list = [&#39;gun&#39;, &#39;shooter&#39;, &#39;shot&#39;, &#39;shooting&#39;, &#39;player&#39;, &#39;coach&#39;, &#39;game&#39;, &#39;movie&#39;, &#39;actor&#39;, &#39;film&#39;, &#39;bullet&#39;, &#39;photo&#39;, &#39;model&#39;, &#39;point&#39;, &#39;suspect&#39;, &#39;victim&#39;, &#39;police&#39;, &#39;fired&#39;]<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w in feature_list : <br />				f[w] = 1.0<br />		features.append(f)<br />	return features<br /><br /><br />#this is the main function you care about; pack all the cleverest features you can think of into here.<br />def get_features(X) :<br />	stopwords = open(&#39;stopwords.txt&#39;).read()<br />	features = []<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w not in stopwords:            <br />				if w not in f : <br />					f[w] = 0<br />				f[w] &#43;= 1                <br />		features.append(f)<br />	return features<br /><br />#vectorize feature dictionaries and return feature and label matricies<br />def get_matricies(data, typ=&#34;unigram&#34;) : <br />	dv = DictVectorizer(sparse=True) <br />	le = LabelEncoder()<br />	y = [d[0] for d in data]<br />	texts = [d[1] for d in data]<br />	if typ == &#34;tree&#34;:<br />		X = get_dtree_features(texts)<br />	else :<br />		X = get_features(texts)<br />	#Here we are returning 5 things, the label vector y and feature matrix X, but also the texts from which the features were extracted and the <br />	#objects that were used to encode them. These will come in handy for your analysis, but you can ignore them for the initial parts of the assignment<br />	return le.fit_transform(y), dv.fit_transform(X), texts, dv, le<br /><br />#train and multinomial naive bayes classifier<br />def train_classifier(X, y):<br />	clf = LogisticRegression()<br />	clf.fit(X,y)<br />	return clf <br /><br />#train a Decision Tree classifier<br />def train_dtree_classifier(X, y):<br />	clf = DecisionTreeClassifier(max_depth=10)<br />	clf.fit(X,y)<br />	return clf<br /><br />#test the classifier<br />def test_classifier(clf, X, y):<br />	return clf.score(X,y)<br /><br />#cross validation	<br />def cross_validate(X, y, dv=None, typ=&#34;unigram&#34;, numfolds=5,):<br />	test_accs = []<br />	split = 1.0 / numfolds<br />	for i in range(numfolds):<br />		x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=i)<br />		if typ == &#34;tree&#34; :<br />			clf = train_dtree_classifier(x_train, y_train)<br />		else :<br />			clf = train_classifier(x_train, y_train)<br />		test_acc = test_classifier(clf, x_test, y_test)<br />		test_accs.append(test_acc)<br />		print &#39;Fold %d : %.05f&#39;%(i,test_acc)<br />	test_average = float(sum(test_accs))/ numfolds<br />	if typ == &#34;tree&#34; :<br />		with open(&#34;output.dot&#34;, &#39;w&#39;) as f:<br />			f = export_graphviz(clf, out_file=f, feature_names=dv.get_feature_names(), class_names=[&#39;Non Gun Related&#39;,&#39;Gun Related&#39;])<br />		create_graph(&#34;decision-tree.png&#34;)<br />	print &#39;Test Average : %.05f&#39;%(test_average)<br />	print<br />	return test_average<br /><br />#run a rule based classifier and calculate the accuracy<br />def rule_based_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO add more keywords, see how well they do alone and in combination<br />		if &#34;shooter&#34; in text or &#34;shot&#34; in text or &#34;shooting&#34; in text :<br />			if &#34;player&#34; in text or &#34;coach&#34; in text or &#34;game&#34; in text: <br />				prediction = &#39;0&#39;<br />			elif &#34;movie&#34; in text or &#34;film&#34; in text or &#34;actor&#34; in text:<br />				prediction = &#39;0&#39;<br />			else : prediction = &#39;1&#39;<br />		if &#34;gun&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Rule-based classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#Extra Credit Rule Based Classifier<br />def extra_credit_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO develop your conditional statements here<br />		if &#34;shooting&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Reverse Engineered classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#train and multinomial naive bayes classifier<br />def get_top_features(X, y, dv):<br />	clf = train_classifier(X, y)<br />	#the DictVectorizer object remembers which column number corresponds to which feature, and return the feature names in the correct order<br />	feature_names = dv.get_feature_names() <br /><br />	#The below code will get the weights from the classifier, and print out the weights of the features you are interested in<br />	features = [] #this will be a list of (feature_idx, weight) tuples<br />	for i,w in enumerate(clf.coef_[0]): <br />		features.append((i,w))<br />	#Sort the list by values, with the largest ones first<br />	features = sorted(features, key=lambda e: e[1], reverse=True)<br /><br />        #Print out the feature names and thier weights<br />	for i,w in features:<br />	  print &#39;%s\t%s&#39;%(feature_names[i], w)<br /><br />def get_misclassified_examples(y, X, texts) :<br />	x_train, x_test, y_train, y_test, train_texts, test_texts = train_test_split(X, y, texts)<br />	clf = train_classifier(x_train, y_train)<br /><br />	#TODO: You will have to write some code to call your classifier on each of the test examples, and check whether its prediction was right or wrong<br /><br />def create_graph(file_name) :<br />	os.system(&#34;dot -Tpng output.dot -o &#34; &#43; file_name)<br />	os.unlink(&#34;output.dot&#34;)<br /><br />if __name__ == &#39;__main__&#39; : <br /><br />	raw_data = get_data(sys.argv[1])<br />	<br />	print &#39;\nRule-based classification&#39;<br />	rule_based_classifier(raw_data)<br /><br />################ Decision Tree ################<br /><br /># print &#39;\nDecision Tree classification&#39;<br /># y, X, texts, dv, le = get_matricies(raw_data, &#34;tree&#34;)<br /># cross_validate(X,y,dv,&#34;tree&#34;)<br /><br />################ Statistical Classification ################<br />print &#39;\nStatistical classification&#39;<br />y, X, texts, dv, le = get_matricies(raw_data)<br />cross_validate(X,y)<br /><br />get_top_features(X, y, dv)<br />get_misclassified_examples(y, X, texts)<br />
</pre>
<p>Thanks in advance for any help you can give.</p>",Statistical Analysis &#34;Killed&#34; - even on biglab,"<p>When my program gets to the Statistical Analysis bit, it makes it through all the words and creates wordcount dictionaries that are appended to the list of features. Then it hangs for a bit until the terminal just prints &#34;Killed&#34; and program ends. Any idea what might be causing this? Not enough memory?</p>",Statistical Analysis &#34;Killed&#34;,,,"<p>I&#39;m confused on what exactly we&#39;re supposed to do for part 6. Are the y dimensions the same as the number of articles? Also, how is logistic regression supposed to be used here?</p>",Statistical Analysis Part 6,0
940849303,4/26/2016 15:19:29,true,1969376228,,4/26/2016 15:14:50,false,neodev,1.0,19132694,LKA,36,Colombo,123.231.124.170,0,0,"<p>Hi, so I read though the other piazza post about how the statistical analysis takes a long time, but I am still having some trouble with my code.</p>
<p></p>
<p>I have already tried waiting 10-15 minutes and removing all stopwords, but for some reason the statistical analysis refuses to complete in a reasonable amount of time</p>
<p></p>
<p>Below is my code:</p>
<pre>def isStopWord(word):
	with open(&#39;stopwords.txt&#39;, &#39;r&#39;) as stopwords:
		for line in stopwords:
			if word in line:
				return True
		return False

def get_features(X) : 
	features = []
	for x in X : 
		f = {}
		#TODO replace this dummy feature function with a unigram model, like we did in class
		for word in x.split():
			if isStopWord(word):
				continue
			if word in f:
				f[word] = f[word] &#43; 1.0
			else:
				f[word] =  1.0
		features.append(f)
	return features</pre>
<p>Not really sure what to do at this point because of this. Also might this be related to using the VM?</p>",Statistical Analysis Taking A Very Long Time,It takes a really long time. Is that expected?,statistical classification runtime,"<p>My script runs forever on my virtual machine, so I&#39;m running it off of biglab. I got the statistical analysis to work once (took about 2 minutes), but after un-commenting get_top_features and get_misclassified, my script gets killed before finishing the initial statistical analysis. I&#39;ve tried several times and I&#39;m unsure what the issue is; I&#39;ve copied my code below.</p>
<p></p>
<pre>#!/bin/python<br /><br />import os<br />import sys<br />import string<br />import random<br />import operator<br />from sklearn.tree import export_graphviz<br />from sklearn.tree import DecisionTreeClassifier<br />from sklearn.naive_bayes import MultinomialNB<br />from sklearn.linear_model import LogisticRegression<br />from sklearn.preprocessing import LabelEncoder<br />from sklearn.feature_extraction import DictVectorizer<br />from sklearn.cross_validation import train_test_split<br />from sklearn.externals.six import StringIO  <br /><br />#read in raw data from file and return a list of (label, article) tuples<br />def get_data(filename): <br />	data = [line.strip().split(&#39;\t&#39;) for line in open(filename).readlines()]<br />	random.shuffle(data)<br />	return data<br /><br />#this function builds the feature matrix for the Decision Tree.<br />def get_dtree_features(X) :<br />	features = []<br />	#TODO : Add the features you would like to use to train the Decision Tree here.<br />	feature_list = [&#39;gun&#39;, &#39;shooter&#39;, &#39;shot&#39;, &#39;shooting&#39;, &#39;player&#39;, &#39;coach&#39;, &#39;game&#39;, &#39;movie&#39;, &#39;actor&#39;, &#39;film&#39;, &#39;bullet&#39;, &#39;photo&#39;, &#39;model&#39;, &#39;point&#39;, &#39;suspect&#39;, &#39;victim&#39;, &#39;police&#39;, &#39;fired&#39;]<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w in feature_list : <br />				f[w] = 1.0<br />		features.append(f)<br />	return features<br /><br /><br />#this is the main function you care about; pack all the cleverest features you can think of into here.<br />def get_features(X) :<br />	stopwords = open(&#39;stopwords.txt&#39;).read()<br />	features = []<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w not in stopwords:            <br />				if w not in f : <br />					f[w] = 0<br />				f[w] &#43;= 1                <br />		features.append(f)<br />	return features<br /><br />#vectorize feature dictionaries and return feature and label matricies<br />def get_matricies(data, typ=&#34;unigram&#34;) : <br />	dv = DictVectorizer(sparse=True) <br />	le = LabelEncoder()<br />	y = [d[0] for d in data]<br />	texts = [d[1] for d in data]<br />	if typ == &#34;tree&#34;:<br />		X = get_dtree_features(texts)<br />	else :<br />		X = get_features(texts)<br />	#Here we are returning 5 things, the label vector y and feature matrix X, but also the texts from which the features were extracted and the <br />	#objects that were used to encode them. These will come in handy for your analysis, but you can ignore them for the initial parts of the assignment<br />	return le.fit_transform(y), dv.fit_transform(X), texts, dv, le<br /><br />#train and multinomial naive bayes classifier<br />def train_classifier(X, y):<br />	clf = LogisticRegression()<br />	clf.fit(X,y)<br />	return clf <br /><br />#train a Decision Tree classifier<br />def train_dtree_classifier(X, y):<br />	clf = DecisionTreeClassifier(max_depth=10)<br />	clf.fit(X,y)<br />	return clf<br /><br />#test the classifier<br />def test_classifier(clf, X, y):<br />	return clf.score(X,y)<br /><br />#cross validation	<br />def cross_validate(X, y, dv=None, typ=&#34;unigram&#34;, numfolds=5,):<br />	test_accs = []<br />	split = 1.0 / numfolds<br />	for i in range(numfolds):<br />		x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=i)<br />		if typ == &#34;tree&#34; :<br />			clf = train_dtree_classifier(x_train, y_train)<br />		else :<br />			clf = train_classifier(x_train, y_train)<br />		test_acc = test_classifier(clf, x_test, y_test)<br />		test_accs.append(test_acc)<br />		print &#39;Fold %d : %.05f&#39;%(i,test_acc)<br />	test_average = float(sum(test_accs))/ numfolds<br />	if typ == &#34;tree&#34; :<br />		with open(&#34;output.dot&#34;, &#39;w&#39;) as f:<br />			f = export_graphviz(clf, out_file=f, feature_names=dv.get_feature_names(), class_names=[&#39;Non Gun Related&#39;,&#39;Gun Related&#39;])<br />		create_graph(&#34;decision-tree.png&#34;)<br />	print &#39;Test Average : %.05f&#39;%(test_average)<br />	print<br />	return test_average<br /><br />#run a rule based classifier and calculate the accuracy<br />def rule_based_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO add more keywords, see how well they do alone and in combination<br />		if &#34;shooter&#34; in text or &#34;shot&#34; in text or &#34;shooting&#34; in text :<br />			if &#34;player&#34; in text or &#34;coach&#34; in text or &#34;game&#34; in text: <br />				prediction = &#39;0&#39;<br />			elif &#34;movie&#34; in text or &#34;film&#34; in text or &#34;actor&#34; in text:<br />				prediction = &#39;0&#39;<br />			else : prediction = &#39;1&#39;<br />		if &#34;gun&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Rule-based classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#Extra Credit Rule Based Classifier<br />def extra_credit_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO develop your conditional statements here<br />		if &#34;shooting&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Reverse Engineered classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#train and multinomial naive bayes classifier<br />def get_top_features(X, y, dv):<br />	clf = train_classifier(X, y)<br />	#the DictVectorizer object remembers which column number corresponds to which feature, and return the feature names in the correct order<br />	feature_names = dv.get_feature_names() <br /><br />	#The below code will get the weights from the classifier, and print out the weights of the features you are interested in<br />	features = [] #this will be a list of (feature_idx, weight) tuples<br />	for i,w in enumerate(clf.coef_[0]): <br />		features.append((i,w))<br />	#Sort the list by values, with the largest ones first<br />	features = sorted(features, key=lambda e: e[1], reverse=True)<br /><br />        #Print out the feature names and thier weights<br />	for i,w in features:<br />	  print &#39;%s\t%s&#39;%(feature_names[i], w)<br /><br />def get_misclassified_examples(y, X, texts) :<br />	x_train, x_test, y_train, y_test, train_texts, test_texts = train_test_split(X, y, texts)<br />	clf = train_classifier(x_train, y_train)<br /><br />	#TODO: You will have to write some code to call your classifier on each of the test examples, and check whether its prediction was right or wrong<br /><br />def create_graph(file_name) :<br />	os.system(&#34;dot -Tpng output.dot -o &#34; &#43; file_name)<br />	os.unlink(&#34;output.dot&#34;)<br /><br />if __name__ == &#39;__main__&#39; : <br /><br />	raw_data = get_data(sys.argv[1])<br />	<br />	print &#39;\nRule-based classification&#39;<br />	rule_based_classifier(raw_data)<br /><br />################ Decision Tree ################<br /><br /># print &#39;\nDecision Tree classification&#39;<br /># y, X, texts, dv, le = get_matricies(raw_data, &#34;tree&#34;)<br /># cross_validate(X,y,dv,&#34;tree&#34;)<br /><br />################ Statistical Classification ################<br />print &#39;\nStatistical classification&#39;<br />y, X, texts, dv, le = get_matricies(raw_data)<br />cross_validate(X,y)<br /><br />get_top_features(X, y, dv)<br />get_misclassified_examples(y, X, texts)<br />
</pre>
<p>Thanks in advance for any help you can give.</p>",Statistical Analysis &#34;Killed&#34; - even on biglab,"<p>When my program gets to the Statistical Analysis bit, it makes it through all the words and creates wordcount dictionaries that are appended to the list of features. Then it hangs for a bit until the terminal just prints &#34;Killed&#34; and program ends. Any idea what might be causing this? Not enough memory?</p>",Statistical Analysis &#34;Killed&#34;,,,"<p>I&#39;m confused on what exactly we&#39;re supposed to do for part 6. Are the y dimensions the same as the number of articles? Also, how is logistic regression supposed to be used here?</p>",Statistical Analysis Part 6,0
940849303,4/26/2016 15:21:31,true,1969379542,,4/26/2016 15:12:37,false,clixsense,0.8889,36052512,PHL,F2,Quezon City,49.149.150.150,0,0,"<p>Hi, so I read though the other piazza post about how the statistical analysis takes a long time, but I am still having some trouble with my code.</p>
<p></p>
<p>I have already tried waiting 10-15 minutes and removing all stopwords, but for some reason the statistical analysis refuses to complete in a reasonable amount of time</p>
<p></p>
<p>Below is my code:</p>
<pre>def isStopWord(word):
	with open(&#39;stopwords.txt&#39;, &#39;r&#39;) as stopwords:
		for line in stopwords:
			if word in line:
				return True
		return False

def get_features(X) : 
	features = []
	for x in X : 
		f = {}
		#TODO replace this dummy feature function with a unigram model, like we did in class
		for word in x.split():
			if isStopWord(word):
				continue
			if word in f:
				f[word] = f[word] &#43; 1.0
			else:
				f[word] =  1.0
		features.append(f)
	return features</pre>
<p>Not really sure what to do at this point because of this. Also might this be related to using the VM?</p>",Statistical Analysis Taking A Very Long Time,It takes a really long time. Is that expected?,statistical classification runtime,"<p>My script runs forever on my virtual machine, so I&#39;m running it off of biglab. I got the statistical analysis to work once (took about 2 minutes), but after un-commenting get_top_features and get_misclassified, my script gets killed before finishing the initial statistical analysis. I&#39;ve tried several times and I&#39;m unsure what the issue is; I&#39;ve copied my code below.</p>
<p></p>
<pre>#!/bin/python<br /><br />import os<br />import sys<br />import string<br />import random<br />import operator<br />from sklearn.tree import export_graphviz<br />from sklearn.tree import DecisionTreeClassifier<br />from sklearn.naive_bayes import MultinomialNB<br />from sklearn.linear_model import LogisticRegression<br />from sklearn.preprocessing import LabelEncoder<br />from sklearn.feature_extraction import DictVectorizer<br />from sklearn.cross_validation import train_test_split<br />from sklearn.externals.six import StringIO  <br /><br />#read in raw data from file and return a list of (label, article) tuples<br />def get_data(filename): <br />	data = [line.strip().split(&#39;\t&#39;) for line in open(filename).readlines()]<br />	random.shuffle(data)<br />	return data<br /><br />#this function builds the feature matrix for the Decision Tree.<br />def get_dtree_features(X) :<br />	features = []<br />	#TODO : Add the features you would like to use to train the Decision Tree here.<br />	feature_list = [&#39;gun&#39;, &#39;shooter&#39;, &#39;shot&#39;, &#39;shooting&#39;, &#39;player&#39;, &#39;coach&#39;, &#39;game&#39;, &#39;movie&#39;, &#39;actor&#39;, &#39;film&#39;, &#39;bullet&#39;, &#39;photo&#39;, &#39;model&#39;, &#39;point&#39;, &#39;suspect&#39;, &#39;victim&#39;, &#39;police&#39;, &#39;fired&#39;]<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w in feature_list : <br />				f[w] = 1.0<br />		features.append(f)<br />	return features<br /><br /><br />#this is the main function you care about; pack all the cleverest features you can think of into here.<br />def get_features(X) :<br />	stopwords = open(&#39;stopwords.txt&#39;).read()<br />	features = []<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w not in stopwords:            <br />				if w not in f : <br />					f[w] = 0<br />				f[w] &#43;= 1                <br />		features.append(f)<br />	return features<br /><br />#vectorize feature dictionaries and return feature and label matricies<br />def get_matricies(data, typ=&#34;unigram&#34;) : <br />	dv = DictVectorizer(sparse=True) <br />	le = LabelEncoder()<br />	y = [d[0] for d in data]<br />	texts = [d[1] for d in data]<br />	if typ == &#34;tree&#34;:<br />		X = get_dtree_features(texts)<br />	else :<br />		X = get_features(texts)<br />	#Here we are returning 5 things, the label vector y and feature matrix X, but also the texts from which the features were extracted and the <br />	#objects that were used to encode them. These will come in handy for your analysis, but you can ignore them for the initial parts of the assignment<br />	return le.fit_transform(y), dv.fit_transform(X), texts, dv, le<br /><br />#train and multinomial naive bayes classifier<br />def train_classifier(X, y):<br />	clf = LogisticRegression()<br />	clf.fit(X,y)<br />	return clf <br /><br />#train a Decision Tree classifier<br />def train_dtree_classifier(X, y):<br />	clf = DecisionTreeClassifier(max_depth=10)<br />	clf.fit(X,y)<br />	return clf<br /><br />#test the classifier<br />def test_classifier(clf, X, y):<br />	return clf.score(X,y)<br /><br />#cross validation	<br />def cross_validate(X, y, dv=None, typ=&#34;unigram&#34;, numfolds=5,):<br />	test_accs = []<br />	split = 1.0 / numfolds<br />	for i in range(numfolds):<br />		x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=i)<br />		if typ == &#34;tree&#34; :<br />			clf = train_dtree_classifier(x_train, y_train)<br />		else :<br />			clf = train_classifier(x_train, y_train)<br />		test_acc = test_classifier(clf, x_test, y_test)<br />		test_accs.append(test_acc)<br />		print &#39;Fold %d : %.05f&#39;%(i,test_acc)<br />	test_average = float(sum(test_accs))/ numfolds<br />	if typ == &#34;tree&#34; :<br />		with open(&#34;output.dot&#34;, &#39;w&#39;) as f:<br />			f = export_graphviz(clf, out_file=f, feature_names=dv.get_feature_names(), class_names=[&#39;Non Gun Related&#39;,&#39;Gun Related&#39;])<br />		create_graph(&#34;decision-tree.png&#34;)<br />	print &#39;Test Average : %.05f&#39;%(test_average)<br />	print<br />	return test_average<br /><br />#run a rule based classifier and calculate the accuracy<br />def rule_based_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO add more keywords, see how well they do alone and in combination<br />		if &#34;shooter&#34; in text or &#34;shot&#34; in text or &#34;shooting&#34; in text :<br />			if &#34;player&#34; in text or &#34;coach&#34; in text or &#34;game&#34; in text: <br />				prediction = &#39;0&#39;<br />			elif &#34;movie&#34; in text or &#34;film&#34; in text or &#34;actor&#34; in text:<br />				prediction = &#39;0&#39;<br />			else : prediction = &#39;1&#39;<br />		if &#34;gun&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Rule-based classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#Extra Credit Rule Based Classifier<br />def extra_credit_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO develop your conditional statements here<br />		if &#34;shooting&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Reverse Engineered classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#train and multinomial naive bayes classifier<br />def get_top_features(X, y, dv):<br />	clf = train_classifier(X, y)<br />	#the DictVectorizer object remembers which column number corresponds to which feature, and return the feature names in the correct order<br />	feature_names = dv.get_feature_names() <br /><br />	#The below code will get the weights from the classifier, and print out the weights of the features you are interested in<br />	features = [] #this will be a list of (feature_idx, weight) tuples<br />	for i,w in enumerate(clf.coef_[0]): <br />		features.append((i,w))<br />	#Sort the list by values, with the largest ones first<br />	features = sorted(features, key=lambda e: e[1], reverse=True)<br /><br />        #Print out the feature names and thier weights<br />	for i,w in features:<br />	  print &#39;%s\t%s&#39;%(feature_names[i], w)<br /><br />def get_misclassified_examples(y, X, texts) :<br />	x_train, x_test, y_train, y_test, train_texts, test_texts = train_test_split(X, y, texts)<br />	clf = train_classifier(x_train, y_train)<br /><br />	#TODO: You will have to write some code to call your classifier on each of the test examples, and check whether its prediction was right or wrong<br /><br />def create_graph(file_name) :<br />	os.system(&#34;dot -Tpng output.dot -o &#34; &#43; file_name)<br />	os.unlink(&#34;output.dot&#34;)<br /><br />if __name__ == &#39;__main__&#39; : <br /><br />	raw_data = get_data(sys.argv[1])<br />	<br />	print &#39;\nRule-based classification&#39;<br />	rule_based_classifier(raw_data)<br /><br />################ Decision Tree ################<br /><br /># print &#39;\nDecision Tree classification&#39;<br /># y, X, texts, dv, le = get_matricies(raw_data, &#34;tree&#34;)<br /># cross_validate(X,y,dv,&#34;tree&#34;)<br /><br />################ Statistical Classification ################<br />print &#39;\nStatistical classification&#39;<br />y, X, texts, dv, le = get_matricies(raw_data)<br />cross_validate(X,y)<br /><br />get_top_features(X, y, dv)<br />get_misclassified_examples(y, X, texts)<br />
</pre>
<p>Thanks in advance for any help you can give.</p>",Statistical Analysis &#34;Killed&#34; - even on biglab,"<p>When my program gets to the Statistical Analysis bit, it makes it through all the words and creates wordcount dictionaries that are appended to the list of features. Then it hangs for a bit until the terminal just prints &#34;Killed&#34; and program ends. Any idea what might be causing this? Not enough memory?</p>",Statistical Analysis &#34;Killed&#34;,,,"<p>I&#39;m confused on what exactly we&#39;re supposed to do for part 6. Are the y dimensions the same as the number of articles? Also, how is logistic regression supposed to be used here?</p>",Statistical Analysis Part 6,0
940849303,4/26/2016 15:21:37,true,1969379715,,4/26/2016 15:19:32,false,clixsense,1.0,7837812,SRB,00,Belgrade,79.101.254.233,0,0,"<p>Hi, so I read though the other piazza post about how the statistical analysis takes a long time, but I am still having some trouble with my code.</p>
<p></p>
<p>I have already tried waiting 10-15 minutes and removing all stopwords, but for some reason the statistical analysis refuses to complete in a reasonable amount of time</p>
<p></p>
<p>Below is my code:</p>
<pre>def isStopWord(word):
	with open(&#39;stopwords.txt&#39;, &#39;r&#39;) as stopwords:
		for line in stopwords:
			if word in line:
				return True
		return False

def get_features(X) : 
	features = []
	for x in X : 
		f = {}
		#TODO replace this dummy feature function with a unigram model, like we did in class
		for word in x.split():
			if isStopWord(word):
				continue
			if word in f:
				f[word] = f[word] &#43; 1.0
			else:
				f[word] =  1.0
		features.append(f)
	return features</pre>
<p>Not really sure what to do at this point because of this. Also might this be related to using the VM?</p>",Statistical Analysis Taking A Very Long Time,It takes a really long time. Is that expected?,statistical classification runtime,"<p>My script runs forever on my virtual machine, so I&#39;m running it off of biglab. I got the statistical analysis to work once (took about 2 minutes), but after un-commenting get_top_features and get_misclassified, my script gets killed before finishing the initial statistical analysis. I&#39;ve tried several times and I&#39;m unsure what the issue is; I&#39;ve copied my code below.</p>
<p></p>
<pre>#!/bin/python<br /><br />import os<br />import sys<br />import string<br />import random<br />import operator<br />from sklearn.tree import export_graphviz<br />from sklearn.tree import DecisionTreeClassifier<br />from sklearn.naive_bayes import MultinomialNB<br />from sklearn.linear_model import LogisticRegression<br />from sklearn.preprocessing import LabelEncoder<br />from sklearn.feature_extraction import DictVectorizer<br />from sklearn.cross_validation import train_test_split<br />from sklearn.externals.six import StringIO  <br /><br />#read in raw data from file and return a list of (label, article) tuples<br />def get_data(filename): <br />	data = [line.strip().split(&#39;\t&#39;) for line in open(filename).readlines()]<br />	random.shuffle(data)<br />	return data<br /><br />#this function builds the feature matrix for the Decision Tree.<br />def get_dtree_features(X) :<br />	features = []<br />	#TODO : Add the features you would like to use to train the Decision Tree here.<br />	feature_list = [&#39;gun&#39;, &#39;shooter&#39;, &#39;shot&#39;, &#39;shooting&#39;, &#39;player&#39;, &#39;coach&#39;, &#39;game&#39;, &#39;movie&#39;, &#39;actor&#39;, &#39;film&#39;, &#39;bullet&#39;, &#39;photo&#39;, &#39;model&#39;, &#39;point&#39;, &#39;suspect&#39;, &#39;victim&#39;, &#39;police&#39;, &#39;fired&#39;]<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w in feature_list : <br />				f[w] = 1.0<br />		features.append(f)<br />	return features<br /><br /><br />#this is the main function you care about; pack all the cleverest features you can think of into here.<br />def get_features(X) :<br />	stopwords = open(&#39;stopwords.txt&#39;).read()<br />	features = []<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w not in stopwords:            <br />				if w not in f : <br />					f[w] = 0<br />				f[w] &#43;= 1                <br />		features.append(f)<br />	return features<br /><br />#vectorize feature dictionaries and return feature and label matricies<br />def get_matricies(data, typ=&#34;unigram&#34;) : <br />	dv = DictVectorizer(sparse=True) <br />	le = LabelEncoder()<br />	y = [d[0] for d in data]<br />	texts = [d[1] for d in data]<br />	if typ == &#34;tree&#34;:<br />		X = get_dtree_features(texts)<br />	else :<br />		X = get_features(texts)<br />	#Here we are returning 5 things, the label vector y and feature matrix X, but also the texts from which the features were extracted and the <br />	#objects that were used to encode them. These will come in handy for your analysis, but you can ignore them for the initial parts of the assignment<br />	return le.fit_transform(y), dv.fit_transform(X), texts, dv, le<br /><br />#train and multinomial naive bayes classifier<br />def train_classifier(X, y):<br />	clf = LogisticRegression()<br />	clf.fit(X,y)<br />	return clf <br /><br />#train a Decision Tree classifier<br />def train_dtree_classifier(X, y):<br />	clf = DecisionTreeClassifier(max_depth=10)<br />	clf.fit(X,y)<br />	return clf<br /><br />#test the classifier<br />def test_classifier(clf, X, y):<br />	return clf.score(X,y)<br /><br />#cross validation	<br />def cross_validate(X, y, dv=None, typ=&#34;unigram&#34;, numfolds=5,):<br />	test_accs = []<br />	split = 1.0 / numfolds<br />	for i in range(numfolds):<br />		x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=i)<br />		if typ == &#34;tree&#34; :<br />			clf = train_dtree_classifier(x_train, y_train)<br />		else :<br />			clf = train_classifier(x_train, y_train)<br />		test_acc = test_classifier(clf, x_test, y_test)<br />		test_accs.append(test_acc)<br />		print &#39;Fold %d : %.05f&#39;%(i,test_acc)<br />	test_average = float(sum(test_accs))/ numfolds<br />	if typ == &#34;tree&#34; :<br />		with open(&#34;output.dot&#34;, &#39;w&#39;) as f:<br />			f = export_graphviz(clf, out_file=f, feature_names=dv.get_feature_names(), class_names=[&#39;Non Gun Related&#39;,&#39;Gun Related&#39;])<br />		create_graph(&#34;decision-tree.png&#34;)<br />	print &#39;Test Average : %.05f&#39;%(test_average)<br />	print<br />	return test_average<br /><br />#run a rule based classifier and calculate the accuracy<br />def rule_based_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO add more keywords, see how well they do alone and in combination<br />		if &#34;shooter&#34; in text or &#34;shot&#34; in text or &#34;shooting&#34; in text :<br />			if &#34;player&#34; in text or &#34;coach&#34; in text or &#34;game&#34; in text: <br />				prediction = &#39;0&#39;<br />			elif &#34;movie&#34; in text or &#34;film&#34; in text or &#34;actor&#34; in text:<br />				prediction = &#39;0&#39;<br />			else : prediction = &#39;1&#39;<br />		if &#34;gun&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Rule-based classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#Extra Credit Rule Based Classifier<br />def extra_credit_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO develop your conditional statements here<br />		if &#34;shooting&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Reverse Engineered classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#train and multinomial naive bayes classifier<br />def get_top_features(X, y, dv):<br />	clf = train_classifier(X, y)<br />	#the DictVectorizer object remembers which column number corresponds to which feature, and return the feature names in the correct order<br />	feature_names = dv.get_feature_names() <br /><br />	#The below code will get the weights from the classifier, and print out the weights of the features you are interested in<br />	features = [] #this will be a list of (feature_idx, weight) tuples<br />	for i,w in enumerate(clf.coef_[0]): <br />		features.append((i,w))<br />	#Sort the list by values, with the largest ones first<br />	features = sorted(features, key=lambda e: e[1], reverse=True)<br /><br />        #Print out the feature names and thier weights<br />	for i,w in features:<br />	  print &#39;%s\t%s&#39;%(feature_names[i], w)<br /><br />def get_misclassified_examples(y, X, texts) :<br />	x_train, x_test, y_train, y_test, train_texts, test_texts = train_test_split(X, y, texts)<br />	clf = train_classifier(x_train, y_train)<br /><br />	#TODO: You will have to write some code to call your classifier on each of the test examples, and check whether its prediction was right or wrong<br /><br />def create_graph(file_name) :<br />	os.system(&#34;dot -Tpng output.dot -o &#34; &#43; file_name)<br />	os.unlink(&#34;output.dot&#34;)<br /><br />if __name__ == &#39;__main__&#39; : <br /><br />	raw_data = get_data(sys.argv[1])<br />	<br />	print &#39;\nRule-based classification&#39;<br />	rule_based_classifier(raw_data)<br /><br />################ Decision Tree ################<br /><br /># print &#39;\nDecision Tree classification&#39;<br /># y, X, texts, dv, le = get_matricies(raw_data, &#34;tree&#34;)<br /># cross_validate(X,y,dv,&#34;tree&#34;)<br /><br />################ Statistical Classification ################<br />print &#39;\nStatistical classification&#39;<br />y, X, texts, dv, le = get_matricies(raw_data)<br />cross_validate(X,y)<br /><br />get_top_features(X, y, dv)<br />get_misclassified_examples(y, X, texts)<br />
</pre>
<p>Thanks in advance for any help you can give.</p>",Statistical Analysis &#34;Killed&#34; - even on biglab,"<p>When my program gets to the Statistical Analysis bit, it makes it through all the words and creates wordcount dictionaries that are appended to the list of features. Then it hangs for a bit until the terminal just prints &#34;Killed&#34; and program ends. Any idea what might be causing this? Not enough memory?</p>",Statistical Analysis &#34;Killed&#34;,,,"<p>I&#39;m confused on what exactly we&#39;re supposed to do for part 6. Are the y dimensions the same as the number of articles? Also, how is logistic regression supposed to be used here?</p>",Statistical Analysis Part 6,0
940849303,4/26/2016 15:24:26,true,1969384555,,4/26/2016 15:21:07,false,elite,1.0,30280423,ITA,15,Siracusa,151.54.84.121,0,0,"<p>Hi, so I read though the other piazza post about how the statistical analysis takes a long time, but I am still having some trouble with my code.</p>
<p></p>
<p>I have already tried waiting 10-15 minutes and removing all stopwords, but for some reason the statistical analysis refuses to complete in a reasonable amount of time</p>
<p></p>
<p>Below is my code:</p>
<pre>def isStopWord(word):
	with open(&#39;stopwords.txt&#39;, &#39;r&#39;) as stopwords:
		for line in stopwords:
			if word in line:
				return True
		return False

def get_features(X) : 
	features = []
	for x in X : 
		f = {}
		#TODO replace this dummy feature function with a unigram model, like we did in class
		for word in x.split():
			if isStopWord(word):
				continue
			if word in f:
				f[word] = f[word] &#43; 1.0
			else:
				f[word] =  1.0
		features.append(f)
	return features</pre>
<p>Not really sure what to do at this point because of this. Also might this be related to using the VM?</p>",Statistical Analysis Taking A Very Long Time,It takes a really long time. Is that expected?,statistical classification runtime,"<p>My script runs forever on my virtual machine, so I&#39;m running it off of biglab. I got the statistical analysis to work once (took about 2 minutes), but after un-commenting get_top_features and get_misclassified, my script gets killed before finishing the initial statistical analysis. I&#39;ve tried several times and I&#39;m unsure what the issue is; I&#39;ve copied my code below.</p>
<p></p>
<pre>#!/bin/python<br /><br />import os<br />import sys<br />import string<br />import random<br />import operator<br />from sklearn.tree import export_graphviz<br />from sklearn.tree import DecisionTreeClassifier<br />from sklearn.naive_bayes import MultinomialNB<br />from sklearn.linear_model import LogisticRegression<br />from sklearn.preprocessing import LabelEncoder<br />from sklearn.feature_extraction import DictVectorizer<br />from sklearn.cross_validation import train_test_split<br />from sklearn.externals.six import StringIO  <br /><br />#read in raw data from file and return a list of (label, article) tuples<br />def get_data(filename): <br />	data = [line.strip().split(&#39;\t&#39;) for line in open(filename).readlines()]<br />	random.shuffle(data)<br />	return data<br /><br />#this function builds the feature matrix for the Decision Tree.<br />def get_dtree_features(X) :<br />	features = []<br />	#TODO : Add the features you would like to use to train the Decision Tree here.<br />	feature_list = [&#39;gun&#39;, &#39;shooter&#39;, &#39;shot&#39;, &#39;shooting&#39;, &#39;player&#39;, &#39;coach&#39;, &#39;game&#39;, &#39;movie&#39;, &#39;actor&#39;, &#39;film&#39;, &#39;bullet&#39;, &#39;photo&#39;, &#39;model&#39;, &#39;point&#39;, &#39;suspect&#39;, &#39;victim&#39;, &#39;police&#39;, &#39;fired&#39;]<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w in feature_list : <br />				f[w] = 1.0<br />		features.append(f)<br />	return features<br /><br /><br />#this is the main function you care about; pack all the cleverest features you can think of into here.<br />def get_features(X) :<br />	stopwords = open(&#39;stopwords.txt&#39;).read()<br />	features = []<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w not in stopwords:            <br />				if w not in f : <br />					f[w] = 0<br />				f[w] &#43;= 1                <br />		features.append(f)<br />	return features<br /><br />#vectorize feature dictionaries and return feature and label matricies<br />def get_matricies(data, typ=&#34;unigram&#34;) : <br />	dv = DictVectorizer(sparse=True) <br />	le = LabelEncoder()<br />	y = [d[0] for d in data]<br />	texts = [d[1] for d in data]<br />	if typ == &#34;tree&#34;:<br />		X = get_dtree_features(texts)<br />	else :<br />		X = get_features(texts)<br />	#Here we are returning 5 things, the label vector y and feature matrix X, but also the texts from which the features were extracted and the <br />	#objects that were used to encode them. These will come in handy for your analysis, but you can ignore them for the initial parts of the assignment<br />	return le.fit_transform(y), dv.fit_transform(X), texts, dv, le<br /><br />#train and multinomial naive bayes classifier<br />def train_classifier(X, y):<br />	clf = LogisticRegression()<br />	clf.fit(X,y)<br />	return clf <br /><br />#train a Decision Tree classifier<br />def train_dtree_classifier(X, y):<br />	clf = DecisionTreeClassifier(max_depth=10)<br />	clf.fit(X,y)<br />	return clf<br /><br />#test the classifier<br />def test_classifier(clf, X, y):<br />	return clf.score(X,y)<br /><br />#cross validation	<br />def cross_validate(X, y, dv=None, typ=&#34;unigram&#34;, numfolds=5,):<br />	test_accs = []<br />	split = 1.0 / numfolds<br />	for i in range(numfolds):<br />		x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=i)<br />		if typ == &#34;tree&#34; :<br />			clf = train_dtree_classifier(x_train, y_train)<br />		else :<br />			clf = train_classifier(x_train, y_train)<br />		test_acc = test_classifier(clf, x_test, y_test)<br />		test_accs.append(test_acc)<br />		print &#39;Fold %d : %.05f&#39;%(i,test_acc)<br />	test_average = float(sum(test_accs))/ numfolds<br />	if typ == &#34;tree&#34; :<br />		with open(&#34;output.dot&#34;, &#39;w&#39;) as f:<br />			f = export_graphviz(clf, out_file=f, feature_names=dv.get_feature_names(), class_names=[&#39;Non Gun Related&#39;,&#39;Gun Related&#39;])<br />		create_graph(&#34;decision-tree.png&#34;)<br />	print &#39;Test Average : %.05f&#39;%(test_average)<br />	print<br />	return test_average<br /><br />#run a rule based classifier and calculate the accuracy<br />def rule_based_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO add more keywords, see how well they do alone and in combination<br />		if &#34;shooter&#34; in text or &#34;shot&#34; in text or &#34;shooting&#34; in text :<br />			if &#34;player&#34; in text or &#34;coach&#34; in text or &#34;game&#34; in text: <br />				prediction = &#39;0&#39;<br />			elif &#34;movie&#34; in text or &#34;film&#34; in text or &#34;actor&#34; in text:<br />				prediction = &#39;0&#39;<br />			else : prediction = &#39;1&#39;<br />		if &#34;gun&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Rule-based classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#Extra Credit Rule Based Classifier<br />def extra_credit_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO develop your conditional statements here<br />		if &#34;shooting&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Reverse Engineered classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#train and multinomial naive bayes classifier<br />def get_top_features(X, y, dv):<br />	clf = train_classifier(X, y)<br />	#the DictVectorizer object remembers which column number corresponds to which feature, and return the feature names in the correct order<br />	feature_names = dv.get_feature_names() <br /><br />	#The below code will get the weights from the classifier, and print out the weights of the features you are interested in<br />	features = [] #this will be a list of (feature_idx, weight) tuples<br />	for i,w in enumerate(clf.coef_[0]): <br />		features.append((i,w))<br />	#Sort the list by values, with the largest ones first<br />	features = sorted(features, key=lambda e: e[1], reverse=True)<br /><br />        #Print out the feature names and thier weights<br />	for i,w in features:<br />	  print &#39;%s\t%s&#39;%(feature_names[i], w)<br /><br />def get_misclassified_examples(y, X, texts) :<br />	x_train, x_test, y_train, y_test, train_texts, test_texts = train_test_split(X, y, texts)<br />	clf = train_classifier(x_train, y_train)<br /><br />	#TODO: You will have to write some code to call your classifier on each of the test examples, and check whether its prediction was right or wrong<br /><br />def create_graph(file_name) :<br />	os.system(&#34;dot -Tpng output.dot -o &#34; &#43; file_name)<br />	os.unlink(&#34;output.dot&#34;)<br /><br />if __name__ == &#39;__main__&#39; : <br /><br />	raw_data = get_data(sys.argv[1])<br />	<br />	print &#39;\nRule-based classification&#39;<br />	rule_based_classifier(raw_data)<br /><br />################ Decision Tree ################<br /><br /># print &#39;\nDecision Tree classification&#39;<br /># y, X, texts, dv, le = get_matricies(raw_data, &#34;tree&#34;)<br /># cross_validate(X,y,dv,&#34;tree&#34;)<br /><br />################ Statistical Classification ################<br />print &#39;\nStatistical classification&#39;<br />y, X, texts, dv, le = get_matricies(raw_data)<br />cross_validate(X,y)<br /><br />get_top_features(X, y, dv)<br />get_misclassified_examples(y, X, texts)<br />
</pre>
<p>Thanks in advance for any help you can give.</p>",Statistical Analysis &#34;Killed&#34; - even on biglab,"<p>When my program gets to the Statistical Analysis bit, it makes it through all the words and creates wordcount dictionaries that are appended to the list of features. Then it hangs for a bit until the terminal just prints &#34;Killed&#34; and program ends. Any idea what might be causing this? Not enough memory?</p>",Statistical Analysis &#34;Killed&#34;,,,"<p>I&#39;m confused on what exactly we&#39;re supposed to do for part 6. Are the y dimensions the same as the number of articles? Also, how is logistic regression supposed to be used here?</p>",Statistical Analysis Part 6,0
940849303,4/26/2016 15:28:06,true,1969389964,,4/26/2016 15:27:30,false,instagc,0.8889,13581319,USA,IL,Waltonville,208.70.36.12,0,0,"<p>Hi, so I read though the other piazza post about how the statistical analysis takes a long time, but I am still having some trouble with my code.</p>
<p></p>
<p>I have already tried waiting 10-15 minutes and removing all stopwords, but for some reason the statistical analysis refuses to complete in a reasonable amount of time</p>
<p></p>
<p>Below is my code:</p>
<pre>def isStopWord(word):
	with open(&#39;stopwords.txt&#39;, &#39;r&#39;) as stopwords:
		for line in stopwords:
			if word in line:
				return True
		return False

def get_features(X) : 
	features = []
	for x in X : 
		f = {}
		#TODO replace this dummy feature function with a unigram model, like we did in class
		for word in x.split():
			if isStopWord(word):
				continue
			if word in f:
				f[word] = f[word] &#43; 1.0
			else:
				f[word] =  1.0
		features.append(f)
	return features</pre>
<p>Not really sure what to do at this point because of this. Also might this be related to using the VM?</p>",Statistical Analysis Taking A Very Long Time,It takes a really long time. Is that expected?,statistical classification runtime,"<p>My script runs forever on my virtual machine, so I&#39;m running it off of biglab. I got the statistical analysis to work once (took about 2 minutes), but after un-commenting get_top_features and get_misclassified, my script gets killed before finishing the initial statistical analysis. I&#39;ve tried several times and I&#39;m unsure what the issue is; I&#39;ve copied my code below.</p>
<p></p>
<pre>#!/bin/python<br /><br />import os<br />import sys<br />import string<br />import random<br />import operator<br />from sklearn.tree import export_graphviz<br />from sklearn.tree import DecisionTreeClassifier<br />from sklearn.naive_bayes import MultinomialNB<br />from sklearn.linear_model import LogisticRegression<br />from sklearn.preprocessing import LabelEncoder<br />from sklearn.feature_extraction import DictVectorizer<br />from sklearn.cross_validation import train_test_split<br />from sklearn.externals.six import StringIO  <br /><br />#read in raw data from file and return a list of (label, article) tuples<br />def get_data(filename): <br />	data = [line.strip().split(&#39;\t&#39;) for line in open(filename).readlines()]<br />	random.shuffle(data)<br />	return data<br /><br />#this function builds the feature matrix for the Decision Tree.<br />def get_dtree_features(X) :<br />	features = []<br />	#TODO : Add the features you would like to use to train the Decision Tree here.<br />	feature_list = [&#39;gun&#39;, &#39;shooter&#39;, &#39;shot&#39;, &#39;shooting&#39;, &#39;player&#39;, &#39;coach&#39;, &#39;game&#39;, &#39;movie&#39;, &#39;actor&#39;, &#39;film&#39;, &#39;bullet&#39;, &#39;photo&#39;, &#39;model&#39;, &#39;point&#39;, &#39;suspect&#39;, &#39;victim&#39;, &#39;police&#39;, &#39;fired&#39;]<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w in feature_list : <br />				f[w] = 1.0<br />		features.append(f)<br />	return features<br /><br /><br />#this is the main function you care about; pack all the cleverest features you can think of into here.<br />def get_features(X) :<br />	stopwords = open(&#39;stopwords.txt&#39;).read()<br />	features = []<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w not in stopwords:            <br />				if w not in f : <br />					f[w] = 0<br />				f[w] &#43;= 1                <br />		features.append(f)<br />	return features<br /><br />#vectorize feature dictionaries and return feature and label matricies<br />def get_matricies(data, typ=&#34;unigram&#34;) : <br />	dv = DictVectorizer(sparse=True) <br />	le = LabelEncoder()<br />	y = [d[0] for d in data]<br />	texts = [d[1] for d in data]<br />	if typ == &#34;tree&#34;:<br />		X = get_dtree_features(texts)<br />	else :<br />		X = get_features(texts)<br />	#Here we are returning 5 things, the label vector y and feature matrix X, but also the texts from which the features were extracted and the <br />	#objects that were used to encode them. These will come in handy for your analysis, but you can ignore them for the initial parts of the assignment<br />	return le.fit_transform(y), dv.fit_transform(X), texts, dv, le<br /><br />#train and multinomial naive bayes classifier<br />def train_classifier(X, y):<br />	clf = LogisticRegression()<br />	clf.fit(X,y)<br />	return clf <br /><br />#train a Decision Tree classifier<br />def train_dtree_classifier(X, y):<br />	clf = DecisionTreeClassifier(max_depth=10)<br />	clf.fit(X,y)<br />	return clf<br /><br />#test the classifier<br />def test_classifier(clf, X, y):<br />	return clf.score(X,y)<br /><br />#cross validation	<br />def cross_validate(X, y, dv=None, typ=&#34;unigram&#34;, numfolds=5,):<br />	test_accs = []<br />	split = 1.0 / numfolds<br />	for i in range(numfolds):<br />		x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=i)<br />		if typ == &#34;tree&#34; :<br />			clf = train_dtree_classifier(x_train, y_train)<br />		else :<br />			clf = train_classifier(x_train, y_train)<br />		test_acc = test_classifier(clf, x_test, y_test)<br />		test_accs.append(test_acc)<br />		print &#39;Fold %d : %.05f&#39;%(i,test_acc)<br />	test_average = float(sum(test_accs))/ numfolds<br />	if typ == &#34;tree&#34; :<br />		with open(&#34;output.dot&#34;, &#39;w&#39;) as f:<br />			f = export_graphviz(clf, out_file=f, feature_names=dv.get_feature_names(), class_names=[&#39;Non Gun Related&#39;,&#39;Gun Related&#39;])<br />		create_graph(&#34;decision-tree.png&#34;)<br />	print &#39;Test Average : %.05f&#39;%(test_average)<br />	print<br />	return test_average<br /><br />#run a rule based classifier and calculate the accuracy<br />def rule_based_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO add more keywords, see how well they do alone and in combination<br />		if &#34;shooter&#34; in text or &#34;shot&#34; in text or &#34;shooting&#34; in text :<br />			if &#34;player&#34; in text or &#34;coach&#34; in text or &#34;game&#34; in text: <br />				prediction = &#39;0&#39;<br />			elif &#34;movie&#34; in text or &#34;film&#34; in text or &#34;actor&#34; in text:<br />				prediction = &#39;0&#39;<br />			else : prediction = &#39;1&#39;<br />		if &#34;gun&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Rule-based classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#Extra Credit Rule Based Classifier<br />def extra_credit_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO develop your conditional statements here<br />		if &#34;shooting&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Reverse Engineered classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#train and multinomial naive bayes classifier<br />def get_top_features(X, y, dv):<br />	clf = train_classifier(X, y)<br />	#the DictVectorizer object remembers which column number corresponds to which feature, and return the feature names in the correct order<br />	feature_names = dv.get_feature_names() <br /><br />	#The below code will get the weights from the classifier, and print out the weights of the features you are interested in<br />	features = [] #this will be a list of (feature_idx, weight) tuples<br />	for i,w in enumerate(clf.coef_[0]): <br />		features.append((i,w))<br />	#Sort the list by values, with the largest ones first<br />	features = sorted(features, key=lambda e: e[1], reverse=True)<br /><br />        #Print out the feature names and thier weights<br />	for i,w in features:<br />	  print &#39;%s\t%s&#39;%(feature_names[i], w)<br /><br />def get_misclassified_examples(y, X, texts) :<br />	x_train, x_test, y_train, y_test, train_texts, test_texts = train_test_split(X, y, texts)<br />	clf = train_classifier(x_train, y_train)<br /><br />	#TODO: You will have to write some code to call your classifier on each of the test examples, and check whether its prediction was right or wrong<br /><br />def create_graph(file_name) :<br />	os.system(&#34;dot -Tpng output.dot -o &#34; &#43; file_name)<br />	os.unlink(&#34;output.dot&#34;)<br /><br />if __name__ == &#39;__main__&#39; : <br /><br />	raw_data = get_data(sys.argv[1])<br />	<br />	print &#39;\nRule-based classification&#39;<br />	rule_based_classifier(raw_data)<br /><br />################ Decision Tree ################<br /><br /># print &#39;\nDecision Tree classification&#39;<br /># y, X, texts, dv, le = get_matricies(raw_data, &#34;tree&#34;)<br /># cross_validate(X,y,dv,&#34;tree&#34;)<br /><br />################ Statistical Classification ################<br />print &#39;\nStatistical classification&#39;<br />y, X, texts, dv, le = get_matricies(raw_data)<br />cross_validate(X,y)<br /><br />get_top_features(X, y, dv)<br />get_misclassified_examples(y, X, texts)<br />
</pre>
<p>Thanks in advance for any help you can give.</p>",Statistical Analysis &#34;Killed&#34; - even on biglab,"<p>When my program gets to the Statistical Analysis bit, it makes it through all the words and creates wordcount dictionaries that are appended to the list of features. Then it hangs for a bit until the terminal just prints &#34;Killed&#34; and program ends. Any idea what might be causing this? Not enough memory?</p>",Statistical Analysis &#34;Killed&#34;,,,"<p>I&#39;m confused on what exactly we&#39;re supposed to do for part 6. Are the y dimensions the same as the number of articles? Also, how is logistic regression supposed to be used here?</p>",Statistical Analysis Part 6,0
940849303,4/26/2016 15:36:01,true,1969392989,,4/26/2016 15:30:10,false,neodev,1.0,13396426,VEN,15,Santa Teresa,190.38.163.149,0,0,"<p>Hi, so I read though the other piazza post about how the statistical analysis takes a long time, but I am still having some trouble with my code.</p>
<p></p>
<p>I have already tried waiting 10-15 minutes and removing all stopwords, but for some reason the statistical analysis refuses to complete in a reasonable amount of time</p>
<p></p>
<p>Below is my code:</p>
<pre>def isStopWord(word):
	with open(&#39;stopwords.txt&#39;, &#39;r&#39;) as stopwords:
		for line in stopwords:
			if word in line:
				return True
		return False

def get_features(X) : 
	features = []
	for x in X : 
		f = {}
		#TODO replace this dummy feature function with a unigram model, like we did in class
		for word in x.split():
			if isStopWord(word):
				continue
			if word in f:
				f[word] = f[word] &#43; 1.0
			else:
				f[word] =  1.0
		features.append(f)
	return features</pre>
<p>Not really sure what to do at this point because of this. Also might this be related to using the VM?</p>",Statistical Analysis Taking A Very Long Time,It takes a really long time. Is that expected?,statistical classification runtime,"<p>My script runs forever on my virtual machine, so I&#39;m running it off of biglab. I got the statistical analysis to work once (took about 2 minutes), but after un-commenting get_top_features and get_misclassified, my script gets killed before finishing the initial statistical analysis. I&#39;ve tried several times and I&#39;m unsure what the issue is; I&#39;ve copied my code below.</p>
<p></p>
<pre>#!/bin/python<br /><br />import os<br />import sys<br />import string<br />import random<br />import operator<br />from sklearn.tree import export_graphviz<br />from sklearn.tree import DecisionTreeClassifier<br />from sklearn.naive_bayes import MultinomialNB<br />from sklearn.linear_model import LogisticRegression<br />from sklearn.preprocessing import LabelEncoder<br />from sklearn.feature_extraction import DictVectorizer<br />from sklearn.cross_validation import train_test_split<br />from sklearn.externals.six import StringIO  <br /><br />#read in raw data from file and return a list of (label, article) tuples<br />def get_data(filename): <br />	data = [line.strip().split(&#39;\t&#39;) for line in open(filename).readlines()]<br />	random.shuffle(data)<br />	return data<br /><br />#this function builds the feature matrix for the Decision Tree.<br />def get_dtree_features(X) :<br />	features = []<br />	#TODO : Add the features you would like to use to train the Decision Tree here.<br />	feature_list = [&#39;gun&#39;, &#39;shooter&#39;, &#39;shot&#39;, &#39;shooting&#39;, &#39;player&#39;, &#39;coach&#39;, &#39;game&#39;, &#39;movie&#39;, &#39;actor&#39;, &#39;film&#39;, &#39;bullet&#39;, &#39;photo&#39;, &#39;model&#39;, &#39;point&#39;, &#39;suspect&#39;, &#39;victim&#39;, &#39;police&#39;, &#39;fired&#39;]<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w in feature_list : <br />				f[w] = 1.0<br />		features.append(f)<br />	return features<br /><br /><br />#this is the main function you care about; pack all the cleverest features you can think of into here.<br />def get_features(X) :<br />	stopwords = open(&#39;stopwords.txt&#39;).read()<br />	features = []<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w not in stopwords:            <br />				if w not in f : <br />					f[w] = 0<br />				f[w] &#43;= 1                <br />		features.append(f)<br />	return features<br /><br />#vectorize feature dictionaries and return feature and label matricies<br />def get_matricies(data, typ=&#34;unigram&#34;) : <br />	dv = DictVectorizer(sparse=True) <br />	le = LabelEncoder()<br />	y = [d[0] for d in data]<br />	texts = [d[1] for d in data]<br />	if typ == &#34;tree&#34;:<br />		X = get_dtree_features(texts)<br />	else :<br />		X = get_features(texts)<br />	#Here we are returning 5 things, the label vector y and feature matrix X, but also the texts from which the features were extracted and the <br />	#objects that were used to encode them. These will come in handy for your analysis, but you can ignore them for the initial parts of the assignment<br />	return le.fit_transform(y), dv.fit_transform(X), texts, dv, le<br /><br />#train and multinomial naive bayes classifier<br />def train_classifier(X, y):<br />	clf = LogisticRegression()<br />	clf.fit(X,y)<br />	return clf <br /><br />#train a Decision Tree classifier<br />def train_dtree_classifier(X, y):<br />	clf = DecisionTreeClassifier(max_depth=10)<br />	clf.fit(X,y)<br />	return clf<br /><br />#test the classifier<br />def test_classifier(clf, X, y):<br />	return clf.score(X,y)<br /><br />#cross validation	<br />def cross_validate(X, y, dv=None, typ=&#34;unigram&#34;, numfolds=5,):<br />	test_accs = []<br />	split = 1.0 / numfolds<br />	for i in range(numfolds):<br />		x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=i)<br />		if typ == &#34;tree&#34; :<br />			clf = train_dtree_classifier(x_train, y_train)<br />		else :<br />			clf = train_classifier(x_train, y_train)<br />		test_acc = test_classifier(clf, x_test, y_test)<br />		test_accs.append(test_acc)<br />		print &#39;Fold %d : %.05f&#39;%(i,test_acc)<br />	test_average = float(sum(test_accs))/ numfolds<br />	if typ == &#34;tree&#34; :<br />		with open(&#34;output.dot&#34;, &#39;w&#39;) as f:<br />			f = export_graphviz(clf, out_file=f, feature_names=dv.get_feature_names(), class_names=[&#39;Non Gun Related&#39;,&#39;Gun Related&#39;])<br />		create_graph(&#34;decision-tree.png&#34;)<br />	print &#39;Test Average : %.05f&#39;%(test_average)<br />	print<br />	return test_average<br /><br />#run a rule based classifier and calculate the accuracy<br />def rule_based_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO add more keywords, see how well they do alone and in combination<br />		if &#34;shooter&#34; in text or &#34;shot&#34; in text or &#34;shooting&#34; in text :<br />			if &#34;player&#34; in text or &#34;coach&#34; in text or &#34;game&#34; in text: <br />				prediction = &#39;0&#39;<br />			elif &#34;movie&#34; in text or &#34;film&#34; in text or &#34;actor&#34; in text:<br />				prediction = &#39;0&#39;<br />			else : prediction = &#39;1&#39;<br />		if &#34;gun&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Rule-based classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#Extra Credit Rule Based Classifier<br />def extra_credit_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO develop your conditional statements here<br />		if &#34;shooting&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Reverse Engineered classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#train and multinomial naive bayes classifier<br />def get_top_features(X, y, dv):<br />	clf = train_classifier(X, y)<br />	#the DictVectorizer object remembers which column number corresponds to which feature, and return the feature names in the correct order<br />	feature_names = dv.get_feature_names() <br /><br />	#The below code will get the weights from the classifier, and print out the weights of the features you are interested in<br />	features = [] #this will be a list of (feature_idx, weight) tuples<br />	for i,w in enumerate(clf.coef_[0]): <br />		features.append((i,w))<br />	#Sort the list by values, with the largest ones first<br />	features = sorted(features, key=lambda e: e[1], reverse=True)<br /><br />        #Print out the feature names and thier weights<br />	for i,w in features:<br />	  print &#39;%s\t%s&#39;%(feature_names[i], w)<br /><br />def get_misclassified_examples(y, X, texts) :<br />	x_train, x_test, y_train, y_test, train_texts, test_texts = train_test_split(X, y, texts)<br />	clf = train_classifier(x_train, y_train)<br /><br />	#TODO: You will have to write some code to call your classifier on each of the test examples, and check whether its prediction was right or wrong<br /><br />def create_graph(file_name) :<br />	os.system(&#34;dot -Tpng output.dot -o &#34; &#43; file_name)<br />	os.unlink(&#34;output.dot&#34;)<br /><br />if __name__ == &#39;__main__&#39; : <br /><br />	raw_data = get_data(sys.argv[1])<br />	<br />	print &#39;\nRule-based classification&#39;<br />	rule_based_classifier(raw_data)<br /><br />################ Decision Tree ################<br /><br /># print &#39;\nDecision Tree classification&#39;<br /># y, X, texts, dv, le = get_matricies(raw_data, &#34;tree&#34;)<br /># cross_validate(X,y,dv,&#34;tree&#34;)<br /><br />################ Statistical Classification ################<br />print &#39;\nStatistical classification&#39;<br />y, X, texts, dv, le = get_matricies(raw_data)<br />cross_validate(X,y)<br /><br />get_top_features(X, y, dv)<br />get_misclassified_examples(y, X, texts)<br />
</pre>
<p>Thanks in advance for any help you can give.</p>",Statistical Analysis &#34;Killed&#34; - even on biglab,"<p>When my program gets to the Statistical Analysis bit, it makes it through all the words and creates wordcount dictionaries that are appended to the list of features. Then it hangs for a bit until the terminal just prints &#34;Killed&#34; and program ends. Any idea what might be causing this? Not enough memory?</p>",Statistical Analysis &#34;Killed&#34;,,,"<p>I&#39;m confused on what exactly we&#39;re supposed to do for part 6. Are the y dimensions the same as the number of articles? Also, how is logistic regression supposed to be used here?</p>",Statistical Analysis Part 6,0
940849303,4/26/2016 15:40:37,true,1969395298,,4/26/2016 15:34:58,false,neodev,0.7778,32569659,USA,MN,Minneapolis,97.127.88.224,0,0,"<p>Hi, so I read though the other piazza post about how the statistical analysis takes a long time, but I am still having some trouble with my code.</p>
<p></p>
<p>I have already tried waiting 10-15 minutes and removing all stopwords, but for some reason the statistical analysis refuses to complete in a reasonable amount of time</p>
<p></p>
<p>Below is my code:</p>
<pre>def isStopWord(word):
	with open(&#39;stopwords.txt&#39;, &#39;r&#39;) as stopwords:
		for line in stopwords:
			if word in line:
				return True
		return False

def get_features(X) : 
	features = []
	for x in X : 
		f = {}
		#TODO replace this dummy feature function with a unigram model, like we did in class
		for word in x.split():
			if isStopWord(word):
				continue
			if word in f:
				f[word] = f[word] &#43; 1.0
			else:
				f[word] =  1.0
		features.append(f)
	return features</pre>
<p>Not really sure what to do at this point because of this. Also might this be related to using the VM?</p>",Statistical Analysis Taking A Very Long Time,It takes a really long time. Is that expected?,statistical classification runtime,"<p>My script runs forever on my virtual machine, so I&#39;m running it off of biglab. I got the statistical analysis to work once (took about 2 minutes), but after un-commenting get_top_features and get_misclassified, my script gets killed before finishing the initial statistical analysis. I&#39;ve tried several times and I&#39;m unsure what the issue is; I&#39;ve copied my code below.</p>
<p></p>
<pre>#!/bin/python<br /><br />import os<br />import sys<br />import string<br />import random<br />import operator<br />from sklearn.tree import export_graphviz<br />from sklearn.tree import DecisionTreeClassifier<br />from sklearn.naive_bayes import MultinomialNB<br />from sklearn.linear_model import LogisticRegression<br />from sklearn.preprocessing import LabelEncoder<br />from sklearn.feature_extraction import DictVectorizer<br />from sklearn.cross_validation import train_test_split<br />from sklearn.externals.six import StringIO  <br /><br />#read in raw data from file and return a list of (label, article) tuples<br />def get_data(filename): <br />	data = [line.strip().split(&#39;\t&#39;) for line in open(filename).readlines()]<br />	random.shuffle(data)<br />	return data<br /><br />#this function builds the feature matrix for the Decision Tree.<br />def get_dtree_features(X) :<br />	features = []<br />	#TODO : Add the features you would like to use to train the Decision Tree here.<br />	feature_list = [&#39;gun&#39;, &#39;shooter&#39;, &#39;shot&#39;, &#39;shooting&#39;, &#39;player&#39;, &#39;coach&#39;, &#39;game&#39;, &#39;movie&#39;, &#39;actor&#39;, &#39;film&#39;, &#39;bullet&#39;, &#39;photo&#39;, &#39;model&#39;, &#39;point&#39;, &#39;suspect&#39;, &#39;victim&#39;, &#39;police&#39;, &#39;fired&#39;]<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w in feature_list : <br />				f[w] = 1.0<br />		features.append(f)<br />	return features<br /><br /><br />#this is the main function you care about; pack all the cleverest features you can think of into here.<br />def get_features(X) :<br />	stopwords = open(&#39;stopwords.txt&#39;).read()<br />	features = []<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w not in stopwords:            <br />				if w not in f : <br />					f[w] = 0<br />				f[w] &#43;= 1                <br />		features.append(f)<br />	return features<br /><br />#vectorize feature dictionaries and return feature and label matricies<br />def get_matricies(data, typ=&#34;unigram&#34;) : <br />	dv = DictVectorizer(sparse=True) <br />	le = LabelEncoder()<br />	y = [d[0] for d in data]<br />	texts = [d[1] for d in data]<br />	if typ == &#34;tree&#34;:<br />		X = get_dtree_features(texts)<br />	else :<br />		X = get_features(texts)<br />	#Here we are returning 5 things, the label vector y and feature matrix X, but also the texts from which the features were extracted and the <br />	#objects that were used to encode them. These will come in handy for your analysis, but you can ignore them for the initial parts of the assignment<br />	return le.fit_transform(y), dv.fit_transform(X), texts, dv, le<br /><br />#train and multinomial naive bayes classifier<br />def train_classifier(X, y):<br />	clf = LogisticRegression()<br />	clf.fit(X,y)<br />	return clf <br /><br />#train a Decision Tree classifier<br />def train_dtree_classifier(X, y):<br />	clf = DecisionTreeClassifier(max_depth=10)<br />	clf.fit(X,y)<br />	return clf<br /><br />#test the classifier<br />def test_classifier(clf, X, y):<br />	return clf.score(X,y)<br /><br />#cross validation	<br />def cross_validate(X, y, dv=None, typ=&#34;unigram&#34;, numfolds=5,):<br />	test_accs = []<br />	split = 1.0 / numfolds<br />	for i in range(numfolds):<br />		x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=i)<br />		if typ == &#34;tree&#34; :<br />			clf = train_dtree_classifier(x_train, y_train)<br />		else :<br />			clf = train_classifier(x_train, y_train)<br />		test_acc = test_classifier(clf, x_test, y_test)<br />		test_accs.append(test_acc)<br />		print &#39;Fold %d : %.05f&#39;%(i,test_acc)<br />	test_average = float(sum(test_accs))/ numfolds<br />	if typ == &#34;tree&#34; :<br />		with open(&#34;output.dot&#34;, &#39;w&#39;) as f:<br />			f = export_graphviz(clf, out_file=f, feature_names=dv.get_feature_names(), class_names=[&#39;Non Gun Related&#39;,&#39;Gun Related&#39;])<br />		create_graph(&#34;decision-tree.png&#34;)<br />	print &#39;Test Average : %.05f&#39;%(test_average)<br />	print<br />	return test_average<br /><br />#run a rule based classifier and calculate the accuracy<br />def rule_based_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO add more keywords, see how well they do alone and in combination<br />		if &#34;shooter&#34; in text or &#34;shot&#34; in text or &#34;shooting&#34; in text :<br />			if &#34;player&#34; in text or &#34;coach&#34; in text or &#34;game&#34; in text: <br />				prediction = &#39;0&#39;<br />			elif &#34;movie&#34; in text or &#34;film&#34; in text or &#34;actor&#34; in text:<br />				prediction = &#39;0&#39;<br />			else : prediction = &#39;1&#39;<br />		if &#34;gun&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Rule-based classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#Extra Credit Rule Based Classifier<br />def extra_credit_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO develop your conditional statements here<br />		if &#34;shooting&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Reverse Engineered classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#train and multinomial naive bayes classifier<br />def get_top_features(X, y, dv):<br />	clf = train_classifier(X, y)<br />	#the DictVectorizer object remembers which column number corresponds to which feature, and return the feature names in the correct order<br />	feature_names = dv.get_feature_names() <br /><br />	#The below code will get the weights from the classifier, and print out the weights of the features you are interested in<br />	features = [] #this will be a list of (feature_idx, weight) tuples<br />	for i,w in enumerate(clf.coef_[0]): <br />		features.append((i,w))<br />	#Sort the list by values, with the largest ones first<br />	features = sorted(features, key=lambda e: e[1], reverse=True)<br /><br />        #Print out the feature names and thier weights<br />	for i,w in features:<br />	  print &#39;%s\t%s&#39;%(feature_names[i], w)<br /><br />def get_misclassified_examples(y, X, texts) :<br />	x_train, x_test, y_train, y_test, train_texts, test_texts = train_test_split(X, y, texts)<br />	clf = train_classifier(x_train, y_train)<br /><br />	#TODO: You will have to write some code to call your classifier on each of the test examples, and check whether its prediction was right or wrong<br /><br />def create_graph(file_name) :<br />	os.system(&#34;dot -Tpng output.dot -o &#34; &#43; file_name)<br />	os.unlink(&#34;output.dot&#34;)<br /><br />if __name__ == &#39;__main__&#39; : <br /><br />	raw_data = get_data(sys.argv[1])<br />	<br />	print &#39;\nRule-based classification&#39;<br />	rule_based_classifier(raw_data)<br /><br />################ Decision Tree ################<br /><br /># print &#39;\nDecision Tree classification&#39;<br /># y, X, texts, dv, le = get_matricies(raw_data, &#34;tree&#34;)<br /># cross_validate(X,y,dv,&#34;tree&#34;)<br /><br />################ Statistical Classification ################<br />print &#39;\nStatistical classification&#39;<br />y, X, texts, dv, le = get_matricies(raw_data)<br />cross_validate(X,y)<br /><br />get_top_features(X, y, dv)<br />get_misclassified_examples(y, X, texts)<br />
</pre>
<p>Thanks in advance for any help you can give.</p>",Statistical Analysis &#34;Killed&#34; - even on biglab,"<p>When my program gets to the Statistical Analysis bit, it makes it through all the words and creates wordcount dictionaries that are appended to the list of features. Then it hangs for a bit until the terminal just prints &#34;Killed&#34; and program ends. Any idea what might be causing this? Not enough memory?</p>",Statistical Analysis &#34;Killed&#34;,,,"<p>I&#39;m confused on what exactly we&#39;re supposed to do for part 6. Are the y dimensions the same as the number of articles? Also, how is logistic regression supposed to be used here?</p>",Statistical Analysis Part 6,0
940849303,4/26/2016 15:44:24,true,1969396768,,4/26/2016 15:31:20,false,neodev,1.0,28875937,PAK,04,Lahore,119.153.154.137,0,0,"<p>Hi, so I read though the other piazza post about how the statistical analysis takes a long time, but I am still having some trouble with my code.</p>
<p></p>
<p>I have already tried waiting 10-15 minutes and removing all stopwords, but for some reason the statistical analysis refuses to complete in a reasonable amount of time</p>
<p></p>
<p>Below is my code:</p>
<pre>def isStopWord(word):
	with open(&#39;stopwords.txt&#39;, &#39;r&#39;) as stopwords:
		for line in stopwords:
			if word in line:
				return True
		return False

def get_features(X) : 
	features = []
	for x in X : 
		f = {}
		#TODO replace this dummy feature function with a unigram model, like we did in class
		for word in x.split():
			if isStopWord(word):
				continue
			if word in f:
				f[word] = f[word] &#43; 1.0
			else:
				f[word] =  1.0
		features.append(f)
	return features</pre>
<p>Not really sure what to do at this point because of this. Also might this be related to using the VM?</p>",Statistical Analysis Taking A Very Long Time,It takes a really long time. Is that expected?,statistical classification runtime,"<p>My script runs forever on my virtual machine, so I&#39;m running it off of biglab. I got the statistical analysis to work once (took about 2 minutes), but after un-commenting get_top_features and get_misclassified, my script gets killed before finishing the initial statistical analysis. I&#39;ve tried several times and I&#39;m unsure what the issue is; I&#39;ve copied my code below.</p>
<p></p>
<pre>#!/bin/python<br /><br />import os<br />import sys<br />import string<br />import random<br />import operator<br />from sklearn.tree import export_graphviz<br />from sklearn.tree import DecisionTreeClassifier<br />from sklearn.naive_bayes import MultinomialNB<br />from sklearn.linear_model import LogisticRegression<br />from sklearn.preprocessing import LabelEncoder<br />from sklearn.feature_extraction import DictVectorizer<br />from sklearn.cross_validation import train_test_split<br />from sklearn.externals.six import StringIO  <br /><br />#read in raw data from file and return a list of (label, article) tuples<br />def get_data(filename): <br />	data = [line.strip().split(&#39;\t&#39;) for line in open(filename).readlines()]<br />	random.shuffle(data)<br />	return data<br /><br />#this function builds the feature matrix for the Decision Tree.<br />def get_dtree_features(X) :<br />	features = []<br />	#TODO : Add the features you would like to use to train the Decision Tree here.<br />	feature_list = [&#39;gun&#39;, &#39;shooter&#39;, &#39;shot&#39;, &#39;shooting&#39;, &#39;player&#39;, &#39;coach&#39;, &#39;game&#39;, &#39;movie&#39;, &#39;actor&#39;, &#39;film&#39;, &#39;bullet&#39;, &#39;photo&#39;, &#39;model&#39;, &#39;point&#39;, &#39;suspect&#39;, &#39;victim&#39;, &#39;police&#39;, &#39;fired&#39;]<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w in feature_list : <br />				f[w] = 1.0<br />		features.append(f)<br />	return features<br /><br /><br />#this is the main function you care about; pack all the cleverest features you can think of into here.<br />def get_features(X) :<br />	stopwords = open(&#39;stopwords.txt&#39;).read()<br />	features = []<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w not in stopwords:            <br />				if w not in f : <br />					f[w] = 0<br />				f[w] &#43;= 1                <br />		features.append(f)<br />	return features<br /><br />#vectorize feature dictionaries and return feature and label matricies<br />def get_matricies(data, typ=&#34;unigram&#34;) : <br />	dv = DictVectorizer(sparse=True) <br />	le = LabelEncoder()<br />	y = [d[0] for d in data]<br />	texts = [d[1] for d in data]<br />	if typ == &#34;tree&#34;:<br />		X = get_dtree_features(texts)<br />	else :<br />		X = get_features(texts)<br />	#Here we are returning 5 things, the label vector y and feature matrix X, but also the texts from which the features were extracted and the <br />	#objects that were used to encode them. These will come in handy for your analysis, but you can ignore them for the initial parts of the assignment<br />	return le.fit_transform(y), dv.fit_transform(X), texts, dv, le<br /><br />#train and multinomial naive bayes classifier<br />def train_classifier(X, y):<br />	clf = LogisticRegression()<br />	clf.fit(X,y)<br />	return clf <br /><br />#train a Decision Tree classifier<br />def train_dtree_classifier(X, y):<br />	clf = DecisionTreeClassifier(max_depth=10)<br />	clf.fit(X,y)<br />	return clf<br /><br />#test the classifier<br />def test_classifier(clf, X, y):<br />	return clf.score(X,y)<br /><br />#cross validation	<br />def cross_validate(X, y, dv=None, typ=&#34;unigram&#34;, numfolds=5,):<br />	test_accs = []<br />	split = 1.0 / numfolds<br />	for i in range(numfolds):<br />		x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=i)<br />		if typ == &#34;tree&#34; :<br />			clf = train_dtree_classifier(x_train, y_train)<br />		else :<br />			clf = train_classifier(x_train, y_train)<br />		test_acc = test_classifier(clf, x_test, y_test)<br />		test_accs.append(test_acc)<br />		print &#39;Fold %d : %.05f&#39;%(i,test_acc)<br />	test_average = float(sum(test_accs))/ numfolds<br />	if typ == &#34;tree&#34; :<br />		with open(&#34;output.dot&#34;, &#39;w&#39;) as f:<br />			f = export_graphviz(clf, out_file=f, feature_names=dv.get_feature_names(), class_names=[&#39;Non Gun Related&#39;,&#39;Gun Related&#39;])<br />		create_graph(&#34;decision-tree.png&#34;)<br />	print &#39;Test Average : %.05f&#39;%(test_average)<br />	print<br />	return test_average<br /><br />#run a rule based classifier and calculate the accuracy<br />def rule_based_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO add more keywords, see how well they do alone and in combination<br />		if &#34;shooter&#34; in text or &#34;shot&#34; in text or &#34;shooting&#34; in text :<br />			if &#34;player&#34; in text or &#34;coach&#34; in text or &#34;game&#34; in text: <br />				prediction = &#39;0&#39;<br />			elif &#34;movie&#34; in text or &#34;film&#34; in text or &#34;actor&#34; in text:<br />				prediction = &#39;0&#39;<br />			else : prediction = &#39;1&#39;<br />		if &#34;gun&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Rule-based classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#Extra Credit Rule Based Classifier<br />def extra_credit_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO develop your conditional statements here<br />		if &#34;shooting&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Reverse Engineered classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#train and multinomial naive bayes classifier<br />def get_top_features(X, y, dv):<br />	clf = train_classifier(X, y)<br />	#the DictVectorizer object remembers which column number corresponds to which feature, and return the feature names in the correct order<br />	feature_names = dv.get_feature_names() <br /><br />	#The below code will get the weights from the classifier, and print out the weights of the features you are interested in<br />	features = [] #this will be a list of (feature_idx, weight) tuples<br />	for i,w in enumerate(clf.coef_[0]): <br />		features.append((i,w))<br />	#Sort the list by values, with the largest ones first<br />	features = sorted(features, key=lambda e: e[1], reverse=True)<br /><br />        #Print out the feature names and thier weights<br />	for i,w in features:<br />	  print &#39;%s\t%s&#39;%(feature_names[i], w)<br /><br />def get_misclassified_examples(y, X, texts) :<br />	x_train, x_test, y_train, y_test, train_texts, test_texts = train_test_split(X, y, texts)<br />	clf = train_classifier(x_train, y_train)<br /><br />	#TODO: You will have to write some code to call your classifier on each of the test examples, and check whether its prediction was right or wrong<br /><br />def create_graph(file_name) :<br />	os.system(&#34;dot -Tpng output.dot -o &#34; &#43; file_name)<br />	os.unlink(&#34;output.dot&#34;)<br /><br />if __name__ == &#39;__main__&#39; : <br /><br />	raw_data = get_data(sys.argv[1])<br />	<br />	print &#39;\nRule-based classification&#39;<br />	rule_based_classifier(raw_data)<br /><br />################ Decision Tree ################<br /><br /># print &#39;\nDecision Tree classification&#39;<br /># y, X, texts, dv, le = get_matricies(raw_data, &#34;tree&#34;)<br /># cross_validate(X,y,dv,&#34;tree&#34;)<br /><br />################ Statistical Classification ################<br />print &#39;\nStatistical classification&#39;<br />y, X, texts, dv, le = get_matricies(raw_data)<br />cross_validate(X,y)<br /><br />get_top_features(X, y, dv)<br />get_misclassified_examples(y, X, texts)<br />
</pre>
<p>Thanks in advance for any help you can give.</p>",Statistical Analysis &#34;Killed&#34; - even on biglab,"<p>When my program gets to the Statistical Analysis bit, it makes it through all the words and creates wordcount dictionaries that are appended to the list of features. Then it hangs for a bit until the terminal just prints &#34;Killed&#34; and program ends. Any idea what might be causing this? Not enough memory?</p>",Statistical Analysis &#34;Killed&#34;,,,"<p>I&#39;m confused on what exactly we&#39;re supposed to do for part 6. Are the y dimensions the same as the number of articles? Also, how is logistic regression supposed to be used here?</p>",Statistical Analysis Part 6,0
940849303,4/26/2016 15:45:33,true,1969397165,,4/26/2016 15:43:24,false,neodev,0.8889,21971187,TTO,08,Valsayn,190.213.132.190,0,0,"<p>Hi, so I read though the other piazza post about how the statistical analysis takes a long time, but I am still having some trouble with my code.</p>
<p></p>
<p>I have already tried waiting 10-15 minutes and removing all stopwords, but for some reason the statistical analysis refuses to complete in a reasonable amount of time</p>
<p></p>
<p>Below is my code:</p>
<pre>def isStopWord(word):
	with open(&#39;stopwords.txt&#39;, &#39;r&#39;) as stopwords:
		for line in stopwords:
			if word in line:
				return True
		return False

def get_features(X) : 
	features = []
	for x in X : 
		f = {}
		#TODO replace this dummy feature function with a unigram model, like we did in class
		for word in x.split():
			if isStopWord(word):
				continue
			if word in f:
				f[word] = f[word] &#43; 1.0
			else:
				f[word] =  1.0
		features.append(f)
	return features</pre>
<p>Not really sure what to do at this point because of this. Also might this be related to using the VM?</p>",Statistical Analysis Taking A Very Long Time,It takes a really long time. Is that expected?,statistical classification runtime,"<p>My script runs forever on my virtual machine, so I&#39;m running it off of biglab. I got the statistical analysis to work once (took about 2 minutes), but after un-commenting get_top_features and get_misclassified, my script gets killed before finishing the initial statistical analysis. I&#39;ve tried several times and I&#39;m unsure what the issue is; I&#39;ve copied my code below.</p>
<p></p>
<pre>#!/bin/python<br /><br />import os<br />import sys<br />import string<br />import random<br />import operator<br />from sklearn.tree import export_graphviz<br />from sklearn.tree import DecisionTreeClassifier<br />from sklearn.naive_bayes import MultinomialNB<br />from sklearn.linear_model import LogisticRegression<br />from sklearn.preprocessing import LabelEncoder<br />from sklearn.feature_extraction import DictVectorizer<br />from sklearn.cross_validation import train_test_split<br />from sklearn.externals.six import StringIO  <br /><br />#read in raw data from file and return a list of (label, article) tuples<br />def get_data(filename): <br />	data = [line.strip().split(&#39;\t&#39;) for line in open(filename).readlines()]<br />	random.shuffle(data)<br />	return data<br /><br />#this function builds the feature matrix for the Decision Tree.<br />def get_dtree_features(X) :<br />	features = []<br />	#TODO : Add the features you would like to use to train the Decision Tree here.<br />	feature_list = [&#39;gun&#39;, &#39;shooter&#39;, &#39;shot&#39;, &#39;shooting&#39;, &#39;player&#39;, &#39;coach&#39;, &#39;game&#39;, &#39;movie&#39;, &#39;actor&#39;, &#39;film&#39;, &#39;bullet&#39;, &#39;photo&#39;, &#39;model&#39;, &#39;point&#39;, &#39;suspect&#39;, &#39;victim&#39;, &#39;police&#39;, &#39;fired&#39;]<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w in feature_list : <br />				f[w] = 1.0<br />		features.append(f)<br />	return features<br /><br /><br />#this is the main function you care about; pack all the cleverest features you can think of into here.<br />def get_features(X) :<br />	stopwords = open(&#39;stopwords.txt&#39;).read()<br />	features = []<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w not in stopwords:            <br />				if w not in f : <br />					f[w] = 0<br />				f[w] &#43;= 1                <br />		features.append(f)<br />	return features<br /><br />#vectorize feature dictionaries and return feature and label matricies<br />def get_matricies(data, typ=&#34;unigram&#34;) : <br />	dv = DictVectorizer(sparse=True) <br />	le = LabelEncoder()<br />	y = [d[0] for d in data]<br />	texts = [d[1] for d in data]<br />	if typ == &#34;tree&#34;:<br />		X = get_dtree_features(texts)<br />	else :<br />		X = get_features(texts)<br />	#Here we are returning 5 things, the label vector y and feature matrix X, but also the texts from which the features were extracted and the <br />	#objects that were used to encode them. These will come in handy for your analysis, but you can ignore them for the initial parts of the assignment<br />	return le.fit_transform(y), dv.fit_transform(X), texts, dv, le<br /><br />#train and multinomial naive bayes classifier<br />def train_classifier(X, y):<br />	clf = LogisticRegression()<br />	clf.fit(X,y)<br />	return clf <br /><br />#train a Decision Tree classifier<br />def train_dtree_classifier(X, y):<br />	clf = DecisionTreeClassifier(max_depth=10)<br />	clf.fit(X,y)<br />	return clf<br /><br />#test the classifier<br />def test_classifier(clf, X, y):<br />	return clf.score(X,y)<br /><br />#cross validation	<br />def cross_validate(X, y, dv=None, typ=&#34;unigram&#34;, numfolds=5,):<br />	test_accs = []<br />	split = 1.0 / numfolds<br />	for i in range(numfolds):<br />		x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=i)<br />		if typ == &#34;tree&#34; :<br />			clf = train_dtree_classifier(x_train, y_train)<br />		else :<br />			clf = train_classifier(x_train, y_train)<br />		test_acc = test_classifier(clf, x_test, y_test)<br />		test_accs.append(test_acc)<br />		print &#39;Fold %d : %.05f&#39;%(i,test_acc)<br />	test_average = float(sum(test_accs))/ numfolds<br />	if typ == &#34;tree&#34; :<br />		with open(&#34;output.dot&#34;, &#39;w&#39;) as f:<br />			f = export_graphviz(clf, out_file=f, feature_names=dv.get_feature_names(), class_names=[&#39;Non Gun Related&#39;,&#39;Gun Related&#39;])<br />		create_graph(&#34;decision-tree.png&#34;)<br />	print &#39;Test Average : %.05f&#39;%(test_average)<br />	print<br />	return test_average<br /><br />#run a rule based classifier and calculate the accuracy<br />def rule_based_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO add more keywords, see how well they do alone and in combination<br />		if &#34;shooter&#34; in text or &#34;shot&#34; in text or &#34;shooting&#34; in text :<br />			if &#34;player&#34; in text or &#34;coach&#34; in text or &#34;game&#34; in text: <br />				prediction = &#39;0&#39;<br />			elif &#34;movie&#34; in text or &#34;film&#34; in text or &#34;actor&#34; in text:<br />				prediction = &#39;0&#39;<br />			else : prediction = &#39;1&#39;<br />		if &#34;gun&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Rule-based classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#Extra Credit Rule Based Classifier<br />def extra_credit_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO develop your conditional statements here<br />		if &#34;shooting&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Reverse Engineered classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#train and multinomial naive bayes classifier<br />def get_top_features(X, y, dv):<br />	clf = train_classifier(X, y)<br />	#the DictVectorizer object remembers which column number corresponds to which feature, and return the feature names in the correct order<br />	feature_names = dv.get_feature_names() <br /><br />	#The below code will get the weights from the classifier, and print out the weights of the features you are interested in<br />	features = [] #this will be a list of (feature_idx, weight) tuples<br />	for i,w in enumerate(clf.coef_[0]): <br />		features.append((i,w))<br />	#Sort the list by values, with the largest ones first<br />	features = sorted(features, key=lambda e: e[1], reverse=True)<br /><br />        #Print out the feature names and thier weights<br />	for i,w in features:<br />	  print &#39;%s\t%s&#39;%(feature_names[i], w)<br /><br />def get_misclassified_examples(y, X, texts) :<br />	x_train, x_test, y_train, y_test, train_texts, test_texts = train_test_split(X, y, texts)<br />	clf = train_classifier(x_train, y_train)<br /><br />	#TODO: You will have to write some code to call your classifier on each of the test examples, and check whether its prediction was right or wrong<br /><br />def create_graph(file_name) :<br />	os.system(&#34;dot -Tpng output.dot -o &#34; &#43; file_name)<br />	os.unlink(&#34;output.dot&#34;)<br /><br />if __name__ == &#39;__main__&#39; : <br /><br />	raw_data = get_data(sys.argv[1])<br />	<br />	print &#39;\nRule-based classification&#39;<br />	rule_based_classifier(raw_data)<br /><br />################ Decision Tree ################<br /><br /># print &#39;\nDecision Tree classification&#39;<br /># y, X, texts, dv, le = get_matricies(raw_data, &#34;tree&#34;)<br /># cross_validate(X,y,dv,&#34;tree&#34;)<br /><br />################ Statistical Classification ################<br />print &#39;\nStatistical classification&#39;<br />y, X, texts, dv, le = get_matricies(raw_data)<br />cross_validate(X,y)<br /><br />get_top_features(X, y, dv)<br />get_misclassified_examples(y, X, texts)<br />
</pre>
<p>Thanks in advance for any help you can give.</p>",Statistical Analysis &#34;Killed&#34; - even on biglab,"<p>When my program gets to the Statistical Analysis bit, it makes it through all the words and creates wordcount dictionaries that are appended to the list of features. Then it hangs for a bit until the terminal just prints &#34;Killed&#34; and program ends. Any idea what might be causing this? Not enough memory?</p>",Statistical Analysis &#34;Killed&#34;,,,"<p>I&#39;m confused on what exactly we&#39;re supposed to do for part 6. Are the y dimensions the same as the number of articles? Also, how is logistic regression supposed to be used here?</p>",Statistical Analysis Part 6,0
940849303,4/26/2016 15:45:34,true,1969397180,,4/26/2016 15:37:11,false,elite,0.8889,36575101,IND,07,New Delhi,112.196.144.2,0,0,"<p>Hi, so I read though the other piazza post about how the statistical analysis takes a long time, but I am still having some trouble with my code.</p>
<p></p>
<p>I have already tried waiting 10-15 minutes and removing all stopwords, but for some reason the statistical analysis refuses to complete in a reasonable amount of time</p>
<p></p>
<p>Below is my code:</p>
<pre>def isStopWord(word):
	with open(&#39;stopwords.txt&#39;, &#39;r&#39;) as stopwords:
		for line in stopwords:
			if word in line:
				return True
		return False

def get_features(X) : 
	features = []
	for x in X : 
		f = {}
		#TODO replace this dummy feature function with a unigram model, like we did in class
		for word in x.split():
			if isStopWord(word):
				continue
			if word in f:
				f[word] = f[word] &#43; 1.0
			else:
				f[word] =  1.0
		features.append(f)
	return features</pre>
<p>Not really sure what to do at this point because of this. Also might this be related to using the VM?</p>",Statistical Analysis Taking A Very Long Time,It takes a really long time. Is that expected?,statistical classification runtime,"<p>My script runs forever on my virtual machine, so I&#39;m running it off of biglab. I got the statistical analysis to work once (took about 2 minutes), but after un-commenting get_top_features and get_misclassified, my script gets killed before finishing the initial statistical analysis. I&#39;ve tried several times and I&#39;m unsure what the issue is; I&#39;ve copied my code below.</p>
<p></p>
<pre>#!/bin/python<br /><br />import os<br />import sys<br />import string<br />import random<br />import operator<br />from sklearn.tree import export_graphviz<br />from sklearn.tree import DecisionTreeClassifier<br />from sklearn.naive_bayes import MultinomialNB<br />from sklearn.linear_model import LogisticRegression<br />from sklearn.preprocessing import LabelEncoder<br />from sklearn.feature_extraction import DictVectorizer<br />from sklearn.cross_validation import train_test_split<br />from sklearn.externals.six import StringIO  <br /><br />#read in raw data from file and return a list of (label, article) tuples<br />def get_data(filename): <br />	data = [line.strip().split(&#39;\t&#39;) for line in open(filename).readlines()]<br />	random.shuffle(data)<br />	return data<br /><br />#this function builds the feature matrix for the Decision Tree.<br />def get_dtree_features(X) :<br />	features = []<br />	#TODO : Add the features you would like to use to train the Decision Tree here.<br />	feature_list = [&#39;gun&#39;, &#39;shooter&#39;, &#39;shot&#39;, &#39;shooting&#39;, &#39;player&#39;, &#39;coach&#39;, &#39;game&#39;, &#39;movie&#39;, &#39;actor&#39;, &#39;film&#39;, &#39;bullet&#39;, &#39;photo&#39;, &#39;model&#39;, &#39;point&#39;, &#39;suspect&#39;, &#39;victim&#39;, &#39;police&#39;, &#39;fired&#39;]<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w in feature_list : <br />				f[w] = 1.0<br />		features.append(f)<br />	return features<br /><br /><br />#this is the main function you care about; pack all the cleverest features you can think of into here.<br />def get_features(X) :<br />	stopwords = open(&#39;stopwords.txt&#39;).read()<br />	features = []<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w not in stopwords:            <br />				if w not in f : <br />					f[w] = 0<br />				f[w] &#43;= 1                <br />		features.append(f)<br />	return features<br /><br />#vectorize feature dictionaries and return feature and label matricies<br />def get_matricies(data, typ=&#34;unigram&#34;) : <br />	dv = DictVectorizer(sparse=True) <br />	le = LabelEncoder()<br />	y = [d[0] for d in data]<br />	texts = [d[1] for d in data]<br />	if typ == &#34;tree&#34;:<br />		X = get_dtree_features(texts)<br />	else :<br />		X = get_features(texts)<br />	#Here we are returning 5 things, the label vector y and feature matrix X, but also the texts from which the features were extracted and the <br />	#objects that were used to encode them. These will come in handy for your analysis, but you can ignore them for the initial parts of the assignment<br />	return le.fit_transform(y), dv.fit_transform(X), texts, dv, le<br /><br />#train and multinomial naive bayes classifier<br />def train_classifier(X, y):<br />	clf = LogisticRegression()<br />	clf.fit(X,y)<br />	return clf <br /><br />#train a Decision Tree classifier<br />def train_dtree_classifier(X, y):<br />	clf = DecisionTreeClassifier(max_depth=10)<br />	clf.fit(X,y)<br />	return clf<br /><br />#test the classifier<br />def test_classifier(clf, X, y):<br />	return clf.score(X,y)<br /><br />#cross validation	<br />def cross_validate(X, y, dv=None, typ=&#34;unigram&#34;, numfolds=5,):<br />	test_accs = []<br />	split = 1.0 / numfolds<br />	for i in range(numfolds):<br />		x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=i)<br />		if typ == &#34;tree&#34; :<br />			clf = train_dtree_classifier(x_train, y_train)<br />		else :<br />			clf = train_classifier(x_train, y_train)<br />		test_acc = test_classifier(clf, x_test, y_test)<br />		test_accs.append(test_acc)<br />		print &#39;Fold %d : %.05f&#39;%(i,test_acc)<br />	test_average = float(sum(test_accs))/ numfolds<br />	if typ == &#34;tree&#34; :<br />		with open(&#34;output.dot&#34;, &#39;w&#39;) as f:<br />			f = export_graphviz(clf, out_file=f, feature_names=dv.get_feature_names(), class_names=[&#39;Non Gun Related&#39;,&#39;Gun Related&#39;])<br />		create_graph(&#34;decision-tree.png&#34;)<br />	print &#39;Test Average : %.05f&#39;%(test_average)<br />	print<br />	return test_average<br /><br />#run a rule based classifier and calculate the accuracy<br />def rule_based_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO add more keywords, see how well they do alone and in combination<br />		if &#34;shooter&#34; in text or &#34;shot&#34; in text or &#34;shooting&#34; in text :<br />			if &#34;player&#34; in text or &#34;coach&#34; in text or &#34;game&#34; in text: <br />				prediction = &#39;0&#39;<br />			elif &#34;movie&#34; in text or &#34;film&#34; in text or &#34;actor&#34; in text:<br />				prediction = &#39;0&#39;<br />			else : prediction = &#39;1&#39;<br />		if &#34;gun&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Rule-based classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#Extra Credit Rule Based Classifier<br />def extra_credit_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO develop your conditional statements here<br />		if &#34;shooting&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Reverse Engineered classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#train and multinomial naive bayes classifier<br />def get_top_features(X, y, dv):<br />	clf = train_classifier(X, y)<br />	#the DictVectorizer object remembers which column number corresponds to which feature, and return the feature names in the correct order<br />	feature_names = dv.get_feature_names() <br /><br />	#The below code will get the weights from the classifier, and print out the weights of the features you are interested in<br />	features = [] #this will be a list of (feature_idx, weight) tuples<br />	for i,w in enumerate(clf.coef_[0]): <br />		features.append((i,w))<br />	#Sort the list by values, with the largest ones first<br />	features = sorted(features, key=lambda e: e[1], reverse=True)<br /><br />        #Print out the feature names and thier weights<br />	for i,w in features:<br />	  print &#39;%s\t%s&#39;%(feature_names[i], w)<br /><br />def get_misclassified_examples(y, X, texts) :<br />	x_train, x_test, y_train, y_test, train_texts, test_texts = train_test_split(X, y, texts)<br />	clf = train_classifier(x_train, y_train)<br /><br />	#TODO: You will have to write some code to call your classifier on each of the test examples, and check whether its prediction was right or wrong<br /><br />def create_graph(file_name) :<br />	os.system(&#34;dot -Tpng output.dot -o &#34; &#43; file_name)<br />	os.unlink(&#34;output.dot&#34;)<br /><br />if __name__ == &#39;__main__&#39; : <br /><br />	raw_data = get_data(sys.argv[1])<br />	<br />	print &#39;\nRule-based classification&#39;<br />	rule_based_classifier(raw_data)<br /><br />################ Decision Tree ################<br /><br /># print &#39;\nDecision Tree classification&#39;<br /># y, X, texts, dv, le = get_matricies(raw_data, &#34;tree&#34;)<br /># cross_validate(X,y,dv,&#34;tree&#34;)<br /><br />################ Statistical Classification ################<br />print &#39;\nStatistical classification&#39;<br />y, X, texts, dv, le = get_matricies(raw_data)<br />cross_validate(X,y)<br /><br />get_top_features(X, y, dv)<br />get_misclassified_examples(y, X, texts)<br />
</pre>
<p>Thanks in advance for any help you can give.</p>",Statistical Analysis &#34;Killed&#34; - even on biglab,"<p>When my program gets to the Statistical Analysis bit, it makes it through all the words and creates wordcount dictionaries that are appended to the list of features. Then it hangs for a bit until the terminal just prints &#34;Killed&#34; and program ends. Any idea what might be causing this? Not enough memory?</p>",Statistical Analysis &#34;Killed&#34;,,,"<p>I&#39;m confused on what exactly we&#39;re supposed to do for part 6. Are the y dimensions the same as the number of articles? Also, how is logistic regression supposed to be used here?</p>",Statistical Analysis Part 6,0
940849303,4/26/2016 15:45:41,true,1969397212,,4/26/2016 15:39:57,false,clixsense,1.0,21875134,GBR,H9,London,87.112.158.81,0,0,"<p>Hi, so I read though the other piazza post about how the statistical analysis takes a long time, but I am still having some trouble with my code.</p>
<p></p>
<p>I have already tried waiting 10-15 minutes and removing all stopwords, but for some reason the statistical analysis refuses to complete in a reasonable amount of time</p>
<p></p>
<p>Below is my code:</p>
<pre>def isStopWord(word):
	with open(&#39;stopwords.txt&#39;, &#39;r&#39;) as stopwords:
		for line in stopwords:
			if word in line:
				return True
		return False

def get_features(X) : 
	features = []
	for x in X : 
		f = {}
		#TODO replace this dummy feature function with a unigram model, like we did in class
		for word in x.split():
			if isStopWord(word):
				continue
			if word in f:
				f[word] = f[word] &#43; 1.0
			else:
				f[word] =  1.0
		features.append(f)
	return features</pre>
<p>Not really sure what to do at this point because of this. Also might this be related to using the VM?</p>",Statistical Analysis Taking A Very Long Time,It takes a really long time. Is that expected?,statistical classification runtime,"<p>My script runs forever on my virtual machine, so I&#39;m running it off of biglab. I got the statistical analysis to work once (took about 2 minutes), but after un-commenting get_top_features and get_misclassified, my script gets killed before finishing the initial statistical analysis. I&#39;ve tried several times and I&#39;m unsure what the issue is; I&#39;ve copied my code below.</p>
<p></p>
<pre>#!/bin/python<br /><br />import os<br />import sys<br />import string<br />import random<br />import operator<br />from sklearn.tree import export_graphviz<br />from sklearn.tree import DecisionTreeClassifier<br />from sklearn.naive_bayes import MultinomialNB<br />from sklearn.linear_model import LogisticRegression<br />from sklearn.preprocessing import LabelEncoder<br />from sklearn.feature_extraction import DictVectorizer<br />from sklearn.cross_validation import train_test_split<br />from sklearn.externals.six import StringIO  <br /><br />#read in raw data from file and return a list of (label, article) tuples<br />def get_data(filename): <br />	data = [line.strip().split(&#39;\t&#39;) for line in open(filename).readlines()]<br />	random.shuffle(data)<br />	return data<br /><br />#this function builds the feature matrix for the Decision Tree.<br />def get_dtree_features(X) :<br />	features = []<br />	#TODO : Add the features you would like to use to train the Decision Tree here.<br />	feature_list = [&#39;gun&#39;, &#39;shooter&#39;, &#39;shot&#39;, &#39;shooting&#39;, &#39;player&#39;, &#39;coach&#39;, &#39;game&#39;, &#39;movie&#39;, &#39;actor&#39;, &#39;film&#39;, &#39;bullet&#39;, &#39;photo&#39;, &#39;model&#39;, &#39;point&#39;, &#39;suspect&#39;, &#39;victim&#39;, &#39;police&#39;, &#39;fired&#39;]<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w in feature_list : <br />				f[w] = 1.0<br />		features.append(f)<br />	return features<br /><br /><br />#this is the main function you care about; pack all the cleverest features you can think of into here.<br />def get_features(X) :<br />	stopwords = open(&#39;stopwords.txt&#39;).read()<br />	features = []<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w not in stopwords:            <br />				if w not in f : <br />					f[w] = 0<br />				f[w] &#43;= 1                <br />		features.append(f)<br />	return features<br /><br />#vectorize feature dictionaries and return feature and label matricies<br />def get_matricies(data, typ=&#34;unigram&#34;) : <br />	dv = DictVectorizer(sparse=True) <br />	le = LabelEncoder()<br />	y = [d[0] for d in data]<br />	texts = [d[1] for d in data]<br />	if typ == &#34;tree&#34;:<br />		X = get_dtree_features(texts)<br />	else :<br />		X = get_features(texts)<br />	#Here we are returning 5 things, the label vector y and feature matrix X, but also the texts from which the features were extracted and the <br />	#objects that were used to encode them. These will come in handy for your analysis, but you can ignore them for the initial parts of the assignment<br />	return le.fit_transform(y), dv.fit_transform(X), texts, dv, le<br /><br />#train and multinomial naive bayes classifier<br />def train_classifier(X, y):<br />	clf = LogisticRegression()<br />	clf.fit(X,y)<br />	return clf <br /><br />#train a Decision Tree classifier<br />def train_dtree_classifier(X, y):<br />	clf = DecisionTreeClassifier(max_depth=10)<br />	clf.fit(X,y)<br />	return clf<br /><br />#test the classifier<br />def test_classifier(clf, X, y):<br />	return clf.score(X,y)<br /><br />#cross validation	<br />def cross_validate(X, y, dv=None, typ=&#34;unigram&#34;, numfolds=5,):<br />	test_accs = []<br />	split = 1.0 / numfolds<br />	for i in range(numfolds):<br />		x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=i)<br />		if typ == &#34;tree&#34; :<br />			clf = train_dtree_classifier(x_train, y_train)<br />		else :<br />			clf = train_classifier(x_train, y_train)<br />		test_acc = test_classifier(clf, x_test, y_test)<br />		test_accs.append(test_acc)<br />		print &#39;Fold %d : %.05f&#39;%(i,test_acc)<br />	test_average = float(sum(test_accs))/ numfolds<br />	if typ == &#34;tree&#34; :<br />		with open(&#34;output.dot&#34;, &#39;w&#39;) as f:<br />			f = export_graphviz(clf, out_file=f, feature_names=dv.get_feature_names(), class_names=[&#39;Non Gun Related&#39;,&#39;Gun Related&#39;])<br />		create_graph(&#34;decision-tree.png&#34;)<br />	print &#39;Test Average : %.05f&#39;%(test_average)<br />	print<br />	return test_average<br /><br />#run a rule based classifier and calculate the accuracy<br />def rule_based_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO add more keywords, see how well they do alone and in combination<br />		if &#34;shooter&#34; in text or &#34;shot&#34; in text or &#34;shooting&#34; in text :<br />			if &#34;player&#34; in text or &#34;coach&#34; in text or &#34;game&#34; in text: <br />				prediction = &#39;0&#39;<br />			elif &#34;movie&#34; in text or &#34;film&#34; in text or &#34;actor&#34; in text:<br />				prediction = &#39;0&#39;<br />			else : prediction = &#39;1&#39;<br />		if &#34;gun&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Rule-based classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#Extra Credit Rule Based Classifier<br />def extra_credit_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO develop your conditional statements here<br />		if &#34;shooting&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Reverse Engineered classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#train and multinomial naive bayes classifier<br />def get_top_features(X, y, dv):<br />	clf = train_classifier(X, y)<br />	#the DictVectorizer object remembers which column number corresponds to which feature, and return the feature names in the correct order<br />	feature_names = dv.get_feature_names() <br /><br />	#The below code will get the weights from the classifier, and print out the weights of the features you are interested in<br />	features = [] #this will be a list of (feature_idx, weight) tuples<br />	for i,w in enumerate(clf.coef_[0]): <br />		features.append((i,w))<br />	#Sort the list by values, with the largest ones first<br />	features = sorted(features, key=lambda e: e[1], reverse=True)<br /><br />        #Print out the feature names and thier weights<br />	for i,w in features:<br />	  print &#39;%s\t%s&#39;%(feature_names[i], w)<br /><br />def get_misclassified_examples(y, X, texts) :<br />	x_train, x_test, y_train, y_test, train_texts, test_texts = train_test_split(X, y, texts)<br />	clf = train_classifier(x_train, y_train)<br /><br />	#TODO: You will have to write some code to call your classifier on each of the test examples, and check whether its prediction was right or wrong<br /><br />def create_graph(file_name) :<br />	os.system(&#34;dot -Tpng output.dot -o &#34; &#43; file_name)<br />	os.unlink(&#34;output.dot&#34;)<br /><br />if __name__ == &#39;__main__&#39; : <br /><br />	raw_data = get_data(sys.argv[1])<br />	<br />	print &#39;\nRule-based classification&#39;<br />	rule_based_classifier(raw_data)<br /><br />################ Decision Tree ################<br /><br /># print &#39;\nDecision Tree classification&#39;<br /># y, X, texts, dv, le = get_matricies(raw_data, &#34;tree&#34;)<br /># cross_validate(X,y,dv,&#34;tree&#34;)<br /><br />################ Statistical Classification ################<br />print &#39;\nStatistical classification&#39;<br />y, X, texts, dv, le = get_matricies(raw_data)<br />cross_validate(X,y)<br /><br />get_top_features(X, y, dv)<br />get_misclassified_examples(y, X, texts)<br />
</pre>
<p>Thanks in advance for any help you can give.</p>",Statistical Analysis &#34;Killed&#34; - even on biglab,"<p>When my program gets to the Statistical Analysis bit, it makes it through all the words and creates wordcount dictionaries that are appended to the list of features. Then it hangs for a bit until the terminal just prints &#34;Killed&#34; and program ends. Any idea what might be causing this? Not enough memory?</p>",Statistical Analysis &#34;Killed&#34;,,,"<p>I&#39;m confused on what exactly we&#39;re supposed to do for part 6. Are the y dimensions the same as the number of articles? Also, how is logistic regression supposed to be used here?</p>",Statistical Analysis Part 6,0
940849303,4/26/2016 15:54:59,true,1969401307,,4/26/2016 15:53:17,false,elite,1.0,33243069,IND,10,Faridabad,116.203.79.150,0,0,"<p>Hi, so I read though the other piazza post about how the statistical analysis takes a long time, but I am still having some trouble with my code.</p>
<p></p>
<p>I have already tried waiting 10-15 minutes and removing all stopwords, but for some reason the statistical analysis refuses to complete in a reasonable amount of time</p>
<p></p>
<p>Below is my code:</p>
<pre>def isStopWord(word):
	with open(&#39;stopwords.txt&#39;, &#39;r&#39;) as stopwords:
		for line in stopwords:
			if word in line:
				return True
		return False

def get_features(X) : 
	features = []
	for x in X : 
		f = {}
		#TODO replace this dummy feature function with a unigram model, like we did in class
		for word in x.split():
			if isStopWord(word):
				continue
			if word in f:
				f[word] = f[word] &#43; 1.0
			else:
				f[word] =  1.0
		features.append(f)
	return features</pre>
<p>Not really sure what to do at this point because of this. Also might this be related to using the VM?</p>",Statistical Analysis Taking A Very Long Time,It takes a really long time. Is that expected?,statistical classification runtime,"<p>My script runs forever on my virtual machine, so I&#39;m running it off of biglab. I got the statistical analysis to work once (took about 2 minutes), but after un-commenting get_top_features and get_misclassified, my script gets killed before finishing the initial statistical analysis. I&#39;ve tried several times and I&#39;m unsure what the issue is; I&#39;ve copied my code below.</p>
<p></p>
<pre>#!/bin/python<br /><br />import os<br />import sys<br />import string<br />import random<br />import operator<br />from sklearn.tree import export_graphviz<br />from sklearn.tree import DecisionTreeClassifier<br />from sklearn.naive_bayes import MultinomialNB<br />from sklearn.linear_model import LogisticRegression<br />from sklearn.preprocessing import LabelEncoder<br />from sklearn.feature_extraction import DictVectorizer<br />from sklearn.cross_validation import train_test_split<br />from sklearn.externals.six import StringIO  <br /><br />#read in raw data from file and return a list of (label, article) tuples<br />def get_data(filename): <br />	data = [line.strip().split(&#39;\t&#39;) for line in open(filename).readlines()]<br />	random.shuffle(data)<br />	return data<br /><br />#this function builds the feature matrix for the Decision Tree.<br />def get_dtree_features(X) :<br />	features = []<br />	#TODO : Add the features you would like to use to train the Decision Tree here.<br />	feature_list = [&#39;gun&#39;, &#39;shooter&#39;, &#39;shot&#39;, &#39;shooting&#39;, &#39;player&#39;, &#39;coach&#39;, &#39;game&#39;, &#39;movie&#39;, &#39;actor&#39;, &#39;film&#39;, &#39;bullet&#39;, &#39;photo&#39;, &#39;model&#39;, &#39;point&#39;, &#39;suspect&#39;, &#39;victim&#39;, &#39;police&#39;, &#39;fired&#39;]<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w in feature_list : <br />				f[w] = 1.0<br />		features.append(f)<br />	return features<br /><br /><br />#this is the main function you care about; pack all the cleverest features you can think of into here.<br />def get_features(X) :<br />	stopwords = open(&#39;stopwords.txt&#39;).read()<br />	features = []<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w not in stopwords:            <br />				if w not in f : <br />					f[w] = 0<br />				f[w] &#43;= 1                <br />		features.append(f)<br />	return features<br /><br />#vectorize feature dictionaries and return feature and label matricies<br />def get_matricies(data, typ=&#34;unigram&#34;) : <br />	dv = DictVectorizer(sparse=True) <br />	le = LabelEncoder()<br />	y = [d[0] for d in data]<br />	texts = [d[1] for d in data]<br />	if typ == &#34;tree&#34;:<br />		X = get_dtree_features(texts)<br />	else :<br />		X = get_features(texts)<br />	#Here we are returning 5 things, the label vector y and feature matrix X, but also the texts from which the features were extracted and the <br />	#objects that were used to encode them. These will come in handy for your analysis, but you can ignore them for the initial parts of the assignment<br />	return le.fit_transform(y), dv.fit_transform(X), texts, dv, le<br /><br />#train and multinomial naive bayes classifier<br />def train_classifier(X, y):<br />	clf = LogisticRegression()<br />	clf.fit(X,y)<br />	return clf <br /><br />#train a Decision Tree classifier<br />def train_dtree_classifier(X, y):<br />	clf = DecisionTreeClassifier(max_depth=10)<br />	clf.fit(X,y)<br />	return clf<br /><br />#test the classifier<br />def test_classifier(clf, X, y):<br />	return clf.score(X,y)<br /><br />#cross validation	<br />def cross_validate(X, y, dv=None, typ=&#34;unigram&#34;, numfolds=5,):<br />	test_accs = []<br />	split = 1.0 / numfolds<br />	for i in range(numfolds):<br />		x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=i)<br />		if typ == &#34;tree&#34; :<br />			clf = train_dtree_classifier(x_train, y_train)<br />		else :<br />			clf = train_classifier(x_train, y_train)<br />		test_acc = test_classifier(clf, x_test, y_test)<br />		test_accs.append(test_acc)<br />		print &#39;Fold %d : %.05f&#39;%(i,test_acc)<br />	test_average = float(sum(test_accs))/ numfolds<br />	if typ == &#34;tree&#34; :<br />		with open(&#34;output.dot&#34;, &#39;w&#39;) as f:<br />			f = export_graphviz(clf, out_file=f, feature_names=dv.get_feature_names(), class_names=[&#39;Non Gun Related&#39;,&#39;Gun Related&#39;])<br />		create_graph(&#34;decision-tree.png&#34;)<br />	print &#39;Test Average : %.05f&#39;%(test_average)<br />	print<br />	return test_average<br /><br />#run a rule based classifier and calculate the accuracy<br />def rule_based_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO add more keywords, see how well they do alone and in combination<br />		if &#34;shooter&#34; in text or &#34;shot&#34; in text or &#34;shooting&#34; in text :<br />			if &#34;player&#34; in text or &#34;coach&#34; in text or &#34;game&#34; in text: <br />				prediction = &#39;0&#39;<br />			elif &#34;movie&#34; in text or &#34;film&#34; in text or &#34;actor&#34; in text:<br />				prediction = &#39;0&#39;<br />			else : prediction = &#39;1&#39;<br />		if &#34;gun&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Rule-based classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#Extra Credit Rule Based Classifier<br />def extra_credit_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO develop your conditional statements here<br />		if &#34;shooting&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Reverse Engineered classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#train and multinomial naive bayes classifier<br />def get_top_features(X, y, dv):<br />	clf = train_classifier(X, y)<br />	#the DictVectorizer object remembers which column number corresponds to which feature, and return the feature names in the correct order<br />	feature_names = dv.get_feature_names() <br /><br />	#The below code will get the weights from the classifier, and print out the weights of the features you are interested in<br />	features = [] #this will be a list of (feature_idx, weight) tuples<br />	for i,w in enumerate(clf.coef_[0]): <br />		features.append((i,w))<br />	#Sort the list by values, with the largest ones first<br />	features = sorted(features, key=lambda e: e[1], reverse=True)<br /><br />        #Print out the feature names and thier weights<br />	for i,w in features:<br />	  print &#39;%s\t%s&#39;%(feature_names[i], w)<br /><br />def get_misclassified_examples(y, X, texts) :<br />	x_train, x_test, y_train, y_test, train_texts, test_texts = train_test_split(X, y, texts)<br />	clf = train_classifier(x_train, y_train)<br /><br />	#TODO: You will have to write some code to call your classifier on each of the test examples, and check whether its prediction was right or wrong<br /><br />def create_graph(file_name) :<br />	os.system(&#34;dot -Tpng output.dot -o &#34; &#43; file_name)<br />	os.unlink(&#34;output.dot&#34;)<br /><br />if __name__ == &#39;__main__&#39; : <br /><br />	raw_data = get_data(sys.argv[1])<br />	<br />	print &#39;\nRule-based classification&#39;<br />	rule_based_classifier(raw_data)<br /><br />################ Decision Tree ################<br /><br /># print &#39;\nDecision Tree classification&#39;<br /># y, X, texts, dv, le = get_matricies(raw_data, &#34;tree&#34;)<br /># cross_validate(X,y,dv,&#34;tree&#34;)<br /><br />################ Statistical Classification ################<br />print &#39;\nStatistical classification&#39;<br />y, X, texts, dv, le = get_matricies(raw_data)<br />cross_validate(X,y)<br /><br />get_top_features(X, y, dv)<br />get_misclassified_examples(y, X, texts)<br />
</pre>
<p>Thanks in advance for any help you can give.</p>",Statistical Analysis &#34;Killed&#34; - even on biglab,"<p>When my program gets to the Statistical Analysis bit, it makes it through all the words and creates wordcount dictionaries that are appended to the list of features. Then it hangs for a bit until the terminal just prints &#34;Killed&#34; and program ends. Any idea what might be causing this? Not enough memory?</p>",Statistical Analysis &#34;Killed&#34;,,,"<p>I&#39;m confused on what exactly we&#39;re supposed to do for part 6. Are the y dimensions the same as the number of articles? Also, how is logistic regression supposed to be used here?</p>",Statistical Analysis Part 6,0
940849303,4/26/2016 16:02:51,true,1969405078,,4/26/2016 16:02:22,false,personaly,1.0,33663352,ARG,01,Mar Del Plata,181.168.213.227,0,0,"<p>Hi, so I read though the other piazza post about how the statistical analysis takes a long time, but I am still having some trouble with my code.</p>
<p></p>
<p>I have already tried waiting 10-15 minutes and removing all stopwords, but for some reason the statistical analysis refuses to complete in a reasonable amount of time</p>
<p></p>
<p>Below is my code:</p>
<pre>def isStopWord(word):
	with open(&#39;stopwords.txt&#39;, &#39;r&#39;) as stopwords:
		for line in stopwords:
			if word in line:
				return True
		return False

def get_features(X) : 
	features = []
	for x in X : 
		f = {}
		#TODO replace this dummy feature function with a unigram model, like we did in class
		for word in x.split():
			if isStopWord(word):
				continue
			if word in f:
				f[word] = f[word] &#43; 1.0
			else:
				f[word] =  1.0
		features.append(f)
	return features</pre>
<p>Not really sure what to do at this point because of this. Also might this be related to using the VM?</p>",Statistical Analysis Taking A Very Long Time,It takes a really long time. Is that expected?,statistical classification runtime,"<p>My script runs forever on my virtual machine, so I&#39;m running it off of biglab. I got the statistical analysis to work once (took about 2 minutes), but after un-commenting get_top_features and get_misclassified, my script gets killed before finishing the initial statistical analysis. I&#39;ve tried several times and I&#39;m unsure what the issue is; I&#39;ve copied my code below.</p>
<p></p>
<pre>#!/bin/python<br /><br />import os<br />import sys<br />import string<br />import random<br />import operator<br />from sklearn.tree import export_graphviz<br />from sklearn.tree import DecisionTreeClassifier<br />from sklearn.naive_bayes import MultinomialNB<br />from sklearn.linear_model import LogisticRegression<br />from sklearn.preprocessing import LabelEncoder<br />from sklearn.feature_extraction import DictVectorizer<br />from sklearn.cross_validation import train_test_split<br />from sklearn.externals.six import StringIO  <br /><br />#read in raw data from file and return a list of (label, article) tuples<br />def get_data(filename): <br />	data = [line.strip().split(&#39;\t&#39;) for line in open(filename).readlines()]<br />	random.shuffle(data)<br />	return data<br /><br />#this function builds the feature matrix for the Decision Tree.<br />def get_dtree_features(X) :<br />	features = []<br />	#TODO : Add the features you would like to use to train the Decision Tree here.<br />	feature_list = [&#39;gun&#39;, &#39;shooter&#39;, &#39;shot&#39;, &#39;shooting&#39;, &#39;player&#39;, &#39;coach&#39;, &#39;game&#39;, &#39;movie&#39;, &#39;actor&#39;, &#39;film&#39;, &#39;bullet&#39;, &#39;photo&#39;, &#39;model&#39;, &#39;point&#39;, &#39;suspect&#39;, &#39;victim&#39;, &#39;police&#39;, &#39;fired&#39;]<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w in feature_list : <br />				f[w] = 1.0<br />		features.append(f)<br />	return features<br /><br /><br />#this is the main function you care about; pack all the cleverest features you can think of into here.<br />def get_features(X) :<br />	stopwords = open(&#39;stopwords.txt&#39;).read()<br />	features = []<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w not in stopwords:            <br />				if w not in f : <br />					f[w] = 0<br />				f[w] &#43;= 1                <br />		features.append(f)<br />	return features<br /><br />#vectorize feature dictionaries and return feature and label matricies<br />def get_matricies(data, typ=&#34;unigram&#34;) : <br />	dv = DictVectorizer(sparse=True) <br />	le = LabelEncoder()<br />	y = [d[0] for d in data]<br />	texts = [d[1] for d in data]<br />	if typ == &#34;tree&#34;:<br />		X = get_dtree_features(texts)<br />	else :<br />		X = get_features(texts)<br />	#Here we are returning 5 things, the label vector y and feature matrix X, but also the texts from which the features were extracted and the <br />	#objects that were used to encode them. These will come in handy for your analysis, but you can ignore them for the initial parts of the assignment<br />	return le.fit_transform(y), dv.fit_transform(X), texts, dv, le<br /><br />#train and multinomial naive bayes classifier<br />def train_classifier(X, y):<br />	clf = LogisticRegression()<br />	clf.fit(X,y)<br />	return clf <br /><br />#train a Decision Tree classifier<br />def train_dtree_classifier(X, y):<br />	clf = DecisionTreeClassifier(max_depth=10)<br />	clf.fit(X,y)<br />	return clf<br /><br />#test the classifier<br />def test_classifier(clf, X, y):<br />	return clf.score(X,y)<br /><br />#cross validation	<br />def cross_validate(X, y, dv=None, typ=&#34;unigram&#34;, numfolds=5,):<br />	test_accs = []<br />	split = 1.0 / numfolds<br />	for i in range(numfolds):<br />		x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=i)<br />		if typ == &#34;tree&#34; :<br />			clf = train_dtree_classifier(x_train, y_train)<br />		else :<br />			clf = train_classifier(x_train, y_train)<br />		test_acc = test_classifier(clf, x_test, y_test)<br />		test_accs.append(test_acc)<br />		print &#39;Fold %d : %.05f&#39;%(i,test_acc)<br />	test_average = float(sum(test_accs))/ numfolds<br />	if typ == &#34;tree&#34; :<br />		with open(&#34;output.dot&#34;, &#39;w&#39;) as f:<br />			f = export_graphviz(clf, out_file=f, feature_names=dv.get_feature_names(), class_names=[&#39;Non Gun Related&#39;,&#39;Gun Related&#39;])<br />		create_graph(&#34;decision-tree.png&#34;)<br />	print &#39;Test Average : %.05f&#39;%(test_average)<br />	print<br />	return test_average<br /><br />#run a rule based classifier and calculate the accuracy<br />def rule_based_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO add more keywords, see how well they do alone and in combination<br />		if &#34;shooter&#34; in text or &#34;shot&#34; in text or &#34;shooting&#34; in text :<br />			if &#34;player&#34; in text or &#34;coach&#34; in text or &#34;game&#34; in text: <br />				prediction = &#39;0&#39;<br />			elif &#34;movie&#34; in text or &#34;film&#34; in text or &#34;actor&#34; in text:<br />				prediction = &#39;0&#39;<br />			else : prediction = &#39;1&#39;<br />		if &#34;gun&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Rule-based classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#Extra Credit Rule Based Classifier<br />def extra_credit_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO develop your conditional statements here<br />		if &#34;shooting&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Reverse Engineered classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#train and multinomial naive bayes classifier<br />def get_top_features(X, y, dv):<br />	clf = train_classifier(X, y)<br />	#the DictVectorizer object remembers which column number corresponds to which feature, and return the feature names in the correct order<br />	feature_names = dv.get_feature_names() <br /><br />	#The below code will get the weights from the classifier, and print out the weights of the features you are interested in<br />	features = [] #this will be a list of (feature_idx, weight) tuples<br />	for i,w in enumerate(clf.coef_[0]): <br />		features.append((i,w))<br />	#Sort the list by values, with the largest ones first<br />	features = sorted(features, key=lambda e: e[1], reverse=True)<br /><br />        #Print out the feature names and thier weights<br />	for i,w in features:<br />	  print &#39;%s\t%s&#39;%(feature_names[i], w)<br /><br />def get_misclassified_examples(y, X, texts) :<br />	x_train, x_test, y_train, y_test, train_texts, test_texts = train_test_split(X, y, texts)<br />	clf = train_classifier(x_train, y_train)<br /><br />	#TODO: You will have to write some code to call your classifier on each of the test examples, and check whether its prediction was right or wrong<br /><br />def create_graph(file_name) :<br />	os.system(&#34;dot -Tpng output.dot -o &#34; &#43; file_name)<br />	os.unlink(&#34;output.dot&#34;)<br /><br />if __name__ == &#39;__main__&#39; : <br /><br />	raw_data = get_data(sys.argv[1])<br />	<br />	print &#39;\nRule-based classification&#39;<br />	rule_based_classifier(raw_data)<br /><br />################ Decision Tree ################<br /><br /># print &#39;\nDecision Tree classification&#39;<br /># y, X, texts, dv, le = get_matricies(raw_data, &#34;tree&#34;)<br /># cross_validate(X,y,dv,&#34;tree&#34;)<br /><br />################ Statistical Classification ################<br />print &#39;\nStatistical classification&#39;<br />y, X, texts, dv, le = get_matricies(raw_data)<br />cross_validate(X,y)<br /><br />get_top_features(X, y, dv)<br />get_misclassified_examples(y, X, texts)<br />
</pre>
<p>Thanks in advance for any help you can give.</p>",Statistical Analysis &#34;Killed&#34; - even on biglab,"<p>When my program gets to the Statistical Analysis bit, it makes it through all the words and creates wordcount dictionaries that are appended to the list of features. Then it hangs for a bit until the terminal just prints &#34;Killed&#34; and program ends. Any idea what might be causing this? Not enough memory?</p>",Statistical Analysis &#34;Killed&#34;,,,"<p>I&#39;m confused on what exactly we&#39;re supposed to do for part 6. Are the y dimensions the same as the number of articles? Also, how is logistic regression supposed to be used here?</p>",Statistical Analysis Part 6,0
940849303,4/26/2016 16:03:35,true,1969405536,,4/26/2016 15:48:02,false,clixsense,0.8889,8057247,PRT,17,Póvoa De Varzim,144.64.25.68,0,0,"<p>Hi, so I read though the other piazza post about how the statistical analysis takes a long time, but I am still having some trouble with my code.</p>
<p></p>
<p>I have already tried waiting 10-15 minutes and removing all stopwords, but for some reason the statistical analysis refuses to complete in a reasonable amount of time</p>
<p></p>
<p>Below is my code:</p>
<pre>def isStopWord(word):
	with open(&#39;stopwords.txt&#39;, &#39;r&#39;) as stopwords:
		for line in stopwords:
			if word in line:
				return True
		return False

def get_features(X) : 
	features = []
	for x in X : 
		f = {}
		#TODO replace this dummy feature function with a unigram model, like we did in class
		for word in x.split():
			if isStopWord(word):
				continue
			if word in f:
				f[word] = f[word] &#43; 1.0
			else:
				f[word] =  1.0
		features.append(f)
	return features</pre>
<p>Not really sure what to do at this point because of this. Also might this be related to using the VM?</p>",Statistical Analysis Taking A Very Long Time,It takes a really long time. Is that expected?,statistical classification runtime,"<p>My script runs forever on my virtual machine, so I&#39;m running it off of biglab. I got the statistical analysis to work once (took about 2 minutes), but after un-commenting get_top_features and get_misclassified, my script gets killed before finishing the initial statistical analysis. I&#39;ve tried several times and I&#39;m unsure what the issue is; I&#39;ve copied my code below.</p>
<p></p>
<pre>#!/bin/python<br /><br />import os<br />import sys<br />import string<br />import random<br />import operator<br />from sklearn.tree import export_graphviz<br />from sklearn.tree import DecisionTreeClassifier<br />from sklearn.naive_bayes import MultinomialNB<br />from sklearn.linear_model import LogisticRegression<br />from sklearn.preprocessing import LabelEncoder<br />from sklearn.feature_extraction import DictVectorizer<br />from sklearn.cross_validation import train_test_split<br />from sklearn.externals.six import StringIO  <br /><br />#read in raw data from file and return a list of (label, article) tuples<br />def get_data(filename): <br />	data = [line.strip().split(&#39;\t&#39;) for line in open(filename).readlines()]<br />	random.shuffle(data)<br />	return data<br /><br />#this function builds the feature matrix for the Decision Tree.<br />def get_dtree_features(X) :<br />	features = []<br />	#TODO : Add the features you would like to use to train the Decision Tree here.<br />	feature_list = [&#39;gun&#39;, &#39;shooter&#39;, &#39;shot&#39;, &#39;shooting&#39;, &#39;player&#39;, &#39;coach&#39;, &#39;game&#39;, &#39;movie&#39;, &#39;actor&#39;, &#39;film&#39;, &#39;bullet&#39;, &#39;photo&#39;, &#39;model&#39;, &#39;point&#39;, &#39;suspect&#39;, &#39;victim&#39;, &#39;police&#39;, &#39;fired&#39;]<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w in feature_list : <br />				f[w] = 1.0<br />		features.append(f)<br />	return features<br /><br /><br />#this is the main function you care about; pack all the cleverest features you can think of into here.<br />def get_features(X) :<br />	stopwords = open(&#39;stopwords.txt&#39;).read()<br />	features = []<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w not in stopwords:            <br />				if w not in f : <br />					f[w] = 0<br />				f[w] &#43;= 1                <br />		features.append(f)<br />	return features<br /><br />#vectorize feature dictionaries and return feature and label matricies<br />def get_matricies(data, typ=&#34;unigram&#34;) : <br />	dv = DictVectorizer(sparse=True) <br />	le = LabelEncoder()<br />	y = [d[0] for d in data]<br />	texts = [d[1] for d in data]<br />	if typ == &#34;tree&#34;:<br />		X = get_dtree_features(texts)<br />	else :<br />		X = get_features(texts)<br />	#Here we are returning 5 things, the label vector y and feature matrix X, but also the texts from which the features were extracted and the <br />	#objects that were used to encode them. These will come in handy for your analysis, but you can ignore them for the initial parts of the assignment<br />	return le.fit_transform(y), dv.fit_transform(X), texts, dv, le<br /><br />#train and multinomial naive bayes classifier<br />def train_classifier(X, y):<br />	clf = LogisticRegression()<br />	clf.fit(X,y)<br />	return clf <br /><br />#train a Decision Tree classifier<br />def train_dtree_classifier(X, y):<br />	clf = DecisionTreeClassifier(max_depth=10)<br />	clf.fit(X,y)<br />	return clf<br /><br />#test the classifier<br />def test_classifier(clf, X, y):<br />	return clf.score(X,y)<br /><br />#cross validation	<br />def cross_validate(X, y, dv=None, typ=&#34;unigram&#34;, numfolds=5,):<br />	test_accs = []<br />	split = 1.0 / numfolds<br />	for i in range(numfolds):<br />		x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=i)<br />		if typ == &#34;tree&#34; :<br />			clf = train_dtree_classifier(x_train, y_train)<br />		else :<br />			clf = train_classifier(x_train, y_train)<br />		test_acc = test_classifier(clf, x_test, y_test)<br />		test_accs.append(test_acc)<br />		print &#39;Fold %d : %.05f&#39;%(i,test_acc)<br />	test_average = float(sum(test_accs))/ numfolds<br />	if typ == &#34;tree&#34; :<br />		with open(&#34;output.dot&#34;, &#39;w&#39;) as f:<br />			f = export_graphviz(clf, out_file=f, feature_names=dv.get_feature_names(), class_names=[&#39;Non Gun Related&#39;,&#39;Gun Related&#39;])<br />		create_graph(&#34;decision-tree.png&#34;)<br />	print &#39;Test Average : %.05f&#39;%(test_average)<br />	print<br />	return test_average<br /><br />#run a rule based classifier and calculate the accuracy<br />def rule_based_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO add more keywords, see how well they do alone and in combination<br />		if &#34;shooter&#34; in text or &#34;shot&#34; in text or &#34;shooting&#34; in text :<br />			if &#34;player&#34; in text or &#34;coach&#34; in text or &#34;game&#34; in text: <br />				prediction = &#39;0&#39;<br />			elif &#34;movie&#34; in text or &#34;film&#34; in text or &#34;actor&#34; in text:<br />				prediction = &#39;0&#39;<br />			else : prediction = &#39;1&#39;<br />		if &#34;gun&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Rule-based classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#Extra Credit Rule Based Classifier<br />def extra_credit_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO develop your conditional statements here<br />		if &#34;shooting&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Reverse Engineered classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#train and multinomial naive bayes classifier<br />def get_top_features(X, y, dv):<br />	clf = train_classifier(X, y)<br />	#the DictVectorizer object remembers which column number corresponds to which feature, and return the feature names in the correct order<br />	feature_names = dv.get_feature_names() <br /><br />	#The below code will get the weights from the classifier, and print out the weights of the features you are interested in<br />	features = [] #this will be a list of (feature_idx, weight) tuples<br />	for i,w in enumerate(clf.coef_[0]): <br />		features.append((i,w))<br />	#Sort the list by values, with the largest ones first<br />	features = sorted(features, key=lambda e: e[1], reverse=True)<br /><br />        #Print out the feature names and thier weights<br />	for i,w in features:<br />	  print &#39;%s\t%s&#39;%(feature_names[i], w)<br /><br />def get_misclassified_examples(y, X, texts) :<br />	x_train, x_test, y_train, y_test, train_texts, test_texts = train_test_split(X, y, texts)<br />	clf = train_classifier(x_train, y_train)<br /><br />	#TODO: You will have to write some code to call your classifier on each of the test examples, and check whether its prediction was right or wrong<br /><br />def create_graph(file_name) :<br />	os.system(&#34;dot -Tpng output.dot -o &#34; &#43; file_name)<br />	os.unlink(&#34;output.dot&#34;)<br /><br />if __name__ == &#39;__main__&#39; : <br /><br />	raw_data = get_data(sys.argv[1])<br />	<br />	print &#39;\nRule-based classification&#39;<br />	rule_based_classifier(raw_data)<br /><br />################ Decision Tree ################<br /><br /># print &#39;\nDecision Tree classification&#39;<br /># y, X, texts, dv, le = get_matricies(raw_data, &#34;tree&#34;)<br /># cross_validate(X,y,dv,&#34;tree&#34;)<br /><br />################ Statistical Classification ################<br />print &#39;\nStatistical classification&#39;<br />y, X, texts, dv, le = get_matricies(raw_data)<br />cross_validate(X,y)<br /><br />get_top_features(X, y, dv)<br />get_misclassified_examples(y, X, texts)<br />
</pre>
<p>Thanks in advance for any help you can give.</p>",Statistical Analysis &#34;Killed&#34; - even on biglab,"<p>When my program gets to the Statistical Analysis bit, it makes it through all the words and creates wordcount dictionaries that are appended to the list of features. Then it hangs for a bit until the terminal just prints &#34;Killed&#34; and program ends. Any idea what might be causing this? Not enough memory?</p>",Statistical Analysis &#34;Killed&#34;,,,"<p>I&#39;m confused on what exactly we&#39;re supposed to do for part 6. Are the y dimensions the same as the number of articles? Also, how is logistic regression supposed to be used here?</p>",Statistical Analysis Part 6,0
940849303,4/26/2016 16:18:19,true,1969413581,,4/26/2016 16:15:37,false,elite,1.0,30128662,BGR,50,Pleven,212.233.177.195,0,0,"<p>Hi, so I read though the other piazza post about how the statistical analysis takes a long time, but I am still having some trouble with my code.</p>
<p></p>
<p>I have already tried waiting 10-15 minutes and removing all stopwords, but for some reason the statistical analysis refuses to complete in a reasonable amount of time</p>
<p></p>
<p>Below is my code:</p>
<pre>def isStopWord(word):
	with open(&#39;stopwords.txt&#39;, &#39;r&#39;) as stopwords:
		for line in stopwords:
			if word in line:
				return True
		return False

def get_features(X) : 
	features = []
	for x in X : 
		f = {}
		#TODO replace this dummy feature function with a unigram model, like we did in class
		for word in x.split():
			if isStopWord(word):
				continue
			if word in f:
				f[word] = f[word] &#43; 1.0
			else:
				f[word] =  1.0
		features.append(f)
	return features</pre>
<p>Not really sure what to do at this point because of this. Also might this be related to using the VM?</p>",Statistical Analysis Taking A Very Long Time,It takes a really long time. Is that expected?,statistical classification runtime,"<p>My script runs forever on my virtual machine, so I&#39;m running it off of biglab. I got the statistical analysis to work once (took about 2 minutes), but after un-commenting get_top_features and get_misclassified, my script gets killed before finishing the initial statistical analysis. I&#39;ve tried several times and I&#39;m unsure what the issue is; I&#39;ve copied my code below.</p>
<p></p>
<pre>#!/bin/python<br /><br />import os<br />import sys<br />import string<br />import random<br />import operator<br />from sklearn.tree import export_graphviz<br />from sklearn.tree import DecisionTreeClassifier<br />from sklearn.naive_bayes import MultinomialNB<br />from sklearn.linear_model import LogisticRegression<br />from sklearn.preprocessing import LabelEncoder<br />from sklearn.feature_extraction import DictVectorizer<br />from sklearn.cross_validation import train_test_split<br />from sklearn.externals.six import StringIO  <br /><br />#read in raw data from file and return a list of (label, article) tuples<br />def get_data(filename): <br />	data = [line.strip().split(&#39;\t&#39;) for line in open(filename).readlines()]<br />	random.shuffle(data)<br />	return data<br /><br />#this function builds the feature matrix for the Decision Tree.<br />def get_dtree_features(X) :<br />	features = []<br />	#TODO : Add the features you would like to use to train the Decision Tree here.<br />	feature_list = [&#39;gun&#39;, &#39;shooter&#39;, &#39;shot&#39;, &#39;shooting&#39;, &#39;player&#39;, &#39;coach&#39;, &#39;game&#39;, &#39;movie&#39;, &#39;actor&#39;, &#39;film&#39;, &#39;bullet&#39;, &#39;photo&#39;, &#39;model&#39;, &#39;point&#39;, &#39;suspect&#39;, &#39;victim&#39;, &#39;police&#39;, &#39;fired&#39;]<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w in feature_list : <br />				f[w] = 1.0<br />		features.append(f)<br />	return features<br /><br /><br />#this is the main function you care about; pack all the cleverest features you can think of into here.<br />def get_features(X) :<br />	stopwords = open(&#39;stopwords.txt&#39;).read()<br />	features = []<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w not in stopwords:            <br />				if w not in f : <br />					f[w] = 0<br />				f[w] &#43;= 1                <br />		features.append(f)<br />	return features<br /><br />#vectorize feature dictionaries and return feature and label matricies<br />def get_matricies(data, typ=&#34;unigram&#34;) : <br />	dv = DictVectorizer(sparse=True) <br />	le = LabelEncoder()<br />	y = [d[0] for d in data]<br />	texts = [d[1] for d in data]<br />	if typ == &#34;tree&#34;:<br />		X = get_dtree_features(texts)<br />	else :<br />		X = get_features(texts)<br />	#Here we are returning 5 things, the label vector y and feature matrix X, but also the texts from which the features were extracted and the <br />	#objects that were used to encode them. These will come in handy for your analysis, but you can ignore them for the initial parts of the assignment<br />	return le.fit_transform(y), dv.fit_transform(X), texts, dv, le<br /><br />#train and multinomial naive bayes classifier<br />def train_classifier(X, y):<br />	clf = LogisticRegression()<br />	clf.fit(X,y)<br />	return clf <br /><br />#train a Decision Tree classifier<br />def train_dtree_classifier(X, y):<br />	clf = DecisionTreeClassifier(max_depth=10)<br />	clf.fit(X,y)<br />	return clf<br /><br />#test the classifier<br />def test_classifier(clf, X, y):<br />	return clf.score(X,y)<br /><br />#cross validation	<br />def cross_validate(X, y, dv=None, typ=&#34;unigram&#34;, numfolds=5,):<br />	test_accs = []<br />	split = 1.0 / numfolds<br />	for i in range(numfolds):<br />		x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=i)<br />		if typ == &#34;tree&#34; :<br />			clf = train_dtree_classifier(x_train, y_train)<br />		else :<br />			clf = train_classifier(x_train, y_train)<br />		test_acc = test_classifier(clf, x_test, y_test)<br />		test_accs.append(test_acc)<br />		print &#39;Fold %d : %.05f&#39;%(i,test_acc)<br />	test_average = float(sum(test_accs))/ numfolds<br />	if typ == &#34;tree&#34; :<br />		with open(&#34;output.dot&#34;, &#39;w&#39;) as f:<br />			f = export_graphviz(clf, out_file=f, feature_names=dv.get_feature_names(), class_names=[&#39;Non Gun Related&#39;,&#39;Gun Related&#39;])<br />		create_graph(&#34;decision-tree.png&#34;)<br />	print &#39;Test Average : %.05f&#39;%(test_average)<br />	print<br />	return test_average<br /><br />#run a rule based classifier and calculate the accuracy<br />def rule_based_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO add more keywords, see how well they do alone and in combination<br />		if &#34;shooter&#34; in text or &#34;shot&#34; in text or &#34;shooting&#34; in text :<br />			if &#34;player&#34; in text or &#34;coach&#34; in text or &#34;game&#34; in text: <br />				prediction = &#39;0&#39;<br />			elif &#34;movie&#34; in text or &#34;film&#34; in text or &#34;actor&#34; in text:<br />				prediction = &#39;0&#39;<br />			else : prediction = &#39;1&#39;<br />		if &#34;gun&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Rule-based classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#Extra Credit Rule Based Classifier<br />def extra_credit_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO develop your conditional statements here<br />		if &#34;shooting&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Reverse Engineered classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#train and multinomial naive bayes classifier<br />def get_top_features(X, y, dv):<br />	clf = train_classifier(X, y)<br />	#the DictVectorizer object remembers which column number corresponds to which feature, and return the feature names in the correct order<br />	feature_names = dv.get_feature_names() <br /><br />	#The below code will get the weights from the classifier, and print out the weights of the features you are interested in<br />	features = [] #this will be a list of (feature_idx, weight) tuples<br />	for i,w in enumerate(clf.coef_[0]): <br />		features.append((i,w))<br />	#Sort the list by values, with the largest ones first<br />	features = sorted(features, key=lambda e: e[1], reverse=True)<br /><br />        #Print out the feature names and thier weights<br />	for i,w in features:<br />	  print &#39;%s\t%s&#39;%(feature_names[i], w)<br /><br />def get_misclassified_examples(y, X, texts) :<br />	x_train, x_test, y_train, y_test, train_texts, test_texts = train_test_split(X, y, texts)<br />	clf = train_classifier(x_train, y_train)<br /><br />	#TODO: You will have to write some code to call your classifier on each of the test examples, and check whether its prediction was right or wrong<br /><br />def create_graph(file_name) :<br />	os.system(&#34;dot -Tpng output.dot -o &#34; &#43; file_name)<br />	os.unlink(&#34;output.dot&#34;)<br /><br />if __name__ == &#39;__main__&#39; : <br /><br />	raw_data = get_data(sys.argv[1])<br />	<br />	print &#39;\nRule-based classification&#39;<br />	rule_based_classifier(raw_data)<br /><br />################ Decision Tree ################<br /><br /># print &#39;\nDecision Tree classification&#39;<br /># y, X, texts, dv, le = get_matricies(raw_data, &#34;tree&#34;)<br /># cross_validate(X,y,dv,&#34;tree&#34;)<br /><br />################ Statistical Classification ################<br />print &#39;\nStatistical classification&#39;<br />y, X, texts, dv, le = get_matricies(raw_data)<br />cross_validate(X,y)<br /><br />get_top_features(X, y, dv)<br />get_misclassified_examples(y, X, texts)<br />
</pre>
<p>Thanks in advance for any help you can give.</p>",Statistical Analysis &#34;Killed&#34; - even on biglab,"<p>When my program gets to the Statistical Analysis bit, it makes it through all the words and creates wordcount dictionaries that are appended to the list of features. Then it hangs for a bit until the terminal just prints &#34;Killed&#34; and program ends. Any idea what might be causing this? Not enough memory?</p>",Statistical Analysis &#34;Killed&#34;,,,"<p>I&#39;m confused on what exactly we&#39;re supposed to do for part 6. Are the y dimensions the same as the number of articles? Also, how is logistic regression supposed to be used here?</p>",Statistical Analysis Part 6,0
940849303,4/26/2016 16:23:45,true,1969418305,,4/26/2016 16:22:19,false,neodev,1.0,29175140,VEN,25,Caracas,190.72.125.134,0,0,"<p>Hi, so I read though the other piazza post about how the statistical analysis takes a long time, but I am still having some trouble with my code.</p>
<p></p>
<p>I have already tried waiting 10-15 minutes and removing all stopwords, but for some reason the statistical analysis refuses to complete in a reasonable amount of time</p>
<p></p>
<p>Below is my code:</p>
<pre>def isStopWord(word):
	with open(&#39;stopwords.txt&#39;, &#39;r&#39;) as stopwords:
		for line in stopwords:
			if word in line:
				return True
		return False

def get_features(X) : 
	features = []
	for x in X : 
		f = {}
		#TODO replace this dummy feature function with a unigram model, like we did in class
		for word in x.split():
			if isStopWord(word):
				continue
			if word in f:
				f[word] = f[word] &#43; 1.0
			else:
				f[word] =  1.0
		features.append(f)
	return features</pre>
<p>Not really sure what to do at this point because of this. Also might this be related to using the VM?</p>",Statistical Analysis Taking A Very Long Time,It takes a really long time. Is that expected?,statistical classification runtime,"<p>My script runs forever on my virtual machine, so I&#39;m running it off of biglab. I got the statistical analysis to work once (took about 2 minutes), but after un-commenting get_top_features and get_misclassified, my script gets killed before finishing the initial statistical analysis. I&#39;ve tried several times and I&#39;m unsure what the issue is; I&#39;ve copied my code below.</p>
<p></p>
<pre>#!/bin/python<br /><br />import os<br />import sys<br />import string<br />import random<br />import operator<br />from sklearn.tree import export_graphviz<br />from sklearn.tree import DecisionTreeClassifier<br />from sklearn.naive_bayes import MultinomialNB<br />from sklearn.linear_model import LogisticRegression<br />from sklearn.preprocessing import LabelEncoder<br />from sklearn.feature_extraction import DictVectorizer<br />from sklearn.cross_validation import train_test_split<br />from sklearn.externals.six import StringIO  <br /><br />#read in raw data from file and return a list of (label, article) tuples<br />def get_data(filename): <br />	data = [line.strip().split(&#39;\t&#39;) for line in open(filename).readlines()]<br />	random.shuffle(data)<br />	return data<br /><br />#this function builds the feature matrix for the Decision Tree.<br />def get_dtree_features(X) :<br />	features = []<br />	#TODO : Add the features you would like to use to train the Decision Tree here.<br />	feature_list = [&#39;gun&#39;, &#39;shooter&#39;, &#39;shot&#39;, &#39;shooting&#39;, &#39;player&#39;, &#39;coach&#39;, &#39;game&#39;, &#39;movie&#39;, &#39;actor&#39;, &#39;film&#39;, &#39;bullet&#39;, &#39;photo&#39;, &#39;model&#39;, &#39;point&#39;, &#39;suspect&#39;, &#39;victim&#39;, &#39;police&#39;, &#39;fired&#39;]<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w in feature_list : <br />				f[w] = 1.0<br />		features.append(f)<br />	return features<br /><br /><br />#this is the main function you care about; pack all the cleverest features you can think of into here.<br />def get_features(X) :<br />	stopwords = open(&#39;stopwords.txt&#39;).read()<br />	features = []<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w not in stopwords:            <br />				if w not in f : <br />					f[w] = 0<br />				f[w] &#43;= 1                <br />		features.append(f)<br />	return features<br /><br />#vectorize feature dictionaries and return feature and label matricies<br />def get_matricies(data, typ=&#34;unigram&#34;) : <br />	dv = DictVectorizer(sparse=True) <br />	le = LabelEncoder()<br />	y = [d[0] for d in data]<br />	texts = [d[1] for d in data]<br />	if typ == &#34;tree&#34;:<br />		X = get_dtree_features(texts)<br />	else :<br />		X = get_features(texts)<br />	#Here we are returning 5 things, the label vector y and feature matrix X, but also the texts from which the features were extracted and the <br />	#objects that were used to encode them. These will come in handy for your analysis, but you can ignore them for the initial parts of the assignment<br />	return le.fit_transform(y), dv.fit_transform(X), texts, dv, le<br /><br />#train and multinomial naive bayes classifier<br />def train_classifier(X, y):<br />	clf = LogisticRegression()<br />	clf.fit(X,y)<br />	return clf <br /><br />#train a Decision Tree classifier<br />def train_dtree_classifier(X, y):<br />	clf = DecisionTreeClassifier(max_depth=10)<br />	clf.fit(X,y)<br />	return clf<br /><br />#test the classifier<br />def test_classifier(clf, X, y):<br />	return clf.score(X,y)<br /><br />#cross validation	<br />def cross_validate(X, y, dv=None, typ=&#34;unigram&#34;, numfolds=5,):<br />	test_accs = []<br />	split = 1.0 / numfolds<br />	for i in range(numfolds):<br />		x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=i)<br />		if typ == &#34;tree&#34; :<br />			clf = train_dtree_classifier(x_train, y_train)<br />		else :<br />			clf = train_classifier(x_train, y_train)<br />		test_acc = test_classifier(clf, x_test, y_test)<br />		test_accs.append(test_acc)<br />		print &#39;Fold %d : %.05f&#39;%(i,test_acc)<br />	test_average = float(sum(test_accs))/ numfolds<br />	if typ == &#34;tree&#34; :<br />		with open(&#34;output.dot&#34;, &#39;w&#39;) as f:<br />			f = export_graphviz(clf, out_file=f, feature_names=dv.get_feature_names(), class_names=[&#39;Non Gun Related&#39;,&#39;Gun Related&#39;])<br />		create_graph(&#34;decision-tree.png&#34;)<br />	print &#39;Test Average : %.05f&#39;%(test_average)<br />	print<br />	return test_average<br /><br />#run a rule based classifier and calculate the accuracy<br />def rule_based_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO add more keywords, see how well they do alone and in combination<br />		if &#34;shooter&#34; in text or &#34;shot&#34; in text or &#34;shooting&#34; in text :<br />			if &#34;player&#34; in text or &#34;coach&#34; in text or &#34;game&#34; in text: <br />				prediction = &#39;0&#39;<br />			elif &#34;movie&#34; in text or &#34;film&#34; in text or &#34;actor&#34; in text:<br />				prediction = &#39;0&#39;<br />			else : prediction = &#39;1&#39;<br />		if &#34;gun&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Rule-based classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#Extra Credit Rule Based Classifier<br />def extra_credit_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO develop your conditional statements here<br />		if &#34;shooting&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Reverse Engineered classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#train and multinomial naive bayes classifier<br />def get_top_features(X, y, dv):<br />	clf = train_classifier(X, y)<br />	#the DictVectorizer object remembers which column number corresponds to which feature, and return the feature names in the correct order<br />	feature_names = dv.get_feature_names() <br /><br />	#The below code will get the weights from the classifier, and print out the weights of the features you are interested in<br />	features = [] #this will be a list of (feature_idx, weight) tuples<br />	for i,w in enumerate(clf.coef_[0]): <br />		features.append((i,w))<br />	#Sort the list by values, with the largest ones first<br />	features = sorted(features, key=lambda e: e[1], reverse=True)<br /><br />        #Print out the feature names and thier weights<br />	for i,w in features:<br />	  print &#39;%s\t%s&#39;%(feature_names[i], w)<br /><br />def get_misclassified_examples(y, X, texts) :<br />	x_train, x_test, y_train, y_test, train_texts, test_texts = train_test_split(X, y, texts)<br />	clf = train_classifier(x_train, y_train)<br /><br />	#TODO: You will have to write some code to call your classifier on each of the test examples, and check whether its prediction was right or wrong<br /><br />def create_graph(file_name) :<br />	os.system(&#34;dot -Tpng output.dot -o &#34; &#43; file_name)<br />	os.unlink(&#34;output.dot&#34;)<br /><br />if __name__ == &#39;__main__&#39; : <br /><br />	raw_data = get_data(sys.argv[1])<br />	<br />	print &#39;\nRule-based classification&#39;<br />	rule_based_classifier(raw_data)<br /><br />################ Decision Tree ################<br /><br /># print &#39;\nDecision Tree classification&#39;<br /># y, X, texts, dv, le = get_matricies(raw_data, &#34;tree&#34;)<br /># cross_validate(X,y,dv,&#34;tree&#34;)<br /><br />################ Statistical Classification ################<br />print &#39;\nStatistical classification&#39;<br />y, X, texts, dv, le = get_matricies(raw_data)<br />cross_validate(X,y)<br /><br />get_top_features(X, y, dv)<br />get_misclassified_examples(y, X, texts)<br />
</pre>
<p>Thanks in advance for any help you can give.</p>",Statistical Analysis &#34;Killed&#34; - even on biglab,"<p>When my program gets to the Statistical Analysis bit, it makes it through all the words and creates wordcount dictionaries that are appended to the list of features. Then it hangs for a bit until the terminal just prints &#34;Killed&#34; and program ends. Any idea what might be causing this? Not enough memory?</p>",Statistical Analysis &#34;Killed&#34;,,,"<p>I&#39;m confused on what exactly we&#39;re supposed to do for part 6. Are the y dimensions the same as the number of articles? Also, how is logistic regression supposed to be used here?</p>",Statistical Analysis Part 6,0
940849303,4/26/2016 16:39:18,true,1969429032,,4/26/2016 16:30:39,false,clixsense,1.0,6329782,IDN,07,Bekonang,202.67.40.31,0,0,"<p>Hi, so I read though the other piazza post about how the statistical analysis takes a long time, but I am still having some trouble with my code.</p>
<p></p>
<p>I have already tried waiting 10-15 minutes and removing all stopwords, but for some reason the statistical analysis refuses to complete in a reasonable amount of time</p>
<p></p>
<p>Below is my code:</p>
<pre>def isStopWord(word):
	with open(&#39;stopwords.txt&#39;, &#39;r&#39;) as stopwords:
		for line in stopwords:
			if word in line:
				return True
		return False

def get_features(X) : 
	features = []
	for x in X : 
		f = {}
		#TODO replace this dummy feature function with a unigram model, like we did in class
		for word in x.split():
			if isStopWord(word):
				continue
			if word in f:
				f[word] = f[word] &#43; 1.0
			else:
				f[word] =  1.0
		features.append(f)
	return features</pre>
<p>Not really sure what to do at this point because of this. Also might this be related to using the VM?</p>",Statistical Analysis Taking A Very Long Time,It takes a really long time. Is that expected?,statistical classification runtime,"<p>My script runs forever on my virtual machine, so I&#39;m running it off of biglab. I got the statistical analysis to work once (took about 2 minutes), but after un-commenting get_top_features and get_misclassified, my script gets killed before finishing the initial statistical analysis. I&#39;ve tried several times and I&#39;m unsure what the issue is; I&#39;ve copied my code below.</p>
<p></p>
<pre>#!/bin/python<br /><br />import os<br />import sys<br />import string<br />import random<br />import operator<br />from sklearn.tree import export_graphviz<br />from sklearn.tree import DecisionTreeClassifier<br />from sklearn.naive_bayes import MultinomialNB<br />from sklearn.linear_model import LogisticRegression<br />from sklearn.preprocessing import LabelEncoder<br />from sklearn.feature_extraction import DictVectorizer<br />from sklearn.cross_validation import train_test_split<br />from sklearn.externals.six import StringIO  <br /><br />#read in raw data from file and return a list of (label, article) tuples<br />def get_data(filename): <br />	data = [line.strip().split(&#39;\t&#39;) for line in open(filename).readlines()]<br />	random.shuffle(data)<br />	return data<br /><br />#this function builds the feature matrix for the Decision Tree.<br />def get_dtree_features(X) :<br />	features = []<br />	#TODO : Add the features you would like to use to train the Decision Tree here.<br />	feature_list = [&#39;gun&#39;, &#39;shooter&#39;, &#39;shot&#39;, &#39;shooting&#39;, &#39;player&#39;, &#39;coach&#39;, &#39;game&#39;, &#39;movie&#39;, &#39;actor&#39;, &#39;film&#39;, &#39;bullet&#39;, &#39;photo&#39;, &#39;model&#39;, &#39;point&#39;, &#39;suspect&#39;, &#39;victim&#39;, &#39;police&#39;, &#39;fired&#39;]<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w in feature_list : <br />				f[w] = 1.0<br />		features.append(f)<br />	return features<br /><br /><br />#this is the main function you care about; pack all the cleverest features you can think of into here.<br />def get_features(X) :<br />	stopwords = open(&#39;stopwords.txt&#39;).read()<br />	features = []<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w not in stopwords:            <br />				if w not in f : <br />					f[w] = 0<br />				f[w] &#43;= 1                <br />		features.append(f)<br />	return features<br /><br />#vectorize feature dictionaries and return feature and label matricies<br />def get_matricies(data, typ=&#34;unigram&#34;) : <br />	dv = DictVectorizer(sparse=True) <br />	le = LabelEncoder()<br />	y = [d[0] for d in data]<br />	texts = [d[1] for d in data]<br />	if typ == &#34;tree&#34;:<br />		X = get_dtree_features(texts)<br />	else :<br />		X = get_features(texts)<br />	#Here we are returning 5 things, the label vector y and feature matrix X, but also the texts from which the features were extracted and the <br />	#objects that were used to encode them. These will come in handy for your analysis, but you can ignore them for the initial parts of the assignment<br />	return le.fit_transform(y), dv.fit_transform(X), texts, dv, le<br /><br />#train and multinomial naive bayes classifier<br />def train_classifier(X, y):<br />	clf = LogisticRegression()<br />	clf.fit(X,y)<br />	return clf <br /><br />#train a Decision Tree classifier<br />def train_dtree_classifier(X, y):<br />	clf = DecisionTreeClassifier(max_depth=10)<br />	clf.fit(X,y)<br />	return clf<br /><br />#test the classifier<br />def test_classifier(clf, X, y):<br />	return clf.score(X,y)<br /><br />#cross validation	<br />def cross_validate(X, y, dv=None, typ=&#34;unigram&#34;, numfolds=5,):<br />	test_accs = []<br />	split = 1.0 / numfolds<br />	for i in range(numfolds):<br />		x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=i)<br />		if typ == &#34;tree&#34; :<br />			clf = train_dtree_classifier(x_train, y_train)<br />		else :<br />			clf = train_classifier(x_train, y_train)<br />		test_acc = test_classifier(clf, x_test, y_test)<br />		test_accs.append(test_acc)<br />		print &#39;Fold %d : %.05f&#39;%(i,test_acc)<br />	test_average = float(sum(test_accs))/ numfolds<br />	if typ == &#34;tree&#34; :<br />		with open(&#34;output.dot&#34;, &#39;w&#39;) as f:<br />			f = export_graphviz(clf, out_file=f, feature_names=dv.get_feature_names(), class_names=[&#39;Non Gun Related&#39;,&#39;Gun Related&#39;])<br />		create_graph(&#34;decision-tree.png&#34;)<br />	print &#39;Test Average : %.05f&#39;%(test_average)<br />	print<br />	return test_average<br /><br />#run a rule based classifier and calculate the accuracy<br />def rule_based_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO add more keywords, see how well they do alone and in combination<br />		if &#34;shooter&#34; in text or &#34;shot&#34; in text or &#34;shooting&#34; in text :<br />			if &#34;player&#34; in text or &#34;coach&#34; in text or &#34;game&#34; in text: <br />				prediction = &#39;0&#39;<br />			elif &#34;movie&#34; in text or &#34;film&#34; in text or &#34;actor&#34; in text:<br />				prediction = &#39;0&#39;<br />			else : prediction = &#39;1&#39;<br />		if &#34;gun&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Rule-based classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#Extra Credit Rule Based Classifier<br />def extra_credit_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO develop your conditional statements here<br />		if &#34;shooting&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Reverse Engineered classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#train and multinomial naive bayes classifier<br />def get_top_features(X, y, dv):<br />	clf = train_classifier(X, y)<br />	#the DictVectorizer object remembers which column number corresponds to which feature, and return the feature names in the correct order<br />	feature_names = dv.get_feature_names() <br /><br />	#The below code will get the weights from the classifier, and print out the weights of the features you are interested in<br />	features = [] #this will be a list of (feature_idx, weight) tuples<br />	for i,w in enumerate(clf.coef_[0]): <br />		features.append((i,w))<br />	#Sort the list by values, with the largest ones first<br />	features = sorted(features, key=lambda e: e[1], reverse=True)<br /><br />        #Print out the feature names and thier weights<br />	for i,w in features:<br />	  print &#39;%s\t%s&#39;%(feature_names[i], w)<br /><br />def get_misclassified_examples(y, X, texts) :<br />	x_train, x_test, y_train, y_test, train_texts, test_texts = train_test_split(X, y, texts)<br />	clf = train_classifier(x_train, y_train)<br /><br />	#TODO: You will have to write some code to call your classifier on each of the test examples, and check whether its prediction was right or wrong<br /><br />def create_graph(file_name) :<br />	os.system(&#34;dot -Tpng output.dot -o &#34; &#43; file_name)<br />	os.unlink(&#34;output.dot&#34;)<br /><br />if __name__ == &#39;__main__&#39; : <br /><br />	raw_data = get_data(sys.argv[1])<br />	<br />	print &#39;\nRule-based classification&#39;<br />	rule_based_classifier(raw_data)<br /><br />################ Decision Tree ################<br /><br /># print &#39;\nDecision Tree classification&#39;<br /># y, X, texts, dv, le = get_matricies(raw_data, &#34;tree&#34;)<br /># cross_validate(X,y,dv,&#34;tree&#34;)<br /><br />################ Statistical Classification ################<br />print &#39;\nStatistical classification&#39;<br />y, X, texts, dv, le = get_matricies(raw_data)<br />cross_validate(X,y)<br /><br />get_top_features(X, y, dv)<br />get_misclassified_examples(y, X, texts)<br />
</pre>
<p>Thanks in advance for any help you can give.</p>",Statistical Analysis &#34;Killed&#34; - even on biglab,"<p>When my program gets to the Statistical Analysis bit, it makes it through all the words and creates wordcount dictionaries that are appended to the list of features. Then it hangs for a bit until the terminal just prints &#34;Killed&#34; and program ends. Any idea what might be causing this? Not enough memory?</p>",Statistical Analysis &#34;Killed&#34;,,,"<p>I&#39;m confused on what exactly we&#39;re supposed to do for part 6. Are the y dimensions the same as the number of articles? Also, how is logistic regression supposed to be used here?</p>",Statistical Analysis Part 6,0
940849303,4/26/2016 16:40:52,true,1969430002,,4/26/2016 16:36:32,false,prodege,1.0,22387641,USA,PA,Jeannette,208.94.42.131,0,0,"<p>Hi, so I read though the other piazza post about how the statistical analysis takes a long time, but I am still having some trouble with my code.</p>
<p></p>
<p>I have already tried waiting 10-15 minutes and removing all stopwords, but for some reason the statistical analysis refuses to complete in a reasonable amount of time</p>
<p></p>
<p>Below is my code:</p>
<pre>def isStopWord(word):
	with open(&#39;stopwords.txt&#39;, &#39;r&#39;) as stopwords:
		for line in stopwords:
			if word in line:
				return True
		return False

def get_features(X) : 
	features = []
	for x in X : 
		f = {}
		#TODO replace this dummy feature function with a unigram model, like we did in class
		for word in x.split():
			if isStopWord(word):
				continue
			if word in f:
				f[word] = f[word] &#43; 1.0
			else:
				f[word] =  1.0
		features.append(f)
	return features</pre>
<p>Not really sure what to do at this point because of this. Also might this be related to using the VM?</p>",Statistical Analysis Taking A Very Long Time,It takes a really long time. Is that expected?,statistical classification runtime,"<p>My script runs forever on my virtual machine, so I&#39;m running it off of biglab. I got the statistical analysis to work once (took about 2 minutes), but after un-commenting get_top_features and get_misclassified, my script gets killed before finishing the initial statistical analysis. I&#39;ve tried several times and I&#39;m unsure what the issue is; I&#39;ve copied my code below.</p>
<p></p>
<pre>#!/bin/python<br /><br />import os<br />import sys<br />import string<br />import random<br />import operator<br />from sklearn.tree import export_graphviz<br />from sklearn.tree import DecisionTreeClassifier<br />from sklearn.naive_bayes import MultinomialNB<br />from sklearn.linear_model import LogisticRegression<br />from sklearn.preprocessing import LabelEncoder<br />from sklearn.feature_extraction import DictVectorizer<br />from sklearn.cross_validation import train_test_split<br />from sklearn.externals.six import StringIO  <br /><br />#read in raw data from file and return a list of (label, article) tuples<br />def get_data(filename): <br />	data = [line.strip().split(&#39;\t&#39;) for line in open(filename).readlines()]<br />	random.shuffle(data)<br />	return data<br /><br />#this function builds the feature matrix for the Decision Tree.<br />def get_dtree_features(X) :<br />	features = []<br />	#TODO : Add the features you would like to use to train the Decision Tree here.<br />	feature_list = [&#39;gun&#39;, &#39;shooter&#39;, &#39;shot&#39;, &#39;shooting&#39;, &#39;player&#39;, &#39;coach&#39;, &#39;game&#39;, &#39;movie&#39;, &#39;actor&#39;, &#39;film&#39;, &#39;bullet&#39;, &#39;photo&#39;, &#39;model&#39;, &#39;point&#39;, &#39;suspect&#39;, &#39;victim&#39;, &#39;police&#39;, &#39;fired&#39;]<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w in feature_list : <br />				f[w] = 1.0<br />		features.append(f)<br />	return features<br /><br /><br />#this is the main function you care about; pack all the cleverest features you can think of into here.<br />def get_features(X) :<br />	stopwords = open(&#39;stopwords.txt&#39;).read()<br />	features = []<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w not in stopwords:            <br />				if w not in f : <br />					f[w] = 0<br />				f[w] &#43;= 1                <br />		features.append(f)<br />	return features<br /><br />#vectorize feature dictionaries and return feature and label matricies<br />def get_matricies(data, typ=&#34;unigram&#34;) : <br />	dv = DictVectorizer(sparse=True) <br />	le = LabelEncoder()<br />	y = [d[0] for d in data]<br />	texts = [d[1] for d in data]<br />	if typ == &#34;tree&#34;:<br />		X = get_dtree_features(texts)<br />	else :<br />		X = get_features(texts)<br />	#Here we are returning 5 things, the label vector y and feature matrix X, but also the texts from which the features were extracted and the <br />	#objects that were used to encode them. These will come in handy for your analysis, but you can ignore them for the initial parts of the assignment<br />	return le.fit_transform(y), dv.fit_transform(X), texts, dv, le<br /><br />#train and multinomial naive bayes classifier<br />def train_classifier(X, y):<br />	clf = LogisticRegression()<br />	clf.fit(X,y)<br />	return clf <br /><br />#train a Decision Tree classifier<br />def train_dtree_classifier(X, y):<br />	clf = DecisionTreeClassifier(max_depth=10)<br />	clf.fit(X,y)<br />	return clf<br /><br />#test the classifier<br />def test_classifier(clf, X, y):<br />	return clf.score(X,y)<br /><br />#cross validation	<br />def cross_validate(X, y, dv=None, typ=&#34;unigram&#34;, numfolds=5,):<br />	test_accs = []<br />	split = 1.0 / numfolds<br />	for i in range(numfolds):<br />		x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=i)<br />		if typ == &#34;tree&#34; :<br />			clf = train_dtree_classifier(x_train, y_train)<br />		else :<br />			clf = train_classifier(x_train, y_train)<br />		test_acc = test_classifier(clf, x_test, y_test)<br />		test_accs.append(test_acc)<br />		print &#39;Fold %d : %.05f&#39;%(i,test_acc)<br />	test_average = float(sum(test_accs))/ numfolds<br />	if typ == &#34;tree&#34; :<br />		with open(&#34;output.dot&#34;, &#39;w&#39;) as f:<br />			f = export_graphviz(clf, out_file=f, feature_names=dv.get_feature_names(), class_names=[&#39;Non Gun Related&#39;,&#39;Gun Related&#39;])<br />		create_graph(&#34;decision-tree.png&#34;)<br />	print &#39;Test Average : %.05f&#39;%(test_average)<br />	print<br />	return test_average<br /><br />#run a rule based classifier and calculate the accuracy<br />def rule_based_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO add more keywords, see how well they do alone and in combination<br />		if &#34;shooter&#34; in text or &#34;shot&#34; in text or &#34;shooting&#34; in text :<br />			if &#34;player&#34; in text or &#34;coach&#34; in text or &#34;game&#34; in text: <br />				prediction = &#39;0&#39;<br />			elif &#34;movie&#34; in text or &#34;film&#34; in text or &#34;actor&#34; in text:<br />				prediction = &#39;0&#39;<br />			else : prediction = &#39;1&#39;<br />		if &#34;gun&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Rule-based classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#Extra Credit Rule Based Classifier<br />def extra_credit_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO develop your conditional statements here<br />		if &#34;shooting&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Reverse Engineered classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#train and multinomial naive bayes classifier<br />def get_top_features(X, y, dv):<br />	clf = train_classifier(X, y)<br />	#the DictVectorizer object remembers which column number corresponds to which feature, and return the feature names in the correct order<br />	feature_names = dv.get_feature_names() <br /><br />	#The below code will get the weights from the classifier, and print out the weights of the features you are interested in<br />	features = [] #this will be a list of (feature_idx, weight) tuples<br />	for i,w in enumerate(clf.coef_[0]): <br />		features.append((i,w))<br />	#Sort the list by values, with the largest ones first<br />	features = sorted(features, key=lambda e: e[1], reverse=True)<br /><br />        #Print out the feature names and thier weights<br />	for i,w in features:<br />	  print &#39;%s\t%s&#39;%(feature_names[i], w)<br /><br />def get_misclassified_examples(y, X, texts) :<br />	x_train, x_test, y_train, y_test, train_texts, test_texts = train_test_split(X, y, texts)<br />	clf = train_classifier(x_train, y_train)<br /><br />	#TODO: You will have to write some code to call your classifier on each of the test examples, and check whether its prediction was right or wrong<br /><br />def create_graph(file_name) :<br />	os.system(&#34;dot -Tpng output.dot -o &#34; &#43; file_name)<br />	os.unlink(&#34;output.dot&#34;)<br /><br />if __name__ == &#39;__main__&#39; : <br /><br />	raw_data = get_data(sys.argv[1])<br />	<br />	print &#39;\nRule-based classification&#39;<br />	rule_based_classifier(raw_data)<br /><br />################ Decision Tree ################<br /><br /># print &#39;\nDecision Tree classification&#39;<br /># y, X, texts, dv, le = get_matricies(raw_data, &#34;tree&#34;)<br /># cross_validate(X,y,dv,&#34;tree&#34;)<br /><br />################ Statistical Classification ################<br />print &#39;\nStatistical classification&#39;<br />y, X, texts, dv, le = get_matricies(raw_data)<br />cross_validate(X,y)<br /><br />get_top_features(X, y, dv)<br />get_misclassified_examples(y, X, texts)<br />
</pre>
<p>Thanks in advance for any help you can give.</p>",Statistical Analysis &#34;Killed&#34; - even on biglab,"<p>When my program gets to the Statistical Analysis bit, it makes it through all the words and creates wordcount dictionaries that are appended to the list of features. Then it hangs for a bit until the terminal just prints &#34;Killed&#34; and program ends. Any idea what might be causing this? Not enough memory?</p>",Statistical Analysis &#34;Killed&#34;,,,"<p>I&#39;m confused on what exactly we&#39;re supposed to do for part 6. Are the y dimensions the same as the number of articles? Also, how is logistic regression supposed to be used here?</p>",Statistical Analysis Part 6,0
940849303,4/26/2016 16:56:40,true,1969439484,,4/26/2016 16:53:35,false,clixsense,1.0,21408115,IDN,07,Semarang,36.79.23.180,0,0,"<p>Hi, so I read though the other piazza post about how the statistical analysis takes a long time, but I am still having some trouble with my code.</p>
<p></p>
<p>I have already tried waiting 10-15 minutes and removing all stopwords, but for some reason the statistical analysis refuses to complete in a reasonable amount of time</p>
<p></p>
<p>Below is my code:</p>
<pre>def isStopWord(word):
	with open(&#39;stopwords.txt&#39;, &#39;r&#39;) as stopwords:
		for line in stopwords:
			if word in line:
				return True
		return False

def get_features(X) : 
	features = []
	for x in X : 
		f = {}
		#TODO replace this dummy feature function with a unigram model, like we did in class
		for word in x.split():
			if isStopWord(word):
				continue
			if word in f:
				f[word] = f[word] &#43; 1.0
			else:
				f[word] =  1.0
		features.append(f)
	return features</pre>
<p>Not really sure what to do at this point because of this. Also might this be related to using the VM?</p>",Statistical Analysis Taking A Very Long Time,It takes a really long time. Is that expected?,statistical classification runtime,"<p>My script runs forever on my virtual machine, so I&#39;m running it off of biglab. I got the statistical analysis to work once (took about 2 minutes), but after un-commenting get_top_features and get_misclassified, my script gets killed before finishing the initial statistical analysis. I&#39;ve tried several times and I&#39;m unsure what the issue is; I&#39;ve copied my code below.</p>
<p></p>
<pre>#!/bin/python<br /><br />import os<br />import sys<br />import string<br />import random<br />import operator<br />from sklearn.tree import export_graphviz<br />from sklearn.tree import DecisionTreeClassifier<br />from sklearn.naive_bayes import MultinomialNB<br />from sklearn.linear_model import LogisticRegression<br />from sklearn.preprocessing import LabelEncoder<br />from sklearn.feature_extraction import DictVectorizer<br />from sklearn.cross_validation import train_test_split<br />from sklearn.externals.six import StringIO  <br /><br />#read in raw data from file and return a list of (label, article) tuples<br />def get_data(filename): <br />	data = [line.strip().split(&#39;\t&#39;) for line in open(filename).readlines()]<br />	random.shuffle(data)<br />	return data<br /><br />#this function builds the feature matrix for the Decision Tree.<br />def get_dtree_features(X) :<br />	features = []<br />	#TODO : Add the features you would like to use to train the Decision Tree here.<br />	feature_list = [&#39;gun&#39;, &#39;shooter&#39;, &#39;shot&#39;, &#39;shooting&#39;, &#39;player&#39;, &#39;coach&#39;, &#39;game&#39;, &#39;movie&#39;, &#39;actor&#39;, &#39;film&#39;, &#39;bullet&#39;, &#39;photo&#39;, &#39;model&#39;, &#39;point&#39;, &#39;suspect&#39;, &#39;victim&#39;, &#39;police&#39;, &#39;fired&#39;]<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w in feature_list : <br />				f[w] = 1.0<br />		features.append(f)<br />	return features<br /><br /><br />#this is the main function you care about; pack all the cleverest features you can think of into here.<br />def get_features(X) :<br />	stopwords = open(&#39;stopwords.txt&#39;).read()<br />	features = []<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w not in stopwords:            <br />				if w not in f : <br />					f[w] = 0<br />				f[w] &#43;= 1                <br />		features.append(f)<br />	return features<br /><br />#vectorize feature dictionaries and return feature and label matricies<br />def get_matricies(data, typ=&#34;unigram&#34;) : <br />	dv = DictVectorizer(sparse=True) <br />	le = LabelEncoder()<br />	y = [d[0] for d in data]<br />	texts = [d[1] for d in data]<br />	if typ == &#34;tree&#34;:<br />		X = get_dtree_features(texts)<br />	else :<br />		X = get_features(texts)<br />	#Here we are returning 5 things, the label vector y and feature matrix X, but also the texts from which the features were extracted and the <br />	#objects that were used to encode them. These will come in handy for your analysis, but you can ignore them for the initial parts of the assignment<br />	return le.fit_transform(y), dv.fit_transform(X), texts, dv, le<br /><br />#train and multinomial naive bayes classifier<br />def train_classifier(X, y):<br />	clf = LogisticRegression()<br />	clf.fit(X,y)<br />	return clf <br /><br />#train a Decision Tree classifier<br />def train_dtree_classifier(X, y):<br />	clf = DecisionTreeClassifier(max_depth=10)<br />	clf.fit(X,y)<br />	return clf<br /><br />#test the classifier<br />def test_classifier(clf, X, y):<br />	return clf.score(X,y)<br /><br />#cross validation	<br />def cross_validate(X, y, dv=None, typ=&#34;unigram&#34;, numfolds=5,):<br />	test_accs = []<br />	split = 1.0 / numfolds<br />	for i in range(numfolds):<br />		x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=i)<br />		if typ == &#34;tree&#34; :<br />			clf = train_dtree_classifier(x_train, y_train)<br />		else :<br />			clf = train_classifier(x_train, y_train)<br />		test_acc = test_classifier(clf, x_test, y_test)<br />		test_accs.append(test_acc)<br />		print &#39;Fold %d : %.05f&#39;%(i,test_acc)<br />	test_average = float(sum(test_accs))/ numfolds<br />	if typ == &#34;tree&#34; :<br />		with open(&#34;output.dot&#34;, &#39;w&#39;) as f:<br />			f = export_graphviz(clf, out_file=f, feature_names=dv.get_feature_names(), class_names=[&#39;Non Gun Related&#39;,&#39;Gun Related&#39;])<br />		create_graph(&#34;decision-tree.png&#34;)<br />	print &#39;Test Average : %.05f&#39;%(test_average)<br />	print<br />	return test_average<br /><br />#run a rule based classifier and calculate the accuracy<br />def rule_based_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO add more keywords, see how well they do alone and in combination<br />		if &#34;shooter&#34; in text or &#34;shot&#34; in text or &#34;shooting&#34; in text :<br />			if &#34;player&#34; in text or &#34;coach&#34; in text or &#34;game&#34; in text: <br />				prediction = &#39;0&#39;<br />			elif &#34;movie&#34; in text or &#34;film&#34; in text or &#34;actor&#34; in text:<br />				prediction = &#39;0&#39;<br />			else : prediction = &#39;1&#39;<br />		if &#34;gun&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Rule-based classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#Extra Credit Rule Based Classifier<br />def extra_credit_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO develop your conditional statements here<br />		if &#34;shooting&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Reverse Engineered classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#train and multinomial naive bayes classifier<br />def get_top_features(X, y, dv):<br />	clf = train_classifier(X, y)<br />	#the DictVectorizer object remembers which column number corresponds to which feature, and return the feature names in the correct order<br />	feature_names = dv.get_feature_names() <br /><br />	#The below code will get the weights from the classifier, and print out the weights of the features you are interested in<br />	features = [] #this will be a list of (feature_idx, weight) tuples<br />	for i,w in enumerate(clf.coef_[0]): <br />		features.append((i,w))<br />	#Sort the list by values, with the largest ones first<br />	features = sorted(features, key=lambda e: e[1], reverse=True)<br /><br />        #Print out the feature names and thier weights<br />	for i,w in features:<br />	  print &#39;%s\t%s&#39;%(feature_names[i], w)<br /><br />def get_misclassified_examples(y, X, texts) :<br />	x_train, x_test, y_train, y_test, train_texts, test_texts = train_test_split(X, y, texts)<br />	clf = train_classifier(x_train, y_train)<br /><br />	#TODO: You will have to write some code to call your classifier on each of the test examples, and check whether its prediction was right or wrong<br /><br />def create_graph(file_name) :<br />	os.system(&#34;dot -Tpng output.dot -o &#34; &#43; file_name)<br />	os.unlink(&#34;output.dot&#34;)<br /><br />if __name__ == &#39;__main__&#39; : <br /><br />	raw_data = get_data(sys.argv[1])<br />	<br />	print &#39;\nRule-based classification&#39;<br />	rule_based_classifier(raw_data)<br /><br />################ Decision Tree ################<br /><br /># print &#39;\nDecision Tree classification&#39;<br /># y, X, texts, dv, le = get_matricies(raw_data, &#34;tree&#34;)<br /># cross_validate(X,y,dv,&#34;tree&#34;)<br /><br />################ Statistical Classification ################<br />print &#39;\nStatistical classification&#39;<br />y, X, texts, dv, le = get_matricies(raw_data)<br />cross_validate(X,y)<br /><br />get_top_features(X, y, dv)<br />get_misclassified_examples(y, X, texts)<br />
</pre>
<p>Thanks in advance for any help you can give.</p>",Statistical Analysis &#34;Killed&#34; - even on biglab,"<p>When my program gets to the Statistical Analysis bit, it makes it through all the words and creates wordcount dictionaries that are appended to the list of features. Then it hangs for a bit until the terminal just prints &#34;Killed&#34; and program ends. Any idea what might be causing this? Not enough memory?</p>",Statistical Analysis &#34;Killed&#34;,,,"<p>I&#39;m confused on what exactly we&#39;re supposed to do for part 6. Are the y dimensions the same as the number of articles? Also, how is logistic regression supposed to be used here?</p>",Statistical Analysis Part 6,0
940849303,4/26/2016 17:02:13,true,1969443062,,4/26/2016 16:59:19,false,neodev,1.0,36167043,GBR,G6,Hull,77.86.101.69,0,0,"<p>Hi, so I read though the other piazza post about how the statistical analysis takes a long time, but I am still having some trouble with my code.</p>
<p></p>
<p>I have already tried waiting 10-15 minutes and removing all stopwords, but for some reason the statistical analysis refuses to complete in a reasonable amount of time</p>
<p></p>
<p>Below is my code:</p>
<pre>def isStopWord(word):
	with open(&#39;stopwords.txt&#39;, &#39;r&#39;) as stopwords:
		for line in stopwords:
			if word in line:
				return True
		return False

def get_features(X) : 
	features = []
	for x in X : 
		f = {}
		#TODO replace this dummy feature function with a unigram model, like we did in class
		for word in x.split():
			if isStopWord(word):
				continue
			if word in f:
				f[word] = f[word] &#43; 1.0
			else:
				f[word] =  1.0
		features.append(f)
	return features</pre>
<p>Not really sure what to do at this point because of this. Also might this be related to using the VM?</p>",Statistical Analysis Taking A Very Long Time,It takes a really long time. Is that expected?,statistical classification runtime,"<p>My script runs forever on my virtual machine, so I&#39;m running it off of biglab. I got the statistical analysis to work once (took about 2 minutes), but after un-commenting get_top_features and get_misclassified, my script gets killed before finishing the initial statistical analysis. I&#39;ve tried several times and I&#39;m unsure what the issue is; I&#39;ve copied my code below.</p>
<p></p>
<pre>#!/bin/python<br /><br />import os<br />import sys<br />import string<br />import random<br />import operator<br />from sklearn.tree import export_graphviz<br />from sklearn.tree import DecisionTreeClassifier<br />from sklearn.naive_bayes import MultinomialNB<br />from sklearn.linear_model import LogisticRegression<br />from sklearn.preprocessing import LabelEncoder<br />from sklearn.feature_extraction import DictVectorizer<br />from sklearn.cross_validation import train_test_split<br />from sklearn.externals.six import StringIO  <br /><br />#read in raw data from file and return a list of (label, article) tuples<br />def get_data(filename): <br />	data = [line.strip().split(&#39;\t&#39;) for line in open(filename).readlines()]<br />	random.shuffle(data)<br />	return data<br /><br />#this function builds the feature matrix for the Decision Tree.<br />def get_dtree_features(X) :<br />	features = []<br />	#TODO : Add the features you would like to use to train the Decision Tree here.<br />	feature_list = [&#39;gun&#39;, &#39;shooter&#39;, &#39;shot&#39;, &#39;shooting&#39;, &#39;player&#39;, &#39;coach&#39;, &#39;game&#39;, &#39;movie&#39;, &#39;actor&#39;, &#39;film&#39;, &#39;bullet&#39;, &#39;photo&#39;, &#39;model&#39;, &#39;point&#39;, &#39;suspect&#39;, &#39;victim&#39;, &#39;police&#39;, &#39;fired&#39;]<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w in feature_list : <br />				f[w] = 1.0<br />		features.append(f)<br />	return features<br /><br /><br />#this is the main function you care about; pack all the cleverest features you can think of into here.<br />def get_features(X) :<br />	stopwords = open(&#39;stopwords.txt&#39;).read()<br />	features = []<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w not in stopwords:            <br />				if w not in f : <br />					f[w] = 0<br />				f[w] &#43;= 1                <br />		features.append(f)<br />	return features<br /><br />#vectorize feature dictionaries and return feature and label matricies<br />def get_matricies(data, typ=&#34;unigram&#34;) : <br />	dv = DictVectorizer(sparse=True) <br />	le = LabelEncoder()<br />	y = [d[0] for d in data]<br />	texts = [d[1] for d in data]<br />	if typ == &#34;tree&#34;:<br />		X = get_dtree_features(texts)<br />	else :<br />		X = get_features(texts)<br />	#Here we are returning 5 things, the label vector y and feature matrix X, but also the texts from which the features were extracted and the <br />	#objects that were used to encode them. These will come in handy for your analysis, but you can ignore them for the initial parts of the assignment<br />	return le.fit_transform(y), dv.fit_transform(X), texts, dv, le<br /><br />#train and multinomial naive bayes classifier<br />def train_classifier(X, y):<br />	clf = LogisticRegression()<br />	clf.fit(X,y)<br />	return clf <br /><br />#train a Decision Tree classifier<br />def train_dtree_classifier(X, y):<br />	clf = DecisionTreeClassifier(max_depth=10)<br />	clf.fit(X,y)<br />	return clf<br /><br />#test the classifier<br />def test_classifier(clf, X, y):<br />	return clf.score(X,y)<br /><br />#cross validation	<br />def cross_validate(X, y, dv=None, typ=&#34;unigram&#34;, numfolds=5,):<br />	test_accs = []<br />	split = 1.0 / numfolds<br />	for i in range(numfolds):<br />		x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=i)<br />		if typ == &#34;tree&#34; :<br />			clf = train_dtree_classifier(x_train, y_train)<br />		else :<br />			clf = train_classifier(x_train, y_train)<br />		test_acc = test_classifier(clf, x_test, y_test)<br />		test_accs.append(test_acc)<br />		print &#39;Fold %d : %.05f&#39;%(i,test_acc)<br />	test_average = float(sum(test_accs))/ numfolds<br />	if typ == &#34;tree&#34; :<br />		with open(&#34;output.dot&#34;, &#39;w&#39;) as f:<br />			f = export_graphviz(clf, out_file=f, feature_names=dv.get_feature_names(), class_names=[&#39;Non Gun Related&#39;,&#39;Gun Related&#39;])<br />		create_graph(&#34;decision-tree.png&#34;)<br />	print &#39;Test Average : %.05f&#39;%(test_average)<br />	print<br />	return test_average<br /><br />#run a rule based classifier and calculate the accuracy<br />def rule_based_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO add more keywords, see how well they do alone and in combination<br />		if &#34;shooter&#34; in text or &#34;shot&#34; in text or &#34;shooting&#34; in text :<br />			if &#34;player&#34; in text or &#34;coach&#34; in text or &#34;game&#34; in text: <br />				prediction = &#39;0&#39;<br />			elif &#34;movie&#34; in text or &#34;film&#34; in text or &#34;actor&#34; in text:<br />				prediction = &#39;0&#39;<br />			else : prediction = &#39;1&#39;<br />		if &#34;gun&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Rule-based classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#Extra Credit Rule Based Classifier<br />def extra_credit_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO develop your conditional statements here<br />		if &#34;shooting&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Reverse Engineered classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#train and multinomial naive bayes classifier<br />def get_top_features(X, y, dv):<br />	clf = train_classifier(X, y)<br />	#the DictVectorizer object remembers which column number corresponds to which feature, and return the feature names in the correct order<br />	feature_names = dv.get_feature_names() <br /><br />	#The below code will get the weights from the classifier, and print out the weights of the features you are interested in<br />	features = [] #this will be a list of (feature_idx, weight) tuples<br />	for i,w in enumerate(clf.coef_[0]): <br />		features.append((i,w))<br />	#Sort the list by values, with the largest ones first<br />	features = sorted(features, key=lambda e: e[1], reverse=True)<br /><br />        #Print out the feature names and thier weights<br />	for i,w in features:<br />	  print &#39;%s\t%s&#39;%(feature_names[i], w)<br /><br />def get_misclassified_examples(y, X, texts) :<br />	x_train, x_test, y_train, y_test, train_texts, test_texts = train_test_split(X, y, texts)<br />	clf = train_classifier(x_train, y_train)<br /><br />	#TODO: You will have to write some code to call your classifier on each of the test examples, and check whether its prediction was right or wrong<br /><br />def create_graph(file_name) :<br />	os.system(&#34;dot -Tpng output.dot -o &#34; &#43; file_name)<br />	os.unlink(&#34;output.dot&#34;)<br /><br />if __name__ == &#39;__main__&#39; : <br /><br />	raw_data = get_data(sys.argv[1])<br />	<br />	print &#39;\nRule-based classification&#39;<br />	rule_based_classifier(raw_data)<br /><br />################ Decision Tree ################<br /><br /># print &#39;\nDecision Tree classification&#39;<br /># y, X, texts, dv, le = get_matricies(raw_data, &#34;tree&#34;)<br /># cross_validate(X,y,dv,&#34;tree&#34;)<br /><br />################ Statistical Classification ################<br />print &#39;\nStatistical classification&#39;<br />y, X, texts, dv, le = get_matricies(raw_data)<br />cross_validate(X,y)<br /><br />get_top_features(X, y, dv)<br />get_misclassified_examples(y, X, texts)<br />
</pre>
<p>Thanks in advance for any help you can give.</p>",Statistical Analysis &#34;Killed&#34; - even on biglab,"<p>When my program gets to the Statistical Analysis bit, it makes it through all the words and creates wordcount dictionaries that are appended to the list of features. Then it hangs for a bit until the terminal just prints &#34;Killed&#34; and program ends. Any idea what might be causing this? Not enough memory?</p>",Statistical Analysis &#34;Killed&#34;,,,"<p>I&#39;m confused on what exactly we&#39;re supposed to do for part 6. Are the y dimensions the same as the number of articles? Also, how is logistic regression supposed to be used here?</p>",Statistical Analysis Part 6,0
940849303,4/26/2016 17:06:41,true,1969445556,,4/26/2016 17:00:39,false,neodev,1.0,33973110,VEN,23,Maracaibo,186.94.238.104,0,0,"<p>Hi, so I read though the other piazza post about how the statistical analysis takes a long time, but I am still having some trouble with my code.</p>
<p></p>
<p>I have already tried waiting 10-15 minutes and removing all stopwords, but for some reason the statistical analysis refuses to complete in a reasonable amount of time</p>
<p></p>
<p>Below is my code:</p>
<pre>def isStopWord(word):
	with open(&#39;stopwords.txt&#39;, &#39;r&#39;) as stopwords:
		for line in stopwords:
			if word in line:
				return True
		return False

def get_features(X) : 
	features = []
	for x in X : 
		f = {}
		#TODO replace this dummy feature function with a unigram model, like we did in class
		for word in x.split():
			if isStopWord(word):
				continue
			if word in f:
				f[word] = f[word] &#43; 1.0
			else:
				f[word] =  1.0
		features.append(f)
	return features</pre>
<p>Not really sure what to do at this point because of this. Also might this be related to using the VM?</p>",Statistical Analysis Taking A Very Long Time,It takes a really long time. Is that expected?,statistical classification runtime,"<p>My script runs forever on my virtual machine, so I&#39;m running it off of biglab. I got the statistical analysis to work once (took about 2 minutes), but after un-commenting get_top_features and get_misclassified, my script gets killed before finishing the initial statistical analysis. I&#39;ve tried several times and I&#39;m unsure what the issue is; I&#39;ve copied my code below.</p>
<p></p>
<pre>#!/bin/python<br /><br />import os<br />import sys<br />import string<br />import random<br />import operator<br />from sklearn.tree import export_graphviz<br />from sklearn.tree import DecisionTreeClassifier<br />from sklearn.naive_bayes import MultinomialNB<br />from sklearn.linear_model import LogisticRegression<br />from sklearn.preprocessing import LabelEncoder<br />from sklearn.feature_extraction import DictVectorizer<br />from sklearn.cross_validation import train_test_split<br />from sklearn.externals.six import StringIO  <br /><br />#read in raw data from file and return a list of (label, article) tuples<br />def get_data(filename): <br />	data = [line.strip().split(&#39;\t&#39;) for line in open(filename).readlines()]<br />	random.shuffle(data)<br />	return data<br /><br />#this function builds the feature matrix for the Decision Tree.<br />def get_dtree_features(X) :<br />	features = []<br />	#TODO : Add the features you would like to use to train the Decision Tree here.<br />	feature_list = [&#39;gun&#39;, &#39;shooter&#39;, &#39;shot&#39;, &#39;shooting&#39;, &#39;player&#39;, &#39;coach&#39;, &#39;game&#39;, &#39;movie&#39;, &#39;actor&#39;, &#39;film&#39;, &#39;bullet&#39;, &#39;photo&#39;, &#39;model&#39;, &#39;point&#39;, &#39;suspect&#39;, &#39;victim&#39;, &#39;police&#39;, &#39;fired&#39;]<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w in feature_list : <br />				f[w] = 1.0<br />		features.append(f)<br />	return features<br /><br /><br />#this is the main function you care about; pack all the cleverest features you can think of into here.<br />def get_features(X) :<br />	stopwords = open(&#39;stopwords.txt&#39;).read()<br />	features = []<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w not in stopwords:            <br />				if w not in f : <br />					f[w] = 0<br />				f[w] &#43;= 1                <br />		features.append(f)<br />	return features<br /><br />#vectorize feature dictionaries and return feature and label matricies<br />def get_matricies(data, typ=&#34;unigram&#34;) : <br />	dv = DictVectorizer(sparse=True) <br />	le = LabelEncoder()<br />	y = [d[0] for d in data]<br />	texts = [d[1] for d in data]<br />	if typ == &#34;tree&#34;:<br />		X = get_dtree_features(texts)<br />	else :<br />		X = get_features(texts)<br />	#Here we are returning 5 things, the label vector y and feature matrix X, but also the texts from which the features were extracted and the <br />	#objects that were used to encode them. These will come in handy for your analysis, but you can ignore them for the initial parts of the assignment<br />	return le.fit_transform(y), dv.fit_transform(X), texts, dv, le<br /><br />#train and multinomial naive bayes classifier<br />def train_classifier(X, y):<br />	clf = LogisticRegression()<br />	clf.fit(X,y)<br />	return clf <br /><br />#train a Decision Tree classifier<br />def train_dtree_classifier(X, y):<br />	clf = DecisionTreeClassifier(max_depth=10)<br />	clf.fit(X,y)<br />	return clf<br /><br />#test the classifier<br />def test_classifier(clf, X, y):<br />	return clf.score(X,y)<br /><br />#cross validation	<br />def cross_validate(X, y, dv=None, typ=&#34;unigram&#34;, numfolds=5,):<br />	test_accs = []<br />	split = 1.0 / numfolds<br />	for i in range(numfolds):<br />		x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=i)<br />		if typ == &#34;tree&#34; :<br />			clf = train_dtree_classifier(x_train, y_train)<br />		else :<br />			clf = train_classifier(x_train, y_train)<br />		test_acc = test_classifier(clf, x_test, y_test)<br />		test_accs.append(test_acc)<br />		print &#39;Fold %d : %.05f&#39;%(i,test_acc)<br />	test_average = float(sum(test_accs))/ numfolds<br />	if typ == &#34;tree&#34; :<br />		with open(&#34;output.dot&#34;, &#39;w&#39;) as f:<br />			f = export_graphviz(clf, out_file=f, feature_names=dv.get_feature_names(), class_names=[&#39;Non Gun Related&#39;,&#39;Gun Related&#39;])<br />		create_graph(&#34;decision-tree.png&#34;)<br />	print &#39;Test Average : %.05f&#39;%(test_average)<br />	print<br />	return test_average<br /><br />#run a rule based classifier and calculate the accuracy<br />def rule_based_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO add more keywords, see how well they do alone and in combination<br />		if &#34;shooter&#34; in text or &#34;shot&#34; in text or &#34;shooting&#34; in text :<br />			if &#34;player&#34; in text or &#34;coach&#34; in text or &#34;game&#34; in text: <br />				prediction = &#39;0&#39;<br />			elif &#34;movie&#34; in text or &#34;film&#34; in text or &#34;actor&#34; in text:<br />				prediction = &#39;0&#39;<br />			else : prediction = &#39;1&#39;<br />		if &#34;gun&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Rule-based classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#Extra Credit Rule Based Classifier<br />def extra_credit_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO develop your conditional statements here<br />		if &#34;shooting&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Reverse Engineered classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#train and multinomial naive bayes classifier<br />def get_top_features(X, y, dv):<br />	clf = train_classifier(X, y)<br />	#the DictVectorizer object remembers which column number corresponds to which feature, and return the feature names in the correct order<br />	feature_names = dv.get_feature_names() <br /><br />	#The below code will get the weights from the classifier, and print out the weights of the features you are interested in<br />	features = [] #this will be a list of (feature_idx, weight) tuples<br />	for i,w in enumerate(clf.coef_[0]): <br />		features.append((i,w))<br />	#Sort the list by values, with the largest ones first<br />	features = sorted(features, key=lambda e: e[1], reverse=True)<br /><br />        #Print out the feature names and thier weights<br />	for i,w in features:<br />	  print &#39;%s\t%s&#39;%(feature_names[i], w)<br /><br />def get_misclassified_examples(y, X, texts) :<br />	x_train, x_test, y_train, y_test, train_texts, test_texts = train_test_split(X, y, texts)<br />	clf = train_classifier(x_train, y_train)<br /><br />	#TODO: You will have to write some code to call your classifier on each of the test examples, and check whether its prediction was right or wrong<br /><br />def create_graph(file_name) :<br />	os.system(&#34;dot -Tpng output.dot -o &#34; &#43; file_name)<br />	os.unlink(&#34;output.dot&#34;)<br /><br />if __name__ == &#39;__main__&#39; : <br /><br />	raw_data = get_data(sys.argv[1])<br />	<br />	print &#39;\nRule-based classification&#39;<br />	rule_based_classifier(raw_data)<br /><br />################ Decision Tree ################<br /><br /># print &#39;\nDecision Tree classification&#39;<br /># y, X, texts, dv, le = get_matricies(raw_data, &#34;tree&#34;)<br /># cross_validate(X,y,dv,&#34;tree&#34;)<br /><br />################ Statistical Classification ################<br />print &#39;\nStatistical classification&#39;<br />y, X, texts, dv, le = get_matricies(raw_data)<br />cross_validate(X,y)<br /><br />get_top_features(X, y, dv)<br />get_misclassified_examples(y, X, texts)<br />
</pre>
<p>Thanks in advance for any help you can give.</p>",Statistical Analysis &#34;Killed&#34; - even on biglab,"<p>When my program gets to the Statistical Analysis bit, it makes it through all the words and creates wordcount dictionaries that are appended to the list of features. Then it hangs for a bit until the terminal just prints &#34;Killed&#34; and program ends. Any idea what might be causing this? Not enough memory?</p>",Statistical Analysis &#34;Killed&#34;,,,"<p>I&#39;m confused on what exactly we&#39;re supposed to do for part 6. Are the y dimensions the same as the number of articles? Also, how is logistic regression supposed to be used here?</p>",Statistical Analysis Part 6,0
940849303,4/26/2016 17:11:04,true,1969448321,,4/26/2016 17:01:16,false,neodev,0.8889,19625264,DZA,41,Chlef,41.102.7.217,0,0,"<p>Hi, so I read though the other piazza post about how the statistical analysis takes a long time, but I am still having some trouble with my code.</p>
<p></p>
<p>I have already tried waiting 10-15 minutes and removing all stopwords, but for some reason the statistical analysis refuses to complete in a reasonable amount of time</p>
<p></p>
<p>Below is my code:</p>
<pre>def isStopWord(word):
	with open(&#39;stopwords.txt&#39;, &#39;r&#39;) as stopwords:
		for line in stopwords:
			if word in line:
				return True
		return False

def get_features(X) : 
	features = []
	for x in X : 
		f = {}
		#TODO replace this dummy feature function with a unigram model, like we did in class
		for word in x.split():
			if isStopWord(word):
				continue
			if word in f:
				f[word] = f[word] &#43; 1.0
			else:
				f[word] =  1.0
		features.append(f)
	return features</pre>
<p>Not really sure what to do at this point because of this. Also might this be related to using the VM?</p>",Statistical Analysis Taking A Very Long Time,It takes a really long time. Is that expected?,statistical classification runtime,"<p>My script runs forever on my virtual machine, so I&#39;m running it off of biglab. I got the statistical analysis to work once (took about 2 minutes), but after un-commenting get_top_features and get_misclassified, my script gets killed before finishing the initial statistical analysis. I&#39;ve tried several times and I&#39;m unsure what the issue is; I&#39;ve copied my code below.</p>
<p></p>
<pre>#!/bin/python<br /><br />import os<br />import sys<br />import string<br />import random<br />import operator<br />from sklearn.tree import export_graphviz<br />from sklearn.tree import DecisionTreeClassifier<br />from sklearn.naive_bayes import MultinomialNB<br />from sklearn.linear_model import LogisticRegression<br />from sklearn.preprocessing import LabelEncoder<br />from sklearn.feature_extraction import DictVectorizer<br />from sklearn.cross_validation import train_test_split<br />from sklearn.externals.six import StringIO  <br /><br />#read in raw data from file and return a list of (label, article) tuples<br />def get_data(filename): <br />	data = [line.strip().split(&#39;\t&#39;) for line in open(filename).readlines()]<br />	random.shuffle(data)<br />	return data<br /><br />#this function builds the feature matrix for the Decision Tree.<br />def get_dtree_features(X) :<br />	features = []<br />	#TODO : Add the features you would like to use to train the Decision Tree here.<br />	feature_list = [&#39;gun&#39;, &#39;shooter&#39;, &#39;shot&#39;, &#39;shooting&#39;, &#39;player&#39;, &#39;coach&#39;, &#39;game&#39;, &#39;movie&#39;, &#39;actor&#39;, &#39;film&#39;, &#39;bullet&#39;, &#39;photo&#39;, &#39;model&#39;, &#39;point&#39;, &#39;suspect&#39;, &#39;victim&#39;, &#39;police&#39;, &#39;fired&#39;]<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w in feature_list : <br />				f[w] = 1.0<br />		features.append(f)<br />	return features<br /><br /><br />#this is the main function you care about; pack all the cleverest features you can think of into here.<br />def get_features(X) :<br />	stopwords = open(&#39;stopwords.txt&#39;).read()<br />	features = []<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w not in stopwords:            <br />				if w not in f : <br />					f[w] = 0<br />				f[w] &#43;= 1                <br />		features.append(f)<br />	return features<br /><br />#vectorize feature dictionaries and return feature and label matricies<br />def get_matricies(data, typ=&#34;unigram&#34;) : <br />	dv = DictVectorizer(sparse=True) <br />	le = LabelEncoder()<br />	y = [d[0] for d in data]<br />	texts = [d[1] for d in data]<br />	if typ == &#34;tree&#34;:<br />		X = get_dtree_features(texts)<br />	else :<br />		X = get_features(texts)<br />	#Here we are returning 5 things, the label vector y and feature matrix X, but also the texts from which the features were extracted and the <br />	#objects that were used to encode them. These will come in handy for your analysis, but you can ignore them for the initial parts of the assignment<br />	return le.fit_transform(y), dv.fit_transform(X), texts, dv, le<br /><br />#train and multinomial naive bayes classifier<br />def train_classifier(X, y):<br />	clf = LogisticRegression()<br />	clf.fit(X,y)<br />	return clf <br /><br />#train a Decision Tree classifier<br />def train_dtree_classifier(X, y):<br />	clf = DecisionTreeClassifier(max_depth=10)<br />	clf.fit(X,y)<br />	return clf<br /><br />#test the classifier<br />def test_classifier(clf, X, y):<br />	return clf.score(X,y)<br /><br />#cross validation	<br />def cross_validate(X, y, dv=None, typ=&#34;unigram&#34;, numfolds=5,):<br />	test_accs = []<br />	split = 1.0 / numfolds<br />	for i in range(numfolds):<br />		x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=i)<br />		if typ == &#34;tree&#34; :<br />			clf = train_dtree_classifier(x_train, y_train)<br />		else :<br />			clf = train_classifier(x_train, y_train)<br />		test_acc = test_classifier(clf, x_test, y_test)<br />		test_accs.append(test_acc)<br />		print &#39;Fold %d : %.05f&#39;%(i,test_acc)<br />	test_average = float(sum(test_accs))/ numfolds<br />	if typ == &#34;tree&#34; :<br />		with open(&#34;output.dot&#34;, &#39;w&#39;) as f:<br />			f = export_graphviz(clf, out_file=f, feature_names=dv.get_feature_names(), class_names=[&#39;Non Gun Related&#39;,&#39;Gun Related&#39;])<br />		create_graph(&#34;decision-tree.png&#34;)<br />	print &#39;Test Average : %.05f&#39;%(test_average)<br />	print<br />	return test_average<br /><br />#run a rule based classifier and calculate the accuracy<br />def rule_based_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO add more keywords, see how well they do alone and in combination<br />		if &#34;shooter&#34; in text or &#34;shot&#34; in text or &#34;shooting&#34; in text :<br />			if &#34;player&#34; in text or &#34;coach&#34; in text or &#34;game&#34; in text: <br />				prediction = &#39;0&#39;<br />			elif &#34;movie&#34; in text or &#34;film&#34; in text or &#34;actor&#34; in text:<br />				prediction = &#39;0&#39;<br />			else : prediction = &#39;1&#39;<br />		if &#34;gun&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Rule-based classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#Extra Credit Rule Based Classifier<br />def extra_credit_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO develop your conditional statements here<br />		if &#34;shooting&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Reverse Engineered classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#train and multinomial naive bayes classifier<br />def get_top_features(X, y, dv):<br />	clf = train_classifier(X, y)<br />	#the DictVectorizer object remembers which column number corresponds to which feature, and return the feature names in the correct order<br />	feature_names = dv.get_feature_names() <br /><br />	#The below code will get the weights from the classifier, and print out the weights of the features you are interested in<br />	features = [] #this will be a list of (feature_idx, weight) tuples<br />	for i,w in enumerate(clf.coef_[0]): <br />		features.append((i,w))<br />	#Sort the list by values, with the largest ones first<br />	features = sorted(features, key=lambda e: e[1], reverse=True)<br /><br />        #Print out the feature names and thier weights<br />	for i,w in features:<br />	  print &#39;%s\t%s&#39;%(feature_names[i], w)<br /><br />def get_misclassified_examples(y, X, texts) :<br />	x_train, x_test, y_train, y_test, train_texts, test_texts = train_test_split(X, y, texts)<br />	clf = train_classifier(x_train, y_train)<br /><br />	#TODO: You will have to write some code to call your classifier on each of the test examples, and check whether its prediction was right or wrong<br /><br />def create_graph(file_name) :<br />	os.system(&#34;dot -Tpng output.dot -o &#34; &#43; file_name)<br />	os.unlink(&#34;output.dot&#34;)<br /><br />if __name__ == &#39;__main__&#39; : <br /><br />	raw_data = get_data(sys.argv[1])<br />	<br />	print &#39;\nRule-based classification&#39;<br />	rule_based_classifier(raw_data)<br /><br />################ Decision Tree ################<br /><br /># print &#39;\nDecision Tree classification&#39;<br /># y, X, texts, dv, le = get_matricies(raw_data, &#34;tree&#34;)<br /># cross_validate(X,y,dv,&#34;tree&#34;)<br /><br />################ Statistical Classification ################<br />print &#39;\nStatistical classification&#39;<br />y, X, texts, dv, le = get_matricies(raw_data)<br />cross_validate(X,y)<br /><br />get_top_features(X, y, dv)<br />get_misclassified_examples(y, X, texts)<br />
</pre>
<p>Thanks in advance for any help you can give.</p>",Statistical Analysis &#34;Killed&#34; - even on biglab,"<p>When my program gets to the Statistical Analysis bit, it makes it through all the words and creates wordcount dictionaries that are appended to the list of features. Then it hangs for a bit until the terminal just prints &#34;Killed&#34; and program ends. Any idea what might be causing this? Not enough memory?</p>",Statistical Analysis &#34;Killed&#34;,,,"<p>I&#39;m confused on what exactly we&#39;re supposed to do for part 6. Are the y dimensions the same as the number of articles? Also, how is logistic regression supposed to be used here?</p>",Statistical Analysis Part 6,0
940849303,4/26/2016 17:24:11,true,1969455986,,4/26/2016 17:22:14,false,elite,1.0,25411289,HRV,"","",31.147.119.175,0,0,"<p>Hi, so I read though the other piazza post about how the statistical analysis takes a long time, but I am still having some trouble with my code.</p>
<p></p>
<p>I have already tried waiting 10-15 minutes and removing all stopwords, but for some reason the statistical analysis refuses to complete in a reasonable amount of time</p>
<p></p>
<p>Below is my code:</p>
<pre>def isStopWord(word):
	with open(&#39;stopwords.txt&#39;, &#39;r&#39;) as stopwords:
		for line in stopwords:
			if word in line:
				return True
		return False

def get_features(X) : 
	features = []
	for x in X : 
		f = {}
		#TODO replace this dummy feature function with a unigram model, like we did in class
		for word in x.split():
			if isStopWord(word):
				continue
			if word in f:
				f[word] = f[word] &#43; 1.0
			else:
				f[word] =  1.0
		features.append(f)
	return features</pre>
<p>Not really sure what to do at this point because of this. Also might this be related to using the VM?</p>",Statistical Analysis Taking A Very Long Time,It takes a really long time. Is that expected?,statistical classification runtime,"<p>My script runs forever on my virtual machine, so I&#39;m running it off of biglab. I got the statistical analysis to work once (took about 2 minutes), but after un-commenting get_top_features and get_misclassified, my script gets killed before finishing the initial statistical analysis. I&#39;ve tried several times and I&#39;m unsure what the issue is; I&#39;ve copied my code below.</p>
<p></p>
<pre>#!/bin/python<br /><br />import os<br />import sys<br />import string<br />import random<br />import operator<br />from sklearn.tree import export_graphviz<br />from sklearn.tree import DecisionTreeClassifier<br />from sklearn.naive_bayes import MultinomialNB<br />from sklearn.linear_model import LogisticRegression<br />from sklearn.preprocessing import LabelEncoder<br />from sklearn.feature_extraction import DictVectorizer<br />from sklearn.cross_validation import train_test_split<br />from sklearn.externals.six import StringIO  <br /><br />#read in raw data from file and return a list of (label, article) tuples<br />def get_data(filename): <br />	data = [line.strip().split(&#39;\t&#39;) for line in open(filename).readlines()]<br />	random.shuffle(data)<br />	return data<br /><br />#this function builds the feature matrix for the Decision Tree.<br />def get_dtree_features(X) :<br />	features = []<br />	#TODO : Add the features you would like to use to train the Decision Tree here.<br />	feature_list = [&#39;gun&#39;, &#39;shooter&#39;, &#39;shot&#39;, &#39;shooting&#39;, &#39;player&#39;, &#39;coach&#39;, &#39;game&#39;, &#39;movie&#39;, &#39;actor&#39;, &#39;film&#39;, &#39;bullet&#39;, &#39;photo&#39;, &#39;model&#39;, &#39;point&#39;, &#39;suspect&#39;, &#39;victim&#39;, &#39;police&#39;, &#39;fired&#39;]<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w in feature_list : <br />				f[w] = 1.0<br />		features.append(f)<br />	return features<br /><br /><br />#this is the main function you care about; pack all the cleverest features you can think of into here.<br />def get_features(X) :<br />	stopwords = open(&#39;stopwords.txt&#39;).read()<br />	features = []<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w not in stopwords:            <br />				if w not in f : <br />					f[w] = 0<br />				f[w] &#43;= 1                <br />		features.append(f)<br />	return features<br /><br />#vectorize feature dictionaries and return feature and label matricies<br />def get_matricies(data, typ=&#34;unigram&#34;) : <br />	dv = DictVectorizer(sparse=True) <br />	le = LabelEncoder()<br />	y = [d[0] for d in data]<br />	texts = [d[1] for d in data]<br />	if typ == &#34;tree&#34;:<br />		X = get_dtree_features(texts)<br />	else :<br />		X = get_features(texts)<br />	#Here we are returning 5 things, the label vector y and feature matrix X, but also the texts from which the features were extracted and the <br />	#objects that were used to encode them. These will come in handy for your analysis, but you can ignore them for the initial parts of the assignment<br />	return le.fit_transform(y), dv.fit_transform(X), texts, dv, le<br /><br />#train and multinomial naive bayes classifier<br />def train_classifier(X, y):<br />	clf = LogisticRegression()<br />	clf.fit(X,y)<br />	return clf <br /><br />#train a Decision Tree classifier<br />def train_dtree_classifier(X, y):<br />	clf = DecisionTreeClassifier(max_depth=10)<br />	clf.fit(X,y)<br />	return clf<br /><br />#test the classifier<br />def test_classifier(clf, X, y):<br />	return clf.score(X,y)<br /><br />#cross validation	<br />def cross_validate(X, y, dv=None, typ=&#34;unigram&#34;, numfolds=5,):<br />	test_accs = []<br />	split = 1.0 / numfolds<br />	for i in range(numfolds):<br />		x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=i)<br />		if typ == &#34;tree&#34; :<br />			clf = train_dtree_classifier(x_train, y_train)<br />		else :<br />			clf = train_classifier(x_train, y_train)<br />		test_acc = test_classifier(clf, x_test, y_test)<br />		test_accs.append(test_acc)<br />		print &#39;Fold %d : %.05f&#39;%(i,test_acc)<br />	test_average = float(sum(test_accs))/ numfolds<br />	if typ == &#34;tree&#34; :<br />		with open(&#34;output.dot&#34;, &#39;w&#39;) as f:<br />			f = export_graphviz(clf, out_file=f, feature_names=dv.get_feature_names(), class_names=[&#39;Non Gun Related&#39;,&#39;Gun Related&#39;])<br />		create_graph(&#34;decision-tree.png&#34;)<br />	print &#39;Test Average : %.05f&#39;%(test_average)<br />	print<br />	return test_average<br /><br />#run a rule based classifier and calculate the accuracy<br />def rule_based_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO add more keywords, see how well they do alone and in combination<br />		if &#34;shooter&#34; in text or &#34;shot&#34; in text or &#34;shooting&#34; in text :<br />			if &#34;player&#34; in text or &#34;coach&#34; in text or &#34;game&#34; in text: <br />				prediction = &#39;0&#39;<br />			elif &#34;movie&#34; in text or &#34;film&#34; in text or &#34;actor&#34; in text:<br />				prediction = &#39;0&#39;<br />			else : prediction = &#39;1&#39;<br />		if &#34;gun&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Rule-based classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#Extra Credit Rule Based Classifier<br />def extra_credit_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO develop your conditional statements here<br />		if &#34;shooting&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Reverse Engineered classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#train and multinomial naive bayes classifier<br />def get_top_features(X, y, dv):<br />	clf = train_classifier(X, y)<br />	#the DictVectorizer object remembers which column number corresponds to which feature, and return the feature names in the correct order<br />	feature_names = dv.get_feature_names() <br /><br />	#The below code will get the weights from the classifier, and print out the weights of the features you are interested in<br />	features = [] #this will be a list of (feature_idx, weight) tuples<br />	for i,w in enumerate(clf.coef_[0]): <br />		features.append((i,w))<br />	#Sort the list by values, with the largest ones first<br />	features = sorted(features, key=lambda e: e[1], reverse=True)<br /><br />        #Print out the feature names and thier weights<br />	for i,w in features:<br />	  print &#39;%s\t%s&#39;%(feature_names[i], w)<br /><br />def get_misclassified_examples(y, X, texts) :<br />	x_train, x_test, y_train, y_test, train_texts, test_texts = train_test_split(X, y, texts)<br />	clf = train_classifier(x_train, y_train)<br /><br />	#TODO: You will have to write some code to call your classifier on each of the test examples, and check whether its prediction was right or wrong<br /><br />def create_graph(file_name) :<br />	os.system(&#34;dot -Tpng output.dot -o &#34; &#43; file_name)<br />	os.unlink(&#34;output.dot&#34;)<br /><br />if __name__ == &#39;__main__&#39; : <br /><br />	raw_data = get_data(sys.argv[1])<br />	<br />	print &#39;\nRule-based classification&#39;<br />	rule_based_classifier(raw_data)<br /><br />################ Decision Tree ################<br /><br /># print &#39;\nDecision Tree classification&#39;<br /># y, X, texts, dv, le = get_matricies(raw_data, &#34;tree&#34;)<br /># cross_validate(X,y,dv,&#34;tree&#34;)<br /><br />################ Statistical Classification ################<br />print &#39;\nStatistical classification&#39;<br />y, X, texts, dv, le = get_matricies(raw_data)<br />cross_validate(X,y)<br /><br />get_top_features(X, y, dv)<br />get_misclassified_examples(y, X, texts)<br />
</pre>
<p>Thanks in advance for any help you can give.</p>",Statistical Analysis &#34;Killed&#34; - even on biglab,"<p>When my program gets to the Statistical Analysis bit, it makes it through all the words and creates wordcount dictionaries that are appended to the list of features. Then it hangs for a bit until the terminal just prints &#34;Killed&#34; and program ends. Any idea what might be causing this? Not enough memory?</p>",Statistical Analysis &#34;Killed&#34;,,,"<p>I&#39;m confused on what exactly we&#39;re supposed to do for part 6. Are the y dimensions the same as the number of articles? Also, how is logistic regression supposed to be used here?</p>",Statistical Analysis Part 6,0
940849303,4/26/2016 17:31:56,true,1969460624,,4/26/2016 17:31:26,false,neodev,0.8889,33131546,IDN,04,Jakarta,139.194.89.60,0,0,"<p>Hi, so I read though the other piazza post about how the statistical analysis takes a long time, but I am still having some trouble with my code.</p>
<p></p>
<p>I have already tried waiting 10-15 minutes and removing all stopwords, but for some reason the statistical analysis refuses to complete in a reasonable amount of time</p>
<p></p>
<p>Below is my code:</p>
<pre>def isStopWord(word):
	with open(&#39;stopwords.txt&#39;, &#39;r&#39;) as stopwords:
		for line in stopwords:
			if word in line:
				return True
		return False

def get_features(X) : 
	features = []
	for x in X : 
		f = {}
		#TODO replace this dummy feature function with a unigram model, like we did in class
		for word in x.split():
			if isStopWord(word):
				continue
			if word in f:
				f[word] = f[word] &#43; 1.0
			else:
				f[word] =  1.0
		features.append(f)
	return features</pre>
<p>Not really sure what to do at this point because of this. Also might this be related to using the VM?</p>",Statistical Analysis Taking A Very Long Time,It takes a really long time. Is that expected?,statistical classification runtime,"<p>My script runs forever on my virtual machine, so I&#39;m running it off of biglab. I got the statistical analysis to work once (took about 2 minutes), but after un-commenting get_top_features and get_misclassified, my script gets killed before finishing the initial statistical analysis. I&#39;ve tried several times and I&#39;m unsure what the issue is; I&#39;ve copied my code below.</p>
<p></p>
<pre>#!/bin/python<br /><br />import os<br />import sys<br />import string<br />import random<br />import operator<br />from sklearn.tree import export_graphviz<br />from sklearn.tree import DecisionTreeClassifier<br />from sklearn.naive_bayes import MultinomialNB<br />from sklearn.linear_model import LogisticRegression<br />from sklearn.preprocessing import LabelEncoder<br />from sklearn.feature_extraction import DictVectorizer<br />from sklearn.cross_validation import train_test_split<br />from sklearn.externals.six import StringIO  <br /><br />#read in raw data from file and return a list of (label, article) tuples<br />def get_data(filename): <br />	data = [line.strip().split(&#39;\t&#39;) for line in open(filename).readlines()]<br />	random.shuffle(data)<br />	return data<br /><br />#this function builds the feature matrix for the Decision Tree.<br />def get_dtree_features(X) :<br />	features = []<br />	#TODO : Add the features you would like to use to train the Decision Tree here.<br />	feature_list = [&#39;gun&#39;, &#39;shooter&#39;, &#39;shot&#39;, &#39;shooting&#39;, &#39;player&#39;, &#39;coach&#39;, &#39;game&#39;, &#39;movie&#39;, &#39;actor&#39;, &#39;film&#39;, &#39;bullet&#39;, &#39;photo&#39;, &#39;model&#39;, &#39;point&#39;, &#39;suspect&#39;, &#39;victim&#39;, &#39;police&#39;, &#39;fired&#39;]<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w in feature_list : <br />				f[w] = 1.0<br />		features.append(f)<br />	return features<br /><br /><br />#this is the main function you care about; pack all the cleverest features you can think of into here.<br />def get_features(X) :<br />	stopwords = open(&#39;stopwords.txt&#39;).read()<br />	features = []<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w not in stopwords:            <br />				if w not in f : <br />					f[w] = 0<br />				f[w] &#43;= 1                <br />		features.append(f)<br />	return features<br /><br />#vectorize feature dictionaries and return feature and label matricies<br />def get_matricies(data, typ=&#34;unigram&#34;) : <br />	dv = DictVectorizer(sparse=True) <br />	le = LabelEncoder()<br />	y = [d[0] for d in data]<br />	texts = [d[1] for d in data]<br />	if typ == &#34;tree&#34;:<br />		X = get_dtree_features(texts)<br />	else :<br />		X = get_features(texts)<br />	#Here we are returning 5 things, the label vector y and feature matrix X, but also the texts from which the features were extracted and the <br />	#objects that were used to encode them. These will come in handy for your analysis, but you can ignore them for the initial parts of the assignment<br />	return le.fit_transform(y), dv.fit_transform(X), texts, dv, le<br /><br />#train and multinomial naive bayes classifier<br />def train_classifier(X, y):<br />	clf = LogisticRegression()<br />	clf.fit(X,y)<br />	return clf <br /><br />#train a Decision Tree classifier<br />def train_dtree_classifier(X, y):<br />	clf = DecisionTreeClassifier(max_depth=10)<br />	clf.fit(X,y)<br />	return clf<br /><br />#test the classifier<br />def test_classifier(clf, X, y):<br />	return clf.score(X,y)<br /><br />#cross validation	<br />def cross_validate(X, y, dv=None, typ=&#34;unigram&#34;, numfolds=5,):<br />	test_accs = []<br />	split = 1.0 / numfolds<br />	for i in range(numfolds):<br />		x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=i)<br />		if typ == &#34;tree&#34; :<br />			clf = train_dtree_classifier(x_train, y_train)<br />		else :<br />			clf = train_classifier(x_train, y_train)<br />		test_acc = test_classifier(clf, x_test, y_test)<br />		test_accs.append(test_acc)<br />		print &#39;Fold %d : %.05f&#39;%(i,test_acc)<br />	test_average = float(sum(test_accs))/ numfolds<br />	if typ == &#34;tree&#34; :<br />		with open(&#34;output.dot&#34;, &#39;w&#39;) as f:<br />			f = export_graphviz(clf, out_file=f, feature_names=dv.get_feature_names(), class_names=[&#39;Non Gun Related&#39;,&#39;Gun Related&#39;])<br />		create_graph(&#34;decision-tree.png&#34;)<br />	print &#39;Test Average : %.05f&#39;%(test_average)<br />	print<br />	return test_average<br /><br />#run a rule based classifier and calculate the accuracy<br />def rule_based_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO add more keywords, see how well they do alone and in combination<br />		if &#34;shooter&#34; in text or &#34;shot&#34; in text or &#34;shooting&#34; in text :<br />			if &#34;player&#34; in text or &#34;coach&#34; in text or &#34;game&#34; in text: <br />				prediction = &#39;0&#39;<br />			elif &#34;movie&#34; in text or &#34;film&#34; in text or &#34;actor&#34; in text:<br />				prediction = &#39;0&#39;<br />			else : prediction = &#39;1&#39;<br />		if &#34;gun&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Rule-based classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#Extra Credit Rule Based Classifier<br />def extra_credit_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO develop your conditional statements here<br />		if &#34;shooting&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Reverse Engineered classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#train and multinomial naive bayes classifier<br />def get_top_features(X, y, dv):<br />	clf = train_classifier(X, y)<br />	#the DictVectorizer object remembers which column number corresponds to which feature, and return the feature names in the correct order<br />	feature_names = dv.get_feature_names() <br /><br />	#The below code will get the weights from the classifier, and print out the weights of the features you are interested in<br />	features = [] #this will be a list of (feature_idx, weight) tuples<br />	for i,w in enumerate(clf.coef_[0]): <br />		features.append((i,w))<br />	#Sort the list by values, with the largest ones first<br />	features = sorted(features, key=lambda e: e[1], reverse=True)<br /><br />        #Print out the feature names and thier weights<br />	for i,w in features:<br />	  print &#39;%s\t%s&#39;%(feature_names[i], w)<br /><br />def get_misclassified_examples(y, X, texts) :<br />	x_train, x_test, y_train, y_test, train_texts, test_texts = train_test_split(X, y, texts)<br />	clf = train_classifier(x_train, y_train)<br /><br />	#TODO: You will have to write some code to call your classifier on each of the test examples, and check whether its prediction was right or wrong<br /><br />def create_graph(file_name) :<br />	os.system(&#34;dot -Tpng output.dot -o &#34; &#43; file_name)<br />	os.unlink(&#34;output.dot&#34;)<br /><br />if __name__ == &#39;__main__&#39; : <br /><br />	raw_data = get_data(sys.argv[1])<br />	<br />	print &#39;\nRule-based classification&#39;<br />	rule_based_classifier(raw_data)<br /><br />################ Decision Tree ################<br /><br /># print &#39;\nDecision Tree classification&#39;<br /># y, X, texts, dv, le = get_matricies(raw_data, &#34;tree&#34;)<br /># cross_validate(X,y,dv,&#34;tree&#34;)<br /><br />################ Statistical Classification ################<br />print &#39;\nStatistical classification&#39;<br />y, X, texts, dv, le = get_matricies(raw_data)<br />cross_validate(X,y)<br /><br />get_top_features(X, y, dv)<br />get_misclassified_examples(y, X, texts)<br />
</pre>
<p>Thanks in advance for any help you can give.</p>",Statistical Analysis &#34;Killed&#34; - even on biglab,"<p>When my program gets to the Statistical Analysis bit, it makes it through all the words and creates wordcount dictionaries that are appended to the list of features. Then it hangs for a bit until the terminal just prints &#34;Killed&#34; and program ends. Any idea what might be causing this? Not enough memory?</p>",Statistical Analysis &#34;Killed&#34;,,,"<p>I&#39;m confused on what exactly we&#39;re supposed to do for part 6. Are the y dimensions the same as the number of articles? Also, how is logistic regression supposed to be used here?</p>",Statistical Analysis Part 6,0
940849303,4/26/2016 17:32:21,true,1969460822,,4/26/2016 17:30:36,false,neodev,1.0,11172894,IND,28,Champdani,117.194.5.117,0,0,"<p>Hi, so I read though the other piazza post about how the statistical analysis takes a long time, but I am still having some trouble with my code.</p>
<p></p>
<p>I have already tried waiting 10-15 minutes and removing all stopwords, but for some reason the statistical analysis refuses to complete in a reasonable amount of time</p>
<p></p>
<p>Below is my code:</p>
<pre>def isStopWord(word):
	with open(&#39;stopwords.txt&#39;, &#39;r&#39;) as stopwords:
		for line in stopwords:
			if word in line:
				return True
		return False

def get_features(X) : 
	features = []
	for x in X : 
		f = {}
		#TODO replace this dummy feature function with a unigram model, like we did in class
		for word in x.split():
			if isStopWord(word):
				continue
			if word in f:
				f[word] = f[word] &#43; 1.0
			else:
				f[word] =  1.0
		features.append(f)
	return features</pre>
<p>Not really sure what to do at this point because of this. Also might this be related to using the VM?</p>",Statistical Analysis Taking A Very Long Time,It takes a really long time. Is that expected?,statistical classification runtime,"<p>My script runs forever on my virtual machine, so I&#39;m running it off of biglab. I got the statistical analysis to work once (took about 2 minutes), but after un-commenting get_top_features and get_misclassified, my script gets killed before finishing the initial statistical analysis. I&#39;ve tried several times and I&#39;m unsure what the issue is; I&#39;ve copied my code below.</p>
<p></p>
<pre>#!/bin/python<br /><br />import os<br />import sys<br />import string<br />import random<br />import operator<br />from sklearn.tree import export_graphviz<br />from sklearn.tree import DecisionTreeClassifier<br />from sklearn.naive_bayes import MultinomialNB<br />from sklearn.linear_model import LogisticRegression<br />from sklearn.preprocessing import LabelEncoder<br />from sklearn.feature_extraction import DictVectorizer<br />from sklearn.cross_validation import train_test_split<br />from sklearn.externals.six import StringIO  <br /><br />#read in raw data from file and return a list of (label, article) tuples<br />def get_data(filename): <br />	data = [line.strip().split(&#39;\t&#39;) for line in open(filename).readlines()]<br />	random.shuffle(data)<br />	return data<br /><br />#this function builds the feature matrix for the Decision Tree.<br />def get_dtree_features(X) :<br />	features = []<br />	#TODO : Add the features you would like to use to train the Decision Tree here.<br />	feature_list = [&#39;gun&#39;, &#39;shooter&#39;, &#39;shot&#39;, &#39;shooting&#39;, &#39;player&#39;, &#39;coach&#39;, &#39;game&#39;, &#39;movie&#39;, &#39;actor&#39;, &#39;film&#39;, &#39;bullet&#39;, &#39;photo&#39;, &#39;model&#39;, &#39;point&#39;, &#39;suspect&#39;, &#39;victim&#39;, &#39;police&#39;, &#39;fired&#39;]<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w in feature_list : <br />				f[w] = 1.0<br />		features.append(f)<br />	return features<br /><br /><br />#this is the main function you care about; pack all the cleverest features you can think of into here.<br />def get_features(X) :<br />	stopwords = open(&#39;stopwords.txt&#39;).read()<br />	features = []<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w not in stopwords:            <br />				if w not in f : <br />					f[w] = 0<br />				f[w] &#43;= 1                <br />		features.append(f)<br />	return features<br /><br />#vectorize feature dictionaries and return feature and label matricies<br />def get_matricies(data, typ=&#34;unigram&#34;) : <br />	dv = DictVectorizer(sparse=True) <br />	le = LabelEncoder()<br />	y = [d[0] for d in data]<br />	texts = [d[1] for d in data]<br />	if typ == &#34;tree&#34;:<br />		X = get_dtree_features(texts)<br />	else :<br />		X = get_features(texts)<br />	#Here we are returning 5 things, the label vector y and feature matrix X, but also the texts from which the features were extracted and the <br />	#objects that were used to encode them. These will come in handy for your analysis, but you can ignore them for the initial parts of the assignment<br />	return le.fit_transform(y), dv.fit_transform(X), texts, dv, le<br /><br />#train and multinomial naive bayes classifier<br />def train_classifier(X, y):<br />	clf = LogisticRegression()<br />	clf.fit(X,y)<br />	return clf <br /><br />#train a Decision Tree classifier<br />def train_dtree_classifier(X, y):<br />	clf = DecisionTreeClassifier(max_depth=10)<br />	clf.fit(X,y)<br />	return clf<br /><br />#test the classifier<br />def test_classifier(clf, X, y):<br />	return clf.score(X,y)<br /><br />#cross validation	<br />def cross_validate(X, y, dv=None, typ=&#34;unigram&#34;, numfolds=5,):<br />	test_accs = []<br />	split = 1.0 / numfolds<br />	for i in range(numfolds):<br />		x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=i)<br />		if typ == &#34;tree&#34; :<br />			clf = train_dtree_classifier(x_train, y_train)<br />		else :<br />			clf = train_classifier(x_train, y_train)<br />		test_acc = test_classifier(clf, x_test, y_test)<br />		test_accs.append(test_acc)<br />		print &#39;Fold %d : %.05f&#39;%(i,test_acc)<br />	test_average = float(sum(test_accs))/ numfolds<br />	if typ == &#34;tree&#34; :<br />		with open(&#34;output.dot&#34;, &#39;w&#39;) as f:<br />			f = export_graphviz(clf, out_file=f, feature_names=dv.get_feature_names(), class_names=[&#39;Non Gun Related&#39;,&#39;Gun Related&#39;])<br />		create_graph(&#34;decision-tree.png&#34;)<br />	print &#39;Test Average : %.05f&#39;%(test_average)<br />	print<br />	return test_average<br /><br />#run a rule based classifier and calculate the accuracy<br />def rule_based_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO add more keywords, see how well they do alone and in combination<br />		if &#34;shooter&#34; in text or &#34;shot&#34; in text or &#34;shooting&#34; in text :<br />			if &#34;player&#34; in text or &#34;coach&#34; in text or &#34;game&#34; in text: <br />				prediction = &#39;0&#39;<br />			elif &#34;movie&#34; in text or &#34;film&#34; in text or &#34;actor&#34; in text:<br />				prediction = &#39;0&#39;<br />			else : prediction = &#39;1&#39;<br />		if &#34;gun&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Rule-based classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#Extra Credit Rule Based Classifier<br />def extra_credit_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO develop your conditional statements here<br />		if &#34;shooting&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Reverse Engineered classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#train and multinomial naive bayes classifier<br />def get_top_features(X, y, dv):<br />	clf = train_classifier(X, y)<br />	#the DictVectorizer object remembers which column number corresponds to which feature, and return the feature names in the correct order<br />	feature_names = dv.get_feature_names() <br /><br />	#The below code will get the weights from the classifier, and print out the weights of the features you are interested in<br />	features = [] #this will be a list of (feature_idx, weight) tuples<br />	for i,w in enumerate(clf.coef_[0]): <br />		features.append((i,w))<br />	#Sort the list by values, with the largest ones first<br />	features = sorted(features, key=lambda e: e[1], reverse=True)<br /><br />        #Print out the feature names and thier weights<br />	for i,w in features:<br />	  print &#39;%s\t%s&#39;%(feature_names[i], w)<br /><br />def get_misclassified_examples(y, X, texts) :<br />	x_train, x_test, y_train, y_test, train_texts, test_texts = train_test_split(X, y, texts)<br />	clf = train_classifier(x_train, y_train)<br /><br />	#TODO: You will have to write some code to call your classifier on each of the test examples, and check whether its prediction was right or wrong<br /><br />def create_graph(file_name) :<br />	os.system(&#34;dot -Tpng output.dot -o &#34; &#43; file_name)<br />	os.unlink(&#34;output.dot&#34;)<br /><br />if __name__ == &#39;__main__&#39; : <br /><br />	raw_data = get_data(sys.argv[1])<br />	<br />	print &#39;\nRule-based classification&#39;<br />	rule_based_classifier(raw_data)<br /><br />################ Decision Tree ################<br /><br /># print &#39;\nDecision Tree classification&#39;<br /># y, X, texts, dv, le = get_matricies(raw_data, &#34;tree&#34;)<br /># cross_validate(X,y,dv,&#34;tree&#34;)<br /><br />################ Statistical Classification ################<br />print &#39;\nStatistical classification&#39;<br />y, X, texts, dv, le = get_matricies(raw_data)<br />cross_validate(X,y)<br /><br />get_top_features(X, y, dv)<br />get_misclassified_examples(y, X, texts)<br />
</pre>
<p>Thanks in advance for any help you can give.</p>",Statistical Analysis &#34;Killed&#34; - even on biglab,"<p>When my program gets to the Statistical Analysis bit, it makes it through all the words and creates wordcount dictionaries that are appended to the list of features. Then it hangs for a bit until the terminal just prints &#34;Killed&#34; and program ends. Any idea what might be causing this? Not enough memory?</p>",Statistical Analysis &#34;Killed&#34;,,,"<p>I&#39;m confused on what exactly we&#39;re supposed to do for part 6. Are the y dimensions the same as the number of articles? Also, how is logistic regression supposed to be used here?</p>",Statistical Analysis Part 6,0
940849303,4/26/2016 17:33:45,true,1969461564,,4/26/2016 17:33:17,false,clixsense,1.0,30712378,ROU,21,Deva,79.119.241.200,0,0,"<p>Hi, so I read though the other piazza post about how the statistical analysis takes a long time, but I am still having some trouble with my code.</p>
<p></p>
<p>I have already tried waiting 10-15 minutes and removing all stopwords, but for some reason the statistical analysis refuses to complete in a reasonable amount of time</p>
<p></p>
<p>Below is my code:</p>
<pre>def isStopWord(word):
	with open(&#39;stopwords.txt&#39;, &#39;r&#39;) as stopwords:
		for line in stopwords:
			if word in line:
				return True
		return False

def get_features(X) : 
	features = []
	for x in X : 
		f = {}
		#TODO replace this dummy feature function with a unigram model, like we did in class
		for word in x.split():
			if isStopWord(word):
				continue
			if word in f:
				f[word] = f[word] &#43; 1.0
			else:
				f[word] =  1.0
		features.append(f)
	return features</pre>
<p>Not really sure what to do at this point because of this. Also might this be related to using the VM?</p>",Statistical Analysis Taking A Very Long Time,It takes a really long time. Is that expected?,statistical classification runtime,"<p>My script runs forever on my virtual machine, so I&#39;m running it off of biglab. I got the statistical analysis to work once (took about 2 minutes), but after un-commenting get_top_features and get_misclassified, my script gets killed before finishing the initial statistical analysis. I&#39;ve tried several times and I&#39;m unsure what the issue is; I&#39;ve copied my code below.</p>
<p></p>
<pre>#!/bin/python<br /><br />import os<br />import sys<br />import string<br />import random<br />import operator<br />from sklearn.tree import export_graphviz<br />from sklearn.tree import DecisionTreeClassifier<br />from sklearn.naive_bayes import MultinomialNB<br />from sklearn.linear_model import LogisticRegression<br />from sklearn.preprocessing import LabelEncoder<br />from sklearn.feature_extraction import DictVectorizer<br />from sklearn.cross_validation import train_test_split<br />from sklearn.externals.six import StringIO  <br /><br />#read in raw data from file and return a list of (label, article) tuples<br />def get_data(filename): <br />	data = [line.strip().split(&#39;\t&#39;) for line in open(filename).readlines()]<br />	random.shuffle(data)<br />	return data<br /><br />#this function builds the feature matrix for the Decision Tree.<br />def get_dtree_features(X) :<br />	features = []<br />	#TODO : Add the features you would like to use to train the Decision Tree here.<br />	feature_list = [&#39;gun&#39;, &#39;shooter&#39;, &#39;shot&#39;, &#39;shooting&#39;, &#39;player&#39;, &#39;coach&#39;, &#39;game&#39;, &#39;movie&#39;, &#39;actor&#39;, &#39;film&#39;, &#39;bullet&#39;, &#39;photo&#39;, &#39;model&#39;, &#39;point&#39;, &#39;suspect&#39;, &#39;victim&#39;, &#39;police&#39;, &#39;fired&#39;]<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w in feature_list : <br />				f[w] = 1.0<br />		features.append(f)<br />	return features<br /><br /><br />#this is the main function you care about; pack all the cleverest features you can think of into here.<br />def get_features(X) :<br />	stopwords = open(&#39;stopwords.txt&#39;).read()<br />	features = []<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w not in stopwords:            <br />				if w not in f : <br />					f[w] = 0<br />				f[w] &#43;= 1                <br />		features.append(f)<br />	return features<br /><br />#vectorize feature dictionaries and return feature and label matricies<br />def get_matricies(data, typ=&#34;unigram&#34;) : <br />	dv = DictVectorizer(sparse=True) <br />	le = LabelEncoder()<br />	y = [d[0] for d in data]<br />	texts = [d[1] for d in data]<br />	if typ == &#34;tree&#34;:<br />		X = get_dtree_features(texts)<br />	else :<br />		X = get_features(texts)<br />	#Here we are returning 5 things, the label vector y and feature matrix X, but also the texts from which the features were extracted and the <br />	#objects that were used to encode them. These will come in handy for your analysis, but you can ignore them for the initial parts of the assignment<br />	return le.fit_transform(y), dv.fit_transform(X), texts, dv, le<br /><br />#train and multinomial naive bayes classifier<br />def train_classifier(X, y):<br />	clf = LogisticRegression()<br />	clf.fit(X,y)<br />	return clf <br /><br />#train a Decision Tree classifier<br />def train_dtree_classifier(X, y):<br />	clf = DecisionTreeClassifier(max_depth=10)<br />	clf.fit(X,y)<br />	return clf<br /><br />#test the classifier<br />def test_classifier(clf, X, y):<br />	return clf.score(X,y)<br /><br />#cross validation	<br />def cross_validate(X, y, dv=None, typ=&#34;unigram&#34;, numfolds=5,):<br />	test_accs = []<br />	split = 1.0 / numfolds<br />	for i in range(numfolds):<br />		x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=i)<br />		if typ == &#34;tree&#34; :<br />			clf = train_dtree_classifier(x_train, y_train)<br />		else :<br />			clf = train_classifier(x_train, y_train)<br />		test_acc = test_classifier(clf, x_test, y_test)<br />		test_accs.append(test_acc)<br />		print &#39;Fold %d : %.05f&#39;%(i,test_acc)<br />	test_average = float(sum(test_accs))/ numfolds<br />	if typ == &#34;tree&#34; :<br />		with open(&#34;output.dot&#34;, &#39;w&#39;) as f:<br />			f = export_graphviz(clf, out_file=f, feature_names=dv.get_feature_names(), class_names=[&#39;Non Gun Related&#39;,&#39;Gun Related&#39;])<br />		create_graph(&#34;decision-tree.png&#34;)<br />	print &#39;Test Average : %.05f&#39;%(test_average)<br />	print<br />	return test_average<br /><br />#run a rule based classifier and calculate the accuracy<br />def rule_based_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO add more keywords, see how well they do alone and in combination<br />		if &#34;shooter&#34; in text or &#34;shot&#34; in text or &#34;shooting&#34; in text :<br />			if &#34;player&#34; in text or &#34;coach&#34; in text or &#34;game&#34; in text: <br />				prediction = &#39;0&#39;<br />			elif &#34;movie&#34; in text or &#34;film&#34; in text or &#34;actor&#34; in text:<br />				prediction = &#39;0&#39;<br />			else : prediction = &#39;1&#39;<br />		if &#34;gun&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Rule-based classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#Extra Credit Rule Based Classifier<br />def extra_credit_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO develop your conditional statements here<br />		if &#34;shooting&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Reverse Engineered classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#train and multinomial naive bayes classifier<br />def get_top_features(X, y, dv):<br />	clf = train_classifier(X, y)<br />	#the DictVectorizer object remembers which column number corresponds to which feature, and return the feature names in the correct order<br />	feature_names = dv.get_feature_names() <br /><br />	#The below code will get the weights from the classifier, and print out the weights of the features you are interested in<br />	features = [] #this will be a list of (feature_idx, weight) tuples<br />	for i,w in enumerate(clf.coef_[0]): <br />		features.append((i,w))<br />	#Sort the list by values, with the largest ones first<br />	features = sorted(features, key=lambda e: e[1], reverse=True)<br /><br />        #Print out the feature names and thier weights<br />	for i,w in features:<br />	  print &#39;%s\t%s&#39;%(feature_names[i], w)<br /><br />def get_misclassified_examples(y, X, texts) :<br />	x_train, x_test, y_train, y_test, train_texts, test_texts = train_test_split(X, y, texts)<br />	clf = train_classifier(x_train, y_train)<br /><br />	#TODO: You will have to write some code to call your classifier on each of the test examples, and check whether its prediction was right or wrong<br /><br />def create_graph(file_name) :<br />	os.system(&#34;dot -Tpng output.dot -o &#34; &#43; file_name)<br />	os.unlink(&#34;output.dot&#34;)<br /><br />if __name__ == &#39;__main__&#39; : <br /><br />	raw_data = get_data(sys.argv[1])<br />	<br />	print &#39;\nRule-based classification&#39;<br />	rule_based_classifier(raw_data)<br /><br />################ Decision Tree ################<br /><br /># print &#39;\nDecision Tree classification&#39;<br /># y, X, texts, dv, le = get_matricies(raw_data, &#34;tree&#34;)<br /># cross_validate(X,y,dv,&#34;tree&#34;)<br /><br />################ Statistical Classification ################<br />print &#39;\nStatistical classification&#39;<br />y, X, texts, dv, le = get_matricies(raw_data)<br />cross_validate(X,y)<br /><br />get_top_features(X, y, dv)<br />get_misclassified_examples(y, X, texts)<br />
</pre>
<p>Thanks in advance for any help you can give.</p>",Statistical Analysis &#34;Killed&#34; - even on biglab,"<p>When my program gets to the Statistical Analysis bit, it makes it through all the words and creates wordcount dictionaries that are appended to the list of features. Then it hangs for a bit until the terminal just prints &#34;Killed&#34; and program ends. Any idea what might be causing this? Not enough memory?</p>",Statistical Analysis &#34;Killed&#34;,,,"<p>I&#39;m confused on what exactly we&#39;re supposed to do for part 6. Are the y dimensions the same as the number of articles? Also, how is logistic regression supposed to be used here?</p>",Statistical Analysis Part 6,0
940849303,4/26/2016 17:35:56,true,1969462698,,4/26/2016 17:30:54,false,neodev,0.8889,33568303,VEN,23,Cabimas,190.77.7.36,0,0,"<p>Hi, so I read though the other piazza post about how the statistical analysis takes a long time, but I am still having some trouble with my code.</p>
<p></p>
<p>I have already tried waiting 10-15 minutes and removing all stopwords, but for some reason the statistical analysis refuses to complete in a reasonable amount of time</p>
<p></p>
<p>Below is my code:</p>
<pre>def isStopWord(word):
	with open(&#39;stopwords.txt&#39;, &#39;r&#39;) as stopwords:
		for line in stopwords:
			if word in line:
				return True
		return False

def get_features(X) : 
	features = []
	for x in X : 
		f = {}
		#TODO replace this dummy feature function with a unigram model, like we did in class
		for word in x.split():
			if isStopWord(word):
				continue
			if word in f:
				f[word] = f[word] &#43; 1.0
			else:
				f[word] =  1.0
		features.append(f)
	return features</pre>
<p>Not really sure what to do at this point because of this. Also might this be related to using the VM?</p>",Statistical Analysis Taking A Very Long Time,It takes a really long time. Is that expected?,statistical classification runtime,"<p>My script runs forever on my virtual machine, so I&#39;m running it off of biglab. I got the statistical analysis to work once (took about 2 minutes), but after un-commenting get_top_features and get_misclassified, my script gets killed before finishing the initial statistical analysis. I&#39;ve tried several times and I&#39;m unsure what the issue is; I&#39;ve copied my code below.</p>
<p></p>
<pre>#!/bin/python<br /><br />import os<br />import sys<br />import string<br />import random<br />import operator<br />from sklearn.tree import export_graphviz<br />from sklearn.tree import DecisionTreeClassifier<br />from sklearn.naive_bayes import MultinomialNB<br />from sklearn.linear_model import LogisticRegression<br />from sklearn.preprocessing import LabelEncoder<br />from sklearn.feature_extraction import DictVectorizer<br />from sklearn.cross_validation import train_test_split<br />from sklearn.externals.six import StringIO  <br /><br />#read in raw data from file and return a list of (label, article) tuples<br />def get_data(filename): <br />	data = [line.strip().split(&#39;\t&#39;) for line in open(filename).readlines()]<br />	random.shuffle(data)<br />	return data<br /><br />#this function builds the feature matrix for the Decision Tree.<br />def get_dtree_features(X) :<br />	features = []<br />	#TODO : Add the features you would like to use to train the Decision Tree here.<br />	feature_list = [&#39;gun&#39;, &#39;shooter&#39;, &#39;shot&#39;, &#39;shooting&#39;, &#39;player&#39;, &#39;coach&#39;, &#39;game&#39;, &#39;movie&#39;, &#39;actor&#39;, &#39;film&#39;, &#39;bullet&#39;, &#39;photo&#39;, &#39;model&#39;, &#39;point&#39;, &#39;suspect&#39;, &#39;victim&#39;, &#39;police&#39;, &#39;fired&#39;]<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w in feature_list : <br />				f[w] = 1.0<br />		features.append(f)<br />	return features<br /><br /><br />#this is the main function you care about; pack all the cleverest features you can think of into here.<br />def get_features(X) :<br />	stopwords = open(&#39;stopwords.txt&#39;).read()<br />	features = []<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w not in stopwords:            <br />				if w not in f : <br />					f[w] = 0<br />				f[w] &#43;= 1                <br />		features.append(f)<br />	return features<br /><br />#vectorize feature dictionaries and return feature and label matricies<br />def get_matricies(data, typ=&#34;unigram&#34;) : <br />	dv = DictVectorizer(sparse=True) <br />	le = LabelEncoder()<br />	y = [d[0] for d in data]<br />	texts = [d[1] for d in data]<br />	if typ == &#34;tree&#34;:<br />		X = get_dtree_features(texts)<br />	else :<br />		X = get_features(texts)<br />	#Here we are returning 5 things, the label vector y and feature matrix X, but also the texts from which the features were extracted and the <br />	#objects that were used to encode them. These will come in handy for your analysis, but you can ignore them for the initial parts of the assignment<br />	return le.fit_transform(y), dv.fit_transform(X), texts, dv, le<br /><br />#train and multinomial naive bayes classifier<br />def train_classifier(X, y):<br />	clf = LogisticRegression()<br />	clf.fit(X,y)<br />	return clf <br /><br />#train a Decision Tree classifier<br />def train_dtree_classifier(X, y):<br />	clf = DecisionTreeClassifier(max_depth=10)<br />	clf.fit(X,y)<br />	return clf<br /><br />#test the classifier<br />def test_classifier(clf, X, y):<br />	return clf.score(X,y)<br /><br />#cross validation	<br />def cross_validate(X, y, dv=None, typ=&#34;unigram&#34;, numfolds=5,):<br />	test_accs = []<br />	split = 1.0 / numfolds<br />	for i in range(numfolds):<br />		x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=i)<br />		if typ == &#34;tree&#34; :<br />			clf = train_dtree_classifier(x_train, y_train)<br />		else :<br />			clf = train_classifier(x_train, y_train)<br />		test_acc = test_classifier(clf, x_test, y_test)<br />		test_accs.append(test_acc)<br />		print &#39;Fold %d : %.05f&#39;%(i,test_acc)<br />	test_average = float(sum(test_accs))/ numfolds<br />	if typ == &#34;tree&#34; :<br />		with open(&#34;output.dot&#34;, &#39;w&#39;) as f:<br />			f = export_graphviz(clf, out_file=f, feature_names=dv.get_feature_names(), class_names=[&#39;Non Gun Related&#39;,&#39;Gun Related&#39;])<br />		create_graph(&#34;decision-tree.png&#34;)<br />	print &#39;Test Average : %.05f&#39;%(test_average)<br />	print<br />	return test_average<br /><br />#run a rule based classifier and calculate the accuracy<br />def rule_based_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO add more keywords, see how well they do alone and in combination<br />		if &#34;shooter&#34; in text or &#34;shot&#34; in text or &#34;shooting&#34; in text :<br />			if &#34;player&#34; in text or &#34;coach&#34; in text or &#34;game&#34; in text: <br />				prediction = &#39;0&#39;<br />			elif &#34;movie&#34; in text or &#34;film&#34; in text or &#34;actor&#34; in text:<br />				prediction = &#39;0&#39;<br />			else : prediction = &#39;1&#39;<br />		if &#34;gun&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Rule-based classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#Extra Credit Rule Based Classifier<br />def extra_credit_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO develop your conditional statements here<br />		if &#34;shooting&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Reverse Engineered classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#train and multinomial naive bayes classifier<br />def get_top_features(X, y, dv):<br />	clf = train_classifier(X, y)<br />	#the DictVectorizer object remembers which column number corresponds to which feature, and return the feature names in the correct order<br />	feature_names = dv.get_feature_names() <br /><br />	#The below code will get the weights from the classifier, and print out the weights of the features you are interested in<br />	features = [] #this will be a list of (feature_idx, weight) tuples<br />	for i,w in enumerate(clf.coef_[0]): <br />		features.append((i,w))<br />	#Sort the list by values, with the largest ones first<br />	features = sorted(features, key=lambda e: e[1], reverse=True)<br /><br />        #Print out the feature names and thier weights<br />	for i,w in features:<br />	  print &#39;%s\t%s&#39;%(feature_names[i], w)<br /><br />def get_misclassified_examples(y, X, texts) :<br />	x_train, x_test, y_train, y_test, train_texts, test_texts = train_test_split(X, y, texts)<br />	clf = train_classifier(x_train, y_train)<br /><br />	#TODO: You will have to write some code to call your classifier on each of the test examples, and check whether its prediction was right or wrong<br /><br />def create_graph(file_name) :<br />	os.system(&#34;dot -Tpng output.dot -o &#34; &#43; file_name)<br />	os.unlink(&#34;output.dot&#34;)<br /><br />if __name__ == &#39;__main__&#39; : <br /><br />	raw_data = get_data(sys.argv[1])<br />	<br />	print &#39;\nRule-based classification&#39;<br />	rule_based_classifier(raw_data)<br /><br />################ Decision Tree ################<br /><br /># print &#39;\nDecision Tree classification&#39;<br /># y, X, texts, dv, le = get_matricies(raw_data, &#34;tree&#34;)<br /># cross_validate(X,y,dv,&#34;tree&#34;)<br /><br />################ Statistical Classification ################<br />print &#39;\nStatistical classification&#39;<br />y, X, texts, dv, le = get_matricies(raw_data)<br />cross_validate(X,y)<br /><br />get_top_features(X, y, dv)<br />get_misclassified_examples(y, X, texts)<br />
</pre>
<p>Thanks in advance for any help you can give.</p>",Statistical Analysis &#34;Killed&#34; - even on biglab,"<p>When my program gets to the Statistical Analysis bit, it makes it through all the words and creates wordcount dictionaries that are appended to the list of features. Then it hangs for a bit until the terminal just prints &#34;Killed&#34; and program ends. Any idea what might be causing this? Not enough memory?</p>",Statistical Analysis &#34;Killed&#34;,,,"<p>I&#39;m confused on what exactly we&#39;re supposed to do for part 6. Are the y dimensions the same as the number of articles? Also, how is logistic regression supposed to be used here?</p>",Statistical Analysis Part 6,0
940849303,4/26/2016 17:39:39,true,1969464646,,4/26/2016 17:35:23,false,clixsense,1.0,35444326,BRA,07,Brasília,177.15.130.106,0,0,"<p>Hi, so I read though the other piazza post about how the statistical analysis takes a long time, but I am still having some trouble with my code.</p>
<p></p>
<p>I have already tried waiting 10-15 minutes and removing all stopwords, but for some reason the statistical analysis refuses to complete in a reasonable amount of time</p>
<p></p>
<p>Below is my code:</p>
<pre>def isStopWord(word):
	with open(&#39;stopwords.txt&#39;, &#39;r&#39;) as stopwords:
		for line in stopwords:
			if word in line:
				return True
		return False

def get_features(X) : 
	features = []
	for x in X : 
		f = {}
		#TODO replace this dummy feature function with a unigram model, like we did in class
		for word in x.split():
			if isStopWord(word):
				continue
			if word in f:
				f[word] = f[word] &#43; 1.0
			else:
				f[word] =  1.0
		features.append(f)
	return features</pre>
<p>Not really sure what to do at this point because of this. Also might this be related to using the VM?</p>",Statistical Analysis Taking A Very Long Time,It takes a really long time. Is that expected?,statistical classification runtime,"<p>My script runs forever on my virtual machine, so I&#39;m running it off of biglab. I got the statistical analysis to work once (took about 2 minutes), but after un-commenting get_top_features and get_misclassified, my script gets killed before finishing the initial statistical analysis. I&#39;ve tried several times and I&#39;m unsure what the issue is; I&#39;ve copied my code below.</p>
<p></p>
<pre>#!/bin/python<br /><br />import os<br />import sys<br />import string<br />import random<br />import operator<br />from sklearn.tree import export_graphviz<br />from sklearn.tree import DecisionTreeClassifier<br />from sklearn.naive_bayes import MultinomialNB<br />from sklearn.linear_model import LogisticRegression<br />from sklearn.preprocessing import LabelEncoder<br />from sklearn.feature_extraction import DictVectorizer<br />from sklearn.cross_validation import train_test_split<br />from sklearn.externals.six import StringIO  <br /><br />#read in raw data from file and return a list of (label, article) tuples<br />def get_data(filename): <br />	data = [line.strip().split(&#39;\t&#39;) for line in open(filename).readlines()]<br />	random.shuffle(data)<br />	return data<br /><br />#this function builds the feature matrix for the Decision Tree.<br />def get_dtree_features(X) :<br />	features = []<br />	#TODO : Add the features you would like to use to train the Decision Tree here.<br />	feature_list = [&#39;gun&#39;, &#39;shooter&#39;, &#39;shot&#39;, &#39;shooting&#39;, &#39;player&#39;, &#39;coach&#39;, &#39;game&#39;, &#39;movie&#39;, &#39;actor&#39;, &#39;film&#39;, &#39;bullet&#39;, &#39;photo&#39;, &#39;model&#39;, &#39;point&#39;, &#39;suspect&#39;, &#39;victim&#39;, &#39;police&#39;, &#39;fired&#39;]<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w in feature_list : <br />				f[w] = 1.0<br />		features.append(f)<br />	return features<br /><br /><br />#this is the main function you care about; pack all the cleverest features you can think of into here.<br />def get_features(X) :<br />	stopwords = open(&#39;stopwords.txt&#39;).read()<br />	features = []<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w not in stopwords:            <br />				if w not in f : <br />					f[w] = 0<br />				f[w] &#43;= 1                <br />		features.append(f)<br />	return features<br /><br />#vectorize feature dictionaries and return feature and label matricies<br />def get_matricies(data, typ=&#34;unigram&#34;) : <br />	dv = DictVectorizer(sparse=True) <br />	le = LabelEncoder()<br />	y = [d[0] for d in data]<br />	texts = [d[1] for d in data]<br />	if typ == &#34;tree&#34;:<br />		X = get_dtree_features(texts)<br />	else :<br />		X = get_features(texts)<br />	#Here we are returning 5 things, the label vector y and feature matrix X, but also the texts from which the features were extracted and the <br />	#objects that were used to encode them. These will come in handy for your analysis, but you can ignore them for the initial parts of the assignment<br />	return le.fit_transform(y), dv.fit_transform(X), texts, dv, le<br /><br />#train and multinomial naive bayes classifier<br />def train_classifier(X, y):<br />	clf = LogisticRegression()<br />	clf.fit(X,y)<br />	return clf <br /><br />#train a Decision Tree classifier<br />def train_dtree_classifier(X, y):<br />	clf = DecisionTreeClassifier(max_depth=10)<br />	clf.fit(X,y)<br />	return clf<br /><br />#test the classifier<br />def test_classifier(clf, X, y):<br />	return clf.score(X,y)<br /><br />#cross validation	<br />def cross_validate(X, y, dv=None, typ=&#34;unigram&#34;, numfolds=5,):<br />	test_accs = []<br />	split = 1.0 / numfolds<br />	for i in range(numfolds):<br />		x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=i)<br />		if typ == &#34;tree&#34; :<br />			clf = train_dtree_classifier(x_train, y_train)<br />		else :<br />			clf = train_classifier(x_train, y_train)<br />		test_acc = test_classifier(clf, x_test, y_test)<br />		test_accs.append(test_acc)<br />		print &#39;Fold %d : %.05f&#39;%(i,test_acc)<br />	test_average = float(sum(test_accs))/ numfolds<br />	if typ == &#34;tree&#34; :<br />		with open(&#34;output.dot&#34;, &#39;w&#39;) as f:<br />			f = export_graphviz(clf, out_file=f, feature_names=dv.get_feature_names(), class_names=[&#39;Non Gun Related&#39;,&#39;Gun Related&#39;])<br />		create_graph(&#34;decision-tree.png&#34;)<br />	print &#39;Test Average : %.05f&#39;%(test_average)<br />	print<br />	return test_average<br /><br />#run a rule based classifier and calculate the accuracy<br />def rule_based_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO add more keywords, see how well they do alone and in combination<br />		if &#34;shooter&#34; in text or &#34;shot&#34; in text or &#34;shooting&#34; in text :<br />			if &#34;player&#34; in text or &#34;coach&#34; in text or &#34;game&#34; in text: <br />				prediction = &#39;0&#39;<br />			elif &#34;movie&#34; in text or &#34;film&#34; in text or &#34;actor&#34; in text:<br />				prediction = &#39;0&#39;<br />			else : prediction = &#39;1&#39;<br />		if &#34;gun&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Rule-based classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#Extra Credit Rule Based Classifier<br />def extra_credit_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO develop your conditional statements here<br />		if &#34;shooting&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Reverse Engineered classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#train and multinomial naive bayes classifier<br />def get_top_features(X, y, dv):<br />	clf = train_classifier(X, y)<br />	#the DictVectorizer object remembers which column number corresponds to which feature, and return the feature names in the correct order<br />	feature_names = dv.get_feature_names() <br /><br />	#The below code will get the weights from the classifier, and print out the weights of the features you are interested in<br />	features = [] #this will be a list of (feature_idx, weight) tuples<br />	for i,w in enumerate(clf.coef_[0]): <br />		features.append((i,w))<br />	#Sort the list by values, with the largest ones first<br />	features = sorted(features, key=lambda e: e[1], reverse=True)<br /><br />        #Print out the feature names and thier weights<br />	for i,w in features:<br />	  print &#39;%s\t%s&#39;%(feature_names[i], w)<br /><br />def get_misclassified_examples(y, X, texts) :<br />	x_train, x_test, y_train, y_test, train_texts, test_texts = train_test_split(X, y, texts)<br />	clf = train_classifier(x_train, y_train)<br /><br />	#TODO: You will have to write some code to call your classifier on each of the test examples, and check whether its prediction was right or wrong<br /><br />def create_graph(file_name) :<br />	os.system(&#34;dot -Tpng output.dot -o &#34; &#43; file_name)<br />	os.unlink(&#34;output.dot&#34;)<br /><br />if __name__ == &#39;__main__&#39; : <br /><br />	raw_data = get_data(sys.argv[1])<br />	<br />	print &#39;\nRule-based classification&#39;<br />	rule_based_classifier(raw_data)<br /><br />################ Decision Tree ################<br /><br /># print &#39;\nDecision Tree classification&#39;<br /># y, X, texts, dv, le = get_matricies(raw_data, &#34;tree&#34;)<br /># cross_validate(X,y,dv,&#34;tree&#34;)<br /><br />################ Statistical Classification ################<br />print &#39;\nStatistical classification&#39;<br />y, X, texts, dv, le = get_matricies(raw_data)<br />cross_validate(X,y)<br /><br />get_top_features(X, y, dv)<br />get_misclassified_examples(y, X, texts)<br />
</pre>
<p>Thanks in advance for any help you can give.</p>",Statistical Analysis &#34;Killed&#34; - even on biglab,"<p>When my program gets to the Statistical Analysis bit, it makes it through all the words and creates wordcount dictionaries that are appended to the list of features. Then it hangs for a bit until the terminal just prints &#34;Killed&#34; and program ends. Any idea what might be causing this? Not enough memory?</p>",Statistical Analysis &#34;Killed&#34;,,,"<p>I&#39;m confused on what exactly we&#39;re supposed to do for part 6. Are the y dimensions the same as the number of articles? Also, how is logistic regression supposed to be used here?</p>",Statistical Analysis Part 6,0
940849303,4/26/2016 18:04:05,true,1969477777,,4/26/2016 17:46:51,false,clixsense,0.8889,35338593,ITA,14,Cagliari,151.56.132.145,0,0,"<p>Hi, so I read though the other piazza post about how the statistical analysis takes a long time, but I am still having some trouble with my code.</p>
<p></p>
<p>I have already tried waiting 10-15 minutes and removing all stopwords, but for some reason the statistical analysis refuses to complete in a reasonable amount of time</p>
<p></p>
<p>Below is my code:</p>
<pre>def isStopWord(word):
	with open(&#39;stopwords.txt&#39;, &#39;r&#39;) as stopwords:
		for line in stopwords:
			if word in line:
				return True
		return False

def get_features(X) : 
	features = []
	for x in X : 
		f = {}
		#TODO replace this dummy feature function with a unigram model, like we did in class
		for word in x.split():
			if isStopWord(word):
				continue
			if word in f:
				f[word] = f[word] &#43; 1.0
			else:
				f[word] =  1.0
		features.append(f)
	return features</pre>
<p>Not really sure what to do at this point because of this. Also might this be related to using the VM?</p>",Statistical Analysis Taking A Very Long Time,It takes a really long time. Is that expected?,statistical classification runtime,"<p>My script runs forever on my virtual machine, so I&#39;m running it off of biglab. I got the statistical analysis to work once (took about 2 minutes), but after un-commenting get_top_features and get_misclassified, my script gets killed before finishing the initial statistical analysis. I&#39;ve tried several times and I&#39;m unsure what the issue is; I&#39;ve copied my code below.</p>
<p></p>
<pre>#!/bin/python<br /><br />import os<br />import sys<br />import string<br />import random<br />import operator<br />from sklearn.tree import export_graphviz<br />from sklearn.tree import DecisionTreeClassifier<br />from sklearn.naive_bayes import MultinomialNB<br />from sklearn.linear_model import LogisticRegression<br />from sklearn.preprocessing import LabelEncoder<br />from sklearn.feature_extraction import DictVectorizer<br />from sklearn.cross_validation import train_test_split<br />from sklearn.externals.six import StringIO  <br /><br />#read in raw data from file and return a list of (label, article) tuples<br />def get_data(filename): <br />	data = [line.strip().split(&#39;\t&#39;) for line in open(filename).readlines()]<br />	random.shuffle(data)<br />	return data<br /><br />#this function builds the feature matrix for the Decision Tree.<br />def get_dtree_features(X) :<br />	features = []<br />	#TODO : Add the features you would like to use to train the Decision Tree here.<br />	feature_list = [&#39;gun&#39;, &#39;shooter&#39;, &#39;shot&#39;, &#39;shooting&#39;, &#39;player&#39;, &#39;coach&#39;, &#39;game&#39;, &#39;movie&#39;, &#39;actor&#39;, &#39;film&#39;, &#39;bullet&#39;, &#39;photo&#39;, &#39;model&#39;, &#39;point&#39;, &#39;suspect&#39;, &#39;victim&#39;, &#39;police&#39;, &#39;fired&#39;]<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w in feature_list : <br />				f[w] = 1.0<br />		features.append(f)<br />	return features<br /><br /><br />#this is the main function you care about; pack all the cleverest features you can think of into here.<br />def get_features(X) :<br />	stopwords = open(&#39;stopwords.txt&#39;).read()<br />	features = []<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w not in stopwords:            <br />				if w not in f : <br />					f[w] = 0<br />				f[w] &#43;= 1                <br />		features.append(f)<br />	return features<br /><br />#vectorize feature dictionaries and return feature and label matricies<br />def get_matricies(data, typ=&#34;unigram&#34;) : <br />	dv = DictVectorizer(sparse=True) <br />	le = LabelEncoder()<br />	y = [d[0] for d in data]<br />	texts = [d[1] for d in data]<br />	if typ == &#34;tree&#34;:<br />		X = get_dtree_features(texts)<br />	else :<br />		X = get_features(texts)<br />	#Here we are returning 5 things, the label vector y and feature matrix X, but also the texts from which the features were extracted and the <br />	#objects that were used to encode them. These will come in handy for your analysis, but you can ignore them for the initial parts of the assignment<br />	return le.fit_transform(y), dv.fit_transform(X), texts, dv, le<br /><br />#train and multinomial naive bayes classifier<br />def train_classifier(X, y):<br />	clf = LogisticRegression()<br />	clf.fit(X,y)<br />	return clf <br /><br />#train a Decision Tree classifier<br />def train_dtree_classifier(X, y):<br />	clf = DecisionTreeClassifier(max_depth=10)<br />	clf.fit(X,y)<br />	return clf<br /><br />#test the classifier<br />def test_classifier(clf, X, y):<br />	return clf.score(X,y)<br /><br />#cross validation	<br />def cross_validate(X, y, dv=None, typ=&#34;unigram&#34;, numfolds=5,):<br />	test_accs = []<br />	split = 1.0 / numfolds<br />	for i in range(numfolds):<br />		x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=i)<br />		if typ == &#34;tree&#34; :<br />			clf = train_dtree_classifier(x_train, y_train)<br />		else :<br />			clf = train_classifier(x_train, y_train)<br />		test_acc = test_classifier(clf, x_test, y_test)<br />		test_accs.append(test_acc)<br />		print &#39;Fold %d : %.05f&#39;%(i,test_acc)<br />	test_average = float(sum(test_accs))/ numfolds<br />	if typ == &#34;tree&#34; :<br />		with open(&#34;output.dot&#34;, &#39;w&#39;) as f:<br />			f = export_graphviz(clf, out_file=f, feature_names=dv.get_feature_names(), class_names=[&#39;Non Gun Related&#39;,&#39;Gun Related&#39;])<br />		create_graph(&#34;decision-tree.png&#34;)<br />	print &#39;Test Average : %.05f&#39;%(test_average)<br />	print<br />	return test_average<br /><br />#run a rule based classifier and calculate the accuracy<br />def rule_based_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO add more keywords, see how well they do alone and in combination<br />		if &#34;shooter&#34; in text or &#34;shot&#34; in text or &#34;shooting&#34; in text :<br />			if &#34;player&#34; in text or &#34;coach&#34; in text or &#34;game&#34; in text: <br />				prediction = &#39;0&#39;<br />			elif &#34;movie&#34; in text or &#34;film&#34; in text or &#34;actor&#34; in text:<br />				prediction = &#39;0&#39;<br />			else : prediction = &#39;1&#39;<br />		if &#34;gun&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Rule-based classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#Extra Credit Rule Based Classifier<br />def extra_credit_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO develop your conditional statements here<br />		if &#34;shooting&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Reverse Engineered classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#train and multinomial naive bayes classifier<br />def get_top_features(X, y, dv):<br />	clf = train_classifier(X, y)<br />	#the DictVectorizer object remembers which column number corresponds to which feature, and return the feature names in the correct order<br />	feature_names = dv.get_feature_names() <br /><br />	#The below code will get the weights from the classifier, and print out the weights of the features you are interested in<br />	features = [] #this will be a list of (feature_idx, weight) tuples<br />	for i,w in enumerate(clf.coef_[0]): <br />		features.append((i,w))<br />	#Sort the list by values, with the largest ones first<br />	features = sorted(features, key=lambda e: e[1], reverse=True)<br /><br />        #Print out the feature names and thier weights<br />	for i,w in features:<br />	  print &#39;%s\t%s&#39;%(feature_names[i], w)<br /><br />def get_misclassified_examples(y, X, texts) :<br />	x_train, x_test, y_train, y_test, train_texts, test_texts = train_test_split(X, y, texts)<br />	clf = train_classifier(x_train, y_train)<br /><br />	#TODO: You will have to write some code to call your classifier on each of the test examples, and check whether its prediction was right or wrong<br /><br />def create_graph(file_name) :<br />	os.system(&#34;dot -Tpng output.dot -o &#34; &#43; file_name)<br />	os.unlink(&#34;output.dot&#34;)<br /><br />if __name__ == &#39;__main__&#39; : <br /><br />	raw_data = get_data(sys.argv[1])<br />	<br />	print &#39;\nRule-based classification&#39;<br />	rule_based_classifier(raw_data)<br /><br />################ Decision Tree ################<br /><br /># print &#39;\nDecision Tree classification&#39;<br /># y, X, texts, dv, le = get_matricies(raw_data, &#34;tree&#34;)<br /># cross_validate(X,y,dv,&#34;tree&#34;)<br /><br />################ Statistical Classification ################<br />print &#39;\nStatistical classification&#39;<br />y, X, texts, dv, le = get_matricies(raw_data)<br />cross_validate(X,y)<br /><br />get_top_features(X, y, dv)<br />get_misclassified_examples(y, X, texts)<br />
</pre>
<p>Thanks in advance for any help you can give.</p>",Statistical Analysis &#34;Killed&#34; - even on biglab,"<p>When my program gets to the Statistical Analysis bit, it makes it through all the words and creates wordcount dictionaries that are appended to the list of features. Then it hangs for a bit until the terminal just prints &#34;Killed&#34; and program ends. Any idea what might be causing this? Not enough memory?</p>",Statistical Analysis &#34;Killed&#34;,,,"<p>I&#39;m confused on what exactly we&#39;re supposed to do for part 6. Are the y dimensions the same as the number of articles? Also, how is logistic regression supposed to be used here?</p>",Statistical Analysis Part 6,0
940849303,4/26/2016 18:16:16,true,1969484746,,4/26/2016 18:07:59,false,neodev,0.8889,35550011,VEN,07,Valencia,190.204.238.112,0,0,"<p>Hi, so I read though the other piazza post about how the statistical analysis takes a long time, but I am still having some trouble with my code.</p>
<p></p>
<p>I have already tried waiting 10-15 minutes and removing all stopwords, but for some reason the statistical analysis refuses to complete in a reasonable amount of time</p>
<p></p>
<p>Below is my code:</p>
<pre>def isStopWord(word):
	with open(&#39;stopwords.txt&#39;, &#39;r&#39;) as stopwords:
		for line in stopwords:
			if word in line:
				return True
		return False

def get_features(X) : 
	features = []
	for x in X : 
		f = {}
		#TODO replace this dummy feature function with a unigram model, like we did in class
		for word in x.split():
			if isStopWord(word):
				continue
			if word in f:
				f[word] = f[word] &#43; 1.0
			else:
				f[word] =  1.0
		features.append(f)
	return features</pre>
<p>Not really sure what to do at this point because of this. Also might this be related to using the VM?</p>",Statistical Analysis Taking A Very Long Time,It takes a really long time. Is that expected?,statistical classification runtime,"<p>My script runs forever on my virtual machine, so I&#39;m running it off of biglab. I got the statistical analysis to work once (took about 2 minutes), but after un-commenting get_top_features and get_misclassified, my script gets killed before finishing the initial statistical analysis. I&#39;ve tried several times and I&#39;m unsure what the issue is; I&#39;ve copied my code below.</p>
<p></p>
<pre>#!/bin/python<br /><br />import os<br />import sys<br />import string<br />import random<br />import operator<br />from sklearn.tree import export_graphviz<br />from sklearn.tree import DecisionTreeClassifier<br />from sklearn.naive_bayes import MultinomialNB<br />from sklearn.linear_model import LogisticRegression<br />from sklearn.preprocessing import LabelEncoder<br />from sklearn.feature_extraction import DictVectorizer<br />from sklearn.cross_validation import train_test_split<br />from sklearn.externals.six import StringIO  <br /><br />#read in raw data from file and return a list of (label, article) tuples<br />def get_data(filename): <br />	data = [line.strip().split(&#39;\t&#39;) for line in open(filename).readlines()]<br />	random.shuffle(data)<br />	return data<br /><br />#this function builds the feature matrix for the Decision Tree.<br />def get_dtree_features(X) :<br />	features = []<br />	#TODO : Add the features you would like to use to train the Decision Tree here.<br />	feature_list = [&#39;gun&#39;, &#39;shooter&#39;, &#39;shot&#39;, &#39;shooting&#39;, &#39;player&#39;, &#39;coach&#39;, &#39;game&#39;, &#39;movie&#39;, &#39;actor&#39;, &#39;film&#39;, &#39;bullet&#39;, &#39;photo&#39;, &#39;model&#39;, &#39;point&#39;, &#39;suspect&#39;, &#39;victim&#39;, &#39;police&#39;, &#39;fired&#39;]<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w in feature_list : <br />				f[w] = 1.0<br />		features.append(f)<br />	return features<br /><br /><br />#this is the main function you care about; pack all the cleverest features you can think of into here.<br />def get_features(X) :<br />	stopwords = open(&#39;stopwords.txt&#39;).read()<br />	features = []<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w not in stopwords:            <br />				if w not in f : <br />					f[w] = 0<br />				f[w] &#43;= 1                <br />		features.append(f)<br />	return features<br /><br />#vectorize feature dictionaries and return feature and label matricies<br />def get_matricies(data, typ=&#34;unigram&#34;) : <br />	dv = DictVectorizer(sparse=True) <br />	le = LabelEncoder()<br />	y = [d[0] for d in data]<br />	texts = [d[1] for d in data]<br />	if typ == &#34;tree&#34;:<br />		X = get_dtree_features(texts)<br />	else :<br />		X = get_features(texts)<br />	#Here we are returning 5 things, the label vector y and feature matrix X, but also the texts from which the features were extracted and the <br />	#objects that were used to encode them. These will come in handy for your analysis, but you can ignore them for the initial parts of the assignment<br />	return le.fit_transform(y), dv.fit_transform(X), texts, dv, le<br /><br />#train and multinomial naive bayes classifier<br />def train_classifier(X, y):<br />	clf = LogisticRegression()<br />	clf.fit(X,y)<br />	return clf <br /><br />#train a Decision Tree classifier<br />def train_dtree_classifier(X, y):<br />	clf = DecisionTreeClassifier(max_depth=10)<br />	clf.fit(X,y)<br />	return clf<br /><br />#test the classifier<br />def test_classifier(clf, X, y):<br />	return clf.score(X,y)<br /><br />#cross validation	<br />def cross_validate(X, y, dv=None, typ=&#34;unigram&#34;, numfolds=5,):<br />	test_accs = []<br />	split = 1.0 / numfolds<br />	for i in range(numfolds):<br />		x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=i)<br />		if typ == &#34;tree&#34; :<br />			clf = train_dtree_classifier(x_train, y_train)<br />		else :<br />			clf = train_classifier(x_train, y_train)<br />		test_acc = test_classifier(clf, x_test, y_test)<br />		test_accs.append(test_acc)<br />		print &#39;Fold %d : %.05f&#39;%(i,test_acc)<br />	test_average = float(sum(test_accs))/ numfolds<br />	if typ == &#34;tree&#34; :<br />		with open(&#34;output.dot&#34;, &#39;w&#39;) as f:<br />			f = export_graphviz(clf, out_file=f, feature_names=dv.get_feature_names(), class_names=[&#39;Non Gun Related&#39;,&#39;Gun Related&#39;])<br />		create_graph(&#34;decision-tree.png&#34;)<br />	print &#39;Test Average : %.05f&#39;%(test_average)<br />	print<br />	return test_average<br /><br />#run a rule based classifier and calculate the accuracy<br />def rule_based_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO add more keywords, see how well they do alone and in combination<br />		if &#34;shooter&#34; in text or &#34;shot&#34; in text or &#34;shooting&#34; in text :<br />			if &#34;player&#34; in text or &#34;coach&#34; in text or &#34;game&#34; in text: <br />				prediction = &#39;0&#39;<br />			elif &#34;movie&#34; in text or &#34;film&#34; in text or &#34;actor&#34; in text:<br />				prediction = &#39;0&#39;<br />			else : prediction = &#39;1&#39;<br />		if &#34;gun&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Rule-based classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#Extra Credit Rule Based Classifier<br />def extra_credit_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO develop your conditional statements here<br />		if &#34;shooting&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Reverse Engineered classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#train and multinomial naive bayes classifier<br />def get_top_features(X, y, dv):<br />	clf = train_classifier(X, y)<br />	#the DictVectorizer object remembers which column number corresponds to which feature, and return the feature names in the correct order<br />	feature_names = dv.get_feature_names() <br /><br />	#The below code will get the weights from the classifier, and print out the weights of the features you are interested in<br />	features = [] #this will be a list of (feature_idx, weight) tuples<br />	for i,w in enumerate(clf.coef_[0]): <br />		features.append((i,w))<br />	#Sort the list by values, with the largest ones first<br />	features = sorted(features, key=lambda e: e[1], reverse=True)<br /><br />        #Print out the feature names and thier weights<br />	for i,w in features:<br />	  print &#39;%s\t%s&#39;%(feature_names[i], w)<br /><br />def get_misclassified_examples(y, X, texts) :<br />	x_train, x_test, y_train, y_test, train_texts, test_texts = train_test_split(X, y, texts)<br />	clf = train_classifier(x_train, y_train)<br /><br />	#TODO: You will have to write some code to call your classifier on each of the test examples, and check whether its prediction was right or wrong<br /><br />def create_graph(file_name) :<br />	os.system(&#34;dot -Tpng output.dot -o &#34; &#43; file_name)<br />	os.unlink(&#34;output.dot&#34;)<br /><br />if __name__ == &#39;__main__&#39; : <br /><br />	raw_data = get_data(sys.argv[1])<br />	<br />	print &#39;\nRule-based classification&#39;<br />	rule_based_classifier(raw_data)<br /><br />################ Decision Tree ################<br /><br /># print &#39;\nDecision Tree classification&#39;<br /># y, X, texts, dv, le = get_matricies(raw_data, &#34;tree&#34;)<br /># cross_validate(X,y,dv,&#34;tree&#34;)<br /><br />################ Statistical Classification ################<br />print &#39;\nStatistical classification&#39;<br />y, X, texts, dv, le = get_matricies(raw_data)<br />cross_validate(X,y)<br /><br />get_top_features(X, y, dv)<br />get_misclassified_examples(y, X, texts)<br />
</pre>
<p>Thanks in advance for any help you can give.</p>",Statistical Analysis &#34;Killed&#34; - even on biglab,"<p>When my program gets to the Statistical Analysis bit, it makes it through all the words and creates wordcount dictionaries that are appended to the list of features. Then it hangs for a bit until the terminal just prints &#34;Killed&#34; and program ends. Any idea what might be causing this? Not enough memory?</p>",Statistical Analysis &#34;Killed&#34;,,,"<p>I&#39;m confused on what exactly we&#39;re supposed to do for part 6. Are the y dimensions the same as the number of articles? Also, how is logistic regression supposed to be used here?</p>",Statistical Analysis Part 6,0
940849303,4/26/2016 18:33:38,true,1969493278,,4/26/2016 18:19:15,false,neodev,1.0,29879245,RUS,69,Smolensk,37.144.124.118,0,0,"<p>Hi, so I read though the other piazza post about how the statistical analysis takes a long time, but I am still having some trouble with my code.</p>
<p></p>
<p>I have already tried waiting 10-15 minutes and removing all stopwords, but for some reason the statistical analysis refuses to complete in a reasonable amount of time</p>
<p></p>
<p>Below is my code:</p>
<pre>def isStopWord(word):
	with open(&#39;stopwords.txt&#39;, &#39;r&#39;) as stopwords:
		for line in stopwords:
			if word in line:
				return True
		return False

def get_features(X) : 
	features = []
	for x in X : 
		f = {}
		#TODO replace this dummy feature function with a unigram model, like we did in class
		for word in x.split():
			if isStopWord(word):
				continue
			if word in f:
				f[word] = f[word] &#43; 1.0
			else:
				f[word] =  1.0
		features.append(f)
	return features</pre>
<p>Not really sure what to do at this point because of this. Also might this be related to using the VM?</p>",Statistical Analysis Taking A Very Long Time,It takes a really long time. Is that expected?,statistical classification runtime,"<p>My script runs forever on my virtual machine, so I&#39;m running it off of biglab. I got the statistical analysis to work once (took about 2 minutes), but after un-commenting get_top_features and get_misclassified, my script gets killed before finishing the initial statistical analysis. I&#39;ve tried several times and I&#39;m unsure what the issue is; I&#39;ve copied my code below.</p>
<p></p>
<pre>#!/bin/python<br /><br />import os<br />import sys<br />import string<br />import random<br />import operator<br />from sklearn.tree import export_graphviz<br />from sklearn.tree import DecisionTreeClassifier<br />from sklearn.naive_bayes import MultinomialNB<br />from sklearn.linear_model import LogisticRegression<br />from sklearn.preprocessing import LabelEncoder<br />from sklearn.feature_extraction import DictVectorizer<br />from sklearn.cross_validation import train_test_split<br />from sklearn.externals.six import StringIO  <br /><br />#read in raw data from file and return a list of (label, article) tuples<br />def get_data(filename): <br />	data = [line.strip().split(&#39;\t&#39;) for line in open(filename).readlines()]<br />	random.shuffle(data)<br />	return data<br /><br />#this function builds the feature matrix for the Decision Tree.<br />def get_dtree_features(X) :<br />	features = []<br />	#TODO : Add the features you would like to use to train the Decision Tree here.<br />	feature_list = [&#39;gun&#39;, &#39;shooter&#39;, &#39;shot&#39;, &#39;shooting&#39;, &#39;player&#39;, &#39;coach&#39;, &#39;game&#39;, &#39;movie&#39;, &#39;actor&#39;, &#39;film&#39;, &#39;bullet&#39;, &#39;photo&#39;, &#39;model&#39;, &#39;point&#39;, &#39;suspect&#39;, &#39;victim&#39;, &#39;police&#39;, &#39;fired&#39;]<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w in feature_list : <br />				f[w] = 1.0<br />		features.append(f)<br />	return features<br /><br /><br />#this is the main function you care about; pack all the cleverest features you can think of into here.<br />def get_features(X) :<br />	stopwords = open(&#39;stopwords.txt&#39;).read()<br />	features = []<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w not in stopwords:            <br />				if w not in f : <br />					f[w] = 0<br />				f[w] &#43;= 1                <br />		features.append(f)<br />	return features<br /><br />#vectorize feature dictionaries and return feature and label matricies<br />def get_matricies(data, typ=&#34;unigram&#34;) : <br />	dv = DictVectorizer(sparse=True) <br />	le = LabelEncoder()<br />	y = [d[0] for d in data]<br />	texts = [d[1] for d in data]<br />	if typ == &#34;tree&#34;:<br />		X = get_dtree_features(texts)<br />	else :<br />		X = get_features(texts)<br />	#Here we are returning 5 things, the label vector y and feature matrix X, but also the texts from which the features were extracted and the <br />	#objects that were used to encode them. These will come in handy for your analysis, but you can ignore them for the initial parts of the assignment<br />	return le.fit_transform(y), dv.fit_transform(X), texts, dv, le<br /><br />#train and multinomial naive bayes classifier<br />def train_classifier(X, y):<br />	clf = LogisticRegression()<br />	clf.fit(X,y)<br />	return clf <br /><br />#train a Decision Tree classifier<br />def train_dtree_classifier(X, y):<br />	clf = DecisionTreeClassifier(max_depth=10)<br />	clf.fit(X,y)<br />	return clf<br /><br />#test the classifier<br />def test_classifier(clf, X, y):<br />	return clf.score(X,y)<br /><br />#cross validation	<br />def cross_validate(X, y, dv=None, typ=&#34;unigram&#34;, numfolds=5,):<br />	test_accs = []<br />	split = 1.0 / numfolds<br />	for i in range(numfolds):<br />		x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=i)<br />		if typ == &#34;tree&#34; :<br />			clf = train_dtree_classifier(x_train, y_train)<br />		else :<br />			clf = train_classifier(x_train, y_train)<br />		test_acc = test_classifier(clf, x_test, y_test)<br />		test_accs.append(test_acc)<br />		print &#39;Fold %d : %.05f&#39;%(i,test_acc)<br />	test_average = float(sum(test_accs))/ numfolds<br />	if typ == &#34;tree&#34; :<br />		with open(&#34;output.dot&#34;, &#39;w&#39;) as f:<br />			f = export_graphviz(clf, out_file=f, feature_names=dv.get_feature_names(), class_names=[&#39;Non Gun Related&#39;,&#39;Gun Related&#39;])<br />		create_graph(&#34;decision-tree.png&#34;)<br />	print &#39;Test Average : %.05f&#39;%(test_average)<br />	print<br />	return test_average<br /><br />#run a rule based classifier and calculate the accuracy<br />def rule_based_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO add more keywords, see how well they do alone and in combination<br />		if &#34;shooter&#34; in text or &#34;shot&#34; in text or &#34;shooting&#34; in text :<br />			if &#34;player&#34; in text or &#34;coach&#34; in text or &#34;game&#34; in text: <br />				prediction = &#39;0&#39;<br />			elif &#34;movie&#34; in text or &#34;film&#34; in text or &#34;actor&#34; in text:<br />				prediction = &#39;0&#39;<br />			else : prediction = &#39;1&#39;<br />		if &#34;gun&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Rule-based classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#Extra Credit Rule Based Classifier<br />def extra_credit_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO develop your conditional statements here<br />		if &#34;shooting&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Reverse Engineered classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#train and multinomial naive bayes classifier<br />def get_top_features(X, y, dv):<br />	clf = train_classifier(X, y)<br />	#the DictVectorizer object remembers which column number corresponds to which feature, and return the feature names in the correct order<br />	feature_names = dv.get_feature_names() <br /><br />	#The below code will get the weights from the classifier, and print out the weights of the features you are interested in<br />	features = [] #this will be a list of (feature_idx, weight) tuples<br />	for i,w in enumerate(clf.coef_[0]): <br />		features.append((i,w))<br />	#Sort the list by values, with the largest ones first<br />	features = sorted(features, key=lambda e: e[1], reverse=True)<br /><br />        #Print out the feature names and thier weights<br />	for i,w in features:<br />	  print &#39;%s\t%s&#39;%(feature_names[i], w)<br /><br />def get_misclassified_examples(y, X, texts) :<br />	x_train, x_test, y_train, y_test, train_texts, test_texts = train_test_split(X, y, texts)<br />	clf = train_classifier(x_train, y_train)<br /><br />	#TODO: You will have to write some code to call your classifier on each of the test examples, and check whether its prediction was right or wrong<br /><br />def create_graph(file_name) :<br />	os.system(&#34;dot -Tpng output.dot -o &#34; &#43; file_name)<br />	os.unlink(&#34;output.dot&#34;)<br /><br />if __name__ == &#39;__main__&#39; : <br /><br />	raw_data = get_data(sys.argv[1])<br />	<br />	print &#39;\nRule-based classification&#39;<br />	rule_based_classifier(raw_data)<br /><br />################ Decision Tree ################<br /><br /># print &#39;\nDecision Tree classification&#39;<br /># y, X, texts, dv, le = get_matricies(raw_data, &#34;tree&#34;)<br /># cross_validate(X,y,dv,&#34;tree&#34;)<br /><br />################ Statistical Classification ################<br />print &#39;\nStatistical classification&#39;<br />y, X, texts, dv, le = get_matricies(raw_data)<br />cross_validate(X,y)<br /><br />get_top_features(X, y, dv)<br />get_misclassified_examples(y, X, texts)<br />
</pre>
<p>Thanks in advance for any help you can give.</p>",Statistical Analysis &#34;Killed&#34; - even on biglab,"<p>When my program gets to the Statistical Analysis bit, it makes it through all the words and creates wordcount dictionaries that are appended to the list of features. Then it hangs for a bit until the terminal just prints &#34;Killed&#34; and program ends. Any idea what might be causing this? Not enough memory?</p>",Statistical Analysis &#34;Killed&#34;,,,"<p>I&#39;m confused on what exactly we&#39;re supposed to do for part 6. Are the y dimensions the same as the number of articles? Also, how is logistic regression supposed to be used here?</p>",Statistical Analysis Part 6,0
940849303,4/26/2016 18:41:04,true,1969497530,,4/26/2016 18:31:56,false,neodev,1.0,35974955,VEN,17,Porlamar,190.198.232.239,0,0,"<p>Hi, so I read though the other piazza post about how the statistical analysis takes a long time, but I am still having some trouble with my code.</p>
<p></p>
<p>I have already tried waiting 10-15 minutes and removing all stopwords, but for some reason the statistical analysis refuses to complete in a reasonable amount of time</p>
<p></p>
<p>Below is my code:</p>
<pre>def isStopWord(word):
	with open(&#39;stopwords.txt&#39;, &#39;r&#39;) as stopwords:
		for line in stopwords:
			if word in line:
				return True
		return False

def get_features(X) : 
	features = []
	for x in X : 
		f = {}
		#TODO replace this dummy feature function with a unigram model, like we did in class
		for word in x.split():
			if isStopWord(word):
				continue
			if word in f:
				f[word] = f[word] &#43; 1.0
			else:
				f[word] =  1.0
		features.append(f)
	return features</pre>
<p>Not really sure what to do at this point because of this. Also might this be related to using the VM?</p>",Statistical Analysis Taking A Very Long Time,It takes a really long time. Is that expected?,statistical classification runtime,"<p>My script runs forever on my virtual machine, so I&#39;m running it off of biglab. I got the statistical analysis to work once (took about 2 minutes), but after un-commenting get_top_features and get_misclassified, my script gets killed before finishing the initial statistical analysis. I&#39;ve tried several times and I&#39;m unsure what the issue is; I&#39;ve copied my code below.</p>
<p></p>
<pre>#!/bin/python<br /><br />import os<br />import sys<br />import string<br />import random<br />import operator<br />from sklearn.tree import export_graphviz<br />from sklearn.tree import DecisionTreeClassifier<br />from sklearn.naive_bayes import MultinomialNB<br />from sklearn.linear_model import LogisticRegression<br />from sklearn.preprocessing import LabelEncoder<br />from sklearn.feature_extraction import DictVectorizer<br />from sklearn.cross_validation import train_test_split<br />from sklearn.externals.six import StringIO  <br /><br />#read in raw data from file and return a list of (label, article) tuples<br />def get_data(filename): <br />	data = [line.strip().split(&#39;\t&#39;) for line in open(filename).readlines()]<br />	random.shuffle(data)<br />	return data<br /><br />#this function builds the feature matrix for the Decision Tree.<br />def get_dtree_features(X) :<br />	features = []<br />	#TODO : Add the features you would like to use to train the Decision Tree here.<br />	feature_list = [&#39;gun&#39;, &#39;shooter&#39;, &#39;shot&#39;, &#39;shooting&#39;, &#39;player&#39;, &#39;coach&#39;, &#39;game&#39;, &#39;movie&#39;, &#39;actor&#39;, &#39;film&#39;, &#39;bullet&#39;, &#39;photo&#39;, &#39;model&#39;, &#39;point&#39;, &#39;suspect&#39;, &#39;victim&#39;, &#39;police&#39;, &#39;fired&#39;]<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w in feature_list : <br />				f[w] = 1.0<br />		features.append(f)<br />	return features<br /><br /><br />#this is the main function you care about; pack all the cleverest features you can think of into here.<br />def get_features(X) :<br />	stopwords = open(&#39;stopwords.txt&#39;).read()<br />	features = []<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w not in stopwords:            <br />				if w not in f : <br />					f[w] = 0<br />				f[w] &#43;= 1                <br />		features.append(f)<br />	return features<br /><br />#vectorize feature dictionaries and return feature and label matricies<br />def get_matricies(data, typ=&#34;unigram&#34;) : <br />	dv = DictVectorizer(sparse=True) <br />	le = LabelEncoder()<br />	y = [d[0] for d in data]<br />	texts = [d[1] for d in data]<br />	if typ == &#34;tree&#34;:<br />		X = get_dtree_features(texts)<br />	else :<br />		X = get_features(texts)<br />	#Here we are returning 5 things, the label vector y and feature matrix X, but also the texts from which the features were extracted and the <br />	#objects that were used to encode them. These will come in handy for your analysis, but you can ignore them for the initial parts of the assignment<br />	return le.fit_transform(y), dv.fit_transform(X), texts, dv, le<br /><br />#train and multinomial naive bayes classifier<br />def train_classifier(X, y):<br />	clf = LogisticRegression()<br />	clf.fit(X,y)<br />	return clf <br /><br />#train a Decision Tree classifier<br />def train_dtree_classifier(X, y):<br />	clf = DecisionTreeClassifier(max_depth=10)<br />	clf.fit(X,y)<br />	return clf<br /><br />#test the classifier<br />def test_classifier(clf, X, y):<br />	return clf.score(X,y)<br /><br />#cross validation	<br />def cross_validate(X, y, dv=None, typ=&#34;unigram&#34;, numfolds=5,):<br />	test_accs = []<br />	split = 1.0 / numfolds<br />	for i in range(numfolds):<br />		x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=i)<br />		if typ == &#34;tree&#34; :<br />			clf = train_dtree_classifier(x_train, y_train)<br />		else :<br />			clf = train_classifier(x_train, y_train)<br />		test_acc = test_classifier(clf, x_test, y_test)<br />		test_accs.append(test_acc)<br />		print &#39;Fold %d : %.05f&#39;%(i,test_acc)<br />	test_average = float(sum(test_accs))/ numfolds<br />	if typ == &#34;tree&#34; :<br />		with open(&#34;output.dot&#34;, &#39;w&#39;) as f:<br />			f = export_graphviz(clf, out_file=f, feature_names=dv.get_feature_names(), class_names=[&#39;Non Gun Related&#39;,&#39;Gun Related&#39;])<br />		create_graph(&#34;decision-tree.png&#34;)<br />	print &#39;Test Average : %.05f&#39;%(test_average)<br />	print<br />	return test_average<br /><br />#run a rule based classifier and calculate the accuracy<br />def rule_based_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO add more keywords, see how well they do alone and in combination<br />		if &#34;shooter&#34; in text or &#34;shot&#34; in text or &#34;shooting&#34; in text :<br />			if &#34;player&#34; in text or &#34;coach&#34; in text or &#34;game&#34; in text: <br />				prediction = &#39;0&#39;<br />			elif &#34;movie&#34; in text or &#34;film&#34; in text or &#34;actor&#34; in text:<br />				prediction = &#39;0&#39;<br />			else : prediction = &#39;1&#39;<br />		if &#34;gun&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Rule-based classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#Extra Credit Rule Based Classifier<br />def extra_credit_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO develop your conditional statements here<br />		if &#34;shooting&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Reverse Engineered classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#train and multinomial naive bayes classifier<br />def get_top_features(X, y, dv):<br />	clf = train_classifier(X, y)<br />	#the DictVectorizer object remembers which column number corresponds to which feature, and return the feature names in the correct order<br />	feature_names = dv.get_feature_names() <br /><br />	#The below code will get the weights from the classifier, and print out the weights of the features you are interested in<br />	features = [] #this will be a list of (feature_idx, weight) tuples<br />	for i,w in enumerate(clf.coef_[0]): <br />		features.append((i,w))<br />	#Sort the list by values, with the largest ones first<br />	features = sorted(features, key=lambda e: e[1], reverse=True)<br /><br />        #Print out the feature names and thier weights<br />	for i,w in features:<br />	  print &#39;%s\t%s&#39;%(feature_names[i], w)<br /><br />def get_misclassified_examples(y, X, texts) :<br />	x_train, x_test, y_train, y_test, train_texts, test_texts = train_test_split(X, y, texts)<br />	clf = train_classifier(x_train, y_train)<br /><br />	#TODO: You will have to write some code to call your classifier on each of the test examples, and check whether its prediction was right or wrong<br /><br />def create_graph(file_name) :<br />	os.system(&#34;dot -Tpng output.dot -o &#34; &#43; file_name)<br />	os.unlink(&#34;output.dot&#34;)<br /><br />if __name__ == &#39;__main__&#39; : <br /><br />	raw_data = get_data(sys.argv[1])<br />	<br />	print &#39;\nRule-based classification&#39;<br />	rule_based_classifier(raw_data)<br /><br />################ Decision Tree ################<br /><br /># print &#39;\nDecision Tree classification&#39;<br /># y, X, texts, dv, le = get_matricies(raw_data, &#34;tree&#34;)<br /># cross_validate(X,y,dv,&#34;tree&#34;)<br /><br />################ Statistical Classification ################<br />print &#39;\nStatistical classification&#39;<br />y, X, texts, dv, le = get_matricies(raw_data)<br />cross_validate(X,y)<br /><br />get_top_features(X, y, dv)<br />get_misclassified_examples(y, X, texts)<br />
</pre>
<p>Thanks in advance for any help you can give.</p>",Statistical Analysis &#34;Killed&#34; - even on biglab,"<p>When my program gets to the Statistical Analysis bit, it makes it through all the words and creates wordcount dictionaries that are appended to the list of features. Then it hangs for a bit until the terminal just prints &#34;Killed&#34; and program ends. Any idea what might be causing this? Not enough memory?</p>",Statistical Analysis &#34;Killed&#34;,,,"<p>I&#39;m confused on what exactly we&#39;re supposed to do for part 6. Are the y dimensions the same as the number of articles? Also, how is logistic regression supposed to be used here?</p>",Statistical Analysis Part 6,0
940849303,4/26/2016 19:25:11,true,1969519075,,4/26/2016 19:21:41,false,tremorgames,1.0,25197223,HRV,15,Split,94.253.234.240,0,0,"<p>Hi, so I read though the other piazza post about how the statistical analysis takes a long time, but I am still having some trouble with my code.</p>
<p></p>
<p>I have already tried waiting 10-15 minutes and removing all stopwords, but for some reason the statistical analysis refuses to complete in a reasonable amount of time</p>
<p></p>
<p>Below is my code:</p>
<pre>def isStopWord(word):
	with open(&#39;stopwords.txt&#39;, &#39;r&#39;) as stopwords:
		for line in stopwords:
			if word in line:
				return True
		return False

def get_features(X) : 
	features = []
	for x in X : 
		f = {}
		#TODO replace this dummy feature function with a unigram model, like we did in class
		for word in x.split():
			if isStopWord(word):
				continue
			if word in f:
				f[word] = f[word] &#43; 1.0
			else:
				f[word] =  1.0
		features.append(f)
	return features</pre>
<p>Not really sure what to do at this point because of this. Also might this be related to using the VM?</p>",Statistical Analysis Taking A Very Long Time,It takes a really long time. Is that expected?,statistical classification runtime,"<p>My script runs forever on my virtual machine, so I&#39;m running it off of biglab. I got the statistical analysis to work once (took about 2 minutes), but after un-commenting get_top_features and get_misclassified, my script gets killed before finishing the initial statistical analysis. I&#39;ve tried several times and I&#39;m unsure what the issue is; I&#39;ve copied my code below.</p>
<p></p>
<pre>#!/bin/python<br /><br />import os<br />import sys<br />import string<br />import random<br />import operator<br />from sklearn.tree import export_graphviz<br />from sklearn.tree import DecisionTreeClassifier<br />from sklearn.naive_bayes import MultinomialNB<br />from sklearn.linear_model import LogisticRegression<br />from sklearn.preprocessing import LabelEncoder<br />from sklearn.feature_extraction import DictVectorizer<br />from sklearn.cross_validation import train_test_split<br />from sklearn.externals.six import StringIO  <br /><br />#read in raw data from file and return a list of (label, article) tuples<br />def get_data(filename): <br />	data = [line.strip().split(&#39;\t&#39;) for line in open(filename).readlines()]<br />	random.shuffle(data)<br />	return data<br /><br />#this function builds the feature matrix for the Decision Tree.<br />def get_dtree_features(X) :<br />	features = []<br />	#TODO : Add the features you would like to use to train the Decision Tree here.<br />	feature_list = [&#39;gun&#39;, &#39;shooter&#39;, &#39;shot&#39;, &#39;shooting&#39;, &#39;player&#39;, &#39;coach&#39;, &#39;game&#39;, &#39;movie&#39;, &#39;actor&#39;, &#39;film&#39;, &#39;bullet&#39;, &#39;photo&#39;, &#39;model&#39;, &#39;point&#39;, &#39;suspect&#39;, &#39;victim&#39;, &#39;police&#39;, &#39;fired&#39;]<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w in feature_list : <br />				f[w] = 1.0<br />		features.append(f)<br />	return features<br /><br /><br />#this is the main function you care about; pack all the cleverest features you can think of into here.<br />def get_features(X) :<br />	stopwords = open(&#39;stopwords.txt&#39;).read()<br />	features = []<br />	for x in X :<br />		f = {}<br />		for w in [word.strip(string.punctuation) for word in x.split()]:<br />			if w not in stopwords:            <br />				if w not in f : <br />					f[w] = 0<br />				f[w] &#43;= 1                <br />		features.append(f)<br />	return features<br /><br />#vectorize feature dictionaries and return feature and label matricies<br />def get_matricies(data, typ=&#34;unigram&#34;) : <br />	dv = DictVectorizer(sparse=True) <br />	le = LabelEncoder()<br />	y = [d[0] for d in data]<br />	texts = [d[1] for d in data]<br />	if typ == &#34;tree&#34;:<br />		X = get_dtree_features(texts)<br />	else :<br />		X = get_features(texts)<br />	#Here we are returning 5 things, the label vector y and feature matrix X, but also the texts from which the features were extracted and the <br />	#objects that were used to encode them. These will come in handy for your analysis, but you can ignore them for the initial parts of the assignment<br />	return le.fit_transform(y), dv.fit_transform(X), texts, dv, le<br /><br />#train and multinomial naive bayes classifier<br />def train_classifier(X, y):<br />	clf = LogisticRegression()<br />	clf.fit(X,y)<br />	return clf <br /><br />#train a Decision Tree classifier<br />def train_dtree_classifier(X, y):<br />	clf = DecisionTreeClassifier(max_depth=10)<br />	clf.fit(X,y)<br />	return clf<br /><br />#test the classifier<br />def test_classifier(clf, X, y):<br />	return clf.score(X,y)<br /><br />#cross validation	<br />def cross_validate(X, y, dv=None, typ=&#34;unigram&#34;, numfolds=5,):<br />	test_accs = []<br />	split = 1.0 / numfolds<br />	for i in range(numfolds):<br />		x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=split, random_state=i)<br />		if typ == &#34;tree&#34; :<br />			clf = train_dtree_classifier(x_train, y_train)<br />		else :<br />			clf = train_classifier(x_train, y_train)<br />		test_acc = test_classifier(clf, x_test, y_test)<br />		test_accs.append(test_acc)<br />		print &#39;Fold %d : %.05f&#39;%(i,test_acc)<br />	test_average = float(sum(test_accs))/ numfolds<br />	if typ == &#34;tree&#34; :<br />		with open(&#34;output.dot&#34;, &#39;w&#39;) as f:<br />			f = export_graphviz(clf, out_file=f, feature_names=dv.get_feature_names(), class_names=[&#39;Non Gun Related&#39;,&#39;Gun Related&#39;])<br />		create_graph(&#34;decision-tree.png&#34;)<br />	print &#39;Test Average : %.05f&#39;%(test_average)<br />	print<br />	return test_average<br /><br />#run a rule based classifier and calculate the accuracy<br />def rule_based_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO add more keywords, see how well they do alone and in combination<br />		if &#34;shooter&#34; in text or &#34;shot&#34; in text or &#34;shooting&#34; in text :<br />			if &#34;player&#34; in text or &#34;coach&#34; in text or &#34;game&#34; in text: <br />				prediction = &#39;0&#39;<br />			elif &#34;movie&#34; in text or &#34;film&#34; in text or &#34;actor&#34; in text:<br />				prediction = &#39;0&#39;<br />			else : prediction = &#39;1&#39;<br />		if &#34;gun&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Rule-based classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#Extra Credit Rule Based Classifier<br />def extra_credit_classifier(data):<br />	correct = 0.0; total = 0.0<br />	for label, text in data : <br />		prediction = &#39;0&#39;<br />		#TODO develop your conditional statements here<br />		if &#34;shooting&#34; in text : prediction = &#39;1&#39;<br />		if prediction == label : correct &#43;= 1<br />		total &#43;= 1<br />	print &#39;Reverse Engineered classifier accuracy: %.05f&#39;%(correct / total)<br /><br />#train and multinomial naive bayes classifier<br />def get_top_features(X, y, dv):<br />	clf = train_classifier(X, y)<br />	#the DictVectorizer object remembers which column number corresponds to which feature, and return the feature names in the correct order<br />	feature_names = dv.get_feature_names() <br /><br />	#The below code will get the weights from the classifier, and print out the weights of the features you are interested in<br />	features = [] #this will be a list of (feature_idx, weight) tuples<br />	for i,w in enumerate(clf.coef_[0]): <br />		features.append((i,w))<br />	#Sort the list by values, with the largest ones first<br />	features = sorted(features, key=lambda e: e[1], reverse=True)<br /><br />        #Print out the feature names and thier weights<br />	for i,w in features:<br />	  print &#39;%s\t%s&#39;%(feature_names[i], w)<br /><br />def get_misclassified_examples(y, X, texts) :<br />	x_train, x_test, y_train, y_test, train_texts, test_texts = train_test_split(X, y, texts)<br />	clf = train_classifier(x_train, y_train)<br /><br />	#TODO: You will have to write some code to call your classifier on each of the test examples, and check whether its prediction was right or wrong<br /><br />def create_graph(file_name) :<br />	os.system(&#34;dot -Tpng output.dot -o &#34; &#43; file_name)<br />	os.unlink(&#34;output.dot&#34;)<br /><br />if __name__ == &#39;__main__&#39; : <br /><br />	raw_data = get_data(sys.argv[1])<br />	<br />	print &#39;\nRule-based classification&#39;<br />	rule_based_classifier(raw_data)<br /><br />################ Decision Tree ################<br /><br /># print &#39;\nDecision Tree classification&#39;<br /># y, X, texts, dv, le = get_matricies(raw_data, &#34;tree&#34;)<br /># cross_validate(X,y,dv,&#34;tree&#34;)<br /><br />################ Statistical Classification ################<br />print &#39;\nStatistical classification&#39;<br />y, X, texts, dv, le = get_matricies(raw_data)<br />cross_validate(X,y)<br /><br />get_top_features(X, y, dv)<br />get_misclassified_examples(y, X, texts)<br />
</pre>
<p>Thanks in advance for any help you can give.</p>",Statistical Analysis &#34;Killed&#34; - even on biglab,"<p>When my program gets to the Statistical Analysis bit, it makes it through all the words and creates wordcount dictionaries that are appended to the list of features. Then it hangs for a bit until the terminal just prints &#34;Killed&#34; and program ends. Any idea what might be causing this? Not enough memory?</p>",Statistical Analysis &#34;Killed&#34;,,,"<p>I&#39;m confused on what exactly we&#39;re supposed to do for part 6. Are the y dimensions the same as the number of articles? Also, how is logistic regression supposed to be used here?</p>",Statistical Analysis Part 6,0
940849440,4/26/2016 15:12:15,true,1969364816,,4/26/2016 15:12:00,false,tremorgames,1.0,32635967,LTU,60,Panevezys,78.63.38.165,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I&#39;m attempting to make it to Katie&#39;s office hours that end at 11:50 but wanted to post this question here if I don&#39;t make it.</p>
<p></p>
<p>My partner and I have been trying to get the &#34;No More Rows Remaining&#34; problem resolved on Crowdflower for about a day now. åÊWe have tried using both our accounts and get that error, and I even made another account and we added it to the team (as per someone&#39;s suggestion on Piazza), but still get that error when trying to launch. åÊI submitted a ticket, but Crowdflower still hasn&#39;t responded.</p>
<p></p>",No More Rows Remaining,2
940849440,4/26/2016 15:17:20,true,1969372589,,4/26/2016 15:12:10,false,clixsense,1.0,7837812,SRB,00,Belgrade,79.101.254.233,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I&#39;m attempting to make it to Katie&#39;s office hours that end at 11:50 but wanted to post this question here if I don&#39;t make it.</p>
<p></p>
<p>My partner and I have been trying to get the &#34;No More Rows Remaining&#34; problem resolved on Crowdflower for about a day now. åÊWe have tried using both our accounts and get that error, and I even made another account and we added it to the team (as per someone&#39;s suggestion on Piazza), but still get that error when trying to launch. åÊI submitted a ticket, but Crowdflower still hasn&#39;t responded.</p>
<p></p>",No More Rows Remaining,2
940849440,4/26/2016 15:19:29,true,1969376230,,4/26/2016 15:14:50,false,neodev,1.0,19132694,LKA,36,Colombo,123.231.124.170,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I&#39;m attempting to make it to Katie&#39;s office hours that end at 11:50 but wanted to post this question here if I don&#39;t make it.</p>
<p></p>
<p>My partner and I have been trying to get the &#34;No More Rows Remaining&#34; problem resolved on Crowdflower for about a day now. åÊWe have tried using both our accounts and get that error, and I even made another account and we added it to the team (as per someone&#39;s suggestion on Piazza), but still get that error when trying to launch. åÊI submitted a ticket, but Crowdflower still hasn&#39;t responded.</p>
<p></p>",No More Rows Remaining,2
940849440,4/26/2016 15:21:05,true,1969378837,,4/26/2016 15:15:48,false,elite,1.0,30280423,ITA,15,Siracusa,151.54.84.121,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I&#39;m attempting to make it to Katie&#39;s office hours that end at 11:50 but wanted to post this question here if I don&#39;t make it.</p>
<p></p>
<p>My partner and I have been trying to get the &#34;No More Rows Remaining&#34; problem resolved on Crowdflower for about a day now. åÊWe have tried using both our accounts and get that error, and I even made another account and we added it to the team (as per someone&#39;s suggestion on Piazza), but still get that error when trying to launch. åÊI submitted a ticket, but Crowdflower still hasn&#39;t responded.</p>
<p></p>",No More Rows Remaining,2
940849440,4/26/2016 15:21:31,true,1969379543,,4/26/2016 15:12:37,false,clixsense,0.8889,36052512,PHL,F2,Quezon City,49.149.150.150,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I&#39;m attempting to make it to Katie&#39;s office hours that end at 11:50 but wanted to post this question here if I don&#39;t make it.</p>
<p></p>
<p>My partner and I have been trying to get the &#34;No More Rows Remaining&#34; problem resolved on Crowdflower for about a day now. åÊWe have tried using both our accounts and get that error, and I even made another account and we added it to the team (as per someone&#39;s suggestion on Piazza), but still get that error when trying to launch. åÊI submitted a ticket, but Crowdflower still hasn&#39;t responded.</p>
<p></p>",No More Rows Remaining,2
940849440,4/26/2016 15:23:37,true,1969383213,,4/26/2016 15:20:30,false,clixsense,1.0,24287706,TWN,04,Keelung,61.231.195.173,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I&#39;m attempting to make it to Katie&#39;s office hours that end at 11:50 but wanted to post this question here if I don&#39;t make it.</p>
<p></p>
<p>My partner and I have been trying to get the &#34;No More Rows Remaining&#34; problem resolved on Crowdflower for about a day now. åÊWe have tried using both our accounts and get that error, and I even made another account and we added it to the team (as per someone&#39;s suggestion on Piazza), but still get that error when trying to launch. åÊI submitted a ticket, but Crowdflower still hasn&#39;t responded.</p>
<p></p>",No More Rows Remaining,2
940849440,4/26/2016 15:28:37,true,1969390285,,4/26/2016 15:28:07,false,instagc,0.8889,13581319,USA,IL,Waltonville,208.70.36.12,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I&#39;m attempting to make it to Katie&#39;s office hours that end at 11:50 but wanted to post this question here if I don&#39;t make it.</p>
<p></p>
<p>My partner and I have been trying to get the &#34;No More Rows Remaining&#34; problem resolved on Crowdflower for about a day now. åÊWe have tried using both our accounts and get that error, and I even made another account and we added it to the team (as per someone&#39;s suggestion on Piazza), but still get that error when trying to launch. åÊI submitted a ticket, but Crowdflower still hasn&#39;t responded.</p>
<p></p>",No More Rows Remaining,2
940849440,4/26/2016 15:36:01,true,1969392988,,4/26/2016 15:30:10,false,neodev,1.0,13396426,VEN,15,Santa Teresa,190.38.163.149,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I&#39;m attempting to make it to Katie&#39;s office hours that end at 11:50 but wanted to post this question here if I don&#39;t make it.</p>
<p></p>
<p>My partner and I have been trying to get the &#34;No More Rows Remaining&#34; problem resolved on Crowdflower for about a day now. åÊWe have tried using both our accounts and get that error, and I even made another account and we added it to the team (as per someone&#39;s suggestion on Piazza), but still get that error when trying to launch. åÊI submitted a ticket, but Crowdflower still hasn&#39;t responded.</p>
<p></p>",No More Rows Remaining,2
940849440,4/26/2016 15:39:56,true,1969394932,,4/26/2016 15:27:12,false,clixsense,1.0,21875134,GBR,H9,London,87.112.158.81,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I&#39;m attempting to make it to Katie&#39;s office hours that end at 11:50 but wanted to post this question here if I don&#39;t make it.</p>
<p></p>
<p>My partner and I have been trying to get the &#34;No More Rows Remaining&#34; problem resolved on Crowdflower for about a day now. åÊWe have tried using both our accounts and get that error, and I even made another account and we added it to the team (as per someone&#39;s suggestion on Piazza), but still get that error when trying to launch. åÊI submitted a ticket, but Crowdflower still hasn&#39;t responded.</p>
<p></p>",No More Rows Remaining,2
940849440,4/26/2016 15:44:24,true,1969396764,,4/26/2016 15:31:20,false,neodev,1.0,28875937,PAK,04,Lahore,119.153.154.137,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I&#39;m attempting to make it to Katie&#39;s office hours that end at 11:50 but wanted to post this question here if I don&#39;t make it.</p>
<p></p>
<p>My partner and I have been trying to get the &#34;No More Rows Remaining&#34; problem resolved on Crowdflower for about a day now. åÊWe have tried using both our accounts and get that error, and I even made another account and we added it to the team (as per someone&#39;s suggestion on Piazza), but still get that error when trying to launch. åÊI submitted a ticket, but Crowdflower still hasn&#39;t responded.</p>
<p></p>",No More Rows Remaining,2
940849440,4/26/2016 15:45:33,true,1969397160,,4/26/2016 15:43:24,false,neodev,0.8889,21971187,TTO,08,Valsayn,190.213.132.190,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I&#39;m attempting to make it to Katie&#39;s office hours that end at 11:50 but wanted to post this question here if I don&#39;t make it.</p>
<p></p>
<p>My partner and I have been trying to get the &#34;No More Rows Remaining&#34; problem resolved on Crowdflower for about a day now. åÊWe have tried using both our accounts and get that error, and I even made another account and we added it to the team (as per someone&#39;s suggestion on Piazza), but still get that error when trying to launch. åÊI submitted a ticket, but Crowdflower still hasn&#39;t responded.</p>
<p></p>",No More Rows Remaining,2
940849440,4/26/2016 15:47:45,true,1969398092,,4/26/2016 15:41:08,false,neodev,0.7778,32569659,USA,MN,Minneapolis,97.127.88.224,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I&#39;m attempting to make it to Katie&#39;s office hours that end at 11:50 but wanted to post this question here if I don&#39;t make it.</p>
<p></p>
<p>My partner and I have been trying to get the &#34;No More Rows Remaining&#34; problem resolved on Crowdflower for about a day now. åÊWe have tried using both our accounts and get that error, and I even made another account and we added it to the team (as per someone&#39;s suggestion on Piazza), but still get that error when trying to launch. åÊI submitted a ticket, but Crowdflower still hasn&#39;t responded.</p>
<p></p>",No More Rows Remaining,2
940849440,4/26/2016 15:50:47,true,1969399563,,4/26/2016 15:49:12,false,elite,0.8889,36575101,IND,07,New Delhi,112.196.144.2,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I&#39;m attempting to make it to Katie&#39;s office hours that end at 11:50 but wanted to post this question here if I don&#39;t make it.</p>
<p></p>
<p>My partner and I have been trying to get the &#34;No More Rows Remaining&#34; problem resolved on Crowdflower for about a day now. åÊWe have tried using both our accounts and get that error, and I even made another account and we added it to the team (as per someone&#39;s suggestion on Piazza), but still get that error when trying to launch. åÊI submitted a ticket, but Crowdflower still hasn&#39;t responded.</p>
<p></p>",No More Rows Remaining,2
940849440,4/26/2016 15:58:24,true,1969402711,,4/26/2016 15:56:48,false,elite,1.0,33243069,IND,10,Faridabad,116.203.79.150,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I&#39;m attempting to make it to Katie&#39;s office hours that end at 11:50 but wanted to post this question here if I don&#39;t make it.</p>
<p></p>
<p>My partner and I have been trying to get the &#34;No More Rows Remaining&#34; problem resolved on Crowdflower for about a day now. åÊWe have tried using both our accounts and get that error, and I even made another account and we added it to the team (as per someone&#39;s suggestion on Piazza), but still get that error when trying to launch. åÊI submitted a ticket, but Crowdflower still hasn&#39;t responded.</p>
<p></p>",No More Rows Remaining,2
940849440,4/26/2016 16:01:46,true,1969404618,,4/26/2016 16:01:11,false,personaly,1.0,33663352,ARG,01,Mar Del Plata,181.168.213.227,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I&#39;m attempting to make it to Katie&#39;s office hours that end at 11:50 but wanted to post this question here if I don&#39;t make it.</p>
<p></p>
<p>My partner and I have been trying to get the &#34;No More Rows Remaining&#34; problem resolved on Crowdflower for about a day now. åÊWe have tried using both our accounts and get that error, and I even made another account and we added it to the team (as per someone&#39;s suggestion on Piazza), but still get that error when trying to launch. åÊI submitted a ticket, but Crowdflower still hasn&#39;t responded.</p>
<p></p>",No More Rows Remaining,2
940849440,4/26/2016 16:03:35,true,1969405532,,4/26/2016 15:48:02,false,clixsense,0.8889,8057247,PRT,17,Póvoa De Varzim,144.64.25.68,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I&#39;m attempting to make it to Katie&#39;s office hours that end at 11:50 but wanted to post this question here if I don&#39;t make it.</p>
<p></p>
<p>My partner and I have been trying to get the &#34;No More Rows Remaining&#34; problem resolved on Crowdflower for about a day now. åÊWe have tried using both our accounts and get that error, and I even made another account and we added it to the team (as per someone&#39;s suggestion on Piazza), but still get that error when trying to launch. åÊI submitted a ticket, but Crowdflower still hasn&#39;t responded.</p>
<p></p>",No More Rows Remaining,2
940849440,4/26/2016 16:15:36,true,1969411454,,4/26/2016 16:12:15,false,elite,1.0,30128662,BGR,50,Pleven,212.233.177.195,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I&#39;m attempting to make it to Katie&#39;s office hours that end at 11:50 but wanted to post this question here if I don&#39;t make it.</p>
<p></p>
<p>My partner and I have been trying to get the &#34;No More Rows Remaining&#34; problem resolved on Crowdflower for about a day now. åÊWe have tried using both our accounts and get that error, and I even made another account and we added it to the team (as per someone&#39;s suggestion on Piazza), but still get that error when trying to launch. åÊI submitted a ticket, but Crowdflower still hasn&#39;t responded.</p>
<p></p>",No More Rows Remaining,2
940849440,4/26/2016 16:26:27,true,1969420527,,4/26/2016 16:25:32,false,neodev,1.0,29175140,VEN,25,Caracas,190.72.125.134,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I&#39;m attempting to make it to Katie&#39;s office hours that end at 11:50 but wanted to post this question here if I don&#39;t make it.</p>
<p></p>
<p>My partner and I have been trying to get the &#34;No More Rows Remaining&#34; problem resolved on Crowdflower for about a day now. åÊWe have tried using both our accounts and get that error, and I even made another account and we added it to the team (as per someone&#39;s suggestion on Piazza), but still get that error when trying to launch. åÊI submitted a ticket, but Crowdflower still hasn&#39;t responded.</p>
<p></p>",No More Rows Remaining,2
940849440,4/26/2016 16:39:18,true,1969429040,,4/26/2016 16:30:39,false,clixsense,1.0,6329782,IDN,07,Bekonang,202.67.40.31,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I&#39;m attempting to make it to Katie&#39;s office hours that end at 11:50 but wanted to post this question here if I don&#39;t make it.</p>
<p></p>
<p>My partner and I have been trying to get the &#34;No More Rows Remaining&#34; problem resolved on Crowdflower for about a day now. åÊWe have tried using both our accounts and get that error, and I even made another account and we added it to the team (as per someone&#39;s suggestion on Piazza), but still get that error when trying to launch. åÊI submitted a ticket, but Crowdflower still hasn&#39;t responded.</p>
<p></p>",No More Rows Remaining,2
940849440,4/26/2016 16:40:52,true,1969429997,,4/26/2016 16:36:32,false,prodege,1.0,22387641,USA,PA,Jeannette,208.94.42.131,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I&#39;m attempting to make it to Katie&#39;s office hours that end at 11:50 but wanted to post this question here if I don&#39;t make it.</p>
<p></p>
<p>My partner and I have been trying to get the &#34;No More Rows Remaining&#34; problem resolved on Crowdflower for about a day now. åÊWe have tried using both our accounts and get that error, and I even made another account and we added it to the team (as per someone&#39;s suggestion on Piazza), but still get that error when trying to launch. åÊI submitted a ticket, but Crowdflower still hasn&#39;t responded.</p>
<p></p>",No More Rows Remaining,2
940849440,4/26/2016 16:56:40,true,1969439483,,4/26/2016 16:53:35,false,clixsense,1.0,21408115,IDN,07,Semarang,36.79.23.180,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I&#39;m attempting to make it to Katie&#39;s office hours that end at 11:50 but wanted to post this question here if I don&#39;t make it.</p>
<p></p>
<p>My partner and I have been trying to get the &#34;No More Rows Remaining&#34; problem resolved on Crowdflower for about a day now. åÊWe have tried using both our accounts and get that error, and I even made another account and we added it to the team (as per someone&#39;s suggestion on Piazza), but still get that error when trying to launch. åÊI submitted a ticket, but Crowdflower still hasn&#39;t responded.</p>
<p></p>",No More Rows Remaining,2
940849440,4/26/2016 17:04:17,true,1969444197,,4/26/2016 17:02:14,false,neodev,1.0,36167043,GBR,G6,Hull,77.86.101.69,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I&#39;m attempting to make it to Katie&#39;s office hours that end at 11:50 but wanted to post this question here if I don&#39;t make it.</p>
<p></p>
<p>My partner and I have been trying to get the &#34;No More Rows Remaining&#34; problem resolved on Crowdflower for about a day now. åÊWe have tried using both our accounts and get that error, and I even made another account and we added it to the team (as per someone&#39;s suggestion on Piazza), but still get that error when trying to launch. åÊI submitted a ticket, but Crowdflower still hasn&#39;t responded.</p>
<p></p>",No More Rows Remaining,2
940849440,4/26/2016 17:11:04,true,1969448312,,4/26/2016 17:01:16,false,neodev,0.8889,19625264,DZA,41,Chlef,41.102.7.217,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I&#39;m attempting to make it to Katie&#39;s office hours that end at 11:50 but wanted to post this question here if I don&#39;t make it.</p>
<p></p>
<p>My partner and I have been trying to get the &#34;No More Rows Remaining&#34; problem resolved on Crowdflower for about a day now. åÊWe have tried using both our accounts and get that error, and I even made another account and we added it to the team (as per someone&#39;s suggestion on Piazza), but still get that error when trying to launch. åÊI submitted a ticket, but Crowdflower still hasn&#39;t responded.</p>
<p></p>",No More Rows Remaining,2
940849440,4/26/2016 17:15:16,true,1969450999,,4/26/2016 17:01:58,false,clixsense,0.8889,35338593,ITA,14,Cagliari,151.56.132.145,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I&#39;m attempting to make it to Katie&#39;s office hours that end at 11:50 but wanted to post this question here if I don&#39;t make it.</p>
<p></p>
<p>My partner and I have been trying to get the &#34;No More Rows Remaining&#34; problem resolved on Crowdflower for about a day now. åÊWe have tried using both our accounts and get that error, and I even made another account and we added it to the team (as per someone&#39;s suggestion on Piazza), but still get that error when trying to launch. åÊI submitted a ticket, but Crowdflower still hasn&#39;t responded.</p>
<p></p>",No More Rows Remaining,2
940849440,4/26/2016 17:16:41,true,1969451757,,4/26/2016 17:14:02,false,elite,1.0,25411289,HRV,"","",31.147.119.175,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I&#39;m attempting to make it to Katie&#39;s office hours that end at 11:50 but wanted to post this question here if I don&#39;t make it.</p>
<p></p>
<p>My partner and I have been trying to get the &#34;No More Rows Remaining&#34; problem resolved on Crowdflower for about a day now. åÊWe have tried using both our accounts and get that error, and I even made another account and we added it to the team (as per someone&#39;s suggestion on Piazza), but still get that error when trying to launch. åÊI submitted a ticket, but Crowdflower still hasn&#39;t responded.</p>
<p></p>",No More Rows Remaining,2
940849440,4/26/2016 17:23:10,true,1969455462,,4/26/2016 17:16:53,false,neodev,1.0,33973110,VEN,23,Maracaibo,186.94.238.104,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I&#39;m attempting to make it to Katie&#39;s office hours that end at 11:50 but wanted to post this question here if I don&#39;t make it.</p>
<p></p>
<p>My partner and I have been trying to get the &#34;No More Rows Remaining&#34; problem resolved on Crowdflower for about a day now. åÊWe have tried using both our accounts and get that error, and I even made another account and we added it to the team (as per someone&#39;s suggestion on Piazza), but still get that error when trying to launch. åÊI submitted a ticket, but Crowdflower still hasn&#39;t responded.</p>
<p></p>",No More Rows Remaining,2
940849440,4/26/2016 17:31:21,true,1969460300,,4/26/2016 17:29:58,false,neodev,0.8889,33131546,IDN,04,Jakarta,139.194.89.60,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I&#39;m attempting to make it to Katie&#39;s office hours that end at 11:50 but wanted to post this question here if I don&#39;t make it.</p>
<p></p>
<p>My partner and I have been trying to get the &#34;No More Rows Remaining&#34; problem resolved on Crowdflower for about a day now. åÊWe have tried using both our accounts and get that error, and I even made another account and we added it to the team (as per someone&#39;s suggestion on Piazza), but still get that error when trying to launch. åÊI submitted a ticket, but Crowdflower still hasn&#39;t responded.</p>
<p></p>",No More Rows Remaining,2
940849440,4/26/2016 17:33:45,true,1969461568,,4/26/2016 17:33:17,false,clixsense,1.0,30712378,ROU,21,Deva,79.119.241.200,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I&#39;m attempting to make it to Katie&#39;s office hours that end at 11:50 but wanted to post this question here if I don&#39;t make it.</p>
<p></p>
<p>My partner and I have been trying to get the &#34;No More Rows Remaining&#34; problem resolved on Crowdflower for about a day now. åÊWe have tried using both our accounts and get that error, and I even made another account and we added it to the team (as per someone&#39;s suggestion on Piazza), but still get that error when trying to launch. åÊI submitted a ticket, but Crowdflower still hasn&#39;t responded.</p>
<p></p>",No More Rows Remaining,2
940849440,4/26/2016 17:39:39,true,1969464645,,4/26/2016 17:35:23,false,clixsense,1.0,35444326,BRA,07,Brasília,177.15.130.106,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I&#39;m attempting to make it to Katie&#39;s office hours that end at 11:50 but wanted to post this question here if I don&#39;t make it.</p>
<p></p>
<p>My partner and I have been trying to get the &#34;No More Rows Remaining&#34; problem resolved on Crowdflower for about a day now. åÊWe have tried using both our accounts and get that error, and I even made another account and we added it to the team (as per someone&#39;s suggestion on Piazza), but still get that error when trying to launch. åÊI submitted a ticket, but Crowdflower still hasn&#39;t responded.</p>
<p></p>",No More Rows Remaining,2
940849440,4/26/2016 17:41:58,true,1969466120,,4/26/2016 17:40:30,false,neodev,0.8889,33568303,VEN,23,Cabimas,190.77.7.36,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I&#39;m attempting to make it to Katie&#39;s office hours that end at 11:50 but wanted to post this question here if I don&#39;t make it.</p>
<p></p>
<p>My partner and I have been trying to get the &#34;No More Rows Remaining&#34; problem resolved on Crowdflower for about a day now. åÊWe have tried using both our accounts and get that error, and I even made another account and we added it to the team (as per someone&#39;s suggestion on Piazza), but still get that error when trying to launch. åÊI submitted a ticket, but Crowdflower still hasn&#39;t responded.</p>
<p></p>",No More Rows Remaining,2
940849440,4/26/2016 18:22:36,true,1969487904,,4/26/2016 18:21:16,false,neodev,0.8889,35550011,VEN,07,Valencia,190.204.238.112,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I&#39;m attempting to make it to Katie&#39;s office hours that end at 11:50 but wanted to post this question here if I don&#39;t make it.</p>
<p></p>
<p>My partner and I have been trying to get the &#34;No More Rows Remaining&#34; problem resolved on Crowdflower for about a day now. åÊWe have tried using both our accounts and get that error, and I even made another account and we added it to the team (as per someone&#39;s suggestion on Piazza), but still get that error when trying to launch. åÊI submitted a ticket, but Crowdflower still hasn&#39;t responded.</p>
<p></p>",No More Rows Remaining,2
940849440,4/26/2016 18:41:04,true,1969497529,,4/26/2016 18:31:56,false,neodev,1.0,35974955,VEN,17,Porlamar,190.198.232.239,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I&#39;m attempting to make it to Katie&#39;s office hours that end at 11:50 but wanted to post this question here if I don&#39;t make it.</p>
<p></p>
<p>My partner and I have been trying to get the &#34;No More Rows Remaining&#34; problem resolved on Crowdflower for about a day now. åÊWe have tried using both our accounts and get that error, and I even made another account and we added it to the team (as per someone&#39;s suggestion on Piazza), but still get that error when trying to launch. åÊI submitted a ticket, but Crowdflower still hasn&#39;t responded.</p>
<p></p>",No More Rows Remaining,2
940849440,4/26/2016 18:56:00,true,1969504143,,4/26/2016 18:55:19,false,neodev,1.0,29879245,RUS,69,Smolensk,37.144.124.118,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I&#39;m attempting to make it to Katie&#39;s office hours that end at 11:50 but wanted to post this question here if I don&#39;t make it.</p>
<p></p>
<p>My partner and I have been trying to get the &#34;No More Rows Remaining&#34; problem resolved on Crowdflower for about a day now. åÊWe have tried using both our accounts and get that error, and I even made another account and we added it to the team (as per someone&#39;s suggestion on Piazza), but still get that error when trying to launch. åÊI submitted a ticket, but Crowdflower still hasn&#39;t responded.</p>
<p></p>",No More Rows Remaining,2
940849440,4/26/2016 19:02:49,true,1969507174,,4/26/2016 18:44:58,false,neodev,1.0,11172894,IND,28,Champdani,117.194.5.117,0,0,In hw5 we only had 2 workers judge each url so we cant really figure out a majority vote from our data. What do we do?,Majority Votes,"<p>Hi Ellie, I recorded my video with Quicktime but there is no option for &#34;file save as&#34; and exporting does not seem to work. The progress bar is just stagnant. What should I do?</p>
<p></p>
<p>Thanks!</p>",Quicktime,"Hi Ellie,

It was stated in lecture today that you will be going over Assignment 7 during lecture on Friday. I can&#39;t make the lecture because I will be leaving for NY to get my passport renewed Friday morning. I was just wondering if this session would be recorded?",Review Session on Friday 3/18,"<p>I&#39;m running the get_clean_text function and it has almost taken me 2 hours at this point (on my local computer, not biglab). Is it stuck somewhere? Not sure what to think. I feel as if the character decoding shouldn&#39;t be causing this.åÊ<img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/ikjdw92f8b8r/Screen_Shot_20160212_at_2.37.58_AM.png"" /></p>",get_clean_text taking very long,"<p>The questionnaire says to &#34;Find an example of an article that the classifier wrongly classifies as gun violence, but which is actually not gun violence (a &#34;false positive&#34;),&#34; and it looks like there&#39;s a function stub, <tt>get_misclassified_examples</tt>, that we&#39;re supposed to fill out to do this. However, I&#39;m having trouble figuring out how exactly we can get the misclassified examples using just the LogisticRegression object ÛÓ we can test the overall accuracy using the <tt>score()</tt> function, but I don&#39;t see any functions that would tell us whether or not something is right for a specific example. Are we supposed to use theåÊLogisticRegression functions to do this question?</p>",get_misclassified_examples,"<p>I&#39;m attempting to make it to Katie&#39;s office hours that end at 11:50 but wanted to post this question here if I don&#39;t make it.</p>
<p></p>
<p>My partner and I have been trying to get the &#34;No More Rows Remaining&#34; problem resolved on Crowdflower for about a day now. åÊWe have tried using both our accounts and get that error, and I even made another account and we added it to the team (as per someone&#39;s suggestion on Piazza), but still get that error when trying to launch. åÊI submitted a ticket, but Crowdflower still hasn&#39;t responded.</p>
<p></p>",No More Rows Remaining,2
940849494,4/26/2016 15:10:31,true,1969362382,,4/26/2016 15:08:05,false,tremorgames,1.0,32635967,LTU,60,Panevezys,78.63.38.165,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p>I&#39;m unable to complete the survey because attempting to visit the URL <a href=""http://www.crowd-workers.com/track"">www.crowd-workers.com/track</a>åÊbrings me to a 404 Error.</p>",www.crowd-workers.com/track ERROR 404,"<p>I keep getting the following error when I try to runåÊ</p>
<p>cat list_of_urls.txt | python get_clean_text.py &gt; articles_and_urls.txt and I&#39;m not sure why. Any ideas?åÊ</p>
<p></p>
<pre>Traceback (most recent call last):
  File &#34;get_clean_text.py&#34;, line 25, in &lt;module&gt;
    txt = get_text(url)
  File &#34;get_clean_text.py&#34;, line 14, in get_text
    soup = BeautifulSoup(urllib2.urlopen(url).read(), &#39;html.parser&#39;)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 127, in urlopen
    return _opener.open(url, data, timeout)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 404, in open
    response = self._open(req, data)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 422, in _open
    &#39;_open&#39;, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1214, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1187, in do_open
    r = h.getresponse(buffering=True)
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 1067, in getresponse
    response.begin()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 409, in begin
    version, status, reason = self._read_status()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 365, in _read_status
    line = self.fp.readline(_MAXLINE &#43; 1)
  File &#34;/usr/lib64/python2.7/socket.py&#34;, line 476, in readline
    data = self._sock.recv(self._rbufsize)
socket.error: [Errno 104] Connection reset by peer</pre>
<p></p>",Connection Reset by Peer error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,4
940849494,4/26/2016 15:20:28,true,1969377832,,4/26/2016 15:19:15,false,clixsense,1.0,24287706,TWN,04,Keelung,61.231.195.173,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p>I&#39;m unable to complete the survey because attempting to visit the URL <a href=""http://www.crowd-workers.com/track"">www.crowd-workers.com/track</a>åÊbrings me to a 404 Error.</p>",www.crowd-workers.com/track ERROR 404,"<p>I keep getting the following error when I try to runåÊ</p>
<p>cat list_of_urls.txt | python get_clean_text.py &gt; articles_and_urls.txt and I&#39;m not sure why. Any ideas?åÊ</p>
<p></p>
<pre>Traceback (most recent call last):
  File &#34;get_clean_text.py&#34;, line 25, in &lt;module&gt;
    txt = get_text(url)
  File &#34;get_clean_text.py&#34;, line 14, in get_text
    soup = BeautifulSoup(urllib2.urlopen(url).read(), &#39;html.parser&#39;)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 127, in urlopen
    return _opener.open(url, data, timeout)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 404, in open
    response = self._open(req, data)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 422, in _open
    &#39;_open&#39;, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1214, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1187, in do_open
    r = h.getresponse(buffering=True)
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 1067, in getresponse
    response.begin()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 409, in begin
    version, status, reason = self._read_status()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 365, in _read_status
    line = self.fp.readline(_MAXLINE &#43; 1)
  File &#34;/usr/lib64/python2.7/socket.py&#34;, line 476, in readline
    data = self._sock.recv(self._rbufsize)
socket.error: [Errno 104] Connection reset by peer</pre>
<p></p>",Connection Reset by Peer error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,4
940849494,4/26/2016 15:20:29,true,1969377833,,4/26/2016 15:20:10,false,neodev,1.0,19132694,LKA,36,Colombo,123.231.124.170,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p>I&#39;m unable to complete the survey because attempting to visit the URL <a href=""http://www.crowd-workers.com/track"">www.crowd-workers.com/track</a>åÊbrings me to a 404 Error.</p>",www.crowd-workers.com/track ERROR 404,"<p>I keep getting the following error when I try to runåÊ</p>
<p>cat list_of_urls.txt | python get_clean_text.py &gt; articles_and_urls.txt and I&#39;m not sure why. Any ideas?åÊ</p>
<p></p>
<pre>Traceback (most recent call last):
  File &#34;get_clean_text.py&#34;, line 25, in &lt;module&gt;
    txt = get_text(url)
  File &#34;get_clean_text.py&#34;, line 14, in get_text
    soup = BeautifulSoup(urllib2.urlopen(url).read(), &#39;html.parser&#39;)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 127, in urlopen
    return _opener.open(url, data, timeout)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 404, in open
    response = self._open(req, data)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 422, in _open
    &#39;_open&#39;, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1214, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1187, in do_open
    r = h.getresponse(buffering=True)
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 1067, in getresponse
    response.begin()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 409, in begin
    version, status, reason = self._read_status()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 365, in _read_status
    line = self.fp.readline(_MAXLINE &#43; 1)
  File &#34;/usr/lib64/python2.7/socket.py&#34;, line 476, in readline
    data = self._sock.recv(self._rbufsize)
socket.error: [Errno 104] Connection reset by peer</pre>
<p></p>",Connection Reset by Peer error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,4
940849494,4/26/2016 15:21:05,true,1969378829,,4/26/2016 15:15:48,false,elite,1.0,30280423,ITA,15,Siracusa,151.54.84.121,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p>I&#39;m unable to complete the survey because attempting to visit the URL <a href=""http://www.crowd-workers.com/track"">www.crowd-workers.com/track</a>åÊbrings me to a 404 Error.</p>",www.crowd-workers.com/track ERROR 404,"<p>I keep getting the following error when I try to runåÊ</p>
<p>cat list_of_urls.txt | python get_clean_text.py &gt; articles_and_urls.txt and I&#39;m not sure why. Any ideas?åÊ</p>
<p></p>
<pre>Traceback (most recent call last):
  File &#34;get_clean_text.py&#34;, line 25, in &lt;module&gt;
    txt = get_text(url)
  File &#34;get_clean_text.py&#34;, line 14, in get_text
    soup = BeautifulSoup(urllib2.urlopen(url).read(), &#39;html.parser&#39;)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 127, in urlopen
    return _opener.open(url, data, timeout)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 404, in open
    response = self._open(req, data)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 422, in _open
    &#39;_open&#39;, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1214, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1187, in do_open
    r = h.getresponse(buffering=True)
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 1067, in getresponse
    response.begin()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 409, in begin
    version, status, reason = self._read_status()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 365, in _read_status
    line = self.fp.readline(_MAXLINE &#43; 1)
  File &#34;/usr/lib64/python2.7/socket.py&#34;, line 476, in readline
    data = self._sock.recv(self._rbufsize)
socket.error: [Errno 104] Connection reset by peer</pre>
<p></p>",Connection Reset by Peer error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,4
940849494,4/26/2016 15:21:31,true,1969379545,,4/26/2016 15:12:37,false,clixsense,0.8889,36052512,PHL,F2,Quezon City,49.149.150.150,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p>I&#39;m unable to complete the survey because attempting to visit the URL <a href=""http://www.crowd-workers.com/track"">www.crowd-workers.com/track</a>åÊbrings me to a 404 Error.</p>",www.crowd-workers.com/track ERROR 404,"<p>I keep getting the following error when I try to runåÊ</p>
<p>cat list_of_urls.txt | python get_clean_text.py &gt; articles_and_urls.txt and I&#39;m not sure why. Any ideas?åÊ</p>
<p></p>
<pre>Traceback (most recent call last):
  File &#34;get_clean_text.py&#34;, line 25, in &lt;module&gt;
    txt = get_text(url)
  File &#34;get_clean_text.py&#34;, line 14, in get_text
    soup = BeautifulSoup(urllib2.urlopen(url).read(), &#39;html.parser&#39;)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 127, in urlopen
    return _opener.open(url, data, timeout)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 404, in open
    response = self._open(req, data)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 422, in _open
    &#39;_open&#39;, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1214, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1187, in do_open
    r = h.getresponse(buffering=True)
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 1067, in getresponse
    response.begin()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 409, in begin
    version, status, reason = self._read_status()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 365, in _read_status
    line = self.fp.readline(_MAXLINE &#43; 1)
  File &#34;/usr/lib64/python2.7/socket.py&#34;, line 476, in readline
    data = self._sock.recv(self._rbufsize)
socket.error: [Errno 104] Connection reset by peer</pre>
<p></p>",Connection Reset by Peer error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,4
940849494,4/26/2016 15:26:30,true,1969388020,,4/26/2016 15:24:24,false,clixsense,1.0,7837812,SRB,00,Belgrade,79.101.254.233,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p>I&#39;m unable to complete the survey because attempting to visit the URL <a href=""http://www.crowd-workers.com/track"">www.crowd-workers.com/track</a>åÊbrings me to a 404 Error.</p>",www.crowd-workers.com/track ERROR 404,"<p>I keep getting the following error when I try to runåÊ</p>
<p>cat list_of_urls.txt | python get_clean_text.py &gt; articles_and_urls.txt and I&#39;m not sure why. Any ideas?åÊ</p>
<p></p>
<pre>Traceback (most recent call last):
  File &#34;get_clean_text.py&#34;, line 25, in &lt;module&gt;
    txt = get_text(url)
  File &#34;get_clean_text.py&#34;, line 14, in get_text
    soup = BeautifulSoup(urllib2.urlopen(url).read(), &#39;html.parser&#39;)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 127, in urlopen
    return _opener.open(url, data, timeout)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 404, in open
    response = self._open(req, data)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 422, in _open
    &#39;_open&#39;, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1214, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1187, in do_open
    r = h.getresponse(buffering=True)
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 1067, in getresponse
    response.begin()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 409, in begin
    version, status, reason = self._read_status()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 365, in _read_status
    line = self.fp.readline(_MAXLINE &#43; 1)
  File &#34;/usr/lib64/python2.7/socket.py&#34;, line 476, in readline
    data = self._sock.recv(self._rbufsize)
socket.error: [Errno 104] Connection reset by peer</pre>
<p></p>",Connection Reset by Peer error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,4
940849494,4/26/2016 15:26:56,true,1969388625,,4/26/2016 15:26:14,false,instagc,0.8889,13581319,USA,IL,Waltonville,208.70.36.12,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p>I&#39;m unable to complete the survey because attempting to visit the URL <a href=""http://www.crowd-workers.com/track"">www.crowd-workers.com/track</a>åÊbrings me to a 404 Error.</p>",www.crowd-workers.com/track ERROR 404,"<p>I keep getting the following error when I try to runåÊ</p>
<p>cat list_of_urls.txt | python get_clean_text.py &gt; articles_and_urls.txt and I&#39;m not sure why. Any ideas?åÊ</p>
<p></p>
<pre>Traceback (most recent call last):
  File &#34;get_clean_text.py&#34;, line 25, in &lt;module&gt;
    txt = get_text(url)
  File &#34;get_clean_text.py&#34;, line 14, in get_text
    soup = BeautifulSoup(urllib2.urlopen(url).read(), &#39;html.parser&#39;)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 127, in urlopen
    return _opener.open(url, data, timeout)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 404, in open
    response = self._open(req, data)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 422, in _open
    &#39;_open&#39;, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1214, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1187, in do_open
    r = h.getresponse(buffering=True)
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 1067, in getresponse
    response.begin()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 409, in begin
    version, status, reason = self._read_status()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 365, in _read_status
    line = self.fp.readline(_MAXLINE &#43; 1)
  File &#34;/usr/lib64/python2.7/socket.py&#34;, line 476, in readline
    data = self._sock.recv(self._rbufsize)
socket.error: [Errno 104] Connection reset by peer</pre>
<p></p>",Connection Reset by Peer error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,4
940849494,4/26/2016 15:36:01,true,1969392986,,4/26/2016 15:30:10,false,neodev,1.0,13396426,VEN,15,Santa Teresa,190.38.163.149,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p>I&#39;m unable to complete the survey because attempting to visit the URL <a href=""http://www.crowd-workers.com/track"">www.crowd-workers.com/track</a>åÊbrings me to a 404 Error.</p>",www.crowd-workers.com/track ERROR 404,"<p>I keep getting the following error when I try to runåÊ</p>
<p>cat list_of_urls.txt | python get_clean_text.py &gt; articles_and_urls.txt and I&#39;m not sure why. Any ideas?åÊ</p>
<p></p>
<pre>Traceback (most recent call last):
  File &#34;get_clean_text.py&#34;, line 25, in &lt;module&gt;
    txt = get_text(url)
  File &#34;get_clean_text.py&#34;, line 14, in get_text
    soup = BeautifulSoup(urllib2.urlopen(url).read(), &#39;html.parser&#39;)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 127, in urlopen
    return _opener.open(url, data, timeout)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 404, in open
    response = self._open(req, data)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 422, in _open
    &#39;_open&#39;, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1214, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1187, in do_open
    r = h.getresponse(buffering=True)
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 1067, in getresponse
    response.begin()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 409, in begin
    version, status, reason = self._read_status()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 365, in _read_status
    line = self.fp.readline(_MAXLINE &#43; 1)
  File &#34;/usr/lib64/python2.7/socket.py&#34;, line 476, in readline
    data = self._sock.recv(self._rbufsize)
socket.error: [Errno 104] Connection reset by peer</pre>
<p></p>",Connection Reset by Peer error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,4
940849494,4/26/2016 15:40:37,true,1969395292,,4/26/2016 15:34:58,false,neodev,0.7778,32569659,USA,MN,Minneapolis,97.127.88.224,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p>I&#39;m unable to complete the survey because attempting to visit the URL <a href=""http://www.crowd-workers.com/track"">www.crowd-workers.com/track</a>åÊbrings me to a 404 Error.</p>",www.crowd-workers.com/track ERROR 404,"<p>I keep getting the following error when I try to runåÊ</p>
<p>cat list_of_urls.txt | python get_clean_text.py &gt; articles_and_urls.txt and I&#39;m not sure why. Any ideas?åÊ</p>
<p></p>
<pre>Traceback (most recent call last):
  File &#34;get_clean_text.py&#34;, line 25, in &lt;module&gt;
    txt = get_text(url)
  File &#34;get_clean_text.py&#34;, line 14, in get_text
    soup = BeautifulSoup(urllib2.urlopen(url).read(), &#39;html.parser&#39;)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 127, in urlopen
    return _opener.open(url, data, timeout)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 404, in open
    response = self._open(req, data)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 422, in _open
    &#39;_open&#39;, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1214, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1187, in do_open
    r = h.getresponse(buffering=True)
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 1067, in getresponse
    response.begin()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 409, in begin
    version, status, reason = self._read_status()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 365, in _read_status
    line = self.fp.readline(_MAXLINE &#43; 1)
  File &#34;/usr/lib64/python2.7/socket.py&#34;, line 476, in readline
    data = self._sock.recv(self._rbufsize)
socket.error: [Errno 104] Connection reset by peer</pre>
<p></p>",Connection Reset by Peer error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,4
940849494,4/26/2016 15:45:34,true,1969397175,,4/26/2016 15:37:11,false,elite,0.8889,36575101,IND,07,New Delhi,112.196.144.2,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p>I&#39;m unable to complete the survey because attempting to visit the URL <a href=""http://www.crowd-workers.com/track"">www.crowd-workers.com/track</a>åÊbrings me to a 404 Error.</p>",www.crowd-workers.com/track ERROR 404,"<p>I keep getting the following error when I try to runåÊ</p>
<p>cat list_of_urls.txt | python get_clean_text.py &gt; articles_and_urls.txt and I&#39;m not sure why. Any ideas?åÊ</p>
<p></p>
<pre>Traceback (most recent call last):
  File &#34;get_clean_text.py&#34;, line 25, in &lt;module&gt;
    txt = get_text(url)
  File &#34;get_clean_text.py&#34;, line 14, in get_text
    soup = BeautifulSoup(urllib2.urlopen(url).read(), &#39;html.parser&#39;)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 127, in urlopen
    return _opener.open(url, data, timeout)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 404, in open
    response = self._open(req, data)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 422, in _open
    &#39;_open&#39;, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1214, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1187, in do_open
    r = h.getresponse(buffering=True)
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 1067, in getresponse
    response.begin()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 409, in begin
    version, status, reason = self._read_status()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 365, in _read_status
    line = self.fp.readline(_MAXLINE &#43; 1)
  File &#34;/usr/lib64/python2.7/socket.py&#34;, line 476, in readline
    data = self._sock.recv(self._rbufsize)
socket.error: [Errno 104] Connection reset by peer</pre>
<p></p>",Connection Reset by Peer error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,4
940849494,4/26/2016 15:53:45,true,1969400806,,4/26/2016 15:51:13,false,neodev,1.0,28875937,PAK,08,Islamabad,119.153.105.50,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p>I&#39;m unable to complete the survey because attempting to visit the URL <a href=""http://www.crowd-workers.com/track"">www.crowd-workers.com/track</a>åÊbrings me to a 404 Error.</p>",www.crowd-workers.com/track ERROR 404,"<p>I keep getting the following error when I try to runåÊ</p>
<p>cat list_of_urls.txt | python get_clean_text.py &gt; articles_and_urls.txt and I&#39;m not sure why. Any ideas?åÊ</p>
<p></p>
<pre>Traceback (most recent call last):
  File &#34;get_clean_text.py&#34;, line 25, in &lt;module&gt;
    txt = get_text(url)
  File &#34;get_clean_text.py&#34;, line 14, in get_text
    soup = BeautifulSoup(urllib2.urlopen(url).read(), &#39;html.parser&#39;)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 127, in urlopen
    return _opener.open(url, data, timeout)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 404, in open
    response = self._open(req, data)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 422, in _open
    &#39;_open&#39;, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1214, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1187, in do_open
    r = h.getresponse(buffering=True)
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 1067, in getresponse
    response.begin()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 409, in begin
    version, status, reason = self._read_status()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 365, in _read_status
    line = self.fp.readline(_MAXLINE &#43; 1)
  File &#34;/usr/lib64/python2.7/socket.py&#34;, line 476, in readline
    data = self._sock.recv(self._rbufsize)
socket.error: [Errno 104] Connection reset by peer</pre>
<p></p>",Connection Reset by Peer error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,4
940849494,4/26/2016 15:54:59,true,1969401301,,4/26/2016 15:53:17,false,elite,1.0,33243069,IND,10,Faridabad,116.203.79.150,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p>I&#39;m unable to complete the survey because attempting to visit the URL <a href=""http://www.crowd-workers.com/track"">www.crowd-workers.com/track</a>åÊbrings me to a 404 Error.</p>",www.crowd-workers.com/track ERROR 404,"<p>I keep getting the following error when I try to runåÊ</p>
<p>cat list_of_urls.txt | python get_clean_text.py &gt; articles_and_urls.txt and I&#39;m not sure why. Any ideas?åÊ</p>
<p></p>
<pre>Traceback (most recent call last):
  File &#34;get_clean_text.py&#34;, line 25, in &lt;module&gt;
    txt = get_text(url)
  File &#34;get_clean_text.py&#34;, line 14, in get_text
    soup = BeautifulSoup(urllib2.urlopen(url).read(), &#39;html.parser&#39;)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 127, in urlopen
    return _opener.open(url, data, timeout)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 404, in open
    response = self._open(req, data)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 422, in _open
    &#39;_open&#39;, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1214, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1187, in do_open
    r = h.getresponse(buffering=True)
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 1067, in getresponse
    response.begin()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 409, in begin
    version, status, reason = self._read_status()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 365, in _read_status
    line = self.fp.readline(_MAXLINE &#43; 1)
  File &#34;/usr/lib64/python2.7/socket.py&#34;, line 476, in readline
    data = self._sock.recv(self._rbufsize)
socket.error: [Errno 104] Connection reset by peer</pre>
<p></p>",Connection Reset by Peer error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,4
940849494,4/26/2016 15:56:09,true,1969401801,,4/26/2016 15:45:42,false,clixsense,1.0,21875134,GBR,H9,London,87.112.158.81,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p>I&#39;m unable to complete the survey because attempting to visit the URL <a href=""http://www.crowd-workers.com/track"">www.crowd-workers.com/track</a>åÊbrings me to a 404 Error.</p>",www.crowd-workers.com/track ERROR 404,"<p>I keep getting the following error when I try to runåÊ</p>
<p>cat list_of_urls.txt | python get_clean_text.py &gt; articles_and_urls.txt and I&#39;m not sure why. Any ideas?åÊ</p>
<p></p>
<pre>Traceback (most recent call last):
  File &#34;get_clean_text.py&#34;, line 25, in &lt;module&gt;
    txt = get_text(url)
  File &#34;get_clean_text.py&#34;, line 14, in get_text
    soup = BeautifulSoup(urllib2.urlopen(url).read(), &#39;html.parser&#39;)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 127, in urlopen
    return _opener.open(url, data, timeout)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 404, in open
    response = self._open(req, data)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 422, in _open
    &#39;_open&#39;, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1214, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1187, in do_open
    r = h.getresponse(buffering=True)
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 1067, in getresponse
    response.begin()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 409, in begin
    version, status, reason = self._read_status()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 365, in _read_status
    line = self.fp.readline(_MAXLINE &#43; 1)
  File &#34;/usr/lib64/python2.7/socket.py&#34;, line 476, in readline
    data = self._sock.recv(self._rbufsize)
socket.error: [Errno 104] Connection reset by peer</pre>
<p></p>",Connection Reset by Peer error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,4
940849494,4/26/2016 15:56:15,true,1969401849,,4/26/2016 15:48:57,false,neodev,0.8889,21971187,TTO,08,Valsayn,190.213.132.190,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p>I&#39;m unable to complete the survey because attempting to visit the URL <a href=""http://www.crowd-workers.com/track"">www.crowd-workers.com/track</a>åÊbrings me to a 404 Error.</p>",www.crowd-workers.com/track ERROR 404,"<p>I keep getting the following error when I try to runåÊ</p>
<p>cat list_of_urls.txt | python get_clean_text.py &gt; articles_and_urls.txt and I&#39;m not sure why. Any ideas?åÊ</p>
<p></p>
<pre>Traceback (most recent call last):
  File &#34;get_clean_text.py&#34;, line 25, in &lt;module&gt;
    txt = get_text(url)
  File &#34;get_clean_text.py&#34;, line 14, in get_text
    soup = BeautifulSoup(urllib2.urlopen(url).read(), &#39;html.parser&#39;)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 127, in urlopen
    return _opener.open(url, data, timeout)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 404, in open
    response = self._open(req, data)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 422, in _open
    &#39;_open&#39;, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1214, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1187, in do_open
    r = h.getresponse(buffering=True)
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 1067, in getresponse
    response.begin()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 409, in begin
    version, status, reason = self._read_status()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 365, in _read_status
    line = self.fp.readline(_MAXLINE &#43; 1)
  File &#34;/usr/lib64/python2.7/socket.py&#34;, line 476, in readline
    data = self._sock.recv(self._rbufsize)
socket.error: [Errno 104] Connection reset by peer</pre>
<p></p>",Connection Reset by Peer error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,4
940849494,4/26/2016 16:01:09,true,1969404223,,4/26/2016 16:00:06,false,personaly,1.0,33663352,ARG,01,Mar Del Plata,181.168.213.227,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p>I&#39;m unable to complete the survey because attempting to visit the URL <a href=""http://www.crowd-workers.com/track"">www.crowd-workers.com/track</a>åÊbrings me to a 404 Error.</p>",www.crowd-workers.com/track ERROR 404,"<p>I keep getting the following error when I try to runåÊ</p>
<p>cat list_of_urls.txt | python get_clean_text.py &gt; articles_and_urls.txt and I&#39;m not sure why. Any ideas?åÊ</p>
<p></p>
<pre>Traceback (most recent call last):
  File &#34;get_clean_text.py&#34;, line 25, in &lt;module&gt;
    txt = get_text(url)
  File &#34;get_clean_text.py&#34;, line 14, in get_text
    soup = BeautifulSoup(urllib2.urlopen(url).read(), &#39;html.parser&#39;)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 127, in urlopen
    return _opener.open(url, data, timeout)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 404, in open
    response = self._open(req, data)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 422, in _open
    &#39;_open&#39;, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1214, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1187, in do_open
    r = h.getresponse(buffering=True)
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 1067, in getresponse
    response.begin()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 409, in begin
    version, status, reason = self._read_status()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 365, in _read_status
    line = self.fp.readline(_MAXLINE &#43; 1)
  File &#34;/usr/lib64/python2.7/socket.py&#34;, line 476, in readline
    data = self._sock.recv(self._rbufsize)
socket.error: [Errno 104] Connection reset by peer</pre>
<p></p>",Connection Reset by Peer error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,4
940849494,4/26/2016 16:03:35,true,1969405534,,4/26/2016 15:48:02,false,clixsense,0.8889,8057247,PRT,17,Póvoa De Varzim,144.64.25.68,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p>I&#39;m unable to complete the survey because attempting to visit the URL <a href=""http://www.crowd-workers.com/track"">www.crowd-workers.com/track</a>åÊbrings me to a 404 Error.</p>",www.crowd-workers.com/track ERROR 404,"<p>I keep getting the following error when I try to runåÊ</p>
<p>cat list_of_urls.txt | python get_clean_text.py &gt; articles_and_urls.txt and I&#39;m not sure why. Any ideas?åÊ</p>
<p></p>
<pre>Traceback (most recent call last):
  File &#34;get_clean_text.py&#34;, line 25, in &lt;module&gt;
    txt = get_text(url)
  File &#34;get_clean_text.py&#34;, line 14, in get_text
    soup = BeautifulSoup(urllib2.urlopen(url).read(), &#39;html.parser&#39;)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 127, in urlopen
    return _opener.open(url, data, timeout)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 404, in open
    response = self._open(req, data)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 422, in _open
    &#39;_open&#39;, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1214, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1187, in do_open
    r = h.getresponse(buffering=True)
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 1067, in getresponse
    response.begin()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 409, in begin
    version, status, reason = self._read_status()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 365, in _read_status
    line = self.fp.readline(_MAXLINE &#43; 1)
  File &#34;/usr/lib64/python2.7/socket.py&#34;, line 476, in readline
    data = self._sock.recv(self._rbufsize)
socket.error: [Errno 104] Connection reset by peer</pre>
<p></p>",Connection Reset by Peer error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,4
940849494,4/26/2016 16:15:36,true,1969411461,,4/26/2016 16:12:15,false,elite,1.0,30128662,BGR,50,Pleven,212.233.177.195,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p>I&#39;m unable to complete the survey because attempting to visit the URL <a href=""http://www.crowd-workers.com/track"">www.crowd-workers.com/track</a>åÊbrings me to a 404 Error.</p>",www.crowd-workers.com/track ERROR 404,"<p>I keep getting the following error when I try to runåÊ</p>
<p>cat list_of_urls.txt | python get_clean_text.py &gt; articles_and_urls.txt and I&#39;m not sure why. Any ideas?åÊ</p>
<p></p>
<pre>Traceback (most recent call last):
  File &#34;get_clean_text.py&#34;, line 25, in &lt;module&gt;
    txt = get_text(url)
  File &#34;get_clean_text.py&#34;, line 14, in get_text
    soup = BeautifulSoup(urllib2.urlopen(url).read(), &#39;html.parser&#39;)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 127, in urlopen
    return _opener.open(url, data, timeout)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 404, in open
    response = self._open(req, data)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 422, in _open
    &#39;_open&#39;, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1214, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1187, in do_open
    r = h.getresponse(buffering=True)
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 1067, in getresponse
    response.begin()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 409, in begin
    version, status, reason = self._read_status()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 365, in _read_status
    line = self.fp.readline(_MAXLINE &#43; 1)
  File &#34;/usr/lib64/python2.7/socket.py&#34;, line 476, in readline
    data = self._sock.recv(self._rbufsize)
socket.error: [Errno 104] Connection reset by peer</pre>
<p></p>",Connection Reset by Peer error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,4
940849494,4/26/2016 16:22:17,true,1969417139,,4/26/2016 16:18:27,false,neodev,1.0,29175140,VEN,25,Caracas,190.72.125.134,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p>I&#39;m unable to complete the survey because attempting to visit the URL <a href=""http://www.crowd-workers.com/track"">www.crowd-workers.com/track</a>åÊbrings me to a 404 Error.</p>",www.crowd-workers.com/track ERROR 404,"<p>I keep getting the following error when I try to runåÊ</p>
<p>cat list_of_urls.txt | python get_clean_text.py &gt; articles_and_urls.txt and I&#39;m not sure why. Any ideas?åÊ</p>
<p></p>
<pre>Traceback (most recent call last):
  File &#34;get_clean_text.py&#34;, line 25, in &lt;module&gt;
    txt = get_text(url)
  File &#34;get_clean_text.py&#34;, line 14, in get_text
    soup = BeautifulSoup(urllib2.urlopen(url).read(), &#39;html.parser&#39;)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 127, in urlopen
    return _opener.open(url, data, timeout)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 404, in open
    response = self._open(req, data)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 422, in _open
    &#39;_open&#39;, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1214, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1187, in do_open
    r = h.getresponse(buffering=True)
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 1067, in getresponse
    response.begin()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 409, in begin
    version, status, reason = self._read_status()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 365, in _read_status
    line = self.fp.readline(_MAXLINE &#43; 1)
  File &#34;/usr/lib64/python2.7/socket.py&#34;, line 476, in readline
    data = self._sock.recv(self._rbufsize)
socket.error: [Errno 104] Connection reset by peer</pre>
<p></p>",Connection Reset by Peer error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,4
940849494,4/26/2016 16:44:00,true,1969431828,,4/26/2016 16:41:32,false,clixsense,1.0,6329782,IDN,10,Sleman,202.67.40.222,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p>I&#39;m unable to complete the survey because attempting to visit the URL <a href=""http://www.crowd-workers.com/track"">www.crowd-workers.com/track</a>åÊbrings me to a 404 Error.</p>",www.crowd-workers.com/track ERROR 404,"<p>I keep getting the following error when I try to runåÊ</p>
<p>cat list_of_urls.txt | python get_clean_text.py &gt; articles_and_urls.txt and I&#39;m not sure why. Any ideas?åÊ</p>
<p></p>
<pre>Traceback (most recent call last):
  File &#34;get_clean_text.py&#34;, line 25, in &lt;module&gt;
    txt = get_text(url)
  File &#34;get_clean_text.py&#34;, line 14, in get_text
    soup = BeautifulSoup(urllib2.urlopen(url).read(), &#39;html.parser&#39;)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 127, in urlopen
    return _opener.open(url, data, timeout)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 404, in open
    response = self._open(req, data)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 422, in _open
    &#39;_open&#39;, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1214, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1187, in do_open
    r = h.getresponse(buffering=True)
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 1067, in getresponse
    response.begin()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 409, in begin
    version, status, reason = self._read_status()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 365, in _read_status
    line = self.fp.readline(_MAXLINE &#43; 1)
  File &#34;/usr/lib64/python2.7/socket.py&#34;, line 476, in readline
    data = self._sock.recv(self._rbufsize)
socket.error: [Errno 104] Connection reset by peer</pre>
<p></p>",Connection Reset by Peer error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,4
940849494,4/26/2016 16:56:40,true,1969439476,,4/26/2016 16:53:35,false,clixsense,1.0,21408115,IDN,07,Semarang,36.79.23.180,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p>I&#39;m unable to complete the survey because attempting to visit the URL <a href=""http://www.crowd-workers.com/track"">www.crowd-workers.com/track</a>åÊbrings me to a 404 Error.</p>",www.crowd-workers.com/track ERROR 404,"<p>I keep getting the following error when I try to runåÊ</p>
<p>cat list_of_urls.txt | python get_clean_text.py &gt; articles_and_urls.txt and I&#39;m not sure why. Any ideas?åÊ</p>
<p></p>
<pre>Traceback (most recent call last):
  File &#34;get_clean_text.py&#34;, line 25, in &lt;module&gt;
    txt = get_text(url)
  File &#34;get_clean_text.py&#34;, line 14, in get_text
    soup = BeautifulSoup(urllib2.urlopen(url).read(), &#39;html.parser&#39;)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 127, in urlopen
    return _opener.open(url, data, timeout)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 404, in open
    response = self._open(req, data)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 422, in _open
    &#39;_open&#39;, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1214, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1187, in do_open
    r = h.getresponse(buffering=True)
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 1067, in getresponse
    response.begin()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 409, in begin
    version, status, reason = self._read_status()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 365, in _read_status
    line = self.fp.readline(_MAXLINE &#43; 1)
  File &#34;/usr/lib64/python2.7/socket.py&#34;, line 476, in readline
    data = self._sock.recv(self._rbufsize)
socket.error: [Errno 104] Connection reset by peer</pre>
<p></p>",Connection Reset by Peer error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,4
940849494,4/26/2016 17:05:47,true,1969445080,,4/26/2016 17:04:18,false,neodev,1.0,36167043,GBR,G6,Hull,77.86.101.69,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p>I&#39;m unable to complete the survey because attempting to visit the URL <a href=""http://www.crowd-workers.com/track"">www.crowd-workers.com/track</a>åÊbrings me to a 404 Error.</p>",www.crowd-workers.com/track ERROR 404,"<p>I keep getting the following error when I try to runåÊ</p>
<p>cat list_of_urls.txt | python get_clean_text.py &gt; articles_and_urls.txt and I&#39;m not sure why. Any ideas?åÊ</p>
<p></p>
<pre>Traceback (most recent call last):
  File &#34;get_clean_text.py&#34;, line 25, in &lt;module&gt;
    txt = get_text(url)
  File &#34;get_clean_text.py&#34;, line 14, in get_text
    soup = BeautifulSoup(urllib2.urlopen(url).read(), &#39;html.parser&#39;)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 127, in urlopen
    return _opener.open(url, data, timeout)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 404, in open
    response = self._open(req, data)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 422, in _open
    &#39;_open&#39;, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1214, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1187, in do_open
    r = h.getresponse(buffering=True)
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 1067, in getresponse
    response.begin()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 409, in begin
    version, status, reason = self._read_status()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 365, in _read_status
    line = self.fp.readline(_MAXLINE &#43; 1)
  File &#34;/usr/lib64/python2.7/socket.py&#34;, line 476, in readline
    data = self._sock.recv(self._rbufsize)
socket.error: [Errno 104] Connection reset by peer</pre>
<p></p>",Connection Reset by Peer error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,4
940849494,4/26/2016 17:08:43,true,1969446812,,4/26/2016 17:06:43,false,neodev,1.0,33973110,VEN,23,Maracaibo,186.94.238.104,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p>I&#39;m unable to complete the survey because attempting to visit the URL <a href=""http://www.crowd-workers.com/track"">www.crowd-workers.com/track</a>åÊbrings me to a 404 Error.</p>",www.crowd-workers.com/track ERROR 404,"<p>I keep getting the following error when I try to runåÊ</p>
<p>cat list_of_urls.txt | python get_clean_text.py &gt; articles_and_urls.txt and I&#39;m not sure why. Any ideas?åÊ</p>
<p></p>
<pre>Traceback (most recent call last):
  File &#34;get_clean_text.py&#34;, line 25, in &lt;module&gt;
    txt = get_text(url)
  File &#34;get_clean_text.py&#34;, line 14, in get_text
    soup = BeautifulSoup(urllib2.urlopen(url).read(), &#39;html.parser&#39;)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 127, in urlopen
    return _opener.open(url, data, timeout)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 404, in open
    response = self._open(req, data)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 422, in _open
    &#39;_open&#39;, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1214, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1187, in do_open
    r = h.getresponse(buffering=True)
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 1067, in getresponse
    response.begin()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 409, in begin
    version, status, reason = self._read_status()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 365, in _read_status
    line = self.fp.readline(_MAXLINE &#43; 1)
  File &#34;/usr/lib64/python2.7/socket.py&#34;, line 476, in readline
    data = self._sock.recv(self._rbufsize)
socket.error: [Errno 104] Connection reset by peer</pre>
<p></p>",Connection Reset by Peer error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,4
940849494,4/26/2016 17:14:00,true,1969450327,,4/26/2016 17:10:16,false,elite,1.0,25411289,HRV,"","",31.147.119.175,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p>I&#39;m unable to complete the survey because attempting to visit the URL <a href=""http://www.crowd-workers.com/track"">www.crowd-workers.com/track</a>åÊbrings me to a 404 Error.</p>",www.crowd-workers.com/track ERROR 404,"<p>I keep getting the following error when I try to runåÊ</p>
<p>cat list_of_urls.txt | python get_clean_text.py &gt; articles_and_urls.txt and I&#39;m not sure why. Any ideas?åÊ</p>
<p></p>
<pre>Traceback (most recent call last):
  File &#34;get_clean_text.py&#34;, line 25, in &lt;module&gt;
    txt = get_text(url)
  File &#34;get_clean_text.py&#34;, line 14, in get_text
    soup = BeautifulSoup(urllib2.urlopen(url).read(), &#39;html.parser&#39;)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 127, in urlopen
    return _opener.open(url, data, timeout)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 404, in open
    response = self._open(req, data)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 422, in _open
    &#39;_open&#39;, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1214, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1187, in do_open
    r = h.getresponse(buffering=True)
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 1067, in getresponse
    response.begin()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 409, in begin
    version, status, reason = self._read_status()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 365, in _read_status
    line = self.fp.readline(_MAXLINE &#43; 1)
  File &#34;/usr/lib64/python2.7/socket.py&#34;, line 476, in readline
    data = self._sock.recv(self._rbufsize)
socket.error: [Errno 104] Connection reset by peer</pre>
<p></p>",Connection Reset by Peer error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,4
940849494,4/26/2016 17:32:14,true,1969460762,,4/26/2016 17:15:40,false,clixsense,0.8889,35338593,ITA,14,Cagliari,151.56.132.145,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p>I&#39;m unable to complete the survey because attempting to visit the URL <a href=""http://www.crowd-workers.com/track"">www.crowd-workers.com/track</a>åÊbrings me to a 404 Error.</p>",www.crowd-workers.com/track ERROR 404,"<p>I keep getting the following error when I try to runåÊ</p>
<p>cat list_of_urls.txt | python get_clean_text.py &gt; articles_and_urls.txt and I&#39;m not sure why. Any ideas?åÊ</p>
<p></p>
<pre>Traceback (most recent call last):
  File &#34;get_clean_text.py&#34;, line 25, in &lt;module&gt;
    txt = get_text(url)
  File &#34;get_clean_text.py&#34;, line 14, in get_text
    soup = BeautifulSoup(urllib2.urlopen(url).read(), &#39;html.parser&#39;)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 127, in urlopen
    return _opener.open(url, data, timeout)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 404, in open
    response = self._open(req, data)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 422, in _open
    &#39;_open&#39;, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1214, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1187, in do_open
    r = h.getresponse(buffering=True)
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 1067, in getresponse
    response.begin()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 409, in begin
    version, status, reason = self._read_status()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 365, in _read_status
    line = self.fp.readline(_MAXLINE &#43; 1)
  File &#34;/usr/lib64/python2.7/socket.py&#34;, line 476, in readline
    data = self._sock.recv(self._rbufsize)
socket.error: [Errno 104] Connection reset by peer</pre>
<p></p>",Connection Reset by Peer error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,4
940849494,4/26/2016 17:32:16,true,1969460788,,4/26/2016 17:31:58,false,neodev,0.8889,33131546,IDN,04,Jakarta,139.194.89.60,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p>I&#39;m unable to complete the survey because attempting to visit the URL <a href=""http://www.crowd-workers.com/track"">www.crowd-workers.com/track</a>åÊbrings me to a 404 Error.</p>",www.crowd-workers.com/track ERROR 404,"<p>I keep getting the following error when I try to runåÊ</p>
<p>cat list_of_urls.txt | python get_clean_text.py &gt; articles_and_urls.txt and I&#39;m not sure why. Any ideas?åÊ</p>
<p></p>
<pre>Traceback (most recent call last):
  File &#34;get_clean_text.py&#34;, line 25, in &lt;module&gt;
    txt = get_text(url)
  File &#34;get_clean_text.py&#34;, line 14, in get_text
    soup = BeautifulSoup(urllib2.urlopen(url).read(), &#39;html.parser&#39;)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 127, in urlopen
    return _opener.open(url, data, timeout)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 404, in open
    response = self._open(req, data)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 422, in _open
    &#39;_open&#39;, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1214, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1187, in do_open
    r = h.getresponse(buffering=True)
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 1067, in getresponse
    response.begin()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 409, in begin
    version, status, reason = self._read_status()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 365, in _read_status
    line = self.fp.readline(_MAXLINE &#43; 1)
  File &#34;/usr/lib64/python2.7/socket.py&#34;, line 476, in readline
    data = self._sock.recv(self._rbufsize)
socket.error: [Errno 104] Connection reset by peer</pre>
<p></p>",Connection Reset by Peer error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,4
940849494,4/26/2016 17:32:21,true,1969460817,,4/26/2016 17:30:36,false,neodev,1.0,11172894,IND,28,Champdani,117.194.5.117,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p>I&#39;m unable to complete the survey because attempting to visit the URL <a href=""http://www.crowd-workers.com/track"">www.crowd-workers.com/track</a>åÊbrings me to a 404 Error.</p>",www.crowd-workers.com/track ERROR 404,"<p>I keep getting the following error when I try to runåÊ</p>
<p>cat list_of_urls.txt | python get_clean_text.py &gt; articles_and_urls.txt and I&#39;m not sure why. Any ideas?åÊ</p>
<p></p>
<pre>Traceback (most recent call last):
  File &#34;get_clean_text.py&#34;, line 25, in &lt;module&gt;
    txt = get_text(url)
  File &#34;get_clean_text.py&#34;, line 14, in get_text
    soup = BeautifulSoup(urllib2.urlopen(url).read(), &#39;html.parser&#39;)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 127, in urlopen
    return _opener.open(url, data, timeout)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 404, in open
    response = self._open(req, data)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 422, in _open
    &#39;_open&#39;, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1214, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1187, in do_open
    r = h.getresponse(buffering=True)
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 1067, in getresponse
    response.begin()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 409, in begin
    version, status, reason = self._read_status()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 365, in _read_status
    line = self.fp.readline(_MAXLINE &#43; 1)
  File &#34;/usr/lib64/python2.7/socket.py&#34;, line 476, in readline
    data = self._sock.recv(self._rbufsize)
socket.error: [Errno 104] Connection reset by peer</pre>
<p></p>",Connection Reset by Peer error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,4
940849494,4/26/2016 17:36:01,true,1969462743,,4/26/2016 17:35:20,false,clixsense,1.0,30712378,ROU,21,Deva,79.119.241.200,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p>I&#39;m unable to complete the survey because attempting to visit the URL <a href=""http://www.crowd-workers.com/track"">www.crowd-workers.com/track</a>åÊbrings me to a 404 Error.</p>",www.crowd-workers.com/track ERROR 404,"<p>I keep getting the following error when I try to runåÊ</p>
<p>cat list_of_urls.txt | python get_clean_text.py &gt; articles_and_urls.txt and I&#39;m not sure why. Any ideas?åÊ</p>
<p></p>
<pre>Traceback (most recent call last):
  File &#34;get_clean_text.py&#34;, line 25, in &lt;module&gt;
    txt = get_text(url)
  File &#34;get_clean_text.py&#34;, line 14, in get_text
    soup = BeautifulSoup(urllib2.urlopen(url).read(), &#39;html.parser&#39;)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 127, in urlopen
    return _opener.open(url, data, timeout)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 404, in open
    response = self._open(req, data)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 422, in _open
    &#39;_open&#39;, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1214, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1187, in do_open
    r = h.getresponse(buffering=True)
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 1067, in getresponse
    response.begin()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 409, in begin
    version, status, reason = self._read_status()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 365, in _read_status
    line = self.fp.readline(_MAXLINE &#43; 1)
  File &#34;/usr/lib64/python2.7/socket.py&#34;, line 476, in readline
    data = self._sock.recv(self._rbufsize)
socket.error: [Errno 104] Connection reset by peer</pre>
<p></p>",Connection Reset by Peer error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,4
940849494,4/26/2016 17:39:39,true,1969464647,,4/26/2016 17:35:23,false,clixsense,1.0,35444326,BRA,07,Brasília,177.15.130.106,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p>I&#39;m unable to complete the survey because attempting to visit the URL <a href=""http://www.crowd-workers.com/track"">www.crowd-workers.com/track</a>åÊbrings me to a 404 Error.</p>",www.crowd-workers.com/track ERROR 404,"<p>I keep getting the following error when I try to runåÊ</p>
<p>cat list_of_urls.txt | python get_clean_text.py &gt; articles_and_urls.txt and I&#39;m not sure why. Any ideas?åÊ</p>
<p></p>
<pre>Traceback (most recent call last):
  File &#34;get_clean_text.py&#34;, line 25, in &lt;module&gt;
    txt = get_text(url)
  File &#34;get_clean_text.py&#34;, line 14, in get_text
    soup = BeautifulSoup(urllib2.urlopen(url).read(), &#39;html.parser&#39;)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 127, in urlopen
    return _opener.open(url, data, timeout)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 404, in open
    response = self._open(req, data)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 422, in _open
    &#39;_open&#39;, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1214, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1187, in do_open
    r = h.getresponse(buffering=True)
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 1067, in getresponse
    response.begin()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 409, in begin
    version, status, reason = self._read_status()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 365, in _read_status
    line = self.fp.readline(_MAXLINE &#43; 1)
  File &#34;/usr/lib64/python2.7/socket.py&#34;, line 476, in readline
    data = self._sock.recv(self._rbufsize)
socket.error: [Errno 104] Connection reset by peer</pre>
<p></p>",Connection Reset by Peer error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,4
940849494,4/26/2016 17:40:29,true,1969465253,,4/26/2016 17:39:38,false,neodev,0.8889,33568303,VEN,23,Cabimas,190.77.7.36,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p>I&#39;m unable to complete the survey because attempting to visit the URL <a href=""http://www.crowd-workers.com/track"">www.crowd-workers.com/track</a>åÊbrings me to a 404 Error.</p>",www.crowd-workers.com/track ERROR 404,"<p>I keep getting the following error when I try to runåÊ</p>
<p>cat list_of_urls.txt | python get_clean_text.py &gt; articles_and_urls.txt and I&#39;m not sure why. Any ideas?åÊ</p>
<p></p>
<pre>Traceback (most recent call last):
  File &#34;get_clean_text.py&#34;, line 25, in &lt;module&gt;
    txt = get_text(url)
  File &#34;get_clean_text.py&#34;, line 14, in get_text
    soup = BeautifulSoup(urllib2.urlopen(url).read(), &#39;html.parser&#39;)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 127, in urlopen
    return _opener.open(url, data, timeout)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 404, in open
    response = self._open(req, data)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 422, in _open
    &#39;_open&#39;, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1214, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1187, in do_open
    r = h.getresponse(buffering=True)
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 1067, in getresponse
    response.begin()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 409, in begin
    version, status, reason = self._read_status()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 365, in _read_status
    line = self.fp.readline(_MAXLINE &#43; 1)
  File &#34;/usr/lib64/python2.7/socket.py&#34;, line 476, in readline
    data = self._sock.recv(self._rbufsize)
socket.error: [Errno 104] Connection reset by peer</pre>
<p></p>",Connection Reset by Peer error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,4
940849494,4/26/2016 17:47:57,true,1969469600,,4/26/2016 17:41:51,false,neodev,0.8889,19625264,DZA,41,Chlef,41.102.7.217,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p>I&#39;m unable to complete the survey because attempting to visit the URL <a href=""http://www.crowd-workers.com/track"">www.crowd-workers.com/track</a>åÊbrings me to a 404 Error.</p>",www.crowd-workers.com/track ERROR 404,"<p>I keep getting the following error when I try to runåÊ</p>
<p>cat list_of_urls.txt | python get_clean_text.py &gt; articles_and_urls.txt and I&#39;m not sure why. Any ideas?åÊ</p>
<p></p>
<pre>Traceback (most recent call last):
  File &#34;get_clean_text.py&#34;, line 25, in &lt;module&gt;
    txt = get_text(url)
  File &#34;get_clean_text.py&#34;, line 14, in get_text
    soup = BeautifulSoup(urllib2.urlopen(url).read(), &#39;html.parser&#39;)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 127, in urlopen
    return _opener.open(url, data, timeout)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 404, in open
    response = self._open(req, data)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 422, in _open
    &#39;_open&#39;, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1214, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1187, in do_open
    r = h.getresponse(buffering=True)
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 1067, in getresponse
    response.begin()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 409, in begin
    version, status, reason = self._read_status()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 365, in _read_status
    line = self.fp.readline(_MAXLINE &#43; 1)
  File &#34;/usr/lib64/python2.7/socket.py&#34;, line 476, in readline
    data = self._sock.recv(self._rbufsize)
socket.error: [Errno 104] Connection reset by peer</pre>
<p></p>",Connection Reset by Peer error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,4
940849494,4/26/2016 18:19:09,true,1969486227,,4/26/2016 18:16:24,false,neodev,0.8889,35550011,VEN,07,Valencia,190.204.238.112,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p>I&#39;m unable to complete the survey because attempting to visit the URL <a href=""http://www.crowd-workers.com/track"">www.crowd-workers.com/track</a>åÊbrings me to a 404 Error.</p>",www.crowd-workers.com/track ERROR 404,"<p>I keep getting the following error when I try to runåÊ</p>
<p>cat list_of_urls.txt | python get_clean_text.py &gt; articles_and_urls.txt and I&#39;m not sure why. Any ideas?åÊ</p>
<p></p>
<pre>Traceback (most recent call last):
  File &#34;get_clean_text.py&#34;, line 25, in &lt;module&gt;
    txt = get_text(url)
  File &#34;get_clean_text.py&#34;, line 14, in get_text
    soup = BeautifulSoup(urllib2.urlopen(url).read(), &#39;html.parser&#39;)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 127, in urlopen
    return _opener.open(url, data, timeout)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 404, in open
    response = self._open(req, data)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 422, in _open
    &#39;_open&#39;, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1214, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1187, in do_open
    r = h.getresponse(buffering=True)
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 1067, in getresponse
    response.begin()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 409, in begin
    version, status, reason = self._read_status()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 365, in _read_status
    line = self.fp.readline(_MAXLINE &#43; 1)
  File &#34;/usr/lib64/python2.7/socket.py&#34;, line 476, in readline
    data = self._sock.recv(self._rbufsize)
socket.error: [Errno 104] Connection reset by peer</pre>
<p></p>",Connection Reset by Peer error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,4
940849494,4/26/2016 18:33:38,true,1969493280,,4/26/2016 18:19:15,false,neodev,1.0,29879245,RUS,69,Smolensk,37.144.124.118,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p>I&#39;m unable to complete the survey because attempting to visit the URL <a href=""http://www.crowd-workers.com/track"">www.crowd-workers.com/track</a>åÊbrings me to a 404 Error.</p>",www.crowd-workers.com/track ERROR 404,"<p>I keep getting the following error when I try to runåÊ</p>
<p>cat list_of_urls.txt | python get_clean_text.py &gt; articles_and_urls.txt and I&#39;m not sure why. Any ideas?åÊ</p>
<p></p>
<pre>Traceback (most recent call last):
  File &#34;get_clean_text.py&#34;, line 25, in &lt;module&gt;
    txt = get_text(url)
  File &#34;get_clean_text.py&#34;, line 14, in get_text
    soup = BeautifulSoup(urllib2.urlopen(url).read(), &#39;html.parser&#39;)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 127, in urlopen
    return _opener.open(url, data, timeout)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 404, in open
    response = self._open(req, data)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 422, in _open
    &#39;_open&#39;, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1214, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1187, in do_open
    r = h.getresponse(buffering=True)
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 1067, in getresponse
    response.begin()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 409, in begin
    version, status, reason = self._read_status()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 365, in _read_status
    line = self.fp.readline(_MAXLINE &#43; 1)
  File &#34;/usr/lib64/python2.7/socket.py&#34;, line 476, in readline
    data = self._sock.recv(self._rbufsize)
socket.error: [Errno 104] Connection reset by peer</pre>
<p></p>",Connection Reset by Peer error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,4
940849494,4/26/2016 19:25:11,true,1969519068,,4/26/2016 19:21:41,false,tremorgames,1.0,25197223,HRV,15,Split,94.253.234.240,0,0,"<p>Any idea why this may be happening? I&#39;m passing in two files to the command line.åÊ</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31vxiahonb/Screen_Shot_20160322_at_2.36.08_AM.png"" /><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/ids2xp8h6vn62d/im31w6viylwz/Screen_Shot_20160322_at_2.35.49_AM.png"" /></p>",arguments error,"<p>So Im just trying to run the script, but Im getting a syntax error:</p>
<p></p>
<p>&gt; python classifier_template.py articles åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ~/Desktop/NETS213/HW4åÊ</p>
<p>åÊ File &#34;classifier_template.py&#34;, line 89</p>
<p>åÊ åÊ print &#39;Fold %d : %.05f&#39; %(i,test_acc)</p>
<p>åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ åÊ ^</p>
<p>SyntaxError: invalid syntax</p>
<p></p>
<p>I havent modified the script at all. Do you know what the problem is?</p>",Syntax Error,"<p>Reading &#34;Running Experiments on Amazon Mechanical Turk,&#34; and I came across this claim, which reminded me of problems with the <a href=""http://www.columbia.edu/cu/news/media/01/duncanWatts/"" target=""_blank"">Columbia Small Worlds Problem</a>, where there was very low participation rateåÊ(99K registered, 24K initiated chains, 384 reached targets).</p>
<p></p>
<p>Critiques of Milgram&#39;s &#34;6 degrees of separation&#34; result (people knew how to find a short path to any random person)åÊhighlight the low participation andåÊcompletion rates (64 of the 296 chains reached the target in the 1960s experiment). Would it be possible/worthwhile to recreate the experiment on MTurk?</p>
<p></p>
<p><img src=""https://d1b10bmlvqabco.cloudfront.net/attach/ijblb017ius5zp/hwjpxirvvk76yi/ikrhvartttu/Screen_Shot_20160217_at_6.52.11_PM.png"" /></p>",Re: &#34;Mechanical Turk strongly diminishes the potential for non-response error in online research&#34;,"<p>I&#39;m unable to complete the survey because attempting to visit the URL <a href=""http://www.crowd-workers.com/track"">www.crowd-workers.com/track</a>åÊbrings me to a 404 Error.</p>",www.crowd-workers.com/track ERROR 404,"<p>I keep getting the following error when I try to runåÊ</p>
<p>cat list_of_urls.txt | python get_clean_text.py &gt; articles_and_urls.txt and I&#39;m not sure why. Any ideas?åÊ</p>
<p></p>
<pre>Traceback (most recent call last):
  File &#34;get_clean_text.py&#34;, line 25, in &lt;module&gt;
    txt = get_text(url)
  File &#34;get_clean_text.py&#34;, line 14, in get_text
    soup = BeautifulSoup(urllib2.urlopen(url).read(), &#39;html.parser&#39;)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 127, in urlopen
    return _opener.open(url, data, timeout)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 404, in open
    response = self._open(req, data)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 422, in _open
    &#39;_open&#39;, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 382, in _call_chain
    result = func(*args)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1214, in http_open
    return self.do_open(httplib.HTTPConnection, req)
  File &#34;/usr/lib64/python2.7/urllib2.py&#34;, line 1187, in do_open
    r = h.getresponse(buffering=True)
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 1067, in getresponse
    response.begin()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 409, in begin
    version, status, reason = self._read_status()
  File &#34;/usr/lib64/python2.7/httplib.py&#34;, line 365, in _read_status
    line = self.fp.readline(_MAXLINE &#43; 1)
  File &#34;/usr/lib64/python2.7/socket.py&#34;, line 476, in readline
    data = self._sock.recv(self._rbufsize)
socket.error: [Errno 104] Connection reset by peer</pre>
<p></p>",Connection Reset by Peer error,"<p>File upload failed:<br />The uploaded file contains duplicated headers:åÊ<code>nil</code></p>
<p></p>
<p>I added a header to an empty column called &#34;label&#34; for the label column and the header &#34;url&#34; to the url column. Is there another way I should be doing this?</p>",Crowdflower Error,4
940850927,4/26/2016 15:11:45,true,1969364128,,4/26/2016 15:11:33,false,tremorgames,1.0,32635967,LTU,60,Panevezys,78.63.38.165,0,0,"<p>Is there a way for us to see which files we turned in for a specific assignment, like #5?</p>",turnin,"<p>Hi Everyone,</p>
<p></p>
<p>Many of you had trouble with turnin for the bootcamp, and sent us email submissions. As a general policy, we cannot accept submissions via email. For this assignment, we will not penalize late submissions, so if you have not yet submitted via turnin, or you are still having trouble using scp/ssh, reach out to us and let&#39;s get it figured out.åÊ</p>
<p></p>
<p>For future assignments, we will only consider an assignment to be submitted if it is submitted via turnin before the deadline. Keep in mind our late-day policy: you are allowed 5 total late days during the semester with no penalty. After that, we deduct one point per day late.</p>
<p></p>
<p>Everyone have a wonderful weekend!</p>
<p>Ellie</p>",Late submissions and turnin,"<p>Hi everyone,</p>
<p>åÊ</p>
<p>For your python bootcamp, and for many other assignments this semester, you will be using &#39;turnin&#39;, which is a program on eniac. This requires using the &#39;ssh&#39; and &#39;scp&#39; commands from your terminal. Instructions on how to do this are below, but please come to office hours if this whole process looks foreign to you. It will take about 30 seconds to show you in person, versus much longer than 30 seconds of playing guess-and-check on your own. :)åÊ</p>
<p></p>
<p>You will be turning in your entire iPython notebook. Once you haveåÊyour notebook copied over to the eniac machines and sshed yourself over to the eniac machines (instructions below), you will run.</p>
<p>åÊ</p>
<pre>turnin -c nets213 -p python-bootcamp -v IPythonBootcamp.ipynb</pre>
<p>åÊ</p>
<p>Here is the process, from the beginning. The lines beginning with &#39;$&#39; or &#39;~&gt;&#39; are things you will need to type. Lines beginning with &#39;#&#39; are comments, explaining the commands below. Lines that have no special characters at the show the output you should expect to see.</p>
<p></p>
<pre>#Copy your iPython file (which right now is on your computer) to your account on the eniac. You can do this using the &#39;scp&#39; or &#39;secure copy&#39; command, like below.
$ scp IPythonBootcamp.ipynb epavlick&#64;eniac.seas.upenn.edu:               
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
IPythonBootcamp.ipynb           

#Log on to the  eniac machines (using your usual penn username/password). You should see your command prompt change to say something like &#64;plus or &#64;minus, like below.
$ ssh epavlick&#64;eniac.seas.upenn.edu
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
Last login: Wed Jan 27 14:46:24 2016 from 10.251.134.177
epavlick&#64;plus:~&gt; 

#Make sure your file was copied over successfully. You can check by listing the contents of the current directory using the &#39;ls&#39; command. Hopefully you will see many things listed, and your IPython file should be one of them.
epavlick&#64;plus:~&gt; ls 
Desktop  Documents  Downloads  IPythonBootcamp.ipynb

#You can keep things organized by making a new folder for this assignment using the &#34;mkdir&#34; command, and moving your file into it using the &#34;mv&#34; command. See below.
epavlick&#64;plus:~&gt; mkdir python-bootcamp-submission
epavlick&#64;plus:~&gt; mv IPythonBootcamp.ipynb python-bootcamp-submission/

#To see the list of open projects for our class, type the following. Currently, only the bootcamp and the test project are open.
epavlick&#64;plus:~&gt; turnin -l -c nets213
Current projects for nets213:
python-bootcamp  on (alternate)
turnin-test      off
gun-classifier   off
crowdflower      off
crowdie          off
quality          off
the-end          off
test-turnin      on (current)

# To submit your bootcamp to turnin, use the following command. The 
epavlick&#64;plus:~&gt; turnin -c nets213 -p python-bootcamp -v python-bootcamp-submission/
drwxr-xr-x epavlick/epavlick 0 2016-01-27 15:53 python-bootcamp-submission/
-rw-r----- epavlick/epavlick 16606 2016-01-27 15:48 python-bootcamp-submission/IPythonBootcamp.ipynb
Compressing submitted files... please wait
Your files have been submitted to nets213, python-bootcamp for grading.
</pre>
<p>åÊåÊ</p>
<p>If you want to practice, please try submitting to the test project (called test-turnin), just to make sure everything is working smoothly. You can just submit a blank file, or you are welcome to go on a meaningless rant and submit that. Whatever makes you happy.åÊ</p>
<p></p>
<p>Please let usåÊknow if you have any problems or questions.</p>
<p></p>
<p>Ellie</p>",Submitting assignments via turnin,"When using the $ scp assignment4 pennkey&#64;eniac.seas.upenn.edu command, I am unable to copy over the directory to my account. Not sure what else to do.",Unable to turnin Homework 4,,,"<p>Are the submissions for the latest homework open yet?</p>
<p>I tried to submit and received the error turnin: submissions for -crowdie have been turned off.</p>",HW8 turnin turned off?,0
940850927,4/26/2016 15:17:20,true,1969372594,,4/26/2016 15:12:10,false,clixsense,1.0,7837812,SRB,00,Belgrade,79.101.254.233,0,0,"<p>Is there a way for us to see which files we turned in for a specific assignment, like #5?</p>",turnin,"<p>Hi Everyone,</p>
<p></p>
<p>Many of you had trouble with turnin for the bootcamp, and sent us email submissions. As a general policy, we cannot accept submissions via email. For this assignment, we will not penalize late submissions, so if you have not yet submitted via turnin, or you are still having trouble using scp/ssh, reach out to us and let&#39;s get it figured out.åÊ</p>
<p></p>
<p>For future assignments, we will only consider an assignment to be submitted if it is submitted via turnin before the deadline. Keep in mind our late-day policy: you are allowed 5 total late days during the semester with no penalty. After that, we deduct one point per day late.</p>
<p></p>
<p>Everyone have a wonderful weekend!</p>
<p>Ellie</p>",Late submissions and turnin,"<p>Hi everyone,</p>
<p>åÊ</p>
<p>For your python bootcamp, and for many other assignments this semester, you will be using &#39;turnin&#39;, which is a program on eniac. This requires using the &#39;ssh&#39; and &#39;scp&#39; commands from your terminal. Instructions on how to do this are below, but please come to office hours if this whole process looks foreign to you. It will take about 30 seconds to show you in person, versus much longer than 30 seconds of playing guess-and-check on your own. :)åÊ</p>
<p></p>
<p>You will be turning in your entire iPython notebook. Once you haveåÊyour notebook copied over to the eniac machines and sshed yourself over to the eniac machines (instructions below), you will run.</p>
<p>åÊ</p>
<pre>turnin -c nets213 -p python-bootcamp -v IPythonBootcamp.ipynb</pre>
<p>åÊ</p>
<p>Here is the process, from the beginning. The lines beginning with &#39;$&#39; or &#39;~&gt;&#39; are things you will need to type. Lines beginning with &#39;#&#39; are comments, explaining the commands below. Lines that have no special characters at the show the output you should expect to see.</p>
<p></p>
<pre>#Copy your iPython file (which right now is on your computer) to your account on the eniac. You can do this using the &#39;scp&#39; or &#39;secure copy&#39; command, like below.
$ scp IPythonBootcamp.ipynb epavlick&#64;eniac.seas.upenn.edu:               
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
IPythonBootcamp.ipynb           

#Log on to the  eniac machines (using your usual penn username/password). You should see your command prompt change to say something like &#64;plus or &#64;minus, like below.
$ ssh epavlick&#64;eniac.seas.upenn.edu
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
Last login: Wed Jan 27 14:46:24 2016 from 10.251.134.177
epavlick&#64;plus:~&gt; 

#Make sure your file was copied over successfully. You can check by listing the contents of the current directory using the &#39;ls&#39; command. Hopefully you will see many things listed, and your IPython file should be one of them.
epavlick&#64;plus:~&gt; ls 
Desktop  Documents  Downloads  IPythonBootcamp.ipynb

#You can keep things organized by making a new folder for this assignment using the &#34;mkdir&#34; command, and moving your file into it using the &#34;mv&#34; command. See below.
epavlick&#64;plus:~&gt; mkdir python-bootcamp-submission
epavlick&#64;plus:~&gt; mv IPythonBootcamp.ipynb python-bootcamp-submission/

#To see the list of open projects for our class, type the following. Currently, only the bootcamp and the test project are open.
epavlick&#64;plus:~&gt; turnin -l -c nets213
Current projects for nets213:
python-bootcamp  on (alternate)
turnin-test      off
gun-classifier   off
crowdflower      off
crowdie          off
quality          off
the-end          off
test-turnin      on (current)

# To submit your bootcamp to turnin, use the following command. The 
epavlick&#64;plus:~&gt; turnin -c nets213 -p python-bootcamp -v python-bootcamp-submission/
drwxr-xr-x epavlick/epavlick 0 2016-01-27 15:53 python-bootcamp-submission/
-rw-r----- epavlick/epavlick 16606 2016-01-27 15:48 python-bootcamp-submission/IPythonBootcamp.ipynb
Compressing submitted files... please wait
Your files have been submitted to nets213, python-bootcamp for grading.
</pre>
<p>åÊåÊ</p>
<p>If you want to practice, please try submitting to the test project (called test-turnin), just to make sure everything is working smoothly. You can just submit a blank file, or you are welcome to go on a meaningless rant and submit that. Whatever makes you happy.åÊ</p>
<p></p>
<p>Please let usåÊknow if you have any problems or questions.</p>
<p></p>
<p>Ellie</p>",Submitting assignments via turnin,"When using the $ scp assignment4 pennkey&#64;eniac.seas.upenn.edu command, I am unable to copy over the directory to my account. Not sure what else to do.",Unable to turnin Homework 4,,,"<p>Are the submissions for the latest homework open yet?</p>
<p>I tried to submit and received the error turnin: submissions for -crowdie have been turned off.</p>",HW8 turnin turned off?,0
940850927,4/26/2016 15:19:14,true,1969375922,,4/26/2016 15:15:44,false,clixsense,1.0,24287706,TWN,04,Keelung,61.231.195.173,0,0,"<p>Is there a way for us to see which files we turned in for a specific assignment, like #5?</p>",turnin,"<p>Hi Everyone,</p>
<p></p>
<p>Many of you had trouble with turnin for the bootcamp, and sent us email submissions. As a general policy, we cannot accept submissions via email. For this assignment, we will not penalize late submissions, so if you have not yet submitted via turnin, or you are still having trouble using scp/ssh, reach out to us and let&#39;s get it figured out.åÊ</p>
<p></p>
<p>For future assignments, we will only consider an assignment to be submitted if it is submitted via turnin before the deadline. Keep in mind our late-day policy: you are allowed 5 total late days during the semester with no penalty. After that, we deduct one point per day late.</p>
<p></p>
<p>Everyone have a wonderful weekend!</p>
<p>Ellie</p>",Late submissions and turnin,"<p>Hi everyone,</p>
<p>åÊ</p>
<p>For your python bootcamp, and for many other assignments this semester, you will be using &#39;turnin&#39;, which is a program on eniac. This requires using the &#39;ssh&#39; and &#39;scp&#39; commands from your terminal. Instructions on how to do this are below, but please come to office hours if this whole process looks foreign to you. It will take about 30 seconds to show you in person, versus much longer than 30 seconds of playing guess-and-check on your own. :)åÊ</p>
<p></p>
<p>You will be turning in your entire iPython notebook. Once you haveåÊyour notebook copied over to the eniac machines and sshed yourself over to the eniac machines (instructions below), you will run.</p>
<p>åÊ</p>
<pre>turnin -c nets213 -p python-bootcamp -v IPythonBootcamp.ipynb</pre>
<p>åÊ</p>
<p>Here is the process, from the beginning. The lines beginning with &#39;$&#39; or &#39;~&gt;&#39; are things you will need to type. Lines beginning with &#39;#&#39; are comments, explaining the commands below. Lines that have no special characters at the show the output you should expect to see.</p>
<p></p>
<pre>#Copy your iPython file (which right now is on your computer) to your account on the eniac. You can do this using the &#39;scp&#39; or &#39;secure copy&#39; command, like below.
$ scp IPythonBootcamp.ipynb epavlick&#64;eniac.seas.upenn.edu:               
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
IPythonBootcamp.ipynb           

#Log on to the  eniac machines (using your usual penn username/password). You should see your command prompt change to say something like &#64;plus or &#64;minus, like below.
$ ssh epavlick&#64;eniac.seas.upenn.edu
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
Last login: Wed Jan 27 14:46:24 2016 from 10.251.134.177
epavlick&#64;plus:~&gt; 

#Make sure your file was copied over successfully. You can check by listing the contents of the current directory using the &#39;ls&#39; command. Hopefully you will see many things listed, and your IPython file should be one of them.
epavlick&#64;plus:~&gt; ls 
Desktop  Documents  Downloads  IPythonBootcamp.ipynb

#You can keep things organized by making a new folder for this assignment using the &#34;mkdir&#34; command, and moving your file into it using the &#34;mv&#34; command. See below.
epavlick&#64;plus:~&gt; mkdir python-bootcamp-submission
epavlick&#64;plus:~&gt; mv IPythonBootcamp.ipynb python-bootcamp-submission/

#To see the list of open projects for our class, type the following. Currently, only the bootcamp and the test project are open.
epavlick&#64;plus:~&gt; turnin -l -c nets213
Current projects for nets213:
python-bootcamp  on (alternate)
turnin-test      off
gun-classifier   off
crowdflower      off
crowdie          off
quality          off
the-end          off
test-turnin      on (current)

# To submit your bootcamp to turnin, use the following command. The 
epavlick&#64;plus:~&gt; turnin -c nets213 -p python-bootcamp -v python-bootcamp-submission/
drwxr-xr-x epavlick/epavlick 0 2016-01-27 15:53 python-bootcamp-submission/
-rw-r----- epavlick/epavlick 16606 2016-01-27 15:48 python-bootcamp-submission/IPythonBootcamp.ipynb
Compressing submitted files... please wait
Your files have been submitted to nets213, python-bootcamp for grading.
</pre>
<p>åÊåÊ</p>
<p>If you want to practice, please try submitting to the test project (called test-turnin), just to make sure everything is working smoothly. You can just submit a blank file, or you are welcome to go on a meaningless rant and submit that. Whatever makes you happy.åÊ</p>
<p></p>
<p>Please let usåÊknow if you have any problems or questions.</p>
<p></p>
<p>Ellie</p>",Submitting assignments via turnin,"When using the $ scp assignment4 pennkey&#64;eniac.seas.upenn.edu command, I am unable to copy over the directory to my account. Not sure what else to do.",Unable to turnin Homework 4,,,"<p>Are the submissions for the latest homework open yet?</p>
<p>I tried to submit and received the error turnin: submissions for -crowdie have been turned off.</p>",HW8 turnin turned off?,0
940850927,4/26/2016 15:20:55,true,1969378549,,4/26/2016 15:20:31,false,neodev,1.0,19132694,LKA,36,Colombo,123.231.124.170,0,0,"<p>Is there a way for us to see which files we turned in for a specific assignment, like #5?</p>",turnin,"<p>Hi Everyone,</p>
<p></p>
<p>Many of you had trouble with turnin for the bootcamp, and sent us email submissions. As a general policy, we cannot accept submissions via email. For this assignment, we will not penalize late submissions, so if you have not yet submitted via turnin, or you are still having trouble using scp/ssh, reach out to us and let&#39;s get it figured out.åÊ</p>
<p></p>
<p>For future assignments, we will only consider an assignment to be submitted if it is submitted via turnin before the deadline. Keep in mind our late-day policy: you are allowed 5 total late days during the semester with no penalty. After that, we deduct one point per day late.</p>
<p></p>
<p>Everyone have a wonderful weekend!</p>
<p>Ellie</p>",Late submissions and turnin,"<p>Hi everyone,</p>
<p>åÊ</p>
<p>For your python bootcamp, and for many other assignments this semester, you will be using &#39;turnin&#39;, which is a program on eniac. This requires using the &#39;ssh&#39; and &#39;scp&#39; commands from your terminal. Instructions on how to do this are below, but please come to office hours if this whole process looks foreign to you. It will take about 30 seconds to show you in person, versus much longer than 30 seconds of playing guess-and-check on your own. :)åÊ</p>
<p></p>
<p>You will be turning in your entire iPython notebook. Once you haveåÊyour notebook copied over to the eniac machines and sshed yourself over to the eniac machines (instructions below), you will run.</p>
<p>åÊ</p>
<pre>turnin -c nets213 -p python-bootcamp -v IPythonBootcamp.ipynb</pre>
<p>åÊ</p>
<p>Here is the process, from the beginning. The lines beginning with &#39;$&#39; or &#39;~&gt;&#39; are things you will need to type. Lines beginning with &#39;#&#39; are comments, explaining the commands below. Lines that have no special characters at the show the output you should expect to see.</p>
<p></p>
<pre>#Copy your iPython file (which right now is on your computer) to your account on the eniac. You can do this using the &#39;scp&#39; or &#39;secure copy&#39; command, like below.
$ scp IPythonBootcamp.ipynb epavlick&#64;eniac.seas.upenn.edu:               
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
IPythonBootcamp.ipynb           

#Log on to the  eniac machines (using your usual penn username/password). You should see your command prompt change to say something like &#64;plus or &#64;minus, like below.
$ ssh epavlick&#64;eniac.seas.upenn.edu
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
Last login: Wed Jan 27 14:46:24 2016 from 10.251.134.177
epavlick&#64;plus:~&gt; 

#Make sure your file was copied over successfully. You can check by listing the contents of the current directory using the &#39;ls&#39; command. Hopefully you will see many things listed, and your IPython file should be one of them.
epavlick&#64;plus:~&gt; ls 
Desktop  Documents  Downloads  IPythonBootcamp.ipynb

#You can keep things organized by making a new folder for this assignment using the &#34;mkdir&#34; command, and moving your file into it using the &#34;mv&#34; command. See below.
epavlick&#64;plus:~&gt; mkdir python-bootcamp-submission
epavlick&#64;plus:~&gt; mv IPythonBootcamp.ipynb python-bootcamp-submission/

#To see the list of open projects for our class, type the following. Currently, only the bootcamp and the test project are open.
epavlick&#64;plus:~&gt; turnin -l -c nets213
Current projects for nets213:
python-bootcamp  on (alternate)
turnin-test      off
gun-classifier   off
crowdflower      off
crowdie          off
quality          off
the-end          off
test-turnin      on (current)

# To submit your bootcamp to turnin, use the following command. The 
epavlick&#64;plus:~&gt; turnin -c nets213 -p python-bootcamp -v python-bootcamp-submission/
drwxr-xr-x epavlick/epavlick 0 2016-01-27 15:53 python-bootcamp-submission/
-rw-r----- epavlick/epavlick 16606 2016-01-27 15:48 python-bootcamp-submission/IPythonBootcamp.ipynb
Compressing submitted files... please wait
Your files have been submitted to nets213, python-bootcamp for grading.
</pre>
<p>åÊåÊ</p>
<p>If you want to practice, please try submitting to the test project (called test-turnin), just to make sure everything is working smoothly. You can just submit a blank file, or you are welcome to go on a meaningless rant and submit that. Whatever makes you happy.åÊ</p>
<p></p>
<p>Please let usåÊknow if you have any problems or questions.</p>
<p></p>
<p>Ellie</p>",Submitting assignments via turnin,"When using the $ scp assignment4 pennkey&#64;eniac.seas.upenn.edu command, I am unable to copy over the directory to my account. Not sure what else to do.",Unable to turnin Homework 4,,,"<p>Are the submissions for the latest homework open yet?</p>
<p>I tried to submit and received the error turnin: submissions for -crowdie have been turned off.</p>",HW8 turnin turned off?,0
940850927,4/26/2016 15:26:03,true,1969387320,,4/26/2016 15:24:14,false,instagc,0.8889,13581319,USA,IL,Waltonville,208.70.36.12,0,0,"<p>Is there a way for us to see which files we turned in for a specific assignment, like #5?</p>",turnin,"<p>Hi Everyone,</p>
<p></p>
<p>Many of you had trouble with turnin for the bootcamp, and sent us email submissions. As a general policy, we cannot accept submissions via email. For this assignment, we will not penalize late submissions, so if you have not yet submitted via turnin, or you are still having trouble using scp/ssh, reach out to us and let&#39;s get it figured out.åÊ</p>
<p></p>
<p>For future assignments, we will only consider an assignment to be submitted if it is submitted via turnin before the deadline. Keep in mind our late-day policy: you are allowed 5 total late days during the semester with no penalty. After that, we deduct one point per day late.</p>
<p></p>
<p>Everyone have a wonderful weekend!</p>
<p>Ellie</p>",Late submissions and turnin,"<p>Hi everyone,</p>
<p>åÊ</p>
<p>For your python bootcamp, and for many other assignments this semester, you will be using &#39;turnin&#39;, which is a program on eniac. This requires using the &#39;ssh&#39; and &#39;scp&#39; commands from your terminal. Instructions on how to do this are below, but please come to office hours if this whole process looks foreign to you. It will take about 30 seconds to show you in person, versus much longer than 30 seconds of playing guess-and-check on your own. :)åÊ</p>
<p></p>
<p>You will be turning in your entire iPython notebook. Once you haveåÊyour notebook copied over to the eniac machines and sshed yourself over to the eniac machines (instructions below), you will run.</p>
<p>åÊ</p>
<pre>turnin -c nets213 -p python-bootcamp -v IPythonBootcamp.ipynb</pre>
<p>åÊ</p>
<p>Here is the process, from the beginning. The lines beginning with &#39;$&#39; or &#39;~&gt;&#39; are things you will need to type. Lines beginning with &#39;#&#39; are comments, explaining the commands below. Lines that have no special characters at the show the output you should expect to see.</p>
<p></p>
<pre>#Copy your iPython file (which right now is on your computer) to your account on the eniac. You can do this using the &#39;scp&#39; or &#39;secure copy&#39; command, like below.
$ scp IPythonBootcamp.ipynb epavlick&#64;eniac.seas.upenn.edu:               
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
IPythonBootcamp.ipynb           

#Log on to the  eniac machines (using your usual penn username/password). You should see your command prompt change to say something like &#64;plus or &#64;minus, like below.
$ ssh epavlick&#64;eniac.seas.upenn.edu
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
Last login: Wed Jan 27 14:46:24 2016 from 10.251.134.177
epavlick&#64;plus:~&gt; 

#Make sure your file was copied over successfully. You can check by listing the contents of the current directory using the &#39;ls&#39; command. Hopefully you will see many things listed, and your IPython file should be one of them.
epavlick&#64;plus:~&gt; ls 
Desktop  Documents  Downloads  IPythonBootcamp.ipynb

#You can keep things organized by making a new folder for this assignment using the &#34;mkdir&#34; command, and moving your file into it using the &#34;mv&#34; command. See below.
epavlick&#64;plus:~&gt; mkdir python-bootcamp-submission
epavlick&#64;plus:~&gt; mv IPythonBootcamp.ipynb python-bootcamp-submission/

#To see the list of open projects for our class, type the following. Currently, only the bootcamp and the test project are open.
epavlick&#64;plus:~&gt; turnin -l -c nets213
Current projects for nets213:
python-bootcamp  on (alternate)
turnin-test      off
gun-classifier   off
crowdflower      off
crowdie          off
quality          off
the-end          off
test-turnin      on (current)

# To submit your bootcamp to turnin, use the following command. The 
epavlick&#64;plus:~&gt; turnin -c nets213 -p python-bootcamp -v python-bootcamp-submission/
drwxr-xr-x epavlick/epavlick 0 2016-01-27 15:53 python-bootcamp-submission/
-rw-r----- epavlick/epavlick 16606 2016-01-27 15:48 python-bootcamp-submission/IPythonBootcamp.ipynb
Compressing submitted files... please wait
Your files have been submitted to nets213, python-bootcamp for grading.
</pre>
<p>åÊåÊ</p>
<p>If you want to practice, please try submitting to the test project (called test-turnin), just to make sure everything is working smoothly. You can just submit a blank file, or you are welcome to go on a meaningless rant and submit that. Whatever makes you happy.åÊ</p>
<p></p>
<p>Please let usåÊknow if you have any problems or questions.</p>
<p></p>
<p>Ellie</p>",Submitting assignments via turnin,"When using the $ scp assignment4 pennkey&#64;eniac.seas.upenn.edu command, I am unable to copy over the directory to my account. Not sure what else to do.",Unable to turnin Homework 4,,,"<p>Are the submissions for the latest homework open yet?</p>
<p>I tried to submit and received the error turnin: submissions for -crowdie have been turned off.</p>",HW8 turnin turned off?,0
940850927,4/26/2016 15:27:16,true,1969389141,,4/26/2016 15:24:27,false,elite,1.0,30280423,ITA,15,Siracusa,151.54.84.121,0,0,"<p>Is there a way for us to see which files we turned in for a specific assignment, like #5?</p>",turnin,"<p>Hi Everyone,</p>
<p></p>
<p>Many of you had trouble with turnin for the bootcamp, and sent us email submissions. As a general policy, we cannot accept submissions via email. For this assignment, we will not penalize late submissions, so if you have not yet submitted via turnin, or you are still having trouble using scp/ssh, reach out to us and let&#39;s get it figured out.åÊ</p>
<p></p>
<p>For future assignments, we will only consider an assignment to be submitted if it is submitted via turnin before the deadline. Keep in mind our late-day policy: you are allowed 5 total late days during the semester with no penalty. After that, we deduct one point per day late.</p>
<p></p>
<p>Everyone have a wonderful weekend!</p>
<p>Ellie</p>",Late submissions and turnin,"<p>Hi everyone,</p>
<p>åÊ</p>
<p>For your python bootcamp, and for many other assignments this semester, you will be using &#39;turnin&#39;, which is a program on eniac. This requires using the &#39;ssh&#39; and &#39;scp&#39; commands from your terminal. Instructions on how to do this are below, but please come to office hours if this whole process looks foreign to you. It will take about 30 seconds to show you in person, versus much longer than 30 seconds of playing guess-and-check on your own. :)åÊ</p>
<p></p>
<p>You will be turning in your entire iPython notebook. Once you haveåÊyour notebook copied over to the eniac machines and sshed yourself over to the eniac machines (instructions below), you will run.</p>
<p>åÊ</p>
<pre>turnin -c nets213 -p python-bootcamp -v IPythonBootcamp.ipynb</pre>
<p>åÊ</p>
<p>Here is the process, from the beginning. The lines beginning with &#39;$&#39; or &#39;~&gt;&#39; are things you will need to type. Lines beginning with &#39;#&#39; are comments, explaining the commands below. Lines that have no special characters at the show the output you should expect to see.</p>
<p></p>
<pre>#Copy your iPython file (which right now is on your computer) to your account on the eniac. You can do this using the &#39;scp&#39; or &#39;secure copy&#39; command, like below.
$ scp IPythonBootcamp.ipynb epavlick&#64;eniac.seas.upenn.edu:               
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
IPythonBootcamp.ipynb           

#Log on to the  eniac machines (using your usual penn username/password). You should see your command prompt change to say something like &#64;plus or &#64;minus, like below.
$ ssh epavlick&#64;eniac.seas.upenn.edu
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
Last login: Wed Jan 27 14:46:24 2016 from 10.251.134.177
epavlick&#64;plus:~&gt; 

#Make sure your file was copied over successfully. You can check by listing the contents of the current directory using the &#39;ls&#39; command. Hopefully you will see many things listed, and your IPython file should be one of them.
epavlick&#64;plus:~&gt; ls 
Desktop  Documents  Downloads  IPythonBootcamp.ipynb

#You can keep things organized by making a new folder for this assignment using the &#34;mkdir&#34; command, and moving your file into it using the &#34;mv&#34; command. See below.
epavlick&#64;plus:~&gt; mkdir python-bootcamp-submission
epavlick&#64;plus:~&gt; mv IPythonBootcamp.ipynb python-bootcamp-submission/

#To see the list of open projects for our class, type the following. Currently, only the bootcamp and the test project are open.
epavlick&#64;plus:~&gt; turnin -l -c nets213
Current projects for nets213:
python-bootcamp  on (alternate)
turnin-test      off
gun-classifier   off
crowdflower      off
crowdie          off
quality          off
the-end          off
test-turnin      on (current)

# To submit your bootcamp to turnin, use the following command. The 
epavlick&#64;plus:~&gt; turnin -c nets213 -p python-bootcamp -v python-bootcamp-submission/
drwxr-xr-x epavlick/epavlick 0 2016-01-27 15:53 python-bootcamp-submission/
-rw-r----- epavlick/epavlick 16606 2016-01-27 15:48 python-bootcamp-submission/IPythonBootcamp.ipynb
Compressing submitted files... please wait
Your files have been submitted to nets213, python-bootcamp for grading.
</pre>
<p>åÊåÊ</p>
<p>If you want to practice, please try submitting to the test project (called test-turnin), just to make sure everything is working smoothly. You can just submit a blank file, or you are welcome to go on a meaningless rant and submit that. Whatever makes you happy.åÊ</p>
<p></p>
<p>Please let usåÊknow if you have any problems or questions.</p>
<p></p>
<p>Ellie</p>",Submitting assignments via turnin,"When using the $ scp assignment4 pennkey&#64;eniac.seas.upenn.edu command, I am unable to copy over the directory to my account. Not sure what else to do.",Unable to turnin Homework 4,,,"<p>Are the submissions for the latest homework open yet?</p>
<p>I tried to submit and received the error turnin: submissions for -crowdie have been turned off.</p>",HW8 turnin turned off?,0
940850927,4/26/2016 15:30:05,true,1969390872,,4/26/2016 15:26:19,false,clixsense,0.8889,36052512,PHL,F2,Quezon City,49.149.150.150,0,0,"<p>Is there a way for us to see which files we turned in for a specific assignment, like #5?</p>",turnin,"<p>Hi Everyone,</p>
<p></p>
<p>Many of you had trouble with turnin for the bootcamp, and sent us email submissions. As a general policy, we cannot accept submissions via email. For this assignment, we will not penalize late submissions, so if you have not yet submitted via turnin, or you are still having trouble using scp/ssh, reach out to us and let&#39;s get it figured out.åÊ</p>
<p></p>
<p>For future assignments, we will only consider an assignment to be submitted if it is submitted via turnin before the deadline. Keep in mind our late-day policy: you are allowed 5 total late days during the semester with no penalty. After that, we deduct one point per day late.</p>
<p></p>
<p>Everyone have a wonderful weekend!</p>
<p>Ellie</p>",Late submissions and turnin,"<p>Hi everyone,</p>
<p>åÊ</p>
<p>For your python bootcamp, and for many other assignments this semester, you will be using &#39;turnin&#39;, which is a program on eniac. This requires using the &#39;ssh&#39; and &#39;scp&#39; commands from your terminal. Instructions on how to do this are below, but please come to office hours if this whole process looks foreign to you. It will take about 30 seconds to show you in person, versus much longer than 30 seconds of playing guess-and-check on your own. :)åÊ</p>
<p></p>
<p>You will be turning in your entire iPython notebook. Once you haveåÊyour notebook copied over to the eniac machines and sshed yourself over to the eniac machines (instructions below), you will run.</p>
<p>åÊ</p>
<pre>turnin -c nets213 -p python-bootcamp -v IPythonBootcamp.ipynb</pre>
<p>åÊ</p>
<p>Here is the process, from the beginning. The lines beginning with &#39;$&#39; or &#39;~&gt;&#39; are things you will need to type. Lines beginning with &#39;#&#39; are comments, explaining the commands below. Lines that have no special characters at the show the output you should expect to see.</p>
<p></p>
<pre>#Copy your iPython file (which right now is on your computer) to your account on the eniac. You can do this using the &#39;scp&#39; or &#39;secure copy&#39; command, like below.
$ scp IPythonBootcamp.ipynb epavlick&#64;eniac.seas.upenn.edu:               
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
IPythonBootcamp.ipynb           

#Log on to the  eniac machines (using your usual penn username/password). You should see your command prompt change to say something like &#64;plus or &#64;minus, like below.
$ ssh epavlick&#64;eniac.seas.upenn.edu
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
Last login: Wed Jan 27 14:46:24 2016 from 10.251.134.177
epavlick&#64;plus:~&gt; 

#Make sure your file was copied over successfully. You can check by listing the contents of the current directory using the &#39;ls&#39; command. Hopefully you will see many things listed, and your IPython file should be one of them.
epavlick&#64;plus:~&gt; ls 
Desktop  Documents  Downloads  IPythonBootcamp.ipynb

#You can keep things organized by making a new folder for this assignment using the &#34;mkdir&#34; command, and moving your file into it using the &#34;mv&#34; command. See below.
epavlick&#64;plus:~&gt; mkdir python-bootcamp-submission
epavlick&#64;plus:~&gt; mv IPythonBootcamp.ipynb python-bootcamp-submission/

#To see the list of open projects for our class, type the following. Currently, only the bootcamp and the test project are open.
epavlick&#64;plus:~&gt; turnin -l -c nets213
Current projects for nets213:
python-bootcamp  on (alternate)
turnin-test      off
gun-classifier   off
crowdflower      off
crowdie          off
quality          off
the-end          off
test-turnin      on (current)

# To submit your bootcamp to turnin, use the following command. The 
epavlick&#64;plus:~&gt; turnin -c nets213 -p python-bootcamp -v python-bootcamp-submission/
drwxr-xr-x epavlick/epavlick 0 2016-01-27 15:53 python-bootcamp-submission/
-rw-r----- epavlick/epavlick 16606 2016-01-27 15:48 python-bootcamp-submission/IPythonBootcamp.ipynb
Compressing submitted files... please wait
Your files have been submitted to nets213, python-bootcamp for grading.
</pre>
<p>åÊåÊ</p>
<p>If you want to practice, please try submitting to the test project (called test-turnin), just to make sure everything is working smoothly. You can just submit a blank file, or you are welcome to go on a meaningless rant and submit that. Whatever makes you happy.åÊ</p>
<p></p>
<p>Please let usåÊknow if you have any problems or questions.</p>
<p></p>
<p>Ellie</p>",Submitting assignments via turnin,"When using the $ scp assignment4 pennkey&#64;eniac.seas.upenn.edu command, I am unable to copy over the directory to my account. Not sure what else to do.",Unable to turnin Homework 4,,,"<p>Are the submissions for the latest homework open yet?</p>
<p>I tried to submit and received the error turnin: submissions for -crowdie have been turned off.</p>",HW8 turnin turned off?,0
940850927,4/26/2016 15:36:01,true,1969392987,,4/26/2016 15:30:10,false,neodev,1.0,13396426,VEN,15,Santa Teresa,190.38.163.149,0,0,"<p>Is there a way for us to see which files we turned in for a specific assignment, like #5?</p>",turnin,"<p>Hi Everyone,</p>
<p></p>
<p>Many of you had trouble with turnin for the bootcamp, and sent us email submissions. As a general policy, we cannot accept submissions via email. For this assignment, we will not penalize late submissions, so if you have not yet submitted via turnin, or you are still having trouble using scp/ssh, reach out to us and let&#39;s get it figured out.åÊ</p>
<p></p>
<p>For future assignments, we will only consider an assignment to be submitted if it is submitted via turnin before the deadline. Keep in mind our late-day policy: you are allowed 5 total late days during the semester with no penalty. After that, we deduct one point per day late.</p>
<p></p>
<p>Everyone have a wonderful weekend!</p>
<p>Ellie</p>",Late submissions and turnin,"<p>Hi everyone,</p>
<p>åÊ</p>
<p>For your python bootcamp, and for many other assignments this semester, you will be using &#39;turnin&#39;, which is a program on eniac. This requires using the &#39;ssh&#39; and &#39;scp&#39; commands from your terminal. Instructions on how to do this are below, but please come to office hours if this whole process looks foreign to you. It will take about 30 seconds to show you in person, versus much longer than 30 seconds of playing guess-and-check on your own. :)åÊ</p>
<p></p>
<p>You will be turning in your entire iPython notebook. Once you haveåÊyour notebook copied over to the eniac machines and sshed yourself over to the eniac machines (instructions below), you will run.</p>
<p>åÊ</p>
<pre>turnin -c nets213 -p python-bootcamp -v IPythonBootcamp.ipynb</pre>
<p>åÊ</p>
<p>Here is the process, from the beginning. The lines beginning with &#39;$&#39; or &#39;~&gt;&#39; are things you will need to type. Lines beginning with &#39;#&#39; are comments, explaining the commands below. Lines that have no special characters at the show the output you should expect to see.</p>
<p></p>
<pre>#Copy your iPython file (which right now is on your computer) to your account on the eniac. You can do this using the &#39;scp&#39; or &#39;secure copy&#39; command, like below.
$ scp IPythonBootcamp.ipynb epavlick&#64;eniac.seas.upenn.edu:               
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
IPythonBootcamp.ipynb           

#Log on to the  eniac machines (using your usual penn username/password). You should see your command prompt change to say something like &#64;plus or &#64;minus, like below.
$ ssh epavlick&#64;eniac.seas.upenn.edu
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
Last login: Wed Jan 27 14:46:24 2016 from 10.251.134.177
epavlick&#64;plus:~&gt; 

#Make sure your file was copied over successfully. You can check by listing the contents of the current directory using the &#39;ls&#39; command. Hopefully you will see many things listed, and your IPython file should be one of them.
epavlick&#64;plus:~&gt; ls 
Desktop  Documents  Downloads  IPythonBootcamp.ipynb

#You can keep things organized by making a new folder for this assignment using the &#34;mkdir&#34; command, and moving your file into it using the &#34;mv&#34; command. See below.
epavlick&#64;plus:~&gt; mkdir python-bootcamp-submission
epavlick&#64;plus:~&gt; mv IPythonBootcamp.ipynb python-bootcamp-submission/

#To see the list of open projects for our class, type the following. Currently, only the bootcamp and the test project are open.
epavlick&#64;plus:~&gt; turnin -l -c nets213
Current projects for nets213:
python-bootcamp  on (alternate)
turnin-test      off
gun-classifier   off
crowdflower      off
crowdie          off
quality          off
the-end          off
test-turnin      on (current)

# To submit your bootcamp to turnin, use the following command. The 
epavlick&#64;plus:~&gt; turnin -c nets213 -p python-bootcamp -v python-bootcamp-submission/
drwxr-xr-x epavlick/epavlick 0 2016-01-27 15:53 python-bootcamp-submission/
-rw-r----- epavlick/epavlick 16606 2016-01-27 15:48 python-bootcamp-submission/IPythonBootcamp.ipynb
Compressing submitted files... please wait
Your files have been submitted to nets213, python-bootcamp for grading.
</pre>
<p>åÊåÊ</p>
<p>If you want to practice, please try submitting to the test project (called test-turnin), just to make sure everything is working smoothly. You can just submit a blank file, or you are welcome to go on a meaningless rant and submit that. Whatever makes you happy.åÊ</p>
<p></p>
<p>Please let usåÊknow if you have any problems or questions.</p>
<p></p>
<p>Ellie</p>",Submitting assignments via turnin,"When using the $ scp assignment4 pennkey&#64;eniac.seas.upenn.edu command, I am unable to copy over the directory to my account. Not sure what else to do.",Unable to turnin Homework 4,,,"<p>Are the submissions for the latest homework open yet?</p>
<p>I tried to submit and received the error turnin: submissions for -crowdie have been turned off.</p>",HW8 turnin turned off?,0
940850927,4/26/2016 15:39:56,true,1969394931,,4/26/2016 15:27:12,false,clixsense,1.0,21875134,GBR,H9,London,87.112.158.81,0,0,"<p>Is there a way for us to see which files we turned in for a specific assignment, like #5?</p>",turnin,"<p>Hi Everyone,</p>
<p></p>
<p>Many of you had trouble with turnin for the bootcamp, and sent us email submissions. As a general policy, we cannot accept submissions via email. For this assignment, we will not penalize late submissions, so if you have not yet submitted via turnin, or you are still having trouble using scp/ssh, reach out to us and let&#39;s get it figured out.åÊ</p>
<p></p>
<p>For future assignments, we will only consider an assignment to be submitted if it is submitted via turnin before the deadline. Keep in mind our late-day policy: you are allowed 5 total late days during the semester with no penalty. After that, we deduct one point per day late.</p>
<p></p>
<p>Everyone have a wonderful weekend!</p>
<p>Ellie</p>",Late submissions and turnin,"<p>Hi everyone,</p>
<p>åÊ</p>
<p>For your python bootcamp, and for many other assignments this semester, you will be using &#39;turnin&#39;, which is a program on eniac. This requires using the &#39;ssh&#39; and &#39;scp&#39; commands from your terminal. Instructions on how to do this are below, but please come to office hours if this whole process looks foreign to you. It will take about 30 seconds to show you in person, versus much longer than 30 seconds of playing guess-and-check on your own. :)åÊ</p>
<p></p>
<p>You will be turning in your entire iPython notebook. Once you haveåÊyour notebook copied over to the eniac machines and sshed yourself over to the eniac machines (instructions below), you will run.</p>
<p>åÊ</p>
<pre>turnin -c nets213 -p python-bootcamp -v IPythonBootcamp.ipynb</pre>
<p>åÊ</p>
<p>Here is the process, from the beginning. The lines beginning with &#39;$&#39; or &#39;~&gt;&#39; are things you will need to type. Lines beginning with &#39;#&#39; are comments, explaining the commands below. Lines that have no special characters at the show the output you should expect to see.</p>
<p></p>
<pre>#Copy your iPython file (which right now is on your computer) to your account on the eniac. You can do this using the &#39;scp&#39; or &#39;secure copy&#39; command, like below.
$ scp IPythonBootcamp.ipynb epavlick&#64;eniac.seas.upenn.edu:               
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
IPythonBootcamp.ipynb           

#Log on to the  eniac machines (using your usual penn username/password). You should see your command prompt change to say something like &#64;plus or &#64;minus, like below.
$ ssh epavlick&#64;eniac.seas.upenn.edu
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
Last login: Wed Jan 27 14:46:24 2016 from 10.251.134.177
epavlick&#64;plus:~&gt; 

#Make sure your file was copied over successfully. You can check by listing the contents of the current directory using the &#39;ls&#39; command. Hopefully you will see many things listed, and your IPython file should be one of them.
epavlick&#64;plus:~&gt; ls 
Desktop  Documents  Downloads  IPythonBootcamp.ipynb

#You can keep things organized by making a new folder for this assignment using the &#34;mkdir&#34; command, and moving your file into it using the &#34;mv&#34; command. See below.
epavlick&#64;plus:~&gt; mkdir python-bootcamp-submission
epavlick&#64;plus:~&gt; mv IPythonBootcamp.ipynb python-bootcamp-submission/

#To see the list of open projects for our class, type the following. Currently, only the bootcamp and the test project are open.
epavlick&#64;plus:~&gt; turnin -l -c nets213
Current projects for nets213:
python-bootcamp  on (alternate)
turnin-test      off
gun-classifier   off
crowdflower      off
crowdie          off
quality          off
the-end          off
test-turnin      on (current)

# To submit your bootcamp to turnin, use the following command. The 
epavlick&#64;plus:~&gt; turnin -c nets213 -p python-bootcamp -v python-bootcamp-submission/
drwxr-xr-x epavlick/epavlick 0 2016-01-27 15:53 python-bootcamp-submission/
-rw-r----- epavlick/epavlick 16606 2016-01-27 15:48 python-bootcamp-submission/IPythonBootcamp.ipynb
Compressing submitted files... please wait
Your files have been submitted to nets213, python-bootcamp for grading.
</pre>
<p>åÊåÊ</p>
<p>If you want to practice, please try submitting to the test project (called test-turnin), just to make sure everything is working smoothly. You can just submit a blank file, or you are welcome to go on a meaningless rant and submit that. Whatever makes you happy.åÊ</p>
<p></p>
<p>Please let usåÊknow if you have any problems or questions.</p>
<p></p>
<p>Ellie</p>",Submitting assignments via turnin,"When using the $ scp assignment4 pennkey&#64;eniac.seas.upenn.edu command, I am unable to copy over the directory to my account. Not sure what else to do.",Unable to turnin Homework 4,,,"<p>Are the submissions for the latest homework open yet?</p>
<p>I tried to submit and received the error turnin: submissions for -crowdie have been turned off.</p>",HW8 turnin turned off?,0
940850927,4/26/2016 15:40:37,true,1969395294,true,4/26/2016 15:34:58,false,neodev,0.7778,32569659,USA,MN,Minneapolis,97.127.88.224,1,0,"<p>Is there a way for us to see which files we turned in for a specific assignment, like #5?</p>",turnin,"<p>Hi Everyone,</p>
<p></p>
<p>Many of you had trouble with turnin for the bootcamp, and sent us email submissions. As a general policy, we cannot accept submissions via email. For this assignment, we will not penalize late submissions, so if you have not yet submitted via turnin, or you are still having trouble using scp/ssh, reach out to us and let&#39;s get it figured out.åÊ</p>
<p></p>
<p>For future assignments, we will only consider an assignment to be submitted if it is submitted via turnin before the deadline. Keep in mind our late-day policy: you are allowed 5 total late days during the semester with no penalty. After that, we deduct one point per day late.</p>
<p></p>
<p>Everyone have a wonderful weekend!</p>
<p>Ellie</p>",Late submissions and turnin,"<p>Hi everyone,</p>
<p>åÊ</p>
<p>For your python bootcamp, and for many other assignments this semester, you will be using &#39;turnin&#39;, which is a program on eniac. This requires using the &#39;ssh&#39; and &#39;scp&#39; commands from your terminal. Instructions on how to do this are below, but please come to office hours if this whole process looks foreign to you. It will take about 30 seconds to show you in person, versus much longer than 30 seconds of playing guess-and-check on your own. :)åÊ</p>
<p></p>
<p>You will be turning in your entire iPython notebook. Once you haveåÊyour notebook copied over to the eniac machines and sshed yourself over to the eniac machines (instructions below), you will run.</p>
<p>åÊ</p>
<pre>turnin -c nets213 -p python-bootcamp -v IPythonBootcamp.ipynb</pre>
<p>åÊ</p>
<p>Here is the process, from the beginning. The lines beginning with &#39;$&#39; or &#39;~&gt;&#39; are things you will need to type. Lines beginning with &#39;#&#39; are comments, explaining the commands below. Lines that have no special characters at the show the output you should expect to see.</p>
<p></p>
<pre>#Copy your iPython file (which right now is on your computer) to your account on the eniac. You can do this using the &#39;scp&#39; or &#39;secure copy&#39; command, like below.
$ scp IPythonBootcamp.ipynb epavlick&#64;eniac.seas.upenn.edu:               
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
IPythonBootcamp.ipynb           

#Log on to the  eniac machines (using your usual penn username/password). You should see your command prompt change to say something like &#64;plus or &#64;minus, like below.
$ ssh epavlick&#64;eniac.seas.upenn.edu
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
Last login: Wed Jan 27 14:46:24 2016 from 10.251.134.177
epavlick&#64;plus:~&gt; 

#Make sure your file was copied over successfully. You can check by listing the contents of the current directory using the &#39;ls&#39; command. Hopefully you will see many things listed, and your IPython file should be one of them.
epavlick&#64;plus:~&gt; ls 
Desktop  Documents  Downloads  IPythonBootcamp.ipynb

#You can keep things organized by making a new folder for this assignment using the &#34;mkdir&#34; command, and moving your file into it using the &#34;mv&#34; command. See below.
epavlick&#64;plus:~&gt; mkdir python-bootcamp-submission
epavlick&#64;plus:~&gt; mv IPythonBootcamp.ipynb python-bootcamp-submission/

#To see the list of open projects for our class, type the following. Currently, only the bootcamp and the test project are open.
epavlick&#64;plus:~&gt; turnin -l -c nets213
Current projects for nets213:
python-bootcamp  on (alternate)
turnin-test      off
gun-classifier   off
crowdflower      off
crowdie          off
quality          off
the-end          off
test-turnin      on (current)

# To submit your bootcamp to turnin, use the following command. The 
epavlick&#64;plus:~&gt; turnin -c nets213 -p python-bootcamp -v python-bootcamp-submission/
drwxr-xr-x epavlick/epavlick 0 2016-01-27 15:53 python-bootcamp-submission/
-rw-r----- epavlick/epavlick 16606 2016-01-27 15:48 python-bootcamp-submission/IPythonBootcamp.ipynb
Compressing submitted files... please wait
Your files have been submitted to nets213, python-bootcamp for grading.
</pre>
<p>åÊåÊ</p>
<p>If you want to practice, please try submitting to the test project (called test-turnin), just to make sure everything is working smoothly. You can just submit a blank file, or you are welcome to go on a meaningless rant and submit that. Whatever makes you happy.åÊ</p>
<p></p>
<p>Please let usåÊknow if you have any problems or questions.</p>
<p></p>
<p>Ellie</p>",Submitting assignments via turnin,"When using the $ scp assignment4 pennkey&#64;eniac.seas.upenn.edu command, I am unable to copy over the directory to my account. Not sure what else to do.",Unable to turnin Homework 4,,,"<p>Are the submissions for the latest homework open yet?</p>
<p>I tried to submit and received the error turnin: submissions for -crowdie have been turned off.</p>",HW8 turnin turned off?,0
940850927,4/26/2016 15:44:24,true,1969396765,,4/26/2016 15:31:20,false,neodev,1.0,28875937,PAK,04,Lahore,119.153.154.137,0,0,"<p>Is there a way for us to see which files we turned in for a specific assignment, like #5?</p>",turnin,"<p>Hi Everyone,</p>
<p></p>
<p>Many of you had trouble with turnin for the bootcamp, and sent us email submissions. As a general policy, we cannot accept submissions via email. For this assignment, we will not penalize late submissions, so if you have not yet submitted via turnin, or you are still having trouble using scp/ssh, reach out to us and let&#39;s get it figured out.åÊ</p>
<p></p>
<p>For future assignments, we will only consider an assignment to be submitted if it is submitted via turnin before the deadline. Keep in mind our late-day policy: you are allowed 5 total late days during the semester with no penalty. After that, we deduct one point per day late.</p>
<p></p>
<p>Everyone have a wonderful weekend!</p>
<p>Ellie</p>",Late submissions and turnin,"<p>Hi everyone,</p>
<p>åÊ</p>
<p>For your python bootcamp, and for many other assignments this semester, you will be using &#39;turnin&#39;, which is a program on eniac. This requires using the &#39;ssh&#39; and &#39;scp&#39; commands from your terminal. Instructions on how to do this are below, but please come to office hours if this whole process looks foreign to you. It will take about 30 seconds to show you in person, versus much longer than 30 seconds of playing guess-and-check on your own. :)åÊ</p>
<p></p>
<p>You will be turning in your entire iPython notebook. Once you haveåÊyour notebook copied over to the eniac machines and sshed yourself over to the eniac machines (instructions below), you will run.</p>
<p>åÊ</p>
<pre>turnin -c nets213 -p python-bootcamp -v IPythonBootcamp.ipynb</pre>
<p>åÊ</p>
<p>Here is the process, from the beginning. The lines beginning with &#39;$&#39; or &#39;~&gt;&#39; are things you will need to type. Lines beginning with &#39;#&#39; are comments, explaining the commands below. Lines that have no special characters at the show the output you should expect to see.</p>
<p></p>
<pre>#Copy your iPython file (which right now is on your computer) to your account on the eniac. You can do this using the &#39;scp&#39; or &#39;secure copy&#39; command, like below.
$ scp IPythonBootcamp.ipynb epavlick&#64;eniac.seas.upenn.edu:               
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
IPythonBootcamp.ipynb           

#Log on to the  eniac machines (using your usual penn username/password). You should see your command prompt change to say something like &#64;plus or &#64;minus, like below.
$ ssh epavlick&#64;eniac.seas.upenn.edu
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
Last login: Wed Jan 27 14:46:24 2016 from 10.251.134.177
epavlick&#64;plus:~&gt; 

#Make sure your file was copied over successfully. You can check by listing the contents of the current directory using the &#39;ls&#39; command. Hopefully you will see many things listed, and your IPython file should be one of them.
epavlick&#64;plus:~&gt; ls 
Desktop  Documents  Downloads  IPythonBootcamp.ipynb

#You can keep things organized by making a new folder for this assignment using the &#34;mkdir&#34; command, and moving your file into it using the &#34;mv&#34; command. See below.
epavlick&#64;plus:~&gt; mkdir python-bootcamp-submission
epavlick&#64;plus:~&gt; mv IPythonBootcamp.ipynb python-bootcamp-submission/

#To see the list of open projects for our class, type the following. Currently, only the bootcamp and the test project are open.
epavlick&#64;plus:~&gt; turnin -l -c nets213
Current projects for nets213:
python-bootcamp  on (alternate)
turnin-test      off
gun-classifier   off
crowdflower      off
crowdie          off
quality          off
the-end          off
test-turnin      on (current)

# To submit your bootcamp to turnin, use the following command. The 
epavlick&#64;plus:~&gt; turnin -c nets213 -p python-bootcamp -v python-bootcamp-submission/
drwxr-xr-x epavlick/epavlick 0 2016-01-27 15:53 python-bootcamp-submission/
-rw-r----- epavlick/epavlick 16606 2016-01-27 15:48 python-bootcamp-submission/IPythonBootcamp.ipynb
Compressing submitted files... please wait
Your files have been submitted to nets213, python-bootcamp for grading.
</pre>
<p>åÊåÊ</p>
<p>If you want to practice, please try submitting to the test project (called test-turnin), just to make sure everything is working smoothly. You can just submit a blank file, or you are welcome to go on a meaningless rant and submit that. Whatever makes you happy.åÊ</p>
<p></p>
<p>Please let usåÊknow if you have any problems or questions.</p>
<p></p>
<p>Ellie</p>",Submitting assignments via turnin,"When using the $ scp assignment4 pennkey&#64;eniac.seas.upenn.edu command, I am unable to copy over the directory to my account. Not sure what else to do.",Unable to turnin Homework 4,,,"<p>Are the submissions for the latest homework open yet?</p>
<p>I tried to submit and received the error turnin: submissions for -crowdie have been turned off.</p>",HW8 turnin turned off?,0
940850927,4/26/2016 15:45:33,true,1969397155,,4/26/2016 15:43:24,false,neodev,0.8889,21971187,TTO,08,Valsayn,190.213.132.190,0,0,"<p>Is there a way for us to see which files we turned in for a specific assignment, like #5?</p>",turnin,"<p>Hi Everyone,</p>
<p></p>
<p>Many of you had trouble with turnin for the bootcamp, and sent us email submissions. As a general policy, we cannot accept submissions via email. For this assignment, we will not penalize late submissions, so if you have not yet submitted via turnin, or you are still having trouble using scp/ssh, reach out to us and let&#39;s get it figured out.åÊ</p>
<p></p>
<p>For future assignments, we will only consider an assignment to be submitted if it is submitted via turnin before the deadline. Keep in mind our late-day policy: you are allowed 5 total late days during the semester with no penalty. After that, we deduct one point per day late.</p>
<p></p>
<p>Everyone have a wonderful weekend!</p>
<p>Ellie</p>",Late submissions and turnin,"<p>Hi everyone,</p>
<p>åÊ</p>
<p>For your python bootcamp, and for many other assignments this semester, you will be using &#39;turnin&#39;, which is a program on eniac. This requires using the &#39;ssh&#39; and &#39;scp&#39; commands from your terminal. Instructions on how to do this are below, but please come to office hours if this whole process looks foreign to you. It will take about 30 seconds to show you in person, versus much longer than 30 seconds of playing guess-and-check on your own. :)åÊ</p>
<p></p>
<p>You will be turning in your entire iPython notebook. Once you haveåÊyour notebook copied over to the eniac machines and sshed yourself over to the eniac machines (instructions below), you will run.</p>
<p>åÊ</p>
<pre>turnin -c nets213 -p python-bootcamp -v IPythonBootcamp.ipynb</pre>
<p>åÊ</p>
<p>Here is the process, from the beginning. The lines beginning with &#39;$&#39; or &#39;~&gt;&#39; are things you will need to type. Lines beginning with &#39;#&#39; are comments, explaining the commands below. Lines that have no special characters at the show the output you should expect to see.</p>
<p></p>
<pre>#Copy your iPython file (which right now is on your computer) to your account on the eniac. You can do this using the &#39;scp&#39; or &#39;secure copy&#39; command, like below.
$ scp IPythonBootcamp.ipynb epavlick&#64;eniac.seas.upenn.edu:               
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
IPythonBootcamp.ipynb           

#Log on to the  eniac machines (using your usual penn username/password). You should see your command prompt change to say something like &#64;plus or &#64;minus, like below.
$ ssh epavlick&#64;eniac.seas.upenn.edu
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
Last login: Wed Jan 27 14:46:24 2016 from 10.251.134.177
epavlick&#64;plus:~&gt; 

#Make sure your file was copied over successfully. You can check by listing the contents of the current directory using the &#39;ls&#39; command. Hopefully you will see many things listed, and your IPython file should be one of them.
epavlick&#64;plus:~&gt; ls 
Desktop  Documents  Downloads  IPythonBootcamp.ipynb

#You can keep things organized by making a new folder for this assignment using the &#34;mkdir&#34; command, and moving your file into it using the &#34;mv&#34; command. See below.
epavlick&#64;plus:~&gt; mkdir python-bootcamp-submission
epavlick&#64;plus:~&gt; mv IPythonBootcamp.ipynb python-bootcamp-submission/

#To see the list of open projects for our class, type the following. Currently, only the bootcamp and the test project are open.
epavlick&#64;plus:~&gt; turnin -l -c nets213
Current projects for nets213:
python-bootcamp  on (alternate)
turnin-test      off
gun-classifier   off
crowdflower      off
crowdie          off
quality          off
the-end          off
test-turnin      on (current)

# To submit your bootcamp to turnin, use the following command. The 
epavlick&#64;plus:~&gt; turnin -c nets213 -p python-bootcamp -v python-bootcamp-submission/
drwxr-xr-x epavlick/epavlick 0 2016-01-27 15:53 python-bootcamp-submission/
-rw-r----- epavlick/epavlick 16606 2016-01-27 15:48 python-bootcamp-submission/IPythonBootcamp.ipynb
Compressing submitted files... please wait
Your files have been submitted to nets213, python-bootcamp for grading.
</pre>
<p>åÊåÊ</p>
<p>If you want to practice, please try submitting to the test project (called test-turnin), just to make sure everything is working smoothly. You can just submit a blank file, or you are welcome to go on a meaningless rant and submit that. Whatever makes you happy.åÊ</p>
<p></p>
<p>Please let usåÊknow if you have any problems or questions.</p>
<p></p>
<p>Ellie</p>",Submitting assignments via turnin,"When using the $ scp assignment4 pennkey&#64;eniac.seas.upenn.edu command, I am unable to copy over the directory to my account. Not sure what else to do.",Unable to turnin Homework 4,,,"<p>Are the submissions for the latest homework open yet?</p>
<p>I tried to submit and received the error turnin: submissions for -crowdie have been turned off.</p>",HW8 turnin turned off?,0
940850927,4/26/2016 15:45:34,true,1969397171,,4/26/2016 15:37:11,false,elite,0.8889,36575101,IND,07,New Delhi,112.196.144.2,0,0,"<p>Is there a way for us to see which files we turned in for a specific assignment, like #5?</p>",turnin,"<p>Hi Everyone,</p>
<p></p>
<p>Many of you had trouble with turnin for the bootcamp, and sent us email submissions. As a general policy, we cannot accept submissions via email. For this assignment, we will not penalize late submissions, so if you have not yet submitted via turnin, or you are still having trouble using scp/ssh, reach out to us and let&#39;s get it figured out.åÊ</p>
<p></p>
<p>For future assignments, we will only consider an assignment to be submitted if it is submitted via turnin before the deadline. Keep in mind our late-day policy: you are allowed 5 total late days during the semester with no penalty. After that, we deduct one point per day late.</p>
<p></p>
<p>Everyone have a wonderful weekend!</p>
<p>Ellie</p>",Late submissions and turnin,"<p>Hi everyone,</p>
<p>åÊ</p>
<p>For your python bootcamp, and for many other assignments this semester, you will be using &#39;turnin&#39;, which is a program on eniac. This requires using the &#39;ssh&#39; and &#39;scp&#39; commands from your terminal. Instructions on how to do this are below, but please come to office hours if this whole process looks foreign to you. It will take about 30 seconds to show you in person, versus much longer than 30 seconds of playing guess-and-check on your own. :)åÊ</p>
<p></p>
<p>You will be turning in your entire iPython notebook. Once you haveåÊyour notebook copied over to the eniac machines and sshed yourself over to the eniac machines (instructions below), you will run.</p>
<p>åÊ</p>
<pre>turnin -c nets213 -p python-bootcamp -v IPythonBootcamp.ipynb</pre>
<p>åÊ</p>
<p>Here is the process, from the beginning. The lines beginning with &#39;$&#39; or &#39;~&gt;&#39; are things you will need to type. Lines beginning with &#39;#&#39; are comments, explaining the commands below. Lines that have no special characters at the show the output you should expect to see.</p>
<p></p>
<pre>#Copy your iPython file (which right now is on your computer) to your account on the eniac. You can do this using the &#39;scp&#39; or &#39;secure copy&#39; command, like below.
$ scp IPythonBootcamp.ipynb epavlick&#64;eniac.seas.upenn.edu:               
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
IPythonBootcamp.ipynb           

#Log on to the  eniac machines (using your usual penn username/password). You should see your command prompt change to say something like &#64;plus or &#64;minus, like below.
$ ssh epavlick&#64;eniac.seas.upenn.edu
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
Last login: Wed Jan 27 14:46:24 2016 from 10.251.134.177
epavlick&#64;plus:~&gt; 

#Make sure your file was copied over successfully. You can check by listing the contents of the current directory using the &#39;ls&#39; command. Hopefully you will see many things listed, and your IPython file should be one of them.
epavlick&#64;plus:~&gt; ls 
Desktop  Documents  Downloads  IPythonBootcamp.ipynb

#You can keep things organized by making a new folder for this assignment using the &#34;mkdir&#34; command, and moving your file into it using the &#34;mv&#34; command. See below.
epavlick&#64;plus:~&gt; mkdir python-bootcamp-submission
epavlick&#64;plus:~&gt; mv IPythonBootcamp.ipynb python-bootcamp-submission/

#To see the list of open projects for our class, type the following. Currently, only the bootcamp and the test project are open.
epavlick&#64;plus:~&gt; turnin -l -c nets213
Current projects for nets213:
python-bootcamp  on (alternate)
turnin-test      off
gun-classifier   off
crowdflower      off
crowdie          off
quality          off
the-end          off
test-turnin      on (current)

# To submit your bootcamp to turnin, use the following command. The 
epavlick&#64;plus:~&gt; turnin -c nets213 -p python-bootcamp -v python-bootcamp-submission/
drwxr-xr-x epavlick/epavlick 0 2016-01-27 15:53 python-bootcamp-submission/
-rw-r----- epavlick/epavlick 16606 2016-01-27 15:48 python-bootcamp-submission/IPythonBootcamp.ipynb
Compressing submitted files... please wait
Your files have been submitted to nets213, python-bootcamp for grading.
</pre>
<p>åÊåÊ</p>
<p>If you want to practice, please try submitting to the test project (called test-turnin), just to make sure everything is working smoothly. You can just submit a blank file, or you are welcome to go on a meaningless rant and submit that. Whatever makes you happy.åÊ</p>
<p></p>
<p>Please let usåÊknow if you have any problems or questions.</p>
<p></p>
<p>Ellie</p>",Submitting assignments via turnin,"When using the $ scp assignment4 pennkey&#64;eniac.seas.upenn.edu command, I am unable to copy over the directory to my account. Not sure what else to do.",Unable to turnin Homework 4,,,"<p>Are the submissions for the latest homework open yet?</p>
<p>I tried to submit and received the error turnin: submissions for -crowdie have been turned off.</p>",HW8 turnin turned off?,0
940850927,4/26/2016 15:59:59,true,1969403526,,4/26/2016 15:58:26,false,elite,1.0,33243069,IND,10,Faridabad,116.203.79.150,0,0,"<p>Is there a way for us to see which files we turned in for a specific assignment, like #5?</p>",turnin,"<p>Hi Everyone,</p>
<p></p>
<p>Many of you had trouble with turnin for the bootcamp, and sent us email submissions. As a general policy, we cannot accept submissions via email. For this assignment, we will not penalize late submissions, so if you have not yet submitted via turnin, or you are still having trouble using scp/ssh, reach out to us and let&#39;s get it figured out.åÊ</p>
<p></p>
<p>For future assignments, we will only consider an assignment to be submitted if it is submitted via turnin before the deadline. Keep in mind our late-day policy: you are allowed 5 total late days during the semester with no penalty. After that, we deduct one point per day late.</p>
<p></p>
<p>Everyone have a wonderful weekend!</p>
<p>Ellie</p>",Late submissions and turnin,"<p>Hi everyone,</p>
<p>åÊ</p>
<p>For your python bootcamp, and for many other assignments this semester, you will be using &#39;turnin&#39;, which is a program on eniac. This requires using the &#39;ssh&#39; and &#39;scp&#39; commands from your terminal. Instructions on how to do this are below, but please come to office hours if this whole process looks foreign to you. It will take about 30 seconds to show you in person, versus much longer than 30 seconds of playing guess-and-check on your own. :)åÊ</p>
<p></p>
<p>You will be turning in your entire iPython notebook. Once you haveåÊyour notebook copied over to the eniac machines and sshed yourself over to the eniac machines (instructions below), you will run.</p>
<p>åÊ</p>
<pre>turnin -c nets213 -p python-bootcamp -v IPythonBootcamp.ipynb</pre>
<p>åÊ</p>
<p>Here is the process, from the beginning. The lines beginning with &#39;$&#39; or &#39;~&gt;&#39; are things you will need to type. Lines beginning with &#39;#&#39; are comments, explaining the commands below. Lines that have no special characters at the show the output you should expect to see.</p>
<p></p>
<pre>#Copy your iPython file (which right now is on your computer) to your account on the eniac. You can do this using the &#39;scp&#39; or &#39;secure copy&#39; command, like below.
$ scp IPythonBootcamp.ipynb epavlick&#64;eniac.seas.upenn.edu:               
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
IPythonBootcamp.ipynb           

#Log on to the  eniac machines (using your usual penn username/password). You should see your command prompt change to say something like &#64;plus or &#64;minus, like below.
$ ssh epavlick&#64;eniac.seas.upenn.edu
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
Last login: Wed Jan 27 14:46:24 2016 from 10.251.134.177
epavlick&#64;plus:~&gt; 

#Make sure your file was copied over successfully. You can check by listing the contents of the current directory using the &#39;ls&#39; command. Hopefully you will see many things listed, and your IPython file should be one of them.
epavlick&#64;plus:~&gt; ls 
Desktop  Documents  Downloads  IPythonBootcamp.ipynb

#You can keep things organized by making a new folder for this assignment using the &#34;mkdir&#34; command, and moving your file into it using the &#34;mv&#34; command. See below.
epavlick&#64;plus:~&gt; mkdir python-bootcamp-submission
epavlick&#64;plus:~&gt; mv IPythonBootcamp.ipynb python-bootcamp-submission/

#To see the list of open projects for our class, type the following. Currently, only the bootcamp and the test project are open.
epavlick&#64;plus:~&gt; turnin -l -c nets213
Current projects for nets213:
python-bootcamp  on (alternate)
turnin-test      off
gun-classifier   off
crowdflower      off
crowdie          off
quality          off
the-end          off
test-turnin      on (current)

# To submit your bootcamp to turnin, use the following command. The 
epavlick&#64;plus:~&gt; turnin -c nets213 -p python-bootcamp -v python-bootcamp-submission/
drwxr-xr-x epavlick/epavlick 0 2016-01-27 15:53 python-bootcamp-submission/
-rw-r----- epavlick/epavlick 16606 2016-01-27 15:48 python-bootcamp-submission/IPythonBootcamp.ipynb
Compressing submitted files... please wait
Your files have been submitted to nets213, python-bootcamp for grading.
</pre>
<p>åÊåÊ</p>
<p>If you want to practice, please try submitting to the test project (called test-turnin), just to make sure everything is working smoothly. You can just submit a blank file, or you are welcome to go on a meaningless rant and submit that. Whatever makes you happy.åÊ</p>
<p></p>
<p>Please let usåÊknow if you have any problems or questions.</p>
<p></p>
<p>Ellie</p>",Submitting assignments via turnin,"When using the $ scp assignment4 pennkey&#64;eniac.seas.upenn.edu command, I am unable to copy over the directory to my account. Not sure what else to do.",Unable to turnin Homework 4,,,"<p>Are the submissions for the latest homework open yet?</p>
<p>I tried to submit and received the error turnin: submissions for -crowdie have been turned off.</p>",HW8 turnin turned off?,0
940850927,4/26/2016 16:03:21,true,1969405340,,4/26/2016 16:02:52,false,personaly,1.0,33663352,ARG,01,Mar Del Plata,181.168.213.227,0,0,"<p>Is there a way for us to see which files we turned in for a specific assignment, like #5?</p>",turnin,"<p>Hi Everyone,</p>
<p></p>
<p>Many of you had trouble with turnin for the bootcamp, and sent us email submissions. As a general policy, we cannot accept submissions via email. For this assignment, we will not penalize late submissions, so if you have not yet submitted via turnin, or you are still having trouble using scp/ssh, reach out to us and let&#39;s get it figured out.åÊ</p>
<p></p>
<p>For future assignments, we will only consider an assignment to be submitted if it is submitted via turnin before the deadline. Keep in mind our late-day policy: you are allowed 5 total late days during the semester with no penalty. After that, we deduct one point per day late.</p>
<p></p>
<p>Everyone have a wonderful weekend!</p>
<p>Ellie</p>",Late submissions and turnin,"<p>Hi everyone,</p>
<p>åÊ</p>
<p>For your python bootcamp, and for many other assignments this semester, you will be using &#39;turnin&#39;, which is a program on eniac. This requires using the &#39;ssh&#39; and &#39;scp&#39; commands from your terminal. Instructions on how to do this are below, but please come to office hours if this whole process looks foreign to you. It will take about 30 seconds to show you in person, versus much longer than 30 seconds of playing guess-and-check on your own. :)åÊ</p>
<p></p>
<p>You will be turning in your entire iPython notebook. Once you haveåÊyour notebook copied over to the eniac machines and sshed yourself over to the eniac machines (instructions below), you will run.</p>
<p>åÊ</p>
<pre>turnin -c nets213 -p python-bootcamp -v IPythonBootcamp.ipynb</pre>
<p>åÊ</p>
<p>Here is the process, from the beginning. The lines beginning with &#39;$&#39; or &#39;~&gt;&#39; are things you will need to type. Lines beginning with &#39;#&#39; are comments, explaining the commands below. Lines that have no special characters at the show the output you should expect to see.</p>
<p></p>
<pre>#Copy your iPython file (which right now is on your computer) to your account on the eniac. You can do this using the &#39;scp&#39; or &#39;secure copy&#39; command, like below.
$ scp IPythonBootcamp.ipynb epavlick&#64;eniac.seas.upenn.edu:               
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
IPythonBootcamp.ipynb           

#Log on to the  eniac machines (using your usual penn username/password). You should see your command prompt change to say something like &#64;plus or &#64;minus, like below.
$ ssh epavlick&#64;eniac.seas.upenn.edu
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
Last login: Wed Jan 27 14:46:24 2016 from 10.251.134.177
epavlick&#64;plus:~&gt; 

#Make sure your file was copied over successfully. You can check by listing the contents of the current directory using the &#39;ls&#39; command. Hopefully you will see many things listed, and your IPython file should be one of them.
epavlick&#64;plus:~&gt; ls 
Desktop  Documents  Downloads  IPythonBootcamp.ipynb

#You can keep things organized by making a new folder for this assignment using the &#34;mkdir&#34; command, and moving your file into it using the &#34;mv&#34; command. See below.
epavlick&#64;plus:~&gt; mkdir python-bootcamp-submission
epavlick&#64;plus:~&gt; mv IPythonBootcamp.ipynb python-bootcamp-submission/

#To see the list of open projects for our class, type the following. Currently, only the bootcamp and the test project are open.
epavlick&#64;plus:~&gt; turnin -l -c nets213
Current projects for nets213:
python-bootcamp  on (alternate)
turnin-test      off
gun-classifier   off
crowdflower      off
crowdie          off
quality          off
the-end          off
test-turnin      on (current)

# To submit your bootcamp to turnin, use the following command. The 
epavlick&#64;plus:~&gt; turnin -c nets213 -p python-bootcamp -v python-bootcamp-submission/
drwxr-xr-x epavlick/epavlick 0 2016-01-27 15:53 python-bootcamp-submission/
-rw-r----- epavlick/epavlick 16606 2016-01-27 15:48 python-bootcamp-submission/IPythonBootcamp.ipynb
Compressing submitted files... please wait
Your files have been submitted to nets213, python-bootcamp for grading.
</pre>
<p>åÊåÊ</p>
<p>If you want to practice, please try submitting to the test project (called test-turnin), just to make sure everything is working smoothly. You can just submit a blank file, or you are welcome to go on a meaningless rant and submit that. Whatever makes you happy.åÊ</p>
<p></p>
<p>Please let usåÊknow if you have any problems or questions.</p>
<p></p>
<p>Ellie</p>",Submitting assignments via turnin,"When using the $ scp assignment4 pennkey&#64;eniac.seas.upenn.edu command, I am unable to copy over the directory to my account. Not sure what else to do.",Unable to turnin Homework 4,,,"<p>Are the submissions for the latest homework open yet?</p>
<p>I tried to submit and received the error turnin: submissions for -crowdie have been turned off.</p>",HW8 turnin turned off?,0
940850927,4/26/2016 16:15:36,true,1969411455,,4/26/2016 16:12:15,false,elite,1.0,30128662,BGR,50,Pleven,212.233.177.195,0,0,"<p>Is there a way for us to see which files we turned in for a specific assignment, like #5?</p>",turnin,"<p>Hi Everyone,</p>
<p></p>
<p>Many of you had trouble with turnin for the bootcamp, and sent us email submissions. As a general policy, we cannot accept submissions via email. For this assignment, we will not penalize late submissions, so if you have not yet submitted via turnin, or you are still having trouble using scp/ssh, reach out to us and let&#39;s get it figured out.åÊ</p>
<p></p>
<p>For future assignments, we will only consider an assignment to be submitted if it is submitted via turnin before the deadline. Keep in mind our late-day policy: you are allowed 5 total late days during the semester with no penalty. After that, we deduct one point per day late.</p>
<p></p>
<p>Everyone have a wonderful weekend!</p>
<p>Ellie</p>",Late submissions and turnin,"<p>Hi everyone,</p>
<p>åÊ</p>
<p>For your python bootcamp, and for many other assignments this semester, you will be using &#39;turnin&#39;, which is a program on eniac. This requires using the &#39;ssh&#39; and &#39;scp&#39; commands from your terminal. Instructions on how to do this are below, but please come to office hours if this whole process looks foreign to you. It will take about 30 seconds to show you in person, versus much longer than 30 seconds of playing guess-and-check on your own. :)åÊ</p>
<p></p>
<p>You will be turning in your entire iPython notebook. Once you haveåÊyour notebook copied over to the eniac machines and sshed yourself over to the eniac machines (instructions below), you will run.</p>
<p>åÊ</p>
<pre>turnin -c nets213 -p python-bootcamp -v IPythonBootcamp.ipynb</pre>
<p>åÊ</p>
<p>Here is the process, from the beginning. The lines beginning with &#39;$&#39; or &#39;~&gt;&#39; are things you will need to type. Lines beginning with &#39;#&#39; are comments, explaining the commands below. Lines that have no special characters at the show the output you should expect to see.</p>
<p></p>
<pre>#Copy your iPython file (which right now is on your computer) to your account on the eniac. You can do this using the &#39;scp&#39; or &#39;secure copy&#39; command, like below.
$ scp IPythonBootcamp.ipynb epavlick&#64;eniac.seas.upenn.edu:               
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
IPythonBootcamp.ipynb           

#Log on to the  eniac machines (using your usual penn username/password). You should see your command prompt change to say something like &#64;plus or &#64;minus, like below.
$ ssh epavlick&#64;eniac.seas.upenn.edu
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
Last login: Wed Jan 27 14:46:24 2016 from 10.251.134.177
epavlick&#64;plus:~&gt; 

#Make sure your file was copied over successfully. You can check by listing the contents of the current directory using the &#39;ls&#39; command. Hopefully you will see many things listed, and your IPython file should be one of them.
epavlick&#64;plus:~&gt; ls 
Desktop  Documents  Downloads  IPythonBootcamp.ipynb

#You can keep things organized by making a new folder for this assignment using the &#34;mkdir&#34; command, and moving your file into it using the &#34;mv&#34; command. See below.
epavlick&#64;plus:~&gt; mkdir python-bootcamp-submission
epavlick&#64;plus:~&gt; mv IPythonBootcamp.ipynb python-bootcamp-submission/

#To see the list of open projects for our class, type the following. Currently, only the bootcamp and the test project are open.
epavlick&#64;plus:~&gt; turnin -l -c nets213
Current projects for nets213:
python-bootcamp  on (alternate)
turnin-test      off
gun-classifier   off
crowdflower      off
crowdie          off
quality          off
the-end          off
test-turnin      on (current)

# To submit your bootcamp to turnin, use the following command. The 
epavlick&#64;plus:~&gt; turnin -c nets213 -p python-bootcamp -v python-bootcamp-submission/
drwxr-xr-x epavlick/epavlick 0 2016-01-27 15:53 python-bootcamp-submission/
-rw-r----- epavlick/epavlick 16606 2016-01-27 15:48 python-bootcamp-submission/IPythonBootcamp.ipynb
Compressing submitted files... please wait
Your files have been submitted to nets213, python-bootcamp for grading.
</pre>
<p>åÊåÊ</p>
<p>If you want to practice, please try submitting to the test project (called test-turnin), just to make sure everything is working smoothly. You can just submit a blank file, or you are welcome to go on a meaningless rant and submit that. Whatever makes you happy.åÊ</p>
<p></p>
<p>Please let usåÊknow if you have any problems or questions.</p>
<p></p>
<p>Ellie</p>",Submitting assignments via turnin,"When using the $ scp assignment4 pennkey&#64;eniac.seas.upenn.edu command, I am unable to copy over the directory to my account. Not sure what else to do.",Unable to turnin Homework 4,,,"<p>Are the submissions for the latest homework open yet?</p>
<p>I tried to submit and received the error turnin: submissions for -crowdie have been turned off.</p>",HW8 turnin turned off?,0
940850927,4/26/2016 16:22:17,true,1969417142,,4/26/2016 16:18:27,false,neodev,1.0,29175140,VEN,25,Caracas,190.72.125.134,0,0,"<p>Is there a way for us to see which files we turned in for a specific assignment, like #5?</p>",turnin,"<p>Hi Everyone,</p>
<p></p>
<p>Many of you had trouble with turnin for the bootcamp, and sent us email submissions. As a general policy, we cannot accept submissions via email. For this assignment, we will not penalize late submissions, so if you have not yet submitted via turnin, or you are still having trouble using scp/ssh, reach out to us and let&#39;s get it figured out.åÊ</p>
<p></p>
<p>For future assignments, we will only consider an assignment to be submitted if it is submitted via turnin before the deadline. Keep in mind our late-day policy: you are allowed 5 total late days during the semester with no penalty. After that, we deduct one point per day late.</p>
<p></p>
<p>Everyone have a wonderful weekend!</p>
<p>Ellie</p>",Late submissions and turnin,"<p>Hi everyone,</p>
<p>åÊ</p>
<p>For your python bootcamp, and for many other assignments this semester, you will be using &#39;turnin&#39;, which is a program on eniac. This requires using the &#39;ssh&#39; and &#39;scp&#39; commands from your terminal. Instructions on how to do this are below, but please come to office hours if this whole process looks foreign to you. It will take about 30 seconds to show you in person, versus much longer than 30 seconds of playing guess-and-check on your own. :)åÊ</p>
<p></p>
<p>You will be turning in your entire iPython notebook. Once you haveåÊyour notebook copied over to the eniac machines and sshed yourself over to the eniac machines (instructions below), you will run.</p>
<p>åÊ</p>
<pre>turnin -c nets213 -p python-bootcamp -v IPythonBootcamp.ipynb</pre>
<p>åÊ</p>
<p>Here is the process, from the beginning. The lines beginning with &#39;$&#39; or &#39;~&gt;&#39; are things you will need to type. Lines beginning with &#39;#&#39; are comments, explaining the commands below. Lines that have no special characters at the show the output you should expect to see.</p>
<p></p>
<pre>#Copy your iPython file (which right now is on your computer) to your account on the eniac. You can do this using the &#39;scp&#39; or &#39;secure copy&#39; command, like below.
$ scp IPythonBootcamp.ipynb epavlick&#64;eniac.seas.upenn.edu:               
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
IPythonBootcamp.ipynb           

#Log on to the  eniac machines (using your usual penn username/password). You should see your command prompt change to say something like &#64;plus or &#64;minus, like below.
$ ssh epavlick&#64;eniac.seas.upenn.edu
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
Last login: Wed Jan 27 14:46:24 2016 from 10.251.134.177
epavlick&#64;plus:~&gt; 

#Make sure your file was copied over successfully. You can check by listing the contents of the current directory using the &#39;ls&#39; command. Hopefully you will see many things listed, and your IPython file should be one of them.
epavlick&#64;plus:~&gt; ls 
Desktop  Documents  Downloads  IPythonBootcamp.ipynb

#You can keep things organized by making a new folder for this assignment using the &#34;mkdir&#34; command, and moving your file into it using the &#34;mv&#34; command. See below.
epavlick&#64;plus:~&gt; mkdir python-bootcamp-submission
epavlick&#64;plus:~&gt; mv IPythonBootcamp.ipynb python-bootcamp-submission/

#To see the list of open projects for our class, type the following. Currently, only the bootcamp and the test project are open.
epavlick&#64;plus:~&gt; turnin -l -c nets213
Current projects for nets213:
python-bootcamp  on (alternate)
turnin-test      off
gun-classifier   off
crowdflower      off
crowdie          off
quality          off
the-end          off
test-turnin      on (current)

# To submit your bootcamp to turnin, use the following command. The 
epavlick&#64;plus:~&gt; turnin -c nets213 -p python-bootcamp -v python-bootcamp-submission/
drwxr-xr-x epavlick/epavlick 0 2016-01-27 15:53 python-bootcamp-submission/
-rw-r----- epavlick/epavlick 16606 2016-01-27 15:48 python-bootcamp-submission/IPythonBootcamp.ipynb
Compressing submitted files... please wait
Your files have been submitted to nets213, python-bootcamp for grading.
</pre>
<p>åÊåÊ</p>
<p>If you want to practice, please try submitting to the test project (called test-turnin), just to make sure everything is working smoothly. You can just submit a blank file, or you are welcome to go on a meaningless rant and submit that. Whatever makes you happy.åÊ</p>
<p></p>
<p>Please let usåÊknow if you have any problems or questions.</p>
<p></p>
<p>Ellie</p>",Submitting assignments via turnin,"When using the $ scp assignment4 pennkey&#64;eniac.seas.upenn.edu command, I am unable to copy over the directory to my account. Not sure what else to do.",Unable to turnin Homework 4,,,"<p>Are the submissions for the latest homework open yet?</p>
<p>I tried to submit and received the error turnin: submissions for -crowdie have been turned off.</p>",HW8 turnin turned off?,0
940850927,4/26/2016 16:32:36,true,1969425037,,4/26/2016 16:13:00,false,clixsense,0.8889,8057247,PRT,17,Póvoa De Varzim,144.64.25.68,0,0,"<p>Is there a way for us to see which files we turned in for a specific assignment, like #5?</p>",turnin,"<p>Hi Everyone,</p>
<p></p>
<p>Many of you had trouble with turnin for the bootcamp, and sent us email submissions. As a general policy, we cannot accept submissions via email. For this assignment, we will not penalize late submissions, so if you have not yet submitted via turnin, or you are still having trouble using scp/ssh, reach out to us and let&#39;s get it figured out.åÊ</p>
<p></p>
<p>For future assignments, we will only consider an assignment to be submitted if it is submitted via turnin before the deadline. Keep in mind our late-day policy: you are allowed 5 total late days during the semester with no penalty. After that, we deduct one point per day late.</p>
<p></p>
<p>Everyone have a wonderful weekend!</p>
<p>Ellie</p>",Late submissions and turnin,"<p>Hi everyone,</p>
<p>åÊ</p>
<p>For your python bootcamp, and for many other assignments this semester, you will be using &#39;turnin&#39;, which is a program on eniac. This requires using the &#39;ssh&#39; and &#39;scp&#39; commands from your terminal. Instructions on how to do this are below, but please come to office hours if this whole process looks foreign to you. It will take about 30 seconds to show you in person, versus much longer than 30 seconds of playing guess-and-check on your own. :)åÊ</p>
<p></p>
<p>You will be turning in your entire iPython notebook. Once you haveåÊyour notebook copied over to the eniac machines and sshed yourself over to the eniac machines (instructions below), you will run.</p>
<p>åÊ</p>
<pre>turnin -c nets213 -p python-bootcamp -v IPythonBootcamp.ipynb</pre>
<p>åÊ</p>
<p>Here is the process, from the beginning. The lines beginning with &#39;$&#39; or &#39;~&gt;&#39; are things you will need to type. Lines beginning with &#39;#&#39; are comments, explaining the commands below. Lines that have no special characters at the show the output you should expect to see.</p>
<p></p>
<pre>#Copy your iPython file (which right now is on your computer) to your account on the eniac. You can do this using the &#39;scp&#39; or &#39;secure copy&#39; command, like below.
$ scp IPythonBootcamp.ipynb epavlick&#64;eniac.seas.upenn.edu:               
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
IPythonBootcamp.ipynb           

#Log on to the  eniac machines (using your usual penn username/password). You should see your command prompt change to say something like &#64;plus or &#64;minus, like below.
$ ssh epavlick&#64;eniac.seas.upenn.edu
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
Last login: Wed Jan 27 14:46:24 2016 from 10.251.134.177
epavlick&#64;plus:~&gt; 

#Make sure your file was copied over successfully. You can check by listing the contents of the current directory using the &#39;ls&#39; command. Hopefully you will see many things listed, and your IPython file should be one of them.
epavlick&#64;plus:~&gt; ls 
Desktop  Documents  Downloads  IPythonBootcamp.ipynb

#You can keep things organized by making a new folder for this assignment using the &#34;mkdir&#34; command, and moving your file into it using the &#34;mv&#34; command. See below.
epavlick&#64;plus:~&gt; mkdir python-bootcamp-submission
epavlick&#64;plus:~&gt; mv IPythonBootcamp.ipynb python-bootcamp-submission/

#To see the list of open projects for our class, type the following. Currently, only the bootcamp and the test project are open.
epavlick&#64;plus:~&gt; turnin -l -c nets213
Current projects for nets213:
python-bootcamp  on (alternate)
turnin-test      off
gun-classifier   off
crowdflower      off
crowdie          off
quality          off
the-end          off
test-turnin      on (current)

# To submit your bootcamp to turnin, use the following command. The 
epavlick&#64;plus:~&gt; turnin -c nets213 -p python-bootcamp -v python-bootcamp-submission/
drwxr-xr-x epavlick/epavlick 0 2016-01-27 15:53 python-bootcamp-submission/
-rw-r----- epavlick/epavlick 16606 2016-01-27 15:48 python-bootcamp-submission/IPythonBootcamp.ipynb
Compressing submitted files... please wait
Your files have been submitted to nets213, python-bootcamp for grading.
</pre>
<p>åÊåÊ</p>
<p>If you want to practice, please try submitting to the test project (called test-turnin), just to make sure everything is working smoothly. You can just submit a blank file, or you are welcome to go on a meaningless rant and submit that. Whatever makes you happy.åÊ</p>
<p></p>
<p>Please let usåÊknow if you have any problems or questions.</p>
<p></p>
<p>Ellie</p>",Submitting assignments via turnin,"When using the $ scp assignment4 pennkey&#64;eniac.seas.upenn.edu command, I am unable to copy over the directory to my account. Not sure what else to do.",Unable to turnin Homework 4,,,"<p>Are the submissions for the latest homework open yet?</p>
<p>I tried to submit and received the error turnin: submissions for -crowdie have been turned off.</p>",HW8 turnin turned off?,0
940850927,4/26/2016 16:40:52,true,1969429999,,4/26/2016 16:36:32,false,prodege,1.0,22387641,USA,PA,Jeannette,208.94.42.131,0,0,"<p>Is there a way for us to see which files we turned in for a specific assignment, like #5?</p>",turnin,"<p>Hi Everyone,</p>
<p></p>
<p>Many of you had trouble with turnin for the bootcamp, and sent us email submissions. As a general policy, we cannot accept submissions via email. For this assignment, we will not penalize late submissions, so if you have not yet submitted via turnin, or you are still having trouble using scp/ssh, reach out to us and let&#39;s get it figured out.åÊ</p>
<p></p>
<p>For future assignments, we will only consider an assignment to be submitted if it is submitted via turnin before the deadline. Keep in mind our late-day policy: you are allowed 5 total late days during the semester with no penalty. After that, we deduct one point per day late.</p>
<p></p>
<p>Everyone have a wonderful weekend!</p>
<p>Ellie</p>",Late submissions and turnin,"<p>Hi everyone,</p>
<p>åÊ</p>
<p>For your python bootcamp, and for many other assignments this semester, you will be using &#39;turnin&#39;, which is a program on eniac. This requires using the &#39;ssh&#39; and &#39;scp&#39; commands from your terminal. Instructions on how to do this are below, but please come to office hours if this whole process looks foreign to you. It will take about 30 seconds to show you in person, versus much longer than 30 seconds of playing guess-and-check on your own. :)åÊ</p>
<p></p>
<p>You will be turning in your entire iPython notebook. Once you haveåÊyour notebook copied over to the eniac machines and sshed yourself over to the eniac machines (instructions below), you will run.</p>
<p>åÊ</p>
<pre>turnin -c nets213 -p python-bootcamp -v IPythonBootcamp.ipynb</pre>
<p>åÊ</p>
<p>Here is the process, from the beginning. The lines beginning with &#39;$&#39; or &#39;~&gt;&#39; are things you will need to type. Lines beginning with &#39;#&#39; are comments, explaining the commands below. Lines that have no special characters at the show the output you should expect to see.</p>
<p></p>
<pre>#Copy your iPython file (which right now is on your computer) to your account on the eniac. You can do this using the &#39;scp&#39; or &#39;secure copy&#39; command, like below.
$ scp IPythonBootcamp.ipynb epavlick&#64;eniac.seas.upenn.edu:               
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
IPythonBootcamp.ipynb           

#Log on to the  eniac machines (using your usual penn username/password). You should see your command prompt change to say something like &#64;plus or &#64;minus, like below.
$ ssh epavlick&#64;eniac.seas.upenn.edu
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
Last login: Wed Jan 27 14:46:24 2016 from 10.251.134.177
epavlick&#64;plus:~&gt; 

#Make sure your file was copied over successfully. You can check by listing the contents of the current directory using the &#39;ls&#39; command. Hopefully you will see many things listed, and your IPython file should be one of them.
epavlick&#64;plus:~&gt; ls 
Desktop  Documents  Downloads  IPythonBootcamp.ipynb

#You can keep things organized by making a new folder for this assignment using the &#34;mkdir&#34; command, and moving your file into it using the &#34;mv&#34; command. See below.
epavlick&#64;plus:~&gt; mkdir python-bootcamp-submission
epavlick&#64;plus:~&gt; mv IPythonBootcamp.ipynb python-bootcamp-submission/

#To see the list of open projects for our class, type the following. Currently, only the bootcamp and the test project are open.
epavlick&#64;plus:~&gt; turnin -l -c nets213
Current projects for nets213:
python-bootcamp  on (alternate)
turnin-test      off
gun-classifier   off
crowdflower      off
crowdie          off
quality          off
the-end          off
test-turnin      on (current)

# To submit your bootcamp to turnin, use the following command. The 
epavlick&#64;plus:~&gt; turnin -c nets213 -p python-bootcamp -v python-bootcamp-submission/
drwxr-xr-x epavlick/epavlick 0 2016-01-27 15:53 python-bootcamp-submission/
-rw-r----- epavlick/epavlick 16606 2016-01-27 15:48 python-bootcamp-submission/IPythonBootcamp.ipynb
Compressing submitted files... please wait
Your files have been submitted to nets213, python-bootcamp for grading.
</pre>
<p>åÊåÊ</p>
<p>If you want to practice, please try submitting to the test project (called test-turnin), just to make sure everything is working smoothly. You can just submit a blank file, or you are welcome to go on a meaningless rant and submit that. Whatever makes you happy.åÊ</p>
<p></p>
<p>Please let usåÊknow if you have any problems or questions.</p>
<p></p>
<p>Ellie</p>",Submitting assignments via turnin,"When using the $ scp assignment4 pennkey&#64;eniac.seas.upenn.edu command, I am unable to copy over the directory to my account. Not sure what else to do.",Unable to turnin Homework 4,,,"<p>Are the submissions for the latest homework open yet?</p>
<p>I tried to submit and received the error turnin: submissions for -crowdie have been turned off.</p>",HW8 turnin turned off?,0
940850927,4/26/2016 16:41:30,true,1969430408,,4/26/2016 16:40:27,false,clixsense,1.0,6329782,IDN,07,Bekonang,202.67.40.31,0,0,"<p>Is there a way for us to see which files we turned in for a specific assignment, like #5?</p>",turnin,"<p>Hi Everyone,</p>
<p></p>
<p>Many of you had trouble with turnin for the bootcamp, and sent us email submissions. As a general policy, we cannot accept submissions via email. For this assignment, we will not penalize late submissions, so if you have not yet submitted via turnin, or you are still having trouble using scp/ssh, reach out to us and let&#39;s get it figured out.åÊ</p>
<p></p>
<p>For future assignments, we will only consider an assignment to be submitted if it is submitted via turnin before the deadline. Keep in mind our late-day policy: you are allowed 5 total late days during the semester with no penalty. After that, we deduct one point per day late.</p>
<p></p>
<p>Everyone have a wonderful weekend!</p>
<p>Ellie</p>",Late submissions and turnin,"<p>Hi everyone,</p>
<p>åÊ</p>
<p>For your python bootcamp, and for many other assignments this semester, you will be using &#39;turnin&#39;, which is a program on eniac. This requires using the &#39;ssh&#39; and &#39;scp&#39; commands from your terminal. Instructions on how to do this are below, but please come to office hours if this whole process looks foreign to you. It will take about 30 seconds to show you in person, versus much longer than 30 seconds of playing guess-and-check on your own. :)åÊ</p>
<p></p>
<p>You will be turning in your entire iPython notebook. Once you haveåÊyour notebook copied over to the eniac machines and sshed yourself over to the eniac machines (instructions below), you will run.</p>
<p>åÊ</p>
<pre>turnin -c nets213 -p python-bootcamp -v IPythonBootcamp.ipynb</pre>
<p>åÊ</p>
<p>Here is the process, from the beginning. The lines beginning with &#39;$&#39; or &#39;~&gt;&#39; are things you will need to type. Lines beginning with &#39;#&#39; are comments, explaining the commands below. Lines that have no special characters at the show the output you should expect to see.</p>
<p></p>
<pre>#Copy your iPython file (which right now is on your computer) to your account on the eniac. You can do this using the &#39;scp&#39; or &#39;secure copy&#39; command, like below.
$ scp IPythonBootcamp.ipynb epavlick&#64;eniac.seas.upenn.edu:               
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
IPythonBootcamp.ipynb           

#Log on to the  eniac machines (using your usual penn username/password). You should see your command prompt change to say something like &#64;plus or &#64;minus, like below.
$ ssh epavlick&#64;eniac.seas.upenn.edu
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
Last login: Wed Jan 27 14:46:24 2016 from 10.251.134.177
epavlick&#64;plus:~&gt; 

#Make sure your file was copied over successfully. You can check by listing the contents of the current directory using the &#39;ls&#39; command. Hopefully you will see many things listed, and your IPython file should be one of them.
epavlick&#64;plus:~&gt; ls 
Desktop  Documents  Downloads  IPythonBootcamp.ipynb

#You can keep things organized by making a new folder for this assignment using the &#34;mkdir&#34; command, and moving your file into it using the &#34;mv&#34; command. See below.
epavlick&#64;plus:~&gt; mkdir python-bootcamp-submission
epavlick&#64;plus:~&gt; mv IPythonBootcamp.ipynb python-bootcamp-submission/

#To see the list of open projects for our class, type the following. Currently, only the bootcamp and the test project are open.
epavlick&#64;plus:~&gt; turnin -l -c nets213
Current projects for nets213:
python-bootcamp  on (alternate)
turnin-test      off
gun-classifier   off
crowdflower      off
crowdie          off
quality          off
the-end          off
test-turnin      on (current)

# To submit your bootcamp to turnin, use the following command. The 
epavlick&#64;plus:~&gt; turnin -c nets213 -p python-bootcamp -v python-bootcamp-submission/
drwxr-xr-x epavlick/epavlick 0 2016-01-27 15:53 python-bootcamp-submission/
-rw-r----- epavlick/epavlick 16606 2016-01-27 15:48 python-bootcamp-submission/IPythonBootcamp.ipynb
Compressing submitted files... please wait
Your files have been submitted to nets213, python-bootcamp for grading.
</pre>
<p>åÊåÊ</p>
<p>If you want to practice, please try submitting to the test project (called test-turnin), just to make sure everything is working smoothly. You can just submit a blank file, or you are welcome to go on a meaningless rant and submit that. Whatever makes you happy.åÊ</p>
<p></p>
<p>Please let usåÊknow if you have any problems or questions.</p>
<p></p>
<p>Ellie</p>",Submitting assignments via turnin,"When using the $ scp assignment4 pennkey&#64;eniac.seas.upenn.edu command, I am unable to copy over the directory to my account. Not sure what else to do.",Unable to turnin Homework 4,,,"<p>Are the submissions for the latest homework open yet?</p>
<p>I tried to submit and received the error turnin: submissions for -crowdie have been turned off.</p>",HW8 turnin turned off?,0
940850927,4/26/2016 17:00:02,true,1969441674,,4/26/2016 16:58:39,false,clixsense,1.0,21408115,IDN,07,Semarang,36.79.23.180,0,0,"<p>Is there a way for us to see which files we turned in for a specific assignment, like #5?</p>",turnin,"<p>Hi Everyone,</p>
<p></p>
<p>Many of you had trouble with turnin for the bootcamp, and sent us email submissions. As a general policy, we cannot accept submissions via email. For this assignment, we will not penalize late submissions, so if you have not yet submitted via turnin, or you are still having trouble using scp/ssh, reach out to us and let&#39;s get it figured out.åÊ</p>
<p></p>
<p>For future assignments, we will only consider an assignment to be submitted if it is submitted via turnin before the deadline. Keep in mind our late-day policy: you are allowed 5 total late days during the semester with no penalty. After that, we deduct one point per day late.</p>
<p></p>
<p>Everyone have a wonderful weekend!</p>
<p>Ellie</p>",Late submissions and turnin,"<p>Hi everyone,</p>
<p>åÊ</p>
<p>For your python bootcamp, and for many other assignments this semester, you will be using &#39;turnin&#39;, which is a program on eniac. This requires using the &#39;ssh&#39; and &#39;scp&#39; commands from your terminal. Instructions on how to do this are below, but please come to office hours if this whole process looks foreign to you. It will take about 30 seconds to show you in person, versus much longer than 30 seconds of playing guess-and-check on your own. :)åÊ</p>
<p></p>
<p>You will be turning in your entire iPython notebook. Once you haveåÊyour notebook copied over to the eniac machines and sshed yourself over to the eniac machines (instructions below), you will run.</p>
<p>åÊ</p>
<pre>turnin -c nets213 -p python-bootcamp -v IPythonBootcamp.ipynb</pre>
<p>åÊ</p>
<p>Here is the process, from the beginning. The lines beginning with &#39;$&#39; or &#39;~&gt;&#39; are things you will need to type. Lines beginning with &#39;#&#39; are comments, explaining the commands below. Lines that have no special characters at the show the output you should expect to see.</p>
<p></p>
<pre>#Copy your iPython file (which right now is on your computer) to your account on the eniac. You can do this using the &#39;scp&#39; or &#39;secure copy&#39; command, like below.
$ scp IPythonBootcamp.ipynb epavlick&#64;eniac.seas.upenn.edu:               
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
IPythonBootcamp.ipynb           

#Log on to the  eniac machines (using your usual penn username/password). You should see your command prompt change to say something like &#64;plus or &#64;minus, like below.
$ ssh epavlick&#64;eniac.seas.upenn.edu
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
Last login: Wed Jan 27 14:46:24 2016 from 10.251.134.177
epavlick&#64;plus:~&gt; 

#Make sure your file was copied over successfully. You can check by listing the contents of the current directory using the &#39;ls&#39; command. Hopefully you will see many things listed, and your IPython file should be one of them.
epavlick&#64;plus:~&gt; ls 
Desktop  Documents  Downloads  IPythonBootcamp.ipynb

#You can keep things organized by making a new folder for this assignment using the &#34;mkdir&#34; command, and moving your file into it using the &#34;mv&#34; command. See below.
epavlick&#64;plus:~&gt; mkdir python-bootcamp-submission
epavlick&#64;plus:~&gt; mv IPythonBootcamp.ipynb python-bootcamp-submission/

#To see the list of open projects for our class, type the following. Currently, only the bootcamp and the test project are open.
epavlick&#64;plus:~&gt; turnin -l -c nets213
Current projects for nets213:
python-bootcamp  on (alternate)
turnin-test      off
gun-classifier   off
crowdflower      off
crowdie          off
quality          off
the-end          off
test-turnin      on (current)

# To submit your bootcamp to turnin, use the following command. The 
epavlick&#64;plus:~&gt; turnin -c nets213 -p python-bootcamp -v python-bootcamp-submission/
drwxr-xr-x epavlick/epavlick 0 2016-01-27 15:53 python-bootcamp-submission/
-rw-r----- epavlick/epavlick 16606 2016-01-27 15:48 python-bootcamp-submission/IPythonBootcamp.ipynb
Compressing submitted files... please wait
Your files have been submitted to nets213, python-bootcamp for grading.
</pre>
<p>åÊåÊ</p>
<p>If you want to practice, please try submitting to the test project (called test-turnin), just to make sure everything is working smoothly. You can just submit a blank file, or you are welcome to go on a meaningless rant and submit that. Whatever makes you happy.åÊ</p>
<p></p>
<p>Please let usåÊknow if you have any problems or questions.</p>
<p></p>
<p>Ellie</p>",Submitting assignments via turnin,"When using the $ scp assignment4 pennkey&#64;eniac.seas.upenn.edu command, I am unable to copy over the directory to my account. Not sure what else to do.",Unable to turnin Homework 4,,,"<p>Are the submissions for the latest homework open yet?</p>
<p>I tried to submit and received the error turnin: submissions for -crowdie have been turned off.</p>",HW8 turnin turned off?,0
940850927,4/26/2016 17:02:13,true,1969443063,,4/26/2016 16:59:19,false,neodev,1.0,36167043,GBR,G6,Hull,77.86.101.69,0,0,"<p>Is there a way for us to see which files we turned in for a specific assignment, like #5?</p>",turnin,"<p>Hi Everyone,</p>
<p></p>
<p>Many of you had trouble with turnin for the bootcamp, and sent us email submissions. As a general policy, we cannot accept submissions via email. For this assignment, we will not penalize late submissions, so if you have not yet submitted via turnin, or you are still having trouble using scp/ssh, reach out to us and let&#39;s get it figured out.åÊ</p>
<p></p>
<p>For future assignments, we will only consider an assignment to be submitted if it is submitted via turnin before the deadline. Keep in mind our late-day policy: you are allowed 5 total late days during the semester with no penalty. After that, we deduct one point per day late.</p>
<p></p>
<p>Everyone have a wonderful weekend!</p>
<p>Ellie</p>",Late submissions and turnin,"<p>Hi everyone,</p>
<p>åÊ</p>
<p>For your python bootcamp, and for many other assignments this semester, you will be using &#39;turnin&#39;, which is a program on eniac. This requires using the &#39;ssh&#39; and &#39;scp&#39; commands from your terminal. Instructions on how to do this are below, but please come to office hours if this whole process looks foreign to you. It will take about 30 seconds to show you in person, versus much longer than 30 seconds of playing guess-and-check on your own. :)åÊ</p>
<p></p>
<p>You will be turning in your entire iPython notebook. Once you haveåÊyour notebook copied over to the eniac machines and sshed yourself over to the eniac machines (instructions below), you will run.</p>
<p>åÊ</p>
<pre>turnin -c nets213 -p python-bootcamp -v IPythonBootcamp.ipynb</pre>
<p>åÊ</p>
<p>Here is the process, from the beginning. The lines beginning with &#39;$&#39; or &#39;~&gt;&#39; are things you will need to type. Lines beginning with &#39;#&#39; are comments, explaining the commands below. Lines that have no special characters at the show the output you should expect to see.</p>
<p></p>
<pre>#Copy your iPython file (which right now is on your computer) to your account on the eniac. You can do this using the &#39;scp&#39; or &#39;secure copy&#39; command, like below.
$ scp IPythonBootcamp.ipynb epavlick&#64;eniac.seas.upenn.edu:               
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
IPythonBootcamp.ipynb           

#Log on to the  eniac machines (using your usual penn username/password). You should see your command prompt change to say something like &#64;plus or &#64;minus, like below.
$ ssh epavlick&#64;eniac.seas.upenn.edu
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
Last login: Wed Jan 27 14:46:24 2016 from 10.251.134.177
epavlick&#64;plus:~&gt; 

#Make sure your file was copied over successfully. You can check by listing the contents of the current directory using the &#39;ls&#39; command. Hopefully you will see many things listed, and your IPython file should be one of them.
epavlick&#64;plus:~&gt; ls 
Desktop  Documents  Downloads  IPythonBootcamp.ipynb

#You can keep things organized by making a new folder for this assignment using the &#34;mkdir&#34; command, and moving your file into it using the &#34;mv&#34; command. See below.
epavlick&#64;plus:~&gt; mkdir python-bootcamp-submission
epavlick&#64;plus:~&gt; mv IPythonBootcamp.ipynb python-bootcamp-submission/

#To see the list of open projects for our class, type the following. Currently, only the bootcamp and the test project are open.
epavlick&#64;plus:~&gt; turnin -l -c nets213
Current projects for nets213:
python-bootcamp  on (alternate)
turnin-test      off
gun-classifier   off
crowdflower      off
crowdie          off
quality          off
the-end          off
test-turnin      on (current)

# To submit your bootcamp to turnin, use the following command. The 
epavlick&#64;plus:~&gt; turnin -c nets213 -p python-bootcamp -v python-bootcamp-submission/
drwxr-xr-x epavlick/epavlick 0 2016-01-27 15:53 python-bootcamp-submission/
-rw-r----- epavlick/epavlick 16606 2016-01-27 15:48 python-bootcamp-submission/IPythonBootcamp.ipynb
Compressing submitted files... please wait
Your files have been submitted to nets213, python-bootcamp for grading.
</pre>
<p>åÊåÊ</p>
<p>If you want to practice, please try submitting to the test project (called test-turnin), just to make sure everything is working smoothly. You can just submit a blank file, or you are welcome to go on a meaningless rant and submit that. Whatever makes you happy.åÊ</p>
<p></p>
<p>Please let usåÊknow if you have any problems or questions.</p>
<p></p>
<p>Ellie</p>",Submitting assignments via turnin,"When using the $ scp assignment4 pennkey&#64;eniac.seas.upenn.edu command, I am unable to copy over the directory to my account. Not sure what else to do.",Unable to turnin Homework 4,,,"<p>Are the submissions for the latest homework open yet?</p>
<p>I tried to submit and received the error turnin: submissions for -crowdie have been turned off.</p>",HW8 turnin turned off?,0
940850927,4/26/2016 17:06:41,true,1969445559,,4/26/2016 17:00:39,false,neodev,1.0,33973110,VEN,23,Maracaibo,186.94.238.104,0,0,"<p>Is there a way for us to see which files we turned in for a specific assignment, like #5?</p>",turnin,"<p>Hi Everyone,</p>
<p></p>
<p>Many of you had trouble with turnin for the bootcamp, and sent us email submissions. As a general policy, we cannot accept submissions via email. For this assignment, we will not penalize late submissions, so if you have not yet submitted via turnin, or you are still having trouble using scp/ssh, reach out to us and let&#39;s get it figured out.åÊ</p>
<p></p>
<p>For future assignments, we will only consider an assignment to be submitted if it is submitted via turnin before the deadline. Keep in mind our late-day policy: you are allowed 5 total late days during the semester with no penalty. After that, we deduct one point per day late.</p>
<p></p>
<p>Everyone have a wonderful weekend!</p>
<p>Ellie</p>",Late submissions and turnin,"<p>Hi everyone,</p>
<p>åÊ</p>
<p>For your python bootcamp, and for many other assignments this semester, you will be using &#39;turnin&#39;, which is a program on eniac. This requires using the &#39;ssh&#39; and &#39;scp&#39; commands from your terminal. Instructions on how to do this are below, but please come to office hours if this whole process looks foreign to you. It will take about 30 seconds to show you in person, versus much longer than 30 seconds of playing guess-and-check on your own. :)åÊ</p>
<p></p>
<p>You will be turning in your entire iPython notebook. Once you haveåÊyour notebook copied over to the eniac machines and sshed yourself over to the eniac machines (instructions below), you will run.</p>
<p>åÊ</p>
<pre>turnin -c nets213 -p python-bootcamp -v IPythonBootcamp.ipynb</pre>
<p>åÊ</p>
<p>Here is the process, from the beginning. The lines beginning with &#39;$&#39; or &#39;~&gt;&#39; are things you will need to type. Lines beginning with &#39;#&#39; are comments, explaining the commands below. Lines that have no special characters at the show the output you should expect to see.</p>
<p></p>
<pre>#Copy your iPython file (which right now is on your computer) to your account on the eniac. You can do this using the &#39;scp&#39; or &#39;secure copy&#39; command, like below.
$ scp IPythonBootcamp.ipynb epavlick&#64;eniac.seas.upenn.edu:               
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
IPythonBootcamp.ipynb           

#Log on to the  eniac machines (using your usual penn username/password). You should see your command prompt change to say something like &#64;plus or &#64;minus, like below.
$ ssh epavlick&#64;eniac.seas.upenn.edu
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
Last login: Wed Jan 27 14:46:24 2016 from 10.251.134.177
epavlick&#64;plus:~&gt; 

#Make sure your file was copied over successfully. You can check by listing the contents of the current directory using the &#39;ls&#39; command. Hopefully you will see many things listed, and your IPython file should be one of them.
epavlick&#64;plus:~&gt; ls 
Desktop  Documents  Downloads  IPythonBootcamp.ipynb

#You can keep things organized by making a new folder for this assignment using the &#34;mkdir&#34; command, and moving your file into it using the &#34;mv&#34; command. See below.
epavlick&#64;plus:~&gt; mkdir python-bootcamp-submission
epavlick&#64;plus:~&gt; mv IPythonBootcamp.ipynb python-bootcamp-submission/

#To see the list of open projects for our class, type the following. Currently, only the bootcamp and the test project are open.
epavlick&#64;plus:~&gt; turnin -l -c nets213
Current projects for nets213:
python-bootcamp  on (alternate)
turnin-test      off
gun-classifier   off
crowdflower      off
crowdie          off
quality          off
the-end          off
test-turnin      on (current)

# To submit your bootcamp to turnin, use the following command. The 
epavlick&#64;plus:~&gt; turnin -c nets213 -p python-bootcamp -v python-bootcamp-submission/
drwxr-xr-x epavlick/epavlick 0 2016-01-27 15:53 python-bootcamp-submission/
-rw-r----- epavlick/epavlick 16606 2016-01-27 15:48 python-bootcamp-submission/IPythonBootcamp.ipynb
Compressing submitted files... please wait
Your files have been submitted to nets213, python-bootcamp for grading.
</pre>
<p>åÊåÊ</p>
<p>If you want to practice, please try submitting to the test project (called test-turnin), just to make sure everything is working smoothly. You can just submit a blank file, or you are welcome to go on a meaningless rant and submit that. Whatever makes you happy.åÊ</p>
<p></p>
<p>Please let usåÊknow if you have any problems or questions.</p>
<p></p>
<p>Ellie</p>",Submitting assignments via turnin,"When using the $ scp assignment4 pennkey&#64;eniac.seas.upenn.edu command, I am unable to copy over the directory to my account. Not sure what else to do.",Unable to turnin Homework 4,,,"<p>Are the submissions for the latest homework open yet?</p>
<p>I tried to submit and received the error turnin: submissions for -crowdie have been turned off.</p>",HW8 turnin turned off?,0
940850927,4/26/2016 17:15:16,true,1969451002,true,4/26/2016 17:01:58,false,clixsense,0.8889,35338593,ITA,14,Cagliari,151.56.132.145,2,0,"<p>Is there a way for us to see which files we turned in for a specific assignment, like #5?</p>",turnin,"<p>Hi Everyone,</p>
<p></p>
<p>Many of you had trouble with turnin for the bootcamp, and sent us email submissions. As a general policy, we cannot accept submissions via email. For this assignment, we will not penalize late submissions, so if you have not yet submitted via turnin, or you are still having trouble using scp/ssh, reach out to us and let&#39;s get it figured out.åÊ</p>
<p></p>
<p>For future assignments, we will only consider an assignment to be submitted if it is submitted via turnin before the deadline. Keep in mind our late-day policy: you are allowed 5 total late days during the semester with no penalty. After that, we deduct one point per day late.</p>
<p></p>
<p>Everyone have a wonderful weekend!</p>
<p>Ellie</p>",Late submissions and turnin,"<p>Hi everyone,</p>
<p>åÊ</p>
<p>For your python bootcamp, and for many other assignments this semester, you will be using &#39;turnin&#39;, which is a program on eniac. This requires using the &#39;ssh&#39; and &#39;scp&#39; commands from your terminal. Instructions on how to do this are below, but please come to office hours if this whole process looks foreign to you. It will take about 30 seconds to show you in person, versus much longer than 30 seconds of playing guess-and-check on your own. :)åÊ</p>
<p></p>
<p>You will be turning in your entire iPython notebook. Once you haveåÊyour notebook copied over to the eniac machines and sshed yourself over to the eniac machines (instructions below), you will run.</p>
<p>åÊ</p>
<pre>turnin -c nets213 -p python-bootcamp -v IPythonBootcamp.ipynb</pre>
<p>åÊ</p>
<p>Here is the process, from the beginning. The lines beginning with &#39;$&#39; or &#39;~&gt;&#39; are things you will need to type. Lines beginning with &#39;#&#39; are comments, explaining the commands below. Lines that have no special characters at the show the output you should expect to see.</p>
<p></p>
<pre>#Copy your iPython file (which right now is on your computer) to your account on the eniac. You can do this using the &#39;scp&#39; or &#39;secure copy&#39; command, like below.
$ scp IPythonBootcamp.ipynb epavlick&#64;eniac.seas.upenn.edu:               
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
IPythonBootcamp.ipynb           

#Log on to the  eniac machines (using your usual penn username/password). You should see your command prompt change to say something like &#64;plus or &#64;minus, like below.
$ ssh epavlick&#64;eniac.seas.upenn.edu
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
Last login: Wed Jan 27 14:46:24 2016 from 10.251.134.177
epavlick&#64;plus:~&gt; 

#Make sure your file was copied over successfully. You can check by listing the contents of the current directory using the &#39;ls&#39; command. Hopefully you will see many things listed, and your IPython file should be one of them.
epavlick&#64;plus:~&gt; ls 
Desktop  Documents  Downloads  IPythonBootcamp.ipynb

#You can keep things organized by making a new folder for this assignment using the &#34;mkdir&#34; command, and moving your file into it using the &#34;mv&#34; command. See below.
epavlick&#64;plus:~&gt; mkdir python-bootcamp-submission
epavlick&#64;plus:~&gt; mv IPythonBootcamp.ipynb python-bootcamp-submission/

#To see the list of open projects for our class, type the following. Currently, only the bootcamp and the test project are open.
epavlick&#64;plus:~&gt; turnin -l -c nets213
Current projects for nets213:
python-bootcamp  on (alternate)
turnin-test      off
gun-classifier   off
crowdflower      off
crowdie          off
quality          off
the-end          off
test-turnin      on (current)

# To submit your bootcamp to turnin, use the following command. The 
epavlick&#64;plus:~&gt; turnin -c nets213 -p python-bootcamp -v python-bootcamp-submission/
drwxr-xr-x epavlick/epavlick 0 2016-01-27 15:53 python-bootcamp-submission/
-rw-r----- epavlick/epavlick 16606 2016-01-27 15:48 python-bootcamp-submission/IPythonBootcamp.ipynb
Compressing submitted files... please wait
Your files have been submitted to nets213, python-bootcamp for grading.
</pre>
<p>åÊåÊ</p>
<p>If you want to practice, please try submitting to the test project (called test-turnin), just to make sure everything is working smoothly. You can just submit a blank file, or you are welcome to go on a meaningless rant and submit that. Whatever makes you happy.åÊ</p>
<p></p>
<p>Please let usåÊknow if you have any problems or questions.</p>
<p></p>
<p>Ellie</p>",Submitting assignments via turnin,"When using the $ scp assignment4 pennkey&#64;eniac.seas.upenn.edu command, I am unable to copy over the directory to my account. Not sure what else to do.",Unable to turnin Homework 4,,,"<p>Are the submissions for the latest homework open yet?</p>
<p>I tried to submit and received the error turnin: submissions for -crowdie have been turned off.</p>",HW8 turnin turned off?,0
940850927,4/26/2016 17:22:13,true,1969454927,,4/26/2016 17:19:06,false,elite,1.0,25411289,HRV,"","",31.147.119.175,0,0,"<p>Is there a way for us to see which files we turned in for a specific assignment, like #5?</p>",turnin,"<p>Hi Everyone,</p>
<p></p>
<p>Many of you had trouble with turnin for the bootcamp, and sent us email submissions. As a general policy, we cannot accept submissions via email. For this assignment, we will not penalize late submissions, so if you have not yet submitted via turnin, or you are still having trouble using scp/ssh, reach out to us and let&#39;s get it figured out.åÊ</p>
<p></p>
<p>For future assignments, we will only consider an assignment to be submitted if it is submitted via turnin before the deadline. Keep in mind our late-day policy: you are allowed 5 total late days during the semester with no penalty. After that, we deduct one point per day late.</p>
<p></p>
<p>Everyone have a wonderful weekend!</p>
<p>Ellie</p>",Late submissions and turnin,"<p>Hi everyone,</p>
<p>åÊ</p>
<p>For your python bootcamp, and for many other assignments this semester, you will be using &#39;turnin&#39;, which is a program on eniac. This requires using the &#39;ssh&#39; and &#39;scp&#39; commands from your terminal. Instructions on how to do this are below, but please come to office hours if this whole process looks foreign to you. It will take about 30 seconds to show you in person, versus much longer than 30 seconds of playing guess-and-check on your own. :)åÊ</p>
<p></p>
<p>You will be turning in your entire iPython notebook. Once you haveåÊyour notebook copied over to the eniac machines and sshed yourself over to the eniac machines (instructions below), you will run.</p>
<p>åÊ</p>
<pre>turnin -c nets213 -p python-bootcamp -v IPythonBootcamp.ipynb</pre>
<p>åÊ</p>
<p>Here is the process, from the beginning. The lines beginning with &#39;$&#39; or &#39;~&gt;&#39; are things you will need to type. Lines beginning with &#39;#&#39; are comments, explaining the commands below. Lines that have no special characters at the show the output you should expect to see.</p>
<p></p>
<pre>#Copy your iPython file (which right now is on your computer) to your account on the eniac. You can do this using the &#39;scp&#39; or &#39;secure copy&#39; command, like below.
$ scp IPythonBootcamp.ipynb epavlick&#64;eniac.seas.upenn.edu:               
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
IPythonBootcamp.ipynb           

#Log on to the  eniac machines (using your usual penn username/password). You should see your command prompt change to say something like &#64;plus or &#64;minus, like below.
$ ssh epavlick&#64;eniac.seas.upenn.edu
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
Last login: Wed Jan 27 14:46:24 2016 from 10.251.134.177
epavlick&#64;plus:~&gt; 

#Make sure your file was copied over successfully. You can check by listing the contents of the current directory using the &#39;ls&#39; command. Hopefully you will see many things listed, and your IPython file should be one of them.
epavlick&#64;plus:~&gt; ls 
Desktop  Documents  Downloads  IPythonBootcamp.ipynb

#You can keep things organized by making a new folder for this assignment using the &#34;mkdir&#34; command, and moving your file into it using the &#34;mv&#34; command. See below.
epavlick&#64;plus:~&gt; mkdir python-bootcamp-submission
epavlick&#64;plus:~&gt; mv IPythonBootcamp.ipynb python-bootcamp-submission/

#To see the list of open projects for our class, type the following. Currently, only the bootcamp and the test project are open.
epavlick&#64;plus:~&gt; turnin -l -c nets213
Current projects for nets213:
python-bootcamp  on (alternate)
turnin-test      off
gun-classifier   off
crowdflower      off
crowdie          off
quality          off
the-end          off
test-turnin      on (current)

# To submit your bootcamp to turnin, use the following command. The 
epavlick&#64;plus:~&gt; turnin -c nets213 -p python-bootcamp -v python-bootcamp-submission/
drwxr-xr-x epavlick/epavlick 0 2016-01-27 15:53 python-bootcamp-submission/
-rw-r----- epavlick/epavlick 16606 2016-01-27 15:48 python-bootcamp-submission/IPythonBootcamp.ipynb
Compressing submitted files... please wait
Your files have been submitted to nets213, python-bootcamp for grading.
</pre>
<p>åÊåÊ</p>
<p>If you want to practice, please try submitting to the test project (called test-turnin), just to make sure everything is working smoothly. You can just submit a blank file, or you are welcome to go on a meaningless rant and submit that. Whatever makes you happy.åÊ</p>
<p></p>
<p>Please let usåÊknow if you have any problems or questions.</p>
<p></p>
<p>Ellie</p>",Submitting assignments via turnin,"When using the $ scp assignment4 pennkey&#64;eniac.seas.upenn.edu command, I am unable to copy over the directory to my account. Not sure what else to do.",Unable to turnin Homework 4,,,"<p>Are the submissions for the latest homework open yet?</p>
<p>I tried to submit and received the error turnin: submissions for -crowdie have been turned off.</p>",HW8 turnin turned off?,0
940850927,4/26/2016 17:32:09,true,1969460739,,4/26/2016 17:30:32,false,neodev,0.8889,19625264,DZA,41,Chlef,41.102.7.217,0,0,"<p>Is there a way for us to see which files we turned in for a specific assignment, like #5?</p>",turnin,"<p>Hi Everyone,</p>
<p></p>
<p>Many of you had trouble with turnin for the bootcamp, and sent us email submissions. As a general policy, we cannot accept submissions via email. For this assignment, we will not penalize late submissions, so if you have not yet submitted via turnin, or you are still having trouble using scp/ssh, reach out to us and let&#39;s get it figured out.åÊ</p>
<p></p>
<p>For future assignments, we will only consider an assignment to be submitted if it is submitted via turnin before the deadline. Keep in mind our late-day policy: you are allowed 5 total late days during the semester with no penalty. After that, we deduct one point per day late.</p>
<p></p>
<p>Everyone have a wonderful weekend!</p>
<p>Ellie</p>",Late submissions and turnin,"<p>Hi everyone,</p>
<p>åÊ</p>
<p>For your python bootcamp, and for many other assignments this semester, you will be using &#39;turnin&#39;, which is a program on eniac. This requires using the &#39;ssh&#39; and &#39;scp&#39; commands from your terminal. Instructions on how to do this are below, but please come to office hours if this whole process looks foreign to you. It will take about 30 seconds to show you in person, versus much longer than 30 seconds of playing guess-and-check on your own. :)åÊ</p>
<p></p>
<p>You will be turning in your entire iPython notebook. Once you haveåÊyour notebook copied over to the eniac machines and sshed yourself over to the eniac machines (instructions below), you will run.</p>
<p>åÊ</p>
<pre>turnin -c nets213 -p python-bootcamp -v IPythonBootcamp.ipynb</pre>
<p>åÊ</p>
<p>Here is the process, from the beginning. The lines beginning with &#39;$&#39; or &#39;~&gt;&#39; are things you will need to type. Lines beginning with &#39;#&#39; are comments, explaining the commands below. Lines that have no special characters at the show the output you should expect to see.</p>
<p></p>
<pre>#Copy your iPython file (which right now is on your computer) to your account on the eniac. You can do this using the &#39;scp&#39; or &#39;secure copy&#39; command, like below.
$ scp IPythonBootcamp.ipynb epavlick&#64;eniac.seas.upenn.edu:               
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
IPythonBootcamp.ipynb           

#Log on to the  eniac machines (using your usual penn username/password). You should see your command prompt change to say something like &#64;plus or &#64;minus, like below.
$ ssh epavlick&#64;eniac.seas.upenn.edu
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
Last login: Wed Jan 27 14:46:24 2016 from 10.251.134.177
epavlick&#64;plus:~&gt; 

#Make sure your file was copied over successfully. You can check by listing the contents of the current directory using the &#39;ls&#39; command. Hopefully you will see many things listed, and your IPython file should be one of them.
epavlick&#64;plus:~&gt; ls 
Desktop  Documents  Downloads  IPythonBootcamp.ipynb

#You can keep things organized by making a new folder for this assignment using the &#34;mkdir&#34; command, and moving your file into it using the &#34;mv&#34; command. See below.
epavlick&#64;plus:~&gt; mkdir python-bootcamp-submission
epavlick&#64;plus:~&gt; mv IPythonBootcamp.ipynb python-bootcamp-submission/

#To see the list of open projects for our class, type the following. Currently, only the bootcamp and the test project are open.
epavlick&#64;plus:~&gt; turnin -l -c nets213
Current projects for nets213:
python-bootcamp  on (alternate)
turnin-test      off
gun-classifier   off
crowdflower      off
crowdie          off
quality          off
the-end          off
test-turnin      on (current)

# To submit your bootcamp to turnin, use the following command. The 
epavlick&#64;plus:~&gt; turnin -c nets213 -p python-bootcamp -v python-bootcamp-submission/
drwxr-xr-x epavlick/epavlick 0 2016-01-27 15:53 python-bootcamp-submission/
-rw-r----- epavlick/epavlick 16606 2016-01-27 15:48 python-bootcamp-submission/IPythonBootcamp.ipynb
Compressing submitted files... please wait
Your files have been submitted to nets213, python-bootcamp for grading.
</pre>
<p>åÊåÊ</p>
<p>If you want to practice, please try submitting to the test project (called test-turnin), just to make sure everything is working smoothly. You can just submit a blank file, or you are welcome to go on a meaningless rant and submit that. Whatever makes you happy.åÊ</p>
<p></p>
<p>Please let usåÊknow if you have any problems or questions.</p>
<p></p>
<p>Ellie</p>",Submitting assignments via turnin,"When using the $ scp assignment4 pennkey&#64;eniac.seas.upenn.edu command, I am unable to copy over the directory to my account. Not sure what else to do.",Unable to turnin Homework 4,,,"<p>Are the submissions for the latest homework open yet?</p>
<p>I tried to submit and received the error turnin: submissions for -crowdie have been turned off.</p>",HW8 turnin turned off?,0
940850927,4/26/2016 17:32:21,true,1969460823,,4/26/2016 17:30:36,false,neodev,1.0,11172894,IND,28,Champdani,117.194.5.117,0,0,"<p>Is there a way for us to see which files we turned in for a specific assignment, like #5?</p>",turnin,"<p>Hi Everyone,</p>
<p></p>
<p>Many of you had trouble with turnin for the bootcamp, and sent us email submissions. As a general policy, we cannot accept submissions via email. For this assignment, we will not penalize late submissions, so if you have not yet submitted via turnin, or you are still having trouble using scp/ssh, reach out to us and let&#39;s get it figured out.åÊ</p>
<p></p>
<p>For future assignments, we will only consider an assignment to be submitted if it is submitted via turnin before the deadline. Keep in mind our late-day policy: you are allowed 5 total late days during the semester with no penalty. After that, we deduct one point per day late.</p>
<p></p>
<p>Everyone have a wonderful weekend!</p>
<p>Ellie</p>",Late submissions and turnin,"<p>Hi everyone,</p>
<p>åÊ</p>
<p>For your python bootcamp, and for many other assignments this semester, you will be using &#39;turnin&#39;, which is a program on eniac. This requires using the &#39;ssh&#39; and &#39;scp&#39; commands from your terminal. Instructions on how to do this are below, but please come to office hours if this whole process looks foreign to you. It will take about 30 seconds to show you in person, versus much longer than 30 seconds of playing guess-and-check on your own. :)åÊ</p>
<p></p>
<p>You will be turning in your entire iPython notebook. Once you haveåÊyour notebook copied over to the eniac machines and sshed yourself over to the eniac machines (instructions below), you will run.</p>
<p>åÊ</p>
<pre>turnin -c nets213 -p python-bootcamp -v IPythonBootcamp.ipynb</pre>
<p>åÊ</p>
<p>Here is the process, from the beginning. The lines beginning with &#39;$&#39; or &#39;~&gt;&#39; are things you will need to type. Lines beginning with &#39;#&#39; are comments, explaining the commands below. Lines that have no special characters at the show the output you should expect to see.</p>
<p></p>
<pre>#Copy your iPython file (which right now is on your computer) to your account on the eniac. You can do this using the &#39;scp&#39; or &#39;secure copy&#39; command, like below.
$ scp IPythonBootcamp.ipynb epavlick&#64;eniac.seas.upenn.edu:               
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
IPythonBootcamp.ipynb           

#Log on to the  eniac machines (using your usual penn username/password). You should see your command prompt change to say something like &#64;plus or &#64;minus, like below.
$ ssh epavlick&#64;eniac.seas.upenn.edu
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
Last login: Wed Jan 27 14:46:24 2016 from 10.251.134.177
epavlick&#64;plus:~&gt; 

#Make sure your file was copied over successfully. You can check by listing the contents of the current directory using the &#39;ls&#39; command. Hopefully you will see many things listed, and your IPython file should be one of them.
epavlick&#64;plus:~&gt; ls 
Desktop  Documents  Downloads  IPythonBootcamp.ipynb

#You can keep things organized by making a new folder for this assignment using the &#34;mkdir&#34; command, and moving your file into it using the &#34;mv&#34; command. See below.
epavlick&#64;plus:~&gt; mkdir python-bootcamp-submission
epavlick&#64;plus:~&gt; mv IPythonBootcamp.ipynb python-bootcamp-submission/

#To see the list of open projects for our class, type the following. Currently, only the bootcamp and the test project are open.
epavlick&#64;plus:~&gt; turnin -l -c nets213
Current projects for nets213:
python-bootcamp  on (alternate)
turnin-test      off
gun-classifier   off
crowdflower      off
crowdie          off
quality          off
the-end          off
test-turnin      on (current)

# To submit your bootcamp to turnin, use the following command. The 
epavlick&#64;plus:~&gt; turnin -c nets213 -p python-bootcamp -v python-bootcamp-submission/
drwxr-xr-x epavlick/epavlick 0 2016-01-27 15:53 python-bootcamp-submission/
-rw-r----- epavlick/epavlick 16606 2016-01-27 15:48 python-bootcamp-submission/IPythonBootcamp.ipynb
Compressing submitted files... please wait
Your files have been submitted to nets213, python-bootcamp for grading.
</pre>
<p>åÊåÊ</p>
<p>If you want to practice, please try submitting to the test project (called test-turnin), just to make sure everything is working smoothly. You can just submit a blank file, or you are welcome to go on a meaningless rant and submit that. Whatever makes you happy.åÊ</p>
<p></p>
<p>Please let usåÊknow if you have any problems or questions.</p>
<p></p>
<p>Ellie</p>",Submitting assignments via turnin,"When using the $ scp assignment4 pennkey&#64;eniac.seas.upenn.edu command, I am unable to copy over the directory to my account. Not sure what else to do.",Unable to turnin Homework 4,,,"<p>Are the submissions for the latest homework open yet?</p>
<p>I tried to submit and received the error turnin: submissions for -crowdie have been turned off.</p>",HW8 turnin turned off?,0
940850927,4/26/2016 17:32:34,true,1969460925,,4/26/2016 17:32:18,false,neodev,0.8889,33131546,IDN,04,Jakarta,139.194.89.60,0,0,"<p>Is there a way for us to see which files we turned in for a specific assignment, like #5?</p>",turnin,"<p>Hi Everyone,</p>
<p></p>
<p>Many of you had trouble with turnin for the bootcamp, and sent us email submissions. As a general policy, we cannot accept submissions via email. For this assignment, we will not penalize late submissions, so if you have not yet submitted via turnin, or you are still having trouble using scp/ssh, reach out to us and let&#39;s get it figured out.åÊ</p>
<p></p>
<p>For future assignments, we will only consider an assignment to be submitted if it is submitted via turnin before the deadline. Keep in mind our late-day policy: you are allowed 5 total late days during the semester with no penalty. After that, we deduct one point per day late.</p>
<p></p>
<p>Everyone have a wonderful weekend!</p>
<p>Ellie</p>",Late submissions and turnin,"<p>Hi everyone,</p>
<p>åÊ</p>
<p>For your python bootcamp, and for many other assignments this semester, you will be using &#39;turnin&#39;, which is a program on eniac. This requires using the &#39;ssh&#39; and &#39;scp&#39; commands from your terminal. Instructions on how to do this are below, but please come to office hours if this whole process looks foreign to you. It will take about 30 seconds to show you in person, versus much longer than 30 seconds of playing guess-and-check on your own. :)åÊ</p>
<p></p>
<p>You will be turning in your entire iPython notebook. Once you haveåÊyour notebook copied over to the eniac machines and sshed yourself over to the eniac machines (instructions below), you will run.</p>
<p>åÊ</p>
<pre>turnin -c nets213 -p python-bootcamp -v IPythonBootcamp.ipynb</pre>
<p>åÊ</p>
<p>Here is the process, from the beginning. The lines beginning with &#39;$&#39; or &#39;~&gt;&#39; are things you will need to type. Lines beginning with &#39;#&#39; are comments, explaining the commands below. Lines that have no special characters at the show the output you should expect to see.</p>
<p></p>
<pre>#Copy your iPython file (which right now is on your computer) to your account on the eniac. You can do this using the &#39;scp&#39; or &#39;secure copy&#39; command, like below.
$ scp IPythonBootcamp.ipynb epavlick&#64;eniac.seas.upenn.edu:               
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
IPythonBootcamp.ipynb           

#Log on to the  eniac machines (using your usual penn username/password). You should see your command prompt change to say something like &#64;plus or &#64;minus, like below.
$ ssh epavlick&#64;eniac.seas.upenn.edu
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
Last login: Wed Jan 27 14:46:24 2016 from 10.251.134.177
epavlick&#64;plus:~&gt; 

#Make sure your file was copied over successfully. You can check by listing the contents of the current directory using the &#39;ls&#39; command. Hopefully you will see many things listed, and your IPython file should be one of them.
epavlick&#64;plus:~&gt; ls 
Desktop  Documents  Downloads  IPythonBootcamp.ipynb

#You can keep things organized by making a new folder for this assignment using the &#34;mkdir&#34; command, and moving your file into it using the &#34;mv&#34; command. See below.
epavlick&#64;plus:~&gt; mkdir python-bootcamp-submission
epavlick&#64;plus:~&gt; mv IPythonBootcamp.ipynb python-bootcamp-submission/

#To see the list of open projects for our class, type the following. Currently, only the bootcamp and the test project are open.
epavlick&#64;plus:~&gt; turnin -l -c nets213
Current projects for nets213:
python-bootcamp  on (alternate)
turnin-test      off
gun-classifier   off
crowdflower      off
crowdie          off
quality          off
the-end          off
test-turnin      on (current)

# To submit your bootcamp to turnin, use the following command. The 
epavlick&#64;plus:~&gt; turnin -c nets213 -p python-bootcamp -v python-bootcamp-submission/
drwxr-xr-x epavlick/epavlick 0 2016-01-27 15:53 python-bootcamp-submission/
-rw-r----- epavlick/epavlick 16606 2016-01-27 15:48 python-bootcamp-submission/IPythonBootcamp.ipynb
Compressing submitted files... please wait
Your files have been submitted to nets213, python-bootcamp for grading.
</pre>
<p>åÊåÊ</p>
<p>If you want to practice, please try submitting to the test project (called test-turnin), just to make sure everything is working smoothly. You can just submit a blank file, or you are welcome to go on a meaningless rant and submit that. Whatever makes you happy.åÊ</p>
<p></p>
<p>Please let usåÊknow if you have any problems or questions.</p>
<p></p>
<p>Ellie</p>",Submitting assignments via turnin,"When using the $ scp assignment4 pennkey&#64;eniac.seas.upenn.edu command, I am unable to copy over the directory to my account. Not sure what else to do.",Unable to turnin Homework 4,,,"<p>Are the submissions for the latest homework open yet?</p>
<p>I tried to submit and received the error turnin: submissions for -crowdie have been turned off.</p>",HW8 turnin turned off?,0
940850927,4/26/2016 17:35:19,true,1969462333,,4/26/2016 17:34:26,false,clixsense,1.0,30712378,ROU,21,Deva,79.119.241.200,0,0,"<p>Is there a way for us to see which files we turned in for a specific assignment, like #5?</p>",turnin,"<p>Hi Everyone,</p>
<p></p>
<p>Many of you had trouble with turnin for the bootcamp, and sent us email submissions. As a general policy, we cannot accept submissions via email. For this assignment, we will not penalize late submissions, so if you have not yet submitted via turnin, or you are still having trouble using scp/ssh, reach out to us and let&#39;s get it figured out.åÊ</p>
<p></p>
<p>For future assignments, we will only consider an assignment to be submitted if it is submitted via turnin before the deadline. Keep in mind our late-day policy: you are allowed 5 total late days during the semester with no penalty. After that, we deduct one point per day late.</p>
<p></p>
<p>Everyone have a wonderful weekend!</p>
<p>Ellie</p>",Late submissions and turnin,"<p>Hi everyone,</p>
<p>åÊ</p>
<p>For your python bootcamp, and for many other assignments this semester, you will be using &#39;turnin&#39;, which is a program on eniac. This requires using the &#39;ssh&#39; and &#39;scp&#39; commands from your terminal. Instructions on how to do this are below, but please come to office hours if this whole process looks foreign to you. It will take about 30 seconds to show you in person, versus much longer than 30 seconds of playing guess-and-check on your own. :)åÊ</p>
<p></p>
<p>You will be turning in your entire iPython notebook. Once you haveåÊyour notebook copied over to the eniac machines and sshed yourself over to the eniac machines (instructions below), you will run.</p>
<p>åÊ</p>
<pre>turnin -c nets213 -p python-bootcamp -v IPythonBootcamp.ipynb</pre>
<p>åÊ</p>
<p>Here is the process, from the beginning. The lines beginning with &#39;$&#39; or &#39;~&gt;&#39; are things you will need to type. Lines beginning with &#39;#&#39; are comments, explaining the commands below. Lines that have no special characters at the show the output you should expect to see.</p>
<p></p>
<pre>#Copy your iPython file (which right now is on your computer) to your account on the eniac. You can do this using the &#39;scp&#39; or &#39;secure copy&#39; command, like below.
$ scp IPythonBootcamp.ipynb epavlick&#64;eniac.seas.upenn.edu:               
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
IPythonBootcamp.ipynb           

#Log on to the  eniac machines (using your usual penn username/password). You should see your command prompt change to say something like &#64;plus or &#64;minus, like below.
$ ssh epavlick&#64;eniac.seas.upenn.edu
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
Last login: Wed Jan 27 14:46:24 2016 from 10.251.134.177
epavlick&#64;plus:~&gt; 

#Make sure your file was copied over successfully. You can check by listing the contents of the current directory using the &#39;ls&#39; command. Hopefully you will see many things listed, and your IPython file should be one of them.
epavlick&#64;plus:~&gt; ls 
Desktop  Documents  Downloads  IPythonBootcamp.ipynb

#You can keep things organized by making a new folder for this assignment using the &#34;mkdir&#34; command, and moving your file into it using the &#34;mv&#34; command. See below.
epavlick&#64;plus:~&gt; mkdir python-bootcamp-submission
epavlick&#64;plus:~&gt; mv IPythonBootcamp.ipynb python-bootcamp-submission/

#To see the list of open projects for our class, type the following. Currently, only the bootcamp and the test project are open.
epavlick&#64;plus:~&gt; turnin -l -c nets213
Current projects for nets213:
python-bootcamp  on (alternate)
turnin-test      off
gun-classifier   off
crowdflower      off
crowdie          off
quality          off
the-end          off
test-turnin      on (current)

# To submit your bootcamp to turnin, use the following command. The 
epavlick&#64;plus:~&gt; turnin -c nets213 -p python-bootcamp -v python-bootcamp-submission/
drwxr-xr-x epavlick/epavlick 0 2016-01-27 15:53 python-bootcamp-submission/
-rw-r----- epavlick/epavlick 16606 2016-01-27 15:48 python-bootcamp-submission/IPythonBootcamp.ipynb
Compressing submitted files... please wait
Your files have been submitted to nets213, python-bootcamp for grading.
</pre>
<p>åÊåÊ</p>
<p>If you want to practice, please try submitting to the test project (called test-turnin), just to make sure everything is working smoothly. You can just submit a blank file, or you are welcome to go on a meaningless rant and submit that. Whatever makes you happy.åÊ</p>
<p></p>
<p>Please let usåÊknow if you have any problems or questions.</p>
<p></p>
<p>Ellie</p>",Submitting assignments via turnin,"When using the $ scp assignment4 pennkey&#64;eniac.seas.upenn.edu command, I am unable to copy over the directory to my account. Not sure what else to do.",Unable to turnin Homework 4,,,"<p>Are the submissions for the latest homework open yet?</p>
<p>I tried to submit and received the error turnin: submissions for -crowdie have been turned off.</p>",HW8 turnin turned off?,0
940850927,4/26/2016 17:39:37,true,1969464627,,4/26/2016 17:38:56,false,neodev,0.8889,33568303,VEN,23,Cabimas,190.77.7.36,0,0,"<p>Is there a way for us to see which files we turned in for a specific assignment, like #5?</p>",turnin,"<p>Hi Everyone,</p>
<p></p>
<p>Many of you had trouble with turnin for the bootcamp, and sent us email submissions. As a general policy, we cannot accept submissions via email. For this assignment, we will not penalize late submissions, so if you have not yet submitted via turnin, or you are still having trouble using scp/ssh, reach out to us and let&#39;s get it figured out.åÊ</p>
<p></p>
<p>For future assignments, we will only consider an assignment to be submitted if it is submitted via turnin before the deadline. Keep in mind our late-day policy: you are allowed 5 total late days during the semester with no penalty. After that, we deduct one point per day late.</p>
<p></p>
<p>Everyone have a wonderful weekend!</p>
<p>Ellie</p>",Late submissions and turnin,"<p>Hi everyone,</p>
<p>åÊ</p>
<p>For your python bootcamp, and for many other assignments this semester, you will be using &#39;turnin&#39;, which is a program on eniac. This requires using the &#39;ssh&#39; and &#39;scp&#39; commands from your terminal. Instructions on how to do this are below, but please come to office hours if this whole process looks foreign to you. It will take about 30 seconds to show you in person, versus much longer than 30 seconds of playing guess-and-check on your own. :)åÊ</p>
<p></p>
<p>You will be turning in your entire iPython notebook. Once you haveåÊyour notebook copied over to the eniac machines and sshed yourself over to the eniac machines (instructions below), you will run.</p>
<p>åÊ</p>
<pre>turnin -c nets213 -p python-bootcamp -v IPythonBootcamp.ipynb</pre>
<p>åÊ</p>
<p>Here is the process, from the beginning. The lines beginning with &#39;$&#39; or &#39;~&gt;&#39; are things you will need to type. Lines beginning with &#39;#&#39; are comments, explaining the commands below. Lines that have no special characters at the show the output you should expect to see.</p>
<p></p>
<pre>#Copy your iPython file (which right now is on your computer) to your account on the eniac. You can do this using the &#39;scp&#39; or &#39;secure copy&#39; command, like below.
$ scp IPythonBootcamp.ipynb epavlick&#64;eniac.seas.upenn.edu:               
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
IPythonBootcamp.ipynb           

#Log on to the  eniac machines (using your usual penn username/password). You should see your command prompt change to say something like &#64;plus or &#64;minus, like below.
$ ssh epavlick&#64;eniac.seas.upenn.edu
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
Last login: Wed Jan 27 14:46:24 2016 from 10.251.134.177
epavlick&#64;plus:~&gt; 

#Make sure your file was copied over successfully. You can check by listing the contents of the current directory using the &#39;ls&#39; command. Hopefully you will see many things listed, and your IPython file should be one of them.
epavlick&#64;plus:~&gt; ls 
Desktop  Documents  Downloads  IPythonBootcamp.ipynb

#You can keep things organized by making a new folder for this assignment using the &#34;mkdir&#34; command, and moving your file into it using the &#34;mv&#34; command. See below.
epavlick&#64;plus:~&gt; mkdir python-bootcamp-submission
epavlick&#64;plus:~&gt; mv IPythonBootcamp.ipynb python-bootcamp-submission/

#To see the list of open projects for our class, type the following. Currently, only the bootcamp and the test project are open.
epavlick&#64;plus:~&gt; turnin -l -c nets213
Current projects for nets213:
python-bootcamp  on (alternate)
turnin-test      off
gun-classifier   off
crowdflower      off
crowdie          off
quality          off
the-end          off
test-turnin      on (current)

# To submit your bootcamp to turnin, use the following command. The 
epavlick&#64;plus:~&gt; turnin -c nets213 -p python-bootcamp -v python-bootcamp-submission/
drwxr-xr-x epavlick/epavlick 0 2016-01-27 15:53 python-bootcamp-submission/
-rw-r----- epavlick/epavlick 16606 2016-01-27 15:48 python-bootcamp-submission/IPythonBootcamp.ipynb
Compressing submitted files... please wait
Your files have been submitted to nets213, python-bootcamp for grading.
</pre>
<p>åÊåÊ</p>
<p>If you want to practice, please try submitting to the test project (called test-turnin), just to make sure everything is working smoothly. You can just submit a blank file, or you are welcome to go on a meaningless rant and submit that. Whatever makes you happy.åÊ</p>
<p></p>
<p>Please let usåÊknow if you have any problems or questions.</p>
<p></p>
<p>Ellie</p>",Submitting assignments via turnin,"When using the $ scp assignment4 pennkey&#64;eniac.seas.upenn.edu command, I am unable to copy over the directory to my account. Not sure what else to do.",Unable to turnin Homework 4,,,"<p>Are the submissions for the latest homework open yet?</p>
<p>I tried to submit and received the error turnin: submissions for -crowdie have been turned off.</p>",HW8 turnin turned off?,0
940850927,4/26/2016 17:51:16,true,1969471395,,4/26/2016 17:50:22,false,clixsense,1.0,35444326,BRA,07,Brasília,177.15.130.106,0,0,"<p>Is there a way for us to see which files we turned in for a specific assignment, like #5?</p>",turnin,"<p>Hi Everyone,</p>
<p></p>
<p>Many of you had trouble with turnin for the bootcamp, and sent us email submissions. As a general policy, we cannot accept submissions via email. For this assignment, we will not penalize late submissions, so if you have not yet submitted via turnin, or you are still having trouble using scp/ssh, reach out to us and let&#39;s get it figured out.åÊ</p>
<p></p>
<p>For future assignments, we will only consider an assignment to be submitted if it is submitted via turnin before the deadline. Keep in mind our late-day policy: you are allowed 5 total late days during the semester with no penalty. After that, we deduct one point per day late.</p>
<p></p>
<p>Everyone have a wonderful weekend!</p>
<p>Ellie</p>",Late submissions and turnin,"<p>Hi everyone,</p>
<p>åÊ</p>
<p>For your python bootcamp, and for many other assignments this semester, you will be using &#39;turnin&#39;, which is a program on eniac. This requires using the &#39;ssh&#39; and &#39;scp&#39; commands from your terminal. Instructions on how to do this are below, but please come to office hours if this whole process looks foreign to you. It will take about 30 seconds to show you in person, versus much longer than 30 seconds of playing guess-and-check on your own. :)åÊ</p>
<p></p>
<p>You will be turning in your entire iPython notebook. Once you haveåÊyour notebook copied over to the eniac machines and sshed yourself over to the eniac machines (instructions below), you will run.</p>
<p>åÊ</p>
<pre>turnin -c nets213 -p python-bootcamp -v IPythonBootcamp.ipynb</pre>
<p>åÊ</p>
<p>Here is the process, from the beginning. The lines beginning with &#39;$&#39; or &#39;~&gt;&#39; are things you will need to type. Lines beginning with &#39;#&#39; are comments, explaining the commands below. Lines that have no special characters at the show the output you should expect to see.</p>
<p></p>
<pre>#Copy your iPython file (which right now is on your computer) to your account on the eniac. You can do this using the &#39;scp&#39; or &#39;secure copy&#39; command, like below.
$ scp IPythonBootcamp.ipynb epavlick&#64;eniac.seas.upenn.edu:               
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
IPythonBootcamp.ipynb           

#Log on to the  eniac machines (using your usual penn username/password). You should see your command prompt change to say something like &#64;plus or &#64;minus, like below.
$ ssh epavlick&#64;eniac.seas.upenn.edu
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
Last login: Wed Jan 27 14:46:24 2016 from 10.251.134.177
epavlick&#64;plus:~&gt; 

#Make sure your file was copied over successfully. You can check by listing the contents of the current directory using the &#39;ls&#39; command. Hopefully you will see many things listed, and your IPython file should be one of them.
epavlick&#64;plus:~&gt; ls 
Desktop  Documents  Downloads  IPythonBootcamp.ipynb

#You can keep things organized by making a new folder for this assignment using the &#34;mkdir&#34; command, and moving your file into it using the &#34;mv&#34; command. See below.
epavlick&#64;plus:~&gt; mkdir python-bootcamp-submission
epavlick&#64;plus:~&gt; mv IPythonBootcamp.ipynb python-bootcamp-submission/

#To see the list of open projects for our class, type the following. Currently, only the bootcamp and the test project are open.
epavlick&#64;plus:~&gt; turnin -l -c nets213
Current projects for nets213:
python-bootcamp  on (alternate)
turnin-test      off
gun-classifier   off
crowdflower      off
crowdie          off
quality          off
the-end          off
test-turnin      on (current)

# To submit your bootcamp to turnin, use the following command. The 
epavlick&#64;plus:~&gt; turnin -c nets213 -p python-bootcamp -v python-bootcamp-submission/
drwxr-xr-x epavlick/epavlick 0 2016-01-27 15:53 python-bootcamp-submission/
-rw-r----- epavlick/epavlick 16606 2016-01-27 15:48 python-bootcamp-submission/IPythonBootcamp.ipynb
Compressing submitted files... please wait
Your files have been submitted to nets213, python-bootcamp for grading.
</pre>
<p>åÊåÊ</p>
<p>If you want to practice, please try submitting to the test project (called test-turnin), just to make sure everything is working smoothly. You can just submit a blank file, or you are welcome to go on a meaningless rant and submit that. Whatever makes you happy.åÊ</p>
<p></p>
<p>Please let usåÊknow if you have any problems or questions.</p>
<p></p>
<p>Ellie</p>",Submitting assignments via turnin,"When using the $ scp assignment4 pennkey&#64;eniac.seas.upenn.edu command, I am unable to copy over the directory to my account. Not sure what else to do.",Unable to turnin Homework 4,,,"<p>Are the submissions for the latest homework open yet?</p>
<p>I tried to submit and received the error turnin: submissions for -crowdie have been turned off.</p>",HW8 turnin turned off?,0
940850927,4/26/2016 18:16:16,true,1969484741,,4/26/2016 18:07:59,false,neodev,0.8889,35550011,VEN,07,Valencia,190.204.238.112,0,0,"<p>Is there a way for us to see which files we turned in for a specific assignment, like #5?</p>",turnin,"<p>Hi Everyone,</p>
<p></p>
<p>Many of you had trouble with turnin for the bootcamp, and sent us email submissions. As a general policy, we cannot accept submissions via email. For this assignment, we will not penalize late submissions, so if you have not yet submitted via turnin, or you are still having trouble using scp/ssh, reach out to us and let&#39;s get it figured out.åÊ</p>
<p></p>
<p>For future assignments, we will only consider an assignment to be submitted if it is submitted via turnin before the deadline. Keep in mind our late-day policy: you are allowed 5 total late days during the semester with no penalty. After that, we deduct one point per day late.</p>
<p></p>
<p>Everyone have a wonderful weekend!</p>
<p>Ellie</p>",Late submissions and turnin,"<p>Hi everyone,</p>
<p>åÊ</p>
<p>For your python bootcamp, and for many other assignments this semester, you will be using &#39;turnin&#39;, which is a program on eniac. This requires using the &#39;ssh&#39; and &#39;scp&#39; commands from your terminal. Instructions on how to do this are below, but please come to office hours if this whole process looks foreign to you. It will take about 30 seconds to show you in person, versus much longer than 30 seconds of playing guess-and-check on your own. :)åÊ</p>
<p></p>
<p>You will be turning in your entire iPython notebook. Once you haveåÊyour notebook copied over to the eniac machines and sshed yourself over to the eniac machines (instructions below), you will run.</p>
<p>åÊ</p>
<pre>turnin -c nets213 -p python-bootcamp -v IPythonBootcamp.ipynb</pre>
<p>åÊ</p>
<p>Here is the process, from the beginning. The lines beginning with &#39;$&#39; or &#39;~&gt;&#39; are things you will need to type. Lines beginning with &#39;#&#39; are comments, explaining the commands below. Lines that have no special characters at the show the output you should expect to see.</p>
<p></p>
<pre>#Copy your iPython file (which right now is on your computer) to your account on the eniac. You can do this using the &#39;scp&#39; or &#39;secure copy&#39; command, like below.
$ scp IPythonBootcamp.ipynb epavlick&#64;eniac.seas.upenn.edu:               
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
IPythonBootcamp.ipynb           

#Log on to the  eniac machines (using your usual penn username/password). You should see your command prompt change to say something like &#64;plus or &#64;minus, like below.
$ ssh epavlick&#64;eniac.seas.upenn.edu
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
Last login: Wed Jan 27 14:46:24 2016 from 10.251.134.177
epavlick&#64;plus:~&gt; 

#Make sure your file was copied over successfully. You can check by listing the contents of the current directory using the &#39;ls&#39; command. Hopefully you will see many things listed, and your IPython file should be one of them.
epavlick&#64;plus:~&gt; ls 
Desktop  Documents  Downloads  IPythonBootcamp.ipynb

#You can keep things organized by making a new folder for this assignment using the &#34;mkdir&#34; command, and moving your file into it using the &#34;mv&#34; command. See below.
epavlick&#64;plus:~&gt; mkdir python-bootcamp-submission
epavlick&#64;plus:~&gt; mv IPythonBootcamp.ipynb python-bootcamp-submission/

#To see the list of open projects for our class, type the following. Currently, only the bootcamp and the test project are open.
epavlick&#64;plus:~&gt; turnin -l -c nets213
Current projects for nets213:
python-bootcamp  on (alternate)
turnin-test      off
gun-classifier   off
crowdflower      off
crowdie          off
quality          off
the-end          off
test-turnin      on (current)

# To submit your bootcamp to turnin, use the following command. The 
epavlick&#64;plus:~&gt; turnin -c nets213 -p python-bootcamp -v python-bootcamp-submission/
drwxr-xr-x epavlick/epavlick 0 2016-01-27 15:53 python-bootcamp-submission/
-rw-r----- epavlick/epavlick 16606 2016-01-27 15:48 python-bootcamp-submission/IPythonBootcamp.ipynb
Compressing submitted files... please wait
Your files have been submitted to nets213, python-bootcamp for grading.
</pre>
<p>åÊåÊ</p>
<p>If you want to practice, please try submitting to the test project (called test-turnin), just to make sure everything is working smoothly. You can just submit a blank file, or you are welcome to go on a meaningless rant and submit that. Whatever makes you happy.åÊ</p>
<p></p>
<p>Please let usåÊknow if you have any problems or questions.</p>
<p></p>
<p>Ellie</p>",Submitting assignments via turnin,"When using the $ scp assignment4 pennkey&#64;eniac.seas.upenn.edu command, I am unable to copy over the directory to my account. Not sure what else to do.",Unable to turnin Homework 4,,,"<p>Are the submissions for the latest homework open yet?</p>
<p>I tried to submit and received the error turnin: submissions for -crowdie have been turned off.</p>",HW8 turnin turned off?,0
940850927,4/26/2016 18:41:04,true,1969497534,,4/26/2016 18:31:56,false,neodev,1.0,35974955,VEN,17,Porlamar,190.198.232.239,0,0,"<p>Is there a way for us to see which files we turned in for a specific assignment, like #5?</p>",turnin,"<p>Hi Everyone,</p>
<p></p>
<p>Many of you had trouble with turnin for the bootcamp, and sent us email submissions. As a general policy, we cannot accept submissions via email. For this assignment, we will not penalize late submissions, so if you have not yet submitted via turnin, or you are still having trouble using scp/ssh, reach out to us and let&#39;s get it figured out.åÊ</p>
<p></p>
<p>For future assignments, we will only consider an assignment to be submitted if it is submitted via turnin before the deadline. Keep in mind our late-day policy: you are allowed 5 total late days during the semester with no penalty. After that, we deduct one point per day late.</p>
<p></p>
<p>Everyone have a wonderful weekend!</p>
<p>Ellie</p>",Late submissions and turnin,"<p>Hi everyone,</p>
<p>åÊ</p>
<p>For your python bootcamp, and for many other assignments this semester, you will be using &#39;turnin&#39;, which is a program on eniac. This requires using the &#39;ssh&#39; and &#39;scp&#39; commands from your terminal. Instructions on how to do this are below, but please come to office hours if this whole process looks foreign to you. It will take about 30 seconds to show you in person, versus much longer than 30 seconds of playing guess-and-check on your own. :)åÊ</p>
<p></p>
<p>You will be turning in your entire iPython notebook. Once you haveåÊyour notebook copied over to the eniac machines and sshed yourself over to the eniac machines (instructions below), you will run.</p>
<p>åÊ</p>
<pre>turnin -c nets213 -p python-bootcamp -v IPythonBootcamp.ipynb</pre>
<p>åÊ</p>
<p>Here is the process, from the beginning. The lines beginning with &#39;$&#39; or &#39;~&gt;&#39; are things you will need to type. Lines beginning with &#39;#&#39; are comments, explaining the commands below. Lines that have no special characters at the show the output you should expect to see.</p>
<p></p>
<pre>#Copy your iPython file (which right now is on your computer) to your account on the eniac. You can do this using the &#39;scp&#39; or &#39;secure copy&#39; command, like below.
$ scp IPythonBootcamp.ipynb epavlick&#64;eniac.seas.upenn.edu:               
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
IPythonBootcamp.ipynb           

#Log on to the  eniac machines (using your usual penn username/password). You should see your command prompt change to say something like &#64;plus or &#64;minus, like below.
$ ssh epavlick&#64;eniac.seas.upenn.edu
epavlick&#64;eniac.seas.upenn.edu&#39;s password: 
Last login: Wed Jan 27 14:46:24 2016 from 10.251.134.177
epavlick&#64;plus:~&gt; 

#Make sure your file was copied over successfully. You can check by listing the contents of the current directory using the &#39;ls&#39; command. Hopefully you will see many things listed, and your IPython file should be one of them.
epavlick&#64;plus:~&gt; ls 
Desktop  Documents  Downloads  IPythonBootcamp.ipynb

#You can keep things organized by making a new folder for this assignment using the &#34;mkdir&#34; command, and moving your file into it using the &#34;mv&#34; command. See below.
epavlick&#64;plus:~&gt; mkdir python-bootcamp-submission
epavlick&#64;plus:~&gt; mv IPythonBootcamp.ipynb python-bootcamp-submission/

#To see the list of open projects for our class, type the following. Currently, only the bootcamp and the test project are open.
epavlick&#64;plus:~&gt; turnin -l -c nets213
Current projects for nets213:
python-bootcamp  on (alternate)
turnin-test      off
gun-classifier   off
crowdflower      off
crowdie          off
quality          off
the-end          off
test-turnin      on (current)

# To submit your bootcamp to turnin, use the following command. The 
epavlick&#64;plus:~&gt; turnin -c nets213 -p python-bootcamp -v python-bootcamp-submission/
drwxr-xr-x epavlick/epavlick 0 2016-01-27 15:53 python-bootcamp-submission/
-rw-r----- epavlick/epavlick 16606 2016-01-27 15:48 python-bootcamp-submission/IPythonBootcamp.ipynb
Compressing submitted files... please wait
Your files have been submitted to nets213, python-bootcamp for grading.
</pre>
<p>åÊåÊ</p>
<p>If you want to practice, please try submitting to the test project (called test-turnin), just to make sure everything is working smoothly. You can just submit a blank file, or you are welcome to go on a meaningless rant and submit that. Whatever makes you happy.åÊ</p>
<p></p>
<p>Please let usåÊknow if you have any problems or questions.</p>
<p></p>
<p>Ellie</p>",Submitting assignments via turnin,"When using the $ scp assignment4 pennkey&#64;eniac.seas.upenn.edu command, I am unable to copy over the directory to my account. Not sure what else to do.",Unable to turnin Homework 4,,,"<p>Are the submissions for the latest homework open yet?</p>
<p>I tried to submit and received the error turnin: submissions for -crowdie have been turned off.</p>",HW8 turnin turned off?,0
